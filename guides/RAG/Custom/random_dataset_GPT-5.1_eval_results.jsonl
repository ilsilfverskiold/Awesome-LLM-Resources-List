{"type": "entry", "index": 0, "question": "What are pros and cons of hybrid vs knowledge-graph RAG for vague queries?", "answer": "For vague queries, hybrid (sparse+dense) RAG and knowledge\u2011graph RAG behave quite differently.\n\nPros of hybrid RAG (sparse + dense search):\n- Works very well for clearly defined, specific questions, returning highly relevant passages [2].\n- Indexing directly on document content avoids heavy metadata engineering and keeps store creation time relatively low [2].\n\nCons of hybrid RAG for vague queries:\n- When the question is vague, hybrid search tends to pull many loosely related passages, adding \u201cextraneous or unnecessary information\u201d and worsening the \u201cneedle in a haystack\u201d problem [2].\n- This leads to long, cluttered contexts and only modest retrieval accuracy on conversational datasets (\u224845\u201349% on CoQA) [2].\n\nPros of knowledge\u2011graph RAG for vague queries:\n- Structures data as entities and relations, so a query like \u201cwho quit Apple\u201d can target only the small, relevant sub\u2011network instead of every vaguely similar sentence, directly reducing the needle\u2011in\u2011a\u2011haystack issue [2].\n- Graph\u2011based QA (especially GraphRAG) excels on complex, thematic, and multi\u2011hop questions, giving richer, more coherent, and more explainable answers [1][2].\n- Tends to lower hallucination compared with pure semantic search [2] and supports provenance and reasoning\u2011style explanations [1].\n\nCons of knowledge\u2011graph RAG:\n- Building and updating the graph is computationally intensive and slow, so overall information\u2011store creation time is high [2].\n- Compressing text into triples can lose nuance, which can hurt exact\u2011match and F1 style metrics even when hallucinations drop [2].\n- GraphRAG brings added system complexity and higher runtime demands compared with lightweight retrievers [1].\n\nHybrid of KG + vector (HybridRAG) partly combines strengths\u2014better faithfulness, answer relevance, and recall\u2014but still struggles with context precision, meaning some unnecessary material remains in the final context [2].", "tags": ["hybrid_vs_kg"], "full_faithfulness": 1.0, "full_answer_relevancy": 0.9639235528000001, "seed_faithfulness": 0.46428571430000004, "seed_answer_relevancy": 0.9773926354, "context_relevance": 0.7954237852781298, "context_relevance_error": null, "context_relevance_reason": "The context passages provide substantial information relevant to the user's question about the pros and cons of hybrid vs knowledge-graph RAG for vague queries. They discuss the strengths and weaknesses of both approaches, particularly highlighting how knowledge graphs can enhance contextual understanding and reasoning capabilities, while also addressing the limitations of traditional RAG methods. However, some passages are more focused on specific methodologies rather than directly comparing the two approaches, which slightly detracts from the overall relevance.", "structure_citation": 0.715921354975597, "structure_error": null, "structure_reason": "The Actual Output effectively discusses the pros and cons of hybrid and knowledge-graph RAG for vague queries, with multiple claims supported by citations. However, while most citations correspond to relevant snippets in the Context, there are instances where the claims about retrieval accuracy and the 'needle in a haystack' problem could be better substantiated with specific references from the Context. Overall, the response is well-structured and informative, but slight discrepancies in citation support prevent a perfect score.", "hallucination_score": 0.8319241688561643, "hallucination_error": null, "hallucination_reason": "The response effectively outlines the pros and cons of both hybrid and knowledge-graph RAG approaches for vague queries, aligning well with the input's request. It provides specific strengths, such as hybrid RAG's efficiency for clear questions and knowledge-graph RAG's ability to target relevant sub-networks, while also addressing their shortcomings. However, while the output is detailed and informative, it could benefit from a more explicit connection to the context provided, particularly regarding the challenges of traditional RAG methods, which were not fully integrated into the analysis.", "contexts_used": ["1. Introduction\n\nRetrieval-Augmented Generation (RAG) has emerged as a powerful approach in natural language processing, particularly in question-answering systems. By combining the retrieval of relevant documents with the generative capabilities of large language models (LLMs), RAG models are able to generate contextually accurate and informative responses. Traditional RAG approaches typically rely on retrieving passages from vast corpora of unstructured text, which are then fed into a generative model to formulate an answer. While effective, this approach often faces challenges such as irrelevant retrieval, redundancy in answers, and limited capacity to understand structured relationships between concepts.", "Knowledge graph based question-answering (KG-QA) systems have gained significant attention in addressing these challenges. Unlike traditional RAG, which depends heavily on unstructured text retrieval, KG-QA leverages structured knowledge in the form of graph triplets, representing relationships between entities. The system can provide more precise, explainable, and contextually relevant answers by querying these triplets. Knowledge graphs offer a way to formalize the relationships between entities, making tracing and verifying the rationale behind a generated answer easier. When combined with LLMs, KG-QA systems can take advantage of both the structured reasoning capabilities of knowledge graphs and the natural language generation strengths of the LLMs.", "Creating high-quality knowledge graph triplets is crucial for the success of such systems. This process can be achieved through various methods, each with different strengths. Rule-based systems like spaCy leverage pre-defined linguistic rules and patterns to offer precision in extracting triplets from domain-specific texts. These systems could perform well and be finely tuned to some specific domain, such as the medical or legal domain, where the structure and vocabulary could be well defined. However, the actual system performance would depend on how well the rules are defined and how much knowledge the models can capture and embed in itself. At the same time, open-domain extractors like Stanford CoreNLP's OpenIE are more flexible, capturing a wider range of relations. They utilize language semantics and structure to push for identifying a subject, object, and relation in any sentence using limited or no training data. More recently, GraphRAG by Microsoft has emerged as a technique that leverages the power of LLMs for extracting precise structural information hierarchically from unstructured data by utilizing LLM inference at various stages of knowledge graph creation. It can create a more holistic view of the text and answer questions at different levels of abstraction, which can prove quite helpful more often than not.", "A key motivation for this research is to overcome the inherent limitations of traditional RAG approaches, which are adept at extracting localized, fact-based information yet often fall short when tasked with understanding the broader thematic context or handling complex, large-scale texts. By integrating knowledge graphs-capable of capturing structured relationships-into QA systems, we aim to enhance LLMs' reasoning capabilities, resulting in accurate, contextually richer responses, and more explainable. Furthermore, the comparative evaluation of diverse extraction methods such as spaCy, CoreNLP OpenIE, and GraphRAG is crucial to identifying the most effective techniques for constructing high-quality knowledge graphs. Leveraging open-source technologies for this study ensures replicability and practical applicability, ultimately guiding researchers and practitioners toward improved KG-QA systems better equipped to address real-world challenges. Therefore, this study not only fills a current research gap but lays the groundwork for future knowledge graph-based question-answering innovations.", "In this paper, therefore, we present a comparative study of three popular methods for generating graph triplets and integrating them with LLMs for question answering: (1) spaCy, (2) CoreNLP OpenIE, and (3) GraphRAG. Each method represents a unique approach to creating knowledge graphs and using them in QA systems. By evaluating them across broad parameters-such as performance, ease of use, accuracy, customizability, etc.-we aim to provide insights into their strengths and weaknesses and how they contribute to the effectiveness of KG-QA systems. This study aims to guide researchers and practitioners in selecting the most appropriate tools for their knowledge graph-based questionanswering tasks.\n\nThe key points of contribution of our work are as follows:\n\nWe present a fully reproducible framework that extracts relational triplets using spaCy, Stanford CoreNLP-OpenIE, and GraphRAG, constructs knowledge graphs, and integrates them into LLM prompts for enhanced question answering.", "Under identical conditions, we systematically evaluate each method on a whole range of parameters, filling a critical gap in the literature. Our evaluations are conducted on two distinct", "data sources-Shakespeare's play 'All's Well That Ends Well' and the RepliQA dataset-to ensure robustness and generalizability of our findings.", "By embedding structured relations into QA, we demonstrate that GraphRAG performs the best in complex, thematic queries, CoreNLP OpenIE offers the broadest factual coverage, and spaCy provides a lightweight, high-precision baseline.", "In addition to GPT-4 automated scoring, we incorporate domain expert judgments to assess answer coherence, reasoning ability, and contextual richness, yielding a more nuanced understanding of KG-augmented QA performance.", "Based on our dual-dataset empirical analysis, we recommend optimal extraction techniquesbalancing factual coverage, reasoning depth, and computational efficiency-along with best practices for graph-based question answering.", "We identify promising directions for advancing KG-based QA, like dynamic graph updates, hybrid extraction pipelines, multi-modal knowledge integration, and domain-specific adaptations.", "GraphRAG\n\nGraphRAG, a Microsoft Research innovation, represents a significant advancement in retrievalaugmented generation (RAG) by integrating structured knowledge graphs to enhance large language models' (LLMs) contextual understanding of private datasets. Unlike traditional RAG systems that rely on vector similarity searches, GraphRAG constructs an LLM-generated entity-relationship graph from source documents, enabling holistic analysis and semantic connections across disparate data points. GraphRAG addresses critical limitations in baseline RAG systems by transforming unstructured text into structured knowledge representations, particularly in synthesizing crossdocument insights and supporting verifiable assertions through source-linked evidence. It extracts relevant data from the knowledge graph, identifies the appropriate community related to the question, and utilizes the generated graph, entities, relations, claims, summaries, etc., to generate accurate and concise responses.", "4. Results and Discussion\n\n4.2.1.2. 'RepliQA'\n\nGraphRAG demonstrated consistently high performance with and without rubric, reflecting its strength in integrating structured knowledge and contextual cues. The small yet positive gain from rubric inclusion (4.61 to 4.66) further supports its capacity to incorporate guidance effectively. Overall, these results not only underscore GraphRAG's superiority but also suggest that rubric augmentation can enhance model outputs, particularly for models that can flexibly integrate external guidance.", "Then, all of the responses for a particular question, along with the question and the context were given to the GPT-4 model, and asked to relatively rank the responses from best to worst. The following table shows how many times the respective responses were ranked first or second in the rankings given by GPT-4.", "| | Method | Rank 1 Count | Rank 2 Count |\n|---:|---------:|---------------:|:--------------------------|\n| 0 | 9 | 6 | Dataset Answer |\n| 1 | 15 | 3 | SpaCy (without rubric) |\n| 2 | 9 | 12 | SpaCy (with rubric) |\n| 3 | 0 | 4 | CoreNLP (without rubric) |\n| 4 | 2 | 9 | CoreNLP (with rubric) |\n| 5 | 4 | 8 | GraphRAG (without rubric) |\n| 6 | 11 | 8 | GraphRAG (with rubric) |", "The rank-based comparison highlights an interesting distinction between spaCy and GraphRAG. SpaCy (without rubric) secures the highest number of rank-1 placements (15), suggesting that it occasionally produces responses that are judged as the best in isolated comparisons. However, when evaluated in terms of overall reliability, GraphRAG demonstrates greater consistency. GraphRAG with rubric, for example, secures 11 rank-1s and 8 rank-2s (19 in total), reflecting a balance of strong answers without significant drops in quality. When considered alongside the average score results, it becomes clear that while spaCy excels in producing standout answers on occasion, GraphRAG achieves a more stable and dependable performance across the dataset.", "The effect of rubric use is also evident. For both spaCy and CoreNLP, introducing rubric guidance shifts performance: spaCy's rank-1 count drops from 15 to 9, but its rank-2 placements rise significantly from 3 to 12, indicating that rubric enforcement pushes its answers toward consistently 'good' quality, even if fewer are rated as the absolute best. CoreNLP shows a similar trend, with rubric use improving its top-2 presence (from 4 to 11), though it still lags behind the other methods. GraphRAG, in contrast, benefits less dramatically from rubric introduction because it is already strong without rubric (12 top-2 placements versus 19 with rubric). This suggests that rubric guidance stabilizes weaker models (spaCy, CoreNLP) by reducing variability, whereas GraphRAG already exhibits high robustness and only gains marginally with rubric.", "In summary, rubric-based evaluation enhances consistency across models, ensuring fewer poorquality answers, but its effect is most pronounced for methods prone to variability. GraphRAG's relative insensitivity to rubric presence underscores its inherent robustness, while spaCy's shift from rank-1 dominance to rank-2 consistency reveals a trade-off between peak and average performance.", "NOTE: It is important to note that comparing spaCy, CoreNLP, and GraphRAG on speed or memory usage is not meaningful due to their inherently different processing approaches-spaCy uses a lightweight Cython-based pipeline, CoreNLP employs a multi-stage Java framework with deep linguistic analysis, and GraphRAG leverages LLMs for hierarchical graph-based reasoning-making direct benchmarking of resource usage inherently unequal.", "4.2.3. Pre-processing Requirements\n\nThe spaCy-based approach requires extensive manual preprocessing, including text segmentation, custom dependency parsing, and triplet extraction, demanding significant coding effort. The CoreNLP method automates many linguistic analyses through its OpenIE module but still necessitates server setup and some manual processing for question analysis and triplet matching. In contrast, GraphRAG automates the entire preprocessing pipeline, handling text ingestion, knowledge graph construction, and retrieval processes internally, thereby minimizing manual intervention. The following table summarizes the preprocessing requirements for each method:", "| | Aspect | spaCy | CoreNLP | GraphRAG |\n|---:|:-------------------------------------|:--------------|:------------------------------|:-----------------------------------------------------|\n| 0 | Setup complexity | Low | Medium (Java server required) | Very low |\n| 1 | Custom patterns &rules | Built-in | | Dependency parsing Fully automated |\n| 2 | Hand-rolled via patterns | OpenIE module | | Triplet extraction Internal to GraphRAG (using LLMs) |\n| 3 | Question matching/filter Custom code | | Custom code | Handled by framework (using prompts) |\n| 4 | Graph construction | Manual | Manual | Automated |\n| 5 | Total manual effort | High | Moderate | Low |\n\nTable 17: Comparison table for the 'Pre-processing Requirements' subparameter", "4.3. Added GraphRAG advantages\n\nGraphRAG offers several advantages in the context of LLM + graph-based question answering. It excels at handling both structured and unstructured data, leveraging knowledge graphs to capture relationships between entities while extracting insights from unstructured text. This dual capability enhances response accuracy and comprehensiveness. Additionally, GraphRAG supports incremental learning, allowing for dynamic updates to the knowledge graph without requiring full retraining, which is particularly beneficial for rapidly evolving domains.", "The system's schema flexibility allows for easy modification or extension of the underlying schema, supporting dynamic or schema-less environments. Furthermore, GraphRAG is designed for distributed processing, leveraging cloud-native architectures to ensure scalability and high performance. GraphRAG also provides provenance tracking, tracing the origin of each piece of information used in generating responses. This feature enhances transparency and trustworthiness by grounding outputs in structured formats, reducing hallucinations and providing confidence scoring. Overall, GraphRAG's comprehensive capabilities make it a powerful tool for complex AI applications across various domains, offers a robust solution for question answering, ensuring privacy, and supporting advanced reasoning and processing capabilities.", "5. Conclusion and Future Work\n\nIn this work, we have presented a fully reproducible framework that leverages three distinct open-source methods-spaCy, Stanford CoreNLP-OpenIE, and GraphRAG-for extracting relational triplets, constructing knowledge graphs, and integrating them with large language models (LLMs) to enhance question-answering. Through systematic evaluation under identical conditions, we found that spaCy delivers a lightweight, high-precision baseline: it excels at extracting clear, well-formed subject-predicate-object triplets with minimal false positives, yet its coverage diminishes when faced with idiomatic or complex sentence structures. CoreNLP-OpenIE , by contrast, achieves the broadest factual coverage across varied text types, uncovering nuanced relations even in less canonical phrasing; however, this comes at the expense of higher computational overhead and the need for more aggressive filtering to remove noisy or overly granular triplets. GraphRAG stands out for its reasoning prowess: its tight coupling of graph-structured context with LLM prompting yields the richest, most coherent responses on thematic and multi-hop queries, albeit with added system complexity and increased runtime demands. Our empirical results-validated by GPT-4 automated scoring and domain expert judgments-demonstrate that each method occupies a unique point in the precision-coverage-reasoning trade-off space, thereby offering clear guidance on tool selection for diverse QA scenarios.", "Based on these insights, we recommend a hybrid pipeline that begins with spaCy's high-precision filters to eliminate spurious relations, followed by CoreNLP-OpenIE's expansive extraction to maximize recall and culminates with GraphRAG's deep contextual reasoning to generate the final answer. To support real-time and dynamic applications, knowledge graphs should be incrementally updated via streaming triplet ingestion, ensuring that the QA system remains aligned with evolving information. Incorporating lightweight expert-in-the-loop validations-such as crowd-sourced or calibrated domain checks-can further prune residual noise and calibrate LLM prompts for heightened coherence. Finally, practitioners should tune extraction thresholds and prompt templates to the specific demands of their domain, balancing latency, accuracy, and interpretability according to task requirements.", "Looking ahead, a promising research avenue lies in unifying triplet extraction and reasoning within end-to-end hybrid architectures. One possible avenue can be where the LLMs are designed such that building knowledge graphs is a part of their own learning process, using trainable components that can be adjusted during training. This way, the model can automatically learn which relationships and structures are most helpful for answering questions. This could eliminate the need for separate, multistep RAG pipelines, making the system more efficient and integrated. A key question for future work is whether graph-centric retrieval-directly querying over learned graph embeddings-can fully supplant traditional chunk-based RAG methods without sacrificing multi-hop inference capabilities or explainability. To date, our study has been anchored in text-centric extraction; expanding this comparison to include retrieval over vectorized graph representations will yield valuable insights into the fundamental limits of RAG versus graph-based strategies.", "Beyond retrieval and model architecture, scalable evaluation and multimodal integration represent critical frontiers. Automated, graph-based coherence metrics-such as entailment scoring over relation paths-could reduce our reliance on costly expert assessments, while calibrated crowd-sourcing frameworks can facilitate large-scale benchmarking. On the content side, extending knowledge graphs to encapsulate visual, tabular, and temporal data will enable QA systems to operate over charts, images, and time series, unlocking new applications in scientific analysis, financial forecasting, and beyond. Finally, developing domain-specific schema adaptation and incremental graph-updating strategies will ensure that KG-enhanced QA systems remain robust and current across rapidly evolving knowledge domains. These combined efforts will bring us closer to QA systems capable of human-level reasoning, contextual richness, and real-world applicability.", "2 Related Works\n\nSpare and Dense Retriever RAG methods: Hybrid approaches, such as the \"Blended RAG\" framework proposed by Sawarkar et al. [12], demonstrate that combining multiple retrievers can improve the retrieval of relevant passages for clearly defined queries. This framework integrates six distinct retrievers-including traditional sparse methods (e.g., BM25) and dense semantic search techniques-selected based on their performance across benchmark datasets like Natural Questions (NQ) [13], TREC-COVID [14], SqUAD [15], and HotPotQA [16].", "A key strength of this hybrid search technique is its ability to return relevant passages, aligning well with Research Goal 3 for well-specified queries. However, these methods do not fully address Research Goal 3 when handling vaguer questions. In such cases, the hybrid approach can retrieve extraneous or unnecessary information, thus failing to eliminate the \"needle in a haystack\" problem. This shortcoming is evident in evaluations on the CoQA dataset [17], where retrieval accuracies ranged only from 45% to 49%. The reliance on document metadata during retrieval-a common feature in these systems-further complicates matters, as such metadata is often minimal in real-world scenarios (e.g., limited to document names and creation dates), thereby increasing both the creation time of the information store (contradicting Research Research Goal 1) and the risk of returning irrelevant passages (contradicting Research Research Goal 3) leading to longer contexts (contradicting Research Goal 2).", "Figure 3: Results of Priyanka Mandikal et al. on 2 example queries [18].\n\nQuery: Do fibroblasts from CF patients grow at a\n\nQuery: What is the prognosis for patients who have\n\nnormal rate?\n\nhad an episode of respiratory failure?\n\nMethod P@1 P@3 P@5 P@10 VSR 33.33 SPECTERZ Hybrid 66.67 Sample Retrieval This paper analyzes the cell cycle of cystic\n\nThe diagnosis, management and prognosis of 17 infants\n\nfibrosis-derived fibroblasts and statistically\n\nwith cystic fibrosis (CF) and severe respiratory disease\n\ndemonstrates their failure support normal\n\nwere reviewed for the\n\nperiod 1968 to 1972.\n\nThe clinical\n\npopulation doubling time in vitro and indicates that this\n\ncourse these infants was characterized by is due largely\n\nto a failure of the\n\ncystic fibrosis-derived\n\nfailure to thrive and\n\ncells to enter the cell cycle in normal numbers.\n\nWe have malnutrition Despite vigorous therapy including no explanation for significant difference antibiotics there was\n\n60% mortality:\n\nA delay in the\n\nbehavior of these cells from patients with cystic fibrosis\n\ndiagnosis of CF from the onset of respiratory symptoms\n\nvis-a-vis either disease control fibroblasts, with a\n\nmean of six weeks was considered an important\n\nsave to conclude that proceeds from an intrinsic\n\nfactor affecting survival. This data supports the need for\n\nproperty of the cells themselves.", "developing a reliable screen test for CF at birth.", "In contrast, recent work by Mandikal et al. [18] has advanced a hybrid retrieval model that integrates a sparse TF-IDF retriever with a dense retriever based on fine-tuned SPECTER2 embeddings [19]. Their approach employs a weighted scoring mechanism, controlled by a hyperparameter \u03bb , to balance the contributions of the two retrieval methods. Empirical evaluations on a medical database concerning Cystic Fibrosis indicate that while dense retrieval alone underperforms, incorporating approximately 20% of the sparse score (i.e., setting \u03bb = 0 . 2 for the sparse component) significantly improves document retrieval accuracies across different retrieval ranks [Figure 3]. Crucially, this method indexes documents solely based on their content, thereby avoiding the pitfalls of metadata dependency. This contributes to reducing information store creation times (Research Goal 1) and partially mitigates the retrieval of unnecessary information (Research Goal 3), although it does not entirely resolve the challenge of handling vaguer queries.", "In summary, while hybrid retrieval approaches represent a significant advancement by returning more relevant passages for well-defined queries complying with Research Goal 3. However they remain imperfect in filtering out unnecessary information for vaguer questions which can lead to longer contexts. This limitation underscores the need for further refinement to minimize the \"needle in a haystack\" problem in large-scale, real-world applications and make a pipeline which is compliant with Research Goal 2 as well.", "Reranker RAG methods: To address the limitation which Hybrid-Search methods have in adhering to Research Goal 2 for vaguer questions, Reranker methods have been developed to reduce the size and improve the quality of the final context.", "Figure 4: A high level view of what a RAG pipeline with a reranker looks like.\n\nIst Stage 2nd Stage Knowledge Embedding-based LLM-based Docs Corpus retrieval reranking\n\nUsually this is a process called 2-step retrieval where the first step involves retrieving documents such as through sparse or dense retriever techniques and then a second stage where an LLM based reranker comes into play to decide which of the documents retrieved are most relevant to the query and sorts them in order [Figure 4] before passing it to an LLM for generation.", "Prior to the formal conceptualization of Retrieval-Augmented Generation (RAG), information retrieval research explored the use of document rerankers. For example, Noguiera et al.'s 2020 study, Document Ranking with a Pretrained Sequence-to-Sequence Model [20], proposed employing a sequence-to-sequence model as a document reranker. This work evaluated multiple fine-tuned T5 models of varying sizes, as well as a fine-tuned BERT model, on documents initially retrieved using the sparse BM25 keyword method. The incorporation of these rerankers led to improvements in retrieval performance across the Robust04, Core17, and Core18 datasets. However, a critical limitation of this framework was its reliance on large models, leaving unresolved whether the observed performance gains were due to the reranking strategy or simply the use of more powerful models.", "Subsequent advancements in RAG have integrated BERT-based rerankers into new pipelines. Michael Glass et al.'s Re 2 G framework [21] embeds a BERT-based reranker-similar to that of Noguiera et al.-within a RAG system. This integration has shown notable improvements in metrics such as Recall (for 5 documents), overall recall, precision, and accuracy on several datasets, as indicated by performance on the KILT leaderboard. Nevertheless, a significant drawback of this approach is the necessity to host an additional model, which directly conflicts with Goal 4: alleviating the monetary and computational costs associated with deploying full-scale LLMs.", "Further refinement in reranking approaches is demonstrated by Xuegang Ma et al., who fine-tuned a LLaMA-2-7B model for reranking in their RankLLaMA system [22]. Although RankLLaMA achieves superior reranking performance, it again relies on a large model comparable to LLaMA-2-7B. Such reliance on resource-intensive models undermines the overarching objective of Goal 4, as the additional computational and financial burden associated with hosting these rerankers contradicts the need for efficiency and cost reduction.", "In summary, while reranker models can mitigate the 'needle in a haystack' problem (Goal 2) by filtering out least relevant information, they are not a sustainable solution. Their requirement for additional large-scale models significantly increases computational and monetary costs, thereby violating Goal 4. Consequently, alternative strategies that achieve comparable retrieval performance without incurring such overhead are essential for the advancement of practical and efficient RAG systems.", "Knowledge Graph RAG methods: Knowledge graph-based solutions have been proposed as a promising alternative for enhancing the RAG process. The core idea behind employing knowledge graphs is to address semantic search challenges by organizing data according to intrinsic relationships. For example, although not originally applied within a RAG context, Dawei Cheng et al.'s 2020 work on constructing knowledge graphs for a financial price prediction tool [23] illustrates how grouping semantically related data into distinct sub-networks enables a query-such as 'who quit Apple'-to retrieve only the pertinent subset of information, rather than returning all semantically similar sentences (e.g., both 'Steve Jobs quits Apple' and 'David Peter leaves Starbucks'). This targeted retrieval helps mitigate the \"needle in a haystack\" problem (Goal 2) and increases the rate of retrieving relevant passages (Goal 3).", "Building on these insights, recent research has directly integrated knowledge graphs within RAG frameworks. For instance, Zhentao Xu et al. [24] developed a system for LinkedIn's customer question answering service by first", "creating a custom knowledge graph and then retrieving related nodes. The retrieved information is subsequently refined using GPT-4 to produce coherent responses. When compared with a baseline semantic search system, this pipeline significantly improved retrieval metrics-including Mean Reciprocal Rank, Recall, and Normalized Discounted Cumulative Gain-as well as evaluation measures such as BLEU, METEOR, and ROUGE, thereby supporting Goals 2 and 3.", "However, despite these promising advances, knowledge graph-based approaches fall short of meeting Goal 1. The process of constructing and training knowledge graphs is computationally intensive and time-consuming, which directly conflicts with the objective of reducing information store creation times.", "| | Model | EM | Fl Score | Accuracy | Hallucination |\n|---:|:--------------|-----:|:-----------|:-----------|:----------------|\n| 0 | Human | 63 | | | |\n| 1 | MHQA-GRN | 33.2 | | | |\n| 2 | Embedding-RAG | 28 | 37 | 46 | 30 |\n| 3 | KG-RAG | 19 | | 32 | 15 |\n\nFigure 5: Results comparing knowledge graph framework with semantic search methods [25].", "For example, Diego Sanmart\u00edn's 2024 work [25] demonstrated a Knowledge Graph RAG system that leverages LLMs to extract entity-relationship triples from unstructured text and employs a 'Chain of Explorations' retrieval strategy. While this approach shows potential for handling vague queries, the high training times required to build and maintain the knowledge graph significantly increase the overall overhead. Moreover, the compressed storage format-using entity-relationship triples-can lead to a loss of detailed contextual information, thereby limiting the system's ability to fully expand upon relevant topics during retrieval hence resulting in even lower EM, F1, Accuracy scores with only an improvement in Hallucination compared to a standard Semantic Search method [Figure 5].", "In summary, while knowledge graph-based solutions offer clear benefits in terms of improving semantic search and enhancing the retrieval of relevant passages (addressing Goals 2 and 3), they do not currently satisfy Goal 1 due to their high training times and associated computational costs. Addressing these inefficiencies is essential for realizing the full potential of knowledge graphs within efficient and practical RAG frameworks.", "Combining Knowledge Graphs and Vector based Search methods: The limitations inherent in a purely knowledge graph-based RAG framework suggest that combining the strengths of knowledge graphs with traditional semantic search methods may yield superior results. Bhaskarjit Sarmah et al. [26] propose such an integrated approach through their HybridRAG framework. This framework concatenates outputs from a knowledge graph query-implemented using Microsoft's GraphRAG [27] with content indexed by extracting entity relationships in chunk sizes of 1024-with results from a semantic search, which similarly employs chunk sizes of 1024 and the text-embedding-ada-002 model. The aggregated context is then provided to GPT-3.5 to generate the final answer. The dataset for this study was constructed by web scraping earnings call documents of companies listed on the NIFTY 50, and evaluation was performed using the RAGAS evaluation platform [28].", "The HybridRAG framework demonstrated improvements in metrics such as Faithfulness, Answer Relevancy Score, and Context Recall. These enhancements support Goal 2 by reducing the \"needle in a haystack\" problem and Goal 3 by increasing the rate of retrieving relevant passages for answering questions. However, the framework underperformed in Context Precision when compared to standalone implementations of both knowledge graph-based and vector database-based RAG systems. This shortcoming indicates that the current integration does not fully leverage the advantages of traditional vector search techniques, particularly hybrid search techniques, which is critical for fully meeting Goal 3."]}
{"type": "entry", "index": 1, "question": "How do reranker-based RAG pipelines trade off latency, cost, and context precision?", "answer": "Reranker-based RAG pipelines add a second step after initial retrieval: an extra model (often an LLM or large encoder) re-scores the retrieved passages and filters them before generation [2]. This directly shapes three things:\n\n1. Context precision\n\u2013 By filtering out less relevant passages, rerankers tackle the \u201cneedle-in-a-haystack\u201d problem and improve recall, precision, and accuracy of what is passed to the generator [2].\n\u2013 More advanced selectors like Dynamic Passage Selector learn to pick a variable set of passages, reducing missed evidence when K is too small and cutting noise when K is too large, which boosts multi-hop reasoning quality [1].\n\n2. Latency\n\u2013 Reranking introduces an extra inference step over many candidate passages, so end\u2011to\u2011end response time increases.\n\u2013 Similar patterns are seen in KG-based RAG: raising the retrieval limit improves accuracy but increases query time and token usage [3]\u2014reranking more items has a comparable effect.\n\n3. Monetary and computational cost\n\u2013 Hosting a dedicated reranker (e.g., BERT, T5, or LLaMA\u20112\u20117B) significantly increases VRAM and compute requirements, conflicting with goals of low-cost deployment [2]. Studies argue that, despite better passage selection, such large rerankers are \u201cnot a sustainable solution\u201d because of this overhead [2].\n\u2013 Some work finds that reranking top\u2011k (e.g., top\u201130) gives only marginal end\u2011task gains in complex \u201cmixture-of-knowledge\u201d settings, suggesting the extra cost may not always be justified [4].\n\u2013 Alternative designs like KeyKnowledgeRAG deliberately avoid large rerankers, using smaller models and summarization to keep VRAM, cost, and latency low while still improving context quality over naive RAG [2].\n\nIn practice, rerankers trade higher cost and slower responses for cleaner, more focused contexts\u2014and whether that trade is worthwhile depends on query complexity and deployment constraints.", "tags": ["rerankers", "benchmarks_eval", "retrieval_tuning", "scaling_cost"], "full_faithfulness": 1.0, "full_answer_relevancy": 0.9639890720000001, "seed_faithfulness": 1.0, "seed_answer_relevancy": 0.0, "context_relevance": 0.8209369753062011, "context_relevance_error": null, "context_relevance_reason": "The context passages provide substantial information relevant to the user's question about the trade-offs in reranker-based RAG pipelines, particularly regarding latency, cost, and context precision. The passages discuss the limitations of traditional reranking methods, the introduction of the Dynamic Passage Selector (DPS), and the implications of hybrid retrieval methods. However, while the context is informative, it could more explicitly address the specific trade-offs mentioned in the question, particularly in terms of latency and cost, which are less directly covered.", "structure_citation": 0.8050857124175226, "structure_error": null, "structure_reason": "The Actual Output effectively addresses the trade-offs of latency, cost, and context precision in reranker-based RAG pipelines, providing detailed explanations supported by citations. It includes claims with [n] markers that correspond to snippets in the Context, and the cited snippets align well with the claims made. However, there are minor shortcomings, such as the presence of a citation [4] that lacks a corresponding snippet in the Context, which slightly detracts from the overall completeness of the response.", "hallucination_score": 0.8121582261821636, "hallucination_error": null, "hallucination_reason": "The response accurately identifies and discusses the trade-offs of latency, cost, and context precision in reranker-based RAG pipelines, aligning well with the context provided. It effectively explains how rerankers improve context precision by filtering passages and addresses the increase in latency and costs associated with additional models. However, while it mentions the Dynamic Passage Selector, it does not explicitly connect this to the context's emphasis on the need for adaptive evidence selection, which slightly detracts from its completeness.", "contexts_used": ["Abstract\n\nRetrieval-augmented generation (RAG) systems are often bottlenecked by their reranking modules, which typically score passages independently and select a fixed Top-K size. This approach struggles with complex multi-hop queries that require synthesizing evidence across multiple documents, creating a trade-off where small K values omit crucial information and large K values introduce noise. To address this, we introduce the D ynamic P assage S elector (DPS), a novel reranking framework that treats passage selection as a supervised learning problem. Unlike traditional point-wise or list-wise methods, DPS is fine-tuned to capture interpassage dependencies and dynamically select the most relevant set of passages for generation. As a seamless plugand-play module, DPS requires no modifications to the standard RAG pipeline. Comprehensive evaluations on five benchmarks show that DPS consistently outperforms stateof-the-art rerankers and fine-tuning methods. Notably, on the challenging MuSiQue dataset, DPS improves the F1-score by 30.06% and 15.4% over strong baselines like Qwen3reranker and RankingGPT, respectively. Our results demonstrate that by enabling adaptive evidence selection, DPS substantially enhances reasoning capabilities in complex RAG scenarios.", "2 Related Works\n\nSeveral studies have explored and implemented advanced RAG (Retrieval-Augmented Generation) systems to overcome the limitations of the naive RAG approach discussed in the introduction. A significant focus of these works lies in developing novel methods to enhance vector database-centric retrieval systems, evaluated across a diverse set of metrics pertinent to vector-based retrieval. Additionally, some studies assess the performance of RAG solutions leveraging Knowledge Graphs [11], while one investigation examines the outcomes of naively combining Knowledge Graph-based and semantic search results for question-answering tasks. These related works also highlight certain limitations, ranging from increased resource demands to the need for novel strategies or enhancements to improve system performance.", "Addressing the limitations of naive Retrieval-Augmented Generation (RAG) pipelines has led to the integration of hybrid retrieval methods that combine sparse (keyword-based) and dense (semantic) retrievers.", "Spare and Dense Retriever RAG methods: Hybrid approaches, such as the \"Blended RAG\" framework proposed by Sawarkar et al. [12], demonstrate that combining multiple retrievers can improve the retrieval of relevant passages for clearly defined queries. This framework integrates six distinct retrievers-including traditional sparse methods (e.g., BM25) and dense semantic search techniques-selected based on their performance across benchmark datasets like Natural Questions (NQ) [13], TREC-COVID [14], SqUAD [15], and HotPotQA [16].", "A key strength of this hybrid search technique is its ability to return relevant passages, aligning well with Research Goal 3 for well-specified queries. However, these methods do not fully address Research Goal 3 when handling vaguer questions. In such cases, the hybrid approach can retrieve extraneous or unnecessary information, thus failing to eliminate the \"needle in a haystack\" problem. This shortcoming is evident in evaluations on the CoQA dataset [17], where retrieval accuracies ranged only from 45% to 49%. The reliance on document metadata during retrieval-a common feature in these systems-further complicates matters, as such metadata is often minimal in real-world scenarios (e.g., limited to document names and creation dates), thereby increasing both the creation time of the information store (contradicting Research Research Goal 1) and the risk of returning irrelevant passages (contradicting Research Research Goal 3) leading to longer contexts (contradicting Research Goal 2).", "Figure 3: Results of Priyanka Mandikal et al. on 2 example queries [18].\n\nQuery: Do fibroblasts from CF patients grow at a\n\nQuery: What is the prognosis for patients who have\n\nnormal rate?\n\nhad an episode of respiratory failure?\n\nMethod P@1 P@3 P@5 P@10 VSR 33.33 SPECTERZ Hybrid 66.67 Sample Retrieval This paper analyzes the cell cycle of cystic\n\nThe diagnosis, management and prognosis of 17 infants\n\nfibrosis-derived fibroblasts and statistically\n\nwith cystic fibrosis (CF) and severe respiratory disease\n\ndemonstrates their failure support normal\n\nwere reviewed for the\n\nperiod 1968 to 1972.\n\nThe clinical\n\npopulation doubling time in vitro and indicates that this\n\ncourse these infants was characterized by is due largely\n\nto a failure of the\n\ncystic fibrosis-derived\n\nfailure to thrive and\n\ncells to enter the cell cycle in normal numbers.\n\nWe have malnutrition Despite vigorous therapy including no explanation for significant difference antibiotics there was\n\n60% mortality:\n\nA delay in the\n\nbehavior of these cells from patients with cystic fibrosis\n\ndiagnosis of CF from the onset of respiratory symptoms\n\nvis-a-vis either disease control fibroblasts, with a\n\nmean of six weeks was considered an important\n\nsave to conclude that proceeds from an intrinsic\n\nfactor affecting survival. This data supports the need for\n\nproperty of the cells themselves.", "developing a reliable screen test for CF at birth.", "In contrast, recent work by Mandikal et al. [18] has advanced a hybrid retrieval model that integrates a sparse TF-IDF retriever with a dense retriever based on fine-tuned SPECTER2 embeddings [19]. Their approach employs a weighted scoring mechanism, controlled by a hyperparameter \u03bb , to balance the contributions of the two retrieval methods. Empirical evaluations on a medical database concerning Cystic Fibrosis indicate that while dense retrieval alone underperforms, incorporating approximately 20% of the sparse score (i.e., setting \u03bb = 0 . 2 for the sparse component) significantly improves document retrieval accuracies across different retrieval ranks [Figure 3]. Crucially, this method indexes documents solely based on their content, thereby avoiding the pitfalls of metadata dependency. This contributes to reducing information store creation times (Research Goal 1) and partially mitigates the retrieval of unnecessary information (Research Goal 3), although it does not entirely resolve the challenge of handling vaguer queries.", "In summary, while hybrid retrieval approaches represent a significant advancement by returning more relevant passages for well-defined queries complying with Research Goal 3. However they remain imperfect in filtering out unnecessary information for vaguer questions which can lead to longer contexts. This limitation underscores the need for further refinement to minimize the \"needle in a haystack\" problem in large-scale, real-world applications and make a pipeline which is compliant with Research Goal 2 as well.", "Reranker RAG methods: To address the limitation which Hybrid-Search methods have in adhering to Research Goal 2 for vaguer questions, Reranker methods have been developed to reduce the size and improve the quality of the final context.", "Figure 4: A high level view of what a RAG pipeline with a reranker looks like.\n\nIst Stage 2nd Stage Knowledge Embedding-based LLM-based Docs Corpus retrieval reranking\n\nUsually this is a process called 2-step retrieval where the first step involves retrieving documents such as through sparse or dense retriever techniques and then a second stage where an LLM based reranker comes into play to decide which of the documents retrieved are most relevant to the query and sorts them in order [Figure 4] before passing it to an LLM for generation.", "Prior to the formal conceptualization of Retrieval-Augmented Generation (RAG), information retrieval research explored the use of document rerankers. For example, Noguiera et al.'s 2020 study, Document Ranking with a Pretrained Sequence-to-Sequence Model [20], proposed employing a sequence-to-sequence model as a document reranker. This work evaluated multiple fine-tuned T5 models of varying sizes, as well as a fine-tuned BERT model, on documents initially retrieved using the sparse BM25 keyword method. The incorporation of these rerankers led to improvements in retrieval performance across the Robust04, Core17, and Core18 datasets. However, a critical limitation of this framework was its reliance on large models, leaving unresolved whether the observed performance gains were due to the reranking strategy or simply the use of more powerful models.", "Subsequent advancements in RAG have integrated BERT-based rerankers into new pipelines. Michael Glass et al.'s Re 2 G framework [21] embeds a BERT-based reranker-similar to that of Noguiera et al.-within a RAG system. This integration has shown notable improvements in metrics such as Recall (for 5 documents), overall recall, precision, and accuracy on several datasets, as indicated by performance on the KILT leaderboard. Nevertheless, a significant drawback of this approach is the necessity to host an additional model, which directly conflicts with Goal 4: alleviating the monetary and computational costs associated with deploying full-scale LLMs.", "Further refinement in reranking approaches is demonstrated by Xuegang Ma et al., who fine-tuned a LLaMA-2-7B model for reranking in their RankLLaMA system [22]. Although RankLLaMA achieves superior reranking performance, it again relies on a large model comparable to LLaMA-2-7B. Such reliance on resource-intensive models undermines the overarching objective of Goal 4, as the additional computational and financial burden associated with hosting these rerankers contradicts the need for efficiency and cost reduction.", "In summary, while reranker models can mitigate the 'needle in a haystack' problem (Goal 2) by filtering out least relevant information, they are not a sustainable solution. Their requirement for additional large-scale models significantly increases computational and monetary costs, thereby violating Goal 4. Consequently, alternative strategies that achieve comparable retrieval performance without incurring such overhead are essential for the advancement of practical and efficient RAG systems.", "Knowledge Graph RAG methods: Knowledge graph-based solutions have been proposed as a promising alternative for enhancing the RAG process. The core idea behind employing knowledge graphs is to address semantic search challenges by organizing data according to intrinsic relationships. For example, although not originally applied within a RAG context, Dawei Cheng et al.'s 2020 work on constructing knowledge graphs for a financial price prediction tool [23] illustrates how grouping semantically related data into distinct sub-networks enables a query-such as 'who quit Apple'-to retrieve only the pertinent subset of information, rather than returning all semantically similar sentences (e.g., both 'Steve Jobs quits Apple' and 'David Peter leaves Starbucks'). This targeted retrieval helps mitigate the \"needle in a haystack\" problem (Goal 2) and increases the rate of retrieving relevant passages (Goal 3).", "Building on these insights, recent research has directly integrated knowledge graphs within RAG frameworks. For instance, Zhentao Xu et al. [24] developed a system for LinkedIn's customer question answering service by first", "creating a custom knowledge graph and then retrieving related nodes. The retrieved information is subsequently refined using GPT-4 to produce coherent responses. When compared with a baseline semantic search system, this pipeline significantly improved retrieval metrics-including Mean Reciprocal Rank, Recall, and Normalized Discounted Cumulative Gain-as well as evaluation measures such as BLEU, METEOR, and ROUGE, thereby supporting Goals 2 and 3.", "However, despite these promising advances, knowledge graph-based approaches fall short of meeting Goal 1. The process of constructing and training knowledge graphs is computationally intensive and time-consuming, which directly conflicts with the objective of reducing information store creation times.", "| | Model | EM | Fl Score | Accuracy | Hallucination |\n|---:|:--------------|-----:|:-----------|:-----------|:----------------|\n| 0 | Human | 63 | | | |\n| 1 | MHQA-GRN | 33.2 | | | |\n| 2 | Embedding-RAG | 28 | 37 | 46 | 30 |\n| 3 | KG-RAG | 19 | | 32 | 15 |\n\nFigure 5: Results comparing knowledge graph framework with semantic search methods [25].", "For example, Diego Sanmart\u00edn's 2024 work [25] demonstrated a Knowledge Graph RAG system that leverages LLMs to extract entity-relationship triples from unstructured text and employs a 'Chain of Explorations' retrieval strategy. While this approach shows potential for handling vague queries, the high training times required to build and maintain the knowledge graph significantly increase the overall overhead. Moreover, the compressed storage format-using entity-relationship triples-can lead to a loss of detailed contextual information, thereby limiting the system's ability to fully expand upon relevant topics during retrieval hence resulting in even lower EM, F1, Accuracy scores with only an improvement in Hallucination compared to a standard Semantic Search method [Figure 5].", "In summary, while knowledge graph-based solutions offer clear benefits in terms of improving semantic search and enhancing the retrieval of relevant passages (addressing Goals 2 and 3), they do not currently satisfy Goal 1 due to their high training times and associated computational costs. Addressing these inefficiencies is essential for realizing the full potential of knowledge graphs within efficient and practical RAG frameworks.", "5 Discussion\n\n5.1 Achieving Research Goal 1: Reduce information store creation and update times\n\nK 2 RAG effectively reduces the creation times for Knowledge Graph and Sparse and Dense vector databases, such as the Semantic and BM25-based Keyword databases. Performing text summarization on the training corpus led to an average reduction in training times of up to 93% , with only 25 minutes spent on summarization.", "5.1 Achieving Research Goal 1: Reduce information store creation and update times\n\nFor instance, the Semantic database creation, which originally took 4010 seconds (67 minutes) , was reduced to 441 seconds (7.35 minutes) when using the summarized corpus. Even when accounting for the summarization process, the total time was 32.25 minutes , significantly lower than training with the full corpus. Similarly, Knowledge Graph creation time dropped from nearly 18 hours to around 1 hour , thanks to the reuse of the summarized corpus. This approach not only saves time and resources but also reduces the cost of LLM-based Knowledge Graph creation, which is typically expensive to host.", "5.2 Achieving Research Goal 2: Mitigate the Needle-in-a-Haystack problem\n\nK 2 RAG effectively mitigates the \"Needle in a Haystack\" problem, which can hinder LLM performance when handling large contexts. By integrating the Longformer-based led-base-book-summary summarizer at key stages of the retrieval pipeline, the framework reduces context size while preserving essential information.", "5.2 Achieving Research Goal 2: Mitigate the Needle-in-a-Haystack problem\n\nK 2 RAG maintained answer quality and even outperformed naive pipelines on certain questions, as reflected in the higher accuracy and Q 3 quantile. This approach allows for improved LLM performance without a bloated context, preventing the model from missing critical details and enhancing the final answer quality.", "5.3 Achieving Research Goal 3: Improve retrieval accuracy\n\nThe K 2 RAG framework enhances passage retrieval through two key components. First, the knowledge component identifies relevant topics for answering the question. Based on these results, the system further explores each topic within the knowledge graph to extract more concrete and relevant chunks. This approach enables the development of more comprehensive answers. Similar to Research Goal 2, this strategy improves the quality of the final LLM-generated responses.", "5.4 Achieving Research Goal 4: Lower Temporal and Computational Overhead\n\nWhile rerankers enhance passage retrieval accuracy, they require an additional large model, increasing resource demands. Similarly, the naive Knowledge Graph RAG pipeline depends on two LLMs: Mistral-7B-FP16 (14GB VRAM) and Mistral-7B-Q4 (4.1GB VRAM).", "5.4 Achieving Research Goal 4: Lower Temporal and Computational Overhead\n\nIn contrast, the K 2 RAG framework streamlines the process by utilizing only Mistral-7B-Q4 (4.1GB VRAM) for querying the Knowledge Graph, question generation, and final answer generation, along with a small summarization model requiring just 648MB VRAM. This results in a total VRAM usage of 4.8GB, allowing for efficient RAG pipelines on smaller machines while significantly reducing computational and monetary costs.", "5.4 Achieving Research Goal 4: Lower Temporal and Computational Overhead\n\nMoreover, K 2 RAG achieves faster response times and superior answer quality compared to naive RAG pipelines. This is evident in its reduced question-answering times relative to the naive Knowledge Graph Search RAG, enabling K 2 RAG to process more queries in a span of time.", "5.5 Limitations\n\nWhile our RAG framework shows a slight mean accuracy advantage over naive methods, a mean similarity of 0.57, despite a high Q 3 value, indicates limited performance gains on most questions. This suggests it excels only at certain\n\n| | Pipeline | Memory Footprint |\n|---:|:-------------------------|:-------------------|\n| 0 | Semantic Search Pipeline | 14.3GB |\n| 1 | Keyword Search Pipeline | 14.3GB |\n| 2 | Hybrid Search Pipeline | 14.3GB |\n| 3 | Knowledge Graph Search | 18.1GB |\n| 4 | KeyKnowledgeRAG | 5GB |\n\nTable 4: Table of Memory Footprints", "question types, but the narrow scope restricts broader improvement. Reliance on broad knowledge graph searches may also reduce precision. A more focused retrieval strategy could yield more precise topic pinpointing and higher accuracy.", "6 DISCUSSION AND LIMITATION\n\nLessons Learned. During the evaluation of RAGs, we identified critical trade-offs between accuracy and computational cost. Tests revealed that response accuracy depends on the scale of the local KG and the entity retrieval limit per query. Increasing the retrieval limit from the default to more entities markedly improved accuracy but incurred higher token consumption and prolonged query times. This highlights the necessity of balancing query performance and cost in large-scale RAG-based workflows. Additionally, while model predictions occasionally diverged from the constructed hypothesis chains, domain experts emphasized that such discrepancies do not diminish the validity of the chains themselves. They highlighted that rigorously derived hypotheses-even without full alignment with retrieval results-retain significant values, particularly in uncovering novel insights.", "Generalization and Scalability. The hypothesis-driven architecture has demonstrated broad applicability across diverse research domains, particularly in scientific discovery tasks involving KG-based node and link prediction. When integrated with RAG, the system offers a cost-effective and flexible alternative to domain-specific fine-tuning, enabling more efficient adaptation to new research areas. Given the generalizability of RAG and the system's modular KG-based design, most components are transferable across domains, requiring only the substitution of domain-specific KGs and predictive models. Domain experts have also identified promising applications in areas such as protein structure and function prediction via interaction networks, as well as novel materials discovery. However, the system's performance is largely dependent on the quality of the underlying KGs and the effectiveness of the associated predictive models. As advances in textbased KG extraction continue to improve accuracy and robustness, the system's cross-domain applicability is expected to expand further.", "Limitations. While RAG supports text-to-KG conversion, the process remains resource-intensive and prone to inaccuracies, especially in data-rich domains. To reduce risk, we restricted integration to small, verified updates using public KGs. However, the slow pace of KG updates compared to rapid scientific progress highlights the need for more accurate, automated extraction methods. Although RAG helps mitigate hallucinations by retrieving from traceable sources, such issues persist. Future improvements could include fact-verification modules-e.g., cross-referencing authoritative sources or applying confidence thresholds to flag uncertain outputs. Given the complexity of hypothesisdriven discovery, some interaction complexity is unavoidable, though we anticipate continued simplification as AI reliability advances.", "2 Experiments\n\n2.2 RQ1: Effectiveness of RAG under Mixture-of-Knowledge Scenarios.\n\n1.64 1.81 0.62 0.57 0.85 0.45 1.13 0.91 (e) Qwen-3-32B Pubmed Wiki Pes2o C4 Github Math SE Book Arxiv CC SimpleQA SciQ ARC-C MMLU MMLU-Pro CSBench 0.72 16.28 3.14 0.90 1.20 0.88 1.46 4.35 1.10 1.40 0.30 2.00 0.50 1.90 1.70 0.60 2.60 0.17 0.09 0.00 0.26 0.34 0.44 0.25 0.37 0.22 0.39 0.20 0.43 0.68 0.96 0.49 0.64 0.48 0.52 0.82 0.79 0.74 0.40 0.51 0.28 (f) GPT-4o\n\nand mid-sized models, while the gains diminish as model size increases. For the strongest models, retrieval offers marginal or even slightly negative gains, suggesting that larger models are increasingly able to internalize domain knowledge without the need for external retrieval. These results highlight the generalizability of our findings and demonstrate that the impact of retrieval augmentation is robust across different knowledge domains.", "2.3 RQ 2: Instance-level Analysis of Multiple Retrieval Sources\n\nTo investigate whether different retrieval sources provide unique advantages, we conduct an instancelevel study (Figure 2) measuring the proportion of queries that can be solved only by retrieving from a specific corpus-compared to using all sources or no retrieval at all. Our results show that a significant fraction of cases (e.g., 8%-39% for Llama-3.18B) depend exclusively on retrieval from particular corpora. Crucially, no single retrieval source consistently outperforms others across all query types , highlighting the need for dynamic, query-specific routing to the most relevant corpus to maximize RAG performance in heterogeneous, real-world knowledge environments.", "2.4 RQ 3: Effectiveness of Reranking\n\nOne possible reason for the limited improvement in RAG performance is the inherent limitations of", "Figure 3: Performance with rerank on retrieval effectiveness across different datasets and models.\n\nPubmedWiki Pes2o GithubMath Book Arxiv All llama-3.2-3b llama-3.1-8b qwen3-4b qwen3-8b qwen3-32b gpt-4o-mini gpt-4o -0.63 7.11 -0.36\n\n-0.57 -0.23 -0.49 -0.07 -0.58\n\n2.04 8.49 3.02 31.90 4.42 9.05 3.93 4.32 3.67 4.94 3.19 12.8934.09 -0.82 3.50 0.21\n\n-0.70 -0.60 -0.70 -0.48 -0.75\n\n0.77 -0.85 2.93 0.14\n\n-0.76 -0.62 -0.69 -0.45 -0.76\n\n0.69\n3.81\n1.99\n\n-0.71 -0.13 -0.78 -0.68 -0.75 -0.58 -0.81\n\n0.27\n2.53\n-0.46\n\n-0.33 -0.01 -0.39 -0.29 -0.36 -0.26 -0.32\n\n1.89\n-0.24\n0.23\n\n-0.27 -0.24 -0.27 -0.24 -0.25 -0.26 -0.23 -0.17\n\n(a) SimpleQA -0.22 -0.10 0.24 -0.26 0.11 -0.20 -0.32 0.41 -0.01 0.06 0.03 -0.14 0.46 -0.15 0.07 -0.09 0.19 -0.11 -0.19 0.18\n\n-0.25 -0.13 -0.21 -0.01 -0.23 -0.06 -0.20 -0.08 -0.29 -0.02\n\n-0.32 -0.15 -0.24 -0.10 -0.28 -0.11 -0.27 -0.11 -0.35 -0.07 -0.01\n\n-0.20 -0.15 -0.25 -0.10 -0.19 -0.14 -0.18 -0.14 -0.24 -0.09 -0.05\n\n-0.14 -0.13 -0.18 -0.07 -0.13 -0.09 -0.13 -0.10 -0.13 -0.08 -0.04\n\n(b) SciQ", "-0.32 -0.15 -0.24 -0.10 -0.28 -0.11 -0.27 -0.11 -0.35 -0.07 -0.01\n\n-0.20 -0.15 -0.25 -0.10 -0.19 -0.14 -0.18 -0.14 -0.24 -0.09 -0.05\n\n-0.14 -0.13 -0.18 -0.07 -0.13 -0.09 -0.13 -0.10 -0.13 -0.08 -0.04\n\n(c) ARC-C -0.00 0.05 -0.03 0.04 -0.05 0.10 0.16 0.01 0.02 0.15 -0.04 -0.00 -0.02 -0.04 -0.02 -0.05 -0.06 -0.04 -0.01 -0.02\n\n-0.03 -0.01 -0.04 -0.00 -0.05\n\n-0.06 -0.02 -0.03 -0.02 -0.04 -0.02 -0.05 -0.02 -0.05 -0.01\n\n-0.03 -0.02 -0.02 -0.02 -0.03 -0.02 -0.03 -0.02 -0.04 -0.02 -0.01\n\n-0.01 -0.02 -0.02 -0.01 -0.01 -0.01 -0.01 -0.01 -0.01 -0.01 -0.01\n\n(d) MMLU -0.06 -0.00 -0.04 -0.07 0.12 0.31 -0.04\n\n-0.08 -0.02 -0.04 -0.02 -0.05 -0.02 -0.04 -0.01 -0.06 -0.01\n\n-0.06 -0.02 -0.05 -0.03 -0.08 -0.02 -0.07 -0.02 -0.09 -0.01", "-0.02 -0.01 -0.02 -0.02 -0.01 -0.00 -0.01 -0.01 -0.02 -0.01\n\n(e) MMLU-Pro\n0.08\n0.13\n\n-0.12 -0.01 -0.04 -0.01 -0.06\n\n-0.05 -0.01 -0.09 -0.03\n\n-0.14 -0.06 -0.09 -0.05 -0.07 -0.05 -0.08 -0.07 -0.11 -0.04 -0.03\n\n-0.10 -0.06 -0.09 -0.06 -0.06 -0.04 -0.04 -0.04 -0.09 -0.04 -0.05\n\n-0.03 -0.01 -0.02 -0.02 -0.02 -0.01 -0.00 -0.00 -0.01 -0.01\n\n-0.02 -0.01 -0.02\n\n-0.02 -0.01 -0.01 -0.00 -0.01 -0.00 -0.00\n\n(f) CSBench\n\nFigure 4: Performance comparison of routing strategies across MMLU and MMLU-Pro datasets.\n\nmmlu_pro 0.0 0.2 0.4 0.6 0.8 Accuracy no retrieval all sources qwen3-4b (plain) qwen3-8b (plain) qwen3-32b (plain) qwen3-4b (cot) qwen3-8b (cot) qwen3-32b (cot) Upper Bound", "retrievers. To investigate this, inspired by prior work demonstrating benefits from adding reranking to the RAG pipeline (Shao et al., 2024; Yu et al., 2024), we apply reranking to the top-30 retrieved results. As shown in Figure 3, reranking yields only marginal gains across datasets. This suggests that improving retrieval quality through reranking is insufficient in mixture-of-knowledge scenarios. The retriever's limited capacity and restricted access to relevant knowledge highlight the need for deeper integration between knowledge sources, retrieval mechanisms, and generative models.", "2.5 RQ 4: Evaluating LLMs as Routers for Mixture-of-Knowledge Retrieval\n\nPrevious analyses highlight the need for adaptive retrieval mechanisms that dynamically route queries to the most relevant corpus based on topic context. Here, we investigate whether current LLMs can effectively perform query routing. Figure 4 evaluates LLMs from the Qwen-3 series (4B, 8B, and 32B parameters) as 'routers' that select among heterogeneous knowledge sources at inference time. We compare plain prompting versus chain-of-thought prompting on the MMLU and MMLU-Pro datasets, which cover diverse general and professional domains.", "2.5 RQ 4: Evaluating LLMs as Routers for Mixture-of-Knowledge Retrieval\n\nSpecifically, we benchmark: (1) no-retrieval baselines; (2) static retrieval from all corpora ('all sources'); (3) LLM-prompted routing variants (plain and chain-of-thought); and (4) an oracle router upper bound. Surprisingly, neither prompting strategy consistently outperforms static retrieval. In fact, both routing approaches often underperform compared to simply retrieving from all sources, and occasionally fall below no-retrieval baselines. Chain-of-thought prompting provides only marginal improvements, while scaling model size yields negligible or even negative returns.", "2.5 RQ 4: Evaluating LLMs as Routers for Mixture-of-Knowledge Retrieval\n\nWe attribute this failure to two main factors: (1) Inaccurate relevance estimation: Without dedicated training, LLMs struggle to reliably identify which corpus holds the needed information, especially amid overlapping or stylistically diverse", "2.5 RQ 4: Evaluating LLMs as Routers for Mixture-of-Knowledge Retrieval\n\ncorpora. Minor routing errors propagate to poor retrieval quality. (2) Training-inference mismatch: LLM training typically lacks explicit multi-source comparison tasks, limiting the effectiveness of prompt-based routing as a meta-reasoning problem. Future directions should focus on learned routing modules trained with supervision or reinforcement learning, or on tightly integrated RAG systems that jointly optimize source selection and generation."]}
{"type": "entry", "index": 2, "question": "Which benchmarks best capture hallucination rates in hybrid RAG systems?", "answer": "No single benchmark perfectly captures hallucination rates in hybrid RAG systems, but several families come close from different angles.\n\nFor RAG-style agents (including hybrid or multi-modal), challenge-style setups that explicitly log \u201challucination rate\u201d at the query level are the most direct. The CRAG\u2011MM 2025 setting tracks per\u2011query hallucinations, missing answers, and accuracy, and reports a hallucination rate alongside a truthfulness score for each system variant, making it very suitable for evaluating how often a RAG pipeline fabricates content versus abstains or answers correctly [3].\n\nFact\u2011checking/verification datasets such as FEVER, HoVeR, QuanTemp, and AveriTeC are also strong proxies because they require comparing model claims to retrieved evidence and judging whether they are supported, refuted, or unverifiable\u2014exactly the distinction underlying hallucinations in RAG [4]. These capture whether a hybrid system\u2019s generated claims stay faithful to evidence.\n\nEnd\u2011to\u2011end RAG evaluation frameworks like RAGAS, ARES, and CRUX explicitly assess answer faithfulness to retrieved documents and analyze how retrieval errors propagate into generation, which is central for measuring hallucinations in hybrid RAG pipelines [4].\n\nMetaRAG\u2019s own enterprise dataset is an example of a targeted hallucination benchmark, with human labels of whether responses are hallucinated given retrieved context, but it is private and the authors note the need for public benchmarks with minimal pretraining overlap and finer-grained factoid labels to better evaluate hallucinations in RAG systems [1].\n\nFinally, broader factuality benchmarks like TruthfulQA and BioGEN, used in DSCC\u2011HS, track factual consistency and hallucination scores in long-form generation, and can complement RAG-specific tests even though they are not RAG-only benchmarks [2].", "tags": ["hybrid_vs_kg", "benchmarks_eval"], "full_faithfulness": 1.0, "full_answer_relevancy": 0.0, "seed_faithfulness": 1.0, "seed_answer_relevancy": 0.0, "context_relevance": 0.7386454574615074, "context_relevance_error": null, "context_relevance_reason": "The context passages provide substantial information about hallucination detection in hybrid RAG systems, particularly through the evaluation of MetaRAG. The passages discuss experiments, datasets, evaluation protocols, and metrics that are directly relevant to understanding how hallucination rates are captured. However, while the context is informative, it could be more explicitly focused on identifying specific benchmarks that capture hallucination rates, which would strengthen the alignment with the user's question.", "structure_citation": 0.7524463226888918, "structure_error": null, "structure_reason": "The Actual Output effectively discusses various benchmarks for capturing hallucination rates in hybrid RAG systems and includes citations [1], [2], [3], and [4]. Each citation corresponds to relevant snippets in the Context, supporting the claims made. However, while the claims are generally well-supported, there is a slight lack of clarity in how some benchmarks are directly linked to the specific evaluation of hallucination rates, which prevents a perfect score.", "hallucination_score": 0.8566786756537, "hallucination_error": null, "hallucination_reason": "The response accurately identifies multiple benchmarks that capture hallucination rates in hybrid RAG systems, aligning well with the context provided. It discusses specific benchmarks like CRAG-MM 2025, FEVER, and MetaRAG, which are relevant to the evaluation of hallucinations as outlined in the context. The output effectively explains how these benchmarks assess hallucination rates, demonstrating a strong understanding of the topic. However, it could have included a brief mention of the limitations or challenges in using these benchmarks, which would have provided a more balanced view.", "contexts_used": ["4 Experiments\n\nWe conducted experiments to evaluate MetaRAG on its ability to detect hallucinations in retrieval-augmented generation (RAG). The evaluation simulates a realistic enterprise deployment setting, in which a chatbot serves responses generated from internal documentation. Our focus is on the detection stage, that is, identifying when an answer contains unsupported (hallucination) or fabricated information. Prevention and mitigation are important but they are outside the scope of this work.", "4.1 Dataset\n\nThe evaluation dataset is a proprietary collection of 23 internal enterprise documents , including policy manuals, procedural guidelines, and analytical reports, none of which were seen during LLM training. Each document was segmented into chunks of a few hundred tokens, and retrieval used cosine similarity over text-embedding-3-large , with the topk = 3 chunks appended to each query.", "We then collected a set of user queries and corresponding chatbot answers. Each response was labeled by human annotators as either hallucinated or not, using the retrieved context as the reference. The final evaluation set contains 67 responses , of which 36 are labeled as not hallucinated and 31 as hallucinated .", "To preserve confidentiality, we do not release the full annotated dataset. However, the complete annotation guidelines are included in the supplementary material.", "4.2 Evaluation Protocol\n\nMetaRAG produces fine-grained, factoid-level hallucination scores , whereas the available ground truth labels are at the response level . To align with these existing labels, we evaluate MetaRAG as a binary classifier by thresholding the hallucination score H ( Q,A,C ) at \u03c4 = 0 . 5 . We report standard classification metrics: Precision, Recall, F1 score and accuracy. Latency is also recorded to assess feasibility for real-time deployment.", "4.2.1 Case Studies in Identity-Sensitive Domains\n\nBeyond quantitative evaluation, we also provide qualitative illustrations of MetaRAG in identity-sensitive scenarios. To illustrate how MetaRAG's span-level scores can enable identity-aware safeguards without inferring protected attributes, we present two stylized examples. These are not part of the quantitative evaluation in Section 4, but highlight potential deployment scenarios.", "Healthcare (pregnancy). Auser asks: 'Can pregnant women take ibuprofen for back pain?' The model answers: 'Yes, ibuprofen is safe throughout pregnancy .' However, the retrieved context specifies that ibuprofen is contraindicated in the third trimester. MetaRAG flags the span 'safe throughout pregnancy' with a high factoid score ( S i = 0 . 92 ), yielding a response-level score H = 0 . 92 . Under the policy hooks described in Section 3.6, the topic tag pregnancy triggers a stricter threshold ( \u03c4 identity = 0 . 3 , lower than the general case), span highlighting, a forced citation requirement, and possible escalation to human review.", "Migration (refugee rights). A user asks: 'Do LGBTQ+ refugees automatically receive protection in country X?' The model claims that such protections are automatic , but the retrieved legal text provides no evidence of this. MetaRAG flags the unsupported claim 'automatically receive protection' with a moderate score ( S i = 0 . 5 ), yielding a response-level score H = 0 . 5 . Although this score would sit at the decision boundary under a general threshold ( \u03c4 general = 0 . 5 ), the stricter identity-aware threshold ( \u03c4 identity = 0 . 3 ) ensures it is flagged for this case. Under the policy hooks, the topic tag asylum/refugee enforces citation and may escalate the response to a human reviewer. In a chatbot deployment, the system would abstain from returning the unsupported answer and instead notify the user that expert verification is required.", "These qualitative vignettes complement our quantitative evaluation by showing how MetaRAG's flagged spans can be turned into concrete safeguards in identity-sensitive deployments.", "6 Discussion\n6.1 Practical Implications\n\nOperational Efficiency : Detecting hallucinations simultaneously with content delivery reduces the need for costly downstream human verification.", "6.2 Ethical Considerations\n\nBeyond technical performance, hallucination detection intersects directly with questions of fairness, accountability, and identity harms. Hallucinations in chatbot systems pose risks that extend beyond factual inaccuracies: they can reinforce harmful stereotypes, undermine user trust, and misrepresent marginalized communities in identitysensitive contexts.", "Reinforced stereotypes: Language models are known to reproduce and amplify societal biases, as demonstrated by benchmarks such as StereoSet [24] and WinoBias [34]. In identity-sensitive deployments, hallucinated outputs risk reinforcing these biases in subtle but harmful ways.", "Trust erosion: Chatbots are only adopted at scale in high-stakes domains if users trust their outputs. Surveys on hallucination consistently highlight that exposure to unsupported or fabricated content undermines user trust in LLM systems [14, 20].", "7 Conclusion\n\nHallucinations in RAG-based conversational agents remain a significant barrier to trustworthy deployment in real-world applications. We introduced MetaRAG , a metamorphic testing framework for hallucination detection in retrieval-augmented generation (RAG) that operates without requiring ground truth references or access to model internals. Our experiments show that MetaRAG achieves strong detection performance on a challenging proprietary dataset, aligning with prior benchmark studies. Beyond general reliability, MetaRAG's factoid-level localization also supports identity-aware deployment by surfacing unsupported claims in sensitive domains (e.g., healthcare, migration, labor). Looking ahead, we see MetaRAG as a step toward safer and fairer conversational AI, where hallucinations are not only detected but also connected to safeguards that protect users in identity-sensitive contexts. This connection to identity-aware AI ensures that hallucination detection does not treat all users as homogeneous but provides safeguards that reduce disproportionate risks for marginalized groups.", "Identity harms: Misrepresentations in generated responses may distort personal narratives or marginalize underrepresented groups, aligning with broader critiques that technical systems can reproduce social inequities if identity considerations are overlooked [11, 29].", "Figure 4. Pareto front analysis for hallucination detection performance. Each point represents a MetaRAG configuration; Pareto-optimal points (non-dominated) are highlighted. Subplots show: (Left) F1 vs. average token usage, (Center) F1 vs. average total execution time, (Right) Precision vs. Recall. Pareto-optimal points represent configurations with no strictly better alternative in both accuracy and cost. Configuration IDs correspond to Table 2.\n\nFl vs token usage\n\nFl vs total time\n\nRecall vs Precision 0.940 Pareto front 7.0 0.98 70000 0.935 6.5 0.96 0.930 60000 0.94 6.0 5.5 0.92 0.925 50000 5.0 0.90 0.920 4.5 0.88 40000 0.915 4.0 0.86 y = x 0.910 3.5 30000 Average token usage\n\nAverage total time (s)\n\nPrecision (mean)\n\nBy detecting hallucinations in a black-box, reference-free manner, MetaRAG supports safer deployment of RAG-based systems, particularly in settings where fairness, identity, and user well-being are at stake.", "6.3 Limitations and Future Work\n\nWhile MetaRAG demonstrates strong hallucination detection performance, several limitations remain:\n\nDataset Scope: The study relies on a private, domain-specific dataset. This may limit external validity. Future work should focus on curating or constructing public benchmarks designed to avoid overlap with LLM pretraining corpora, enabling more robust generalizability .", "Annotation Granularity: We lack factoid-level ground truth, which reduces our ability to assess fine-grained reasoning accuracy. Providing such annotations in future datasets would support deeper consistency evaluations .", "Policy Hooks Not Evaluated: The identity-aware deployment hooks introduced in Section 3.6 are presented only as a design concept. In our implementation, we used a fixed threshold of \u03c4 = 0 . 5 across all queries. Future research should implement and measure the effectiveness of topic-aware thresholds, forced citation, and escalation strategies in real-world chatbot deployments .", "Topic as Proxy (Design Limitation): In Section 3.6, we suggest topic tags (e.g., pregnancy, asylum, labor) as privacy-preserving signals for stricter safeguards, rather than inferring protected attributes. This was not implemented in our experiments. As a design idea, it may also miss cases where risk is identity-conditioned but the query appears generic. Future work should explore how to operationalize such topic-aware safeguards and investigate richer, privacy-preserving signals that better capture identity-sensitive risks .", "Model Dependency: Current findings hinge on specific LLMs (GPT-4.1 variants). As models evolve, the behavior of MetaRAG\n\nmay shift. Future efforts should validate MetaRAG across opensource and emerging models to reinforce its robustness .", "ABSTRACT\n\nHallucination remains a critical barrier to the reliable deployment of Large Language Models (LLMs) in high-stakes applications. Existing mitigation strategies, such as Retrieval-Augmented Generation (RAG) and post-hoc verification, are often reactive, inefficient, or fail to address the root cause within the generative process. Inspired by dual-process cognitive theory, we propose D ynamic S elfreinforcing C alibration for H allucination S uppression (DSCC-HS), a novel, proactive framework that intervenes directly during autoregressive decoding. DSCC-HS operates via a two-phase mechanism: (1) During training, a compact proxy model is iteratively aligned into two adversarial roles-a Factual Alignment Proxy (FAP) and a Hallucination Detection Proxy (HDP)-through contrastive logit-space optimization using augmented data and parameter-efficient LoRA adaptation. (2) During inference, these frozen proxies dynamically steer a large target model by injecting a real-time, vocabulary-aligned steering vector (computed as the difference between FAP and HDP logits) at each decoding step, requiring no modification to the target model. This plug-and-play approach enables lightweight, scalable, and proactive hallucination suppression. Our experiments on the TruthfulQA and BioGEN benchmarks demonstrate DSCC-HS's state-of-the-art performance. On TruthfulQA, DSCC-HS achieves a 49.82% accuracy and a 99.2% Factual Consistency Rate (FCR) while reducing the hallucination score to 0.8 , significantly outperforming strong baselines like ITI and DOLA. On the long-form BioGEN benchmark, DSCC-HS attains the highest FActScore of 46.50 and the lowest incorrectness rate of 11.49 , showcasing its robustness in generating factually consistent long texts. These results validate DSCC-HS as a principled and efficient solution for enhancing LLM factuality.", "4 Experiments\n\nIn this section, we present our experimental setup and evaluation results. To assess the effectiveness of our multimodal RAG pipeline under settings aligned with the CRAG-MM 2025 challenge. All local experiments were conducted on the same hardware as the challenge environment: a single NVIDIA L40S GPU with 48GB of memory. For the generated answer, we employed the GPT-4o-mini [7] model as the evaluation LLM.", "4.1 Metrics\n\nWeevaluate our system using the official metric suite defined by the CRAG-MM challenge. Specifically, we report the total number of queries, correct answers, missed answers, and hallucinations. From these, we derive key indicators such as accuracy, hallucination rate, and missing rate. Additionally, we compute the truthfulness score to assess factual grounding.", "4.2 Single-source Augmentation\n\nTo evaluate the effectiveness of our method, we conduct an ablation study on Task 1 under local evaluation, as shown in Table 1. The goal is to assess the incremental impact of the key component in our verification-centric pipeline.", "The LLaMA Vision Only baseline refers to the vision-language model (VLM) directly answering questions from image and text input without any external retrieval or augmentation. This setting yields a relatively high accuracy of 25.00%, but suffers from severe hallucination issues (59.62%) and a negative truthfulness score of -34.62%, indicating that the answers often conflict with the ground truth. This is expected, as the VLM attempts to answer all questions regardless of its knowledge limitations, resulting in a low", "| | | Accuracy (%) | Missing Rate (%) | Hallucination Rate (%) | Truthfulness Score (%) |\n|---:|:---------------------------|---------------:|-------------------:|-------------------------:|-------------------------:|\n| 0 | LLaMA Vision Only | 25 | 15.38 | 59.62 | -34.62 |\n| 1 | RAG Agent | 27.88 | 9.62 | 62.5 | -34.62 |\n| 2 | w/o CoV & Self-Consistency | 3.85 | 95.19 | 0.96 | 2.88 |\n| 3 | w/o CoV | 4.81 | 95.19 | 0 | 4.81 |\n| 4 | Ours | 14.42 | 82.69 | 2.88 | 11.54 |\n\nTable 1: Ablation study on Task 1 single-source augmentation under local evaluation.", "missing rate but poor factual reliability. The RAG Agent baseline incorporates external retrieval via the official search API, enhancing access to relevant knowledge. This improves the accuracy to 27.88% and slightly reduces the missing rate to 9.62%. However, hallucination rate increases further to 62.50%, and the truthfulness score remains at -34.62%, showing that naive RAG without verification can introduce more misleading content. To further assess the role of verification, we remove key components from our pipeline to observe their individual effects. The w/o CoV & SelfConsistency variant disables both the self-consistency check and the Chain-of-Verification (CoV). As a result, the model becomes overly conservative, with a missing rate of 95.19% and very low accuracy (3.85%). While hallucination rate drops sharply to 0.96%, the overall coverage and utility of the model are severely degraded. A similar trend is observed in the w/o CoV variant, which enables self-consistency but disables CoV. Although slightly more accurate (4.81%), it still fails to provide meaningful coverage. Our final agent, denoted as Ours , integrates all proposed components. This configuration strikes the best balance between caution and informativeness. It achieves a truthfulness score of 11.54% , the highest among all systems, with an acceptable hallucination rate of 2.88% and an improved accuracy of 14.42%. The results demonstrate that each verification stage contributes to reducing hallucinations while selectively increasing answer confidence, enabling the agent to provide more trustworthy responses.", "Our method is developed specifically for the Single-source Augmentation (Task 1) setting. The same pipeline and implementation are directly applied to Task 2 and Task 3 during the CRAG-MM challenge phase. Thus, we focus our discussion on Task 1.", "6 Datasets and Evaluation Benchmarks\n\nThe evaluation of IR and RAG systems relies on diverse datasets that test different aspects of retrieval and answer generation capabilities, including the retrieval, ranking, and answer quality.", "6.1 IR Specific\n\nInformation retrieval has a long history of evaluation campaigns, including those from TREC, CLEF, NTCIR, and FIRE. The queries used in these collections are often developed to strike a balance between having too many relevant documents in the target collection (which can be too easy to retrieve and too difficult to properly annotate (Voorhees et al., 2022)) and too few relevant documents. Sometimes challenging topics are also developed deliberately (Voorhees, 2005). The topic development process often involves manual reformulation of queries to ensure good coverage of relevance assessments. Mirroring the annotation process itself, it can be beneficial for automated retrieval systems to also perform various forms of query understanding (expansion or rewriting) to help ensure high recall. Hence, the approaches described in Section 3 show performance gains.", "Comprehensive benchmarks like BEIR (Thakur et al., 2021) offer heterogeneous evaluation across 17 diverse datasets spanning multiple domains and retrieval tasks, enabling zero-shot generalization assessment. In addition, evaluation sets such as TREC DL (Deep Learning) and its variants (Craswell et al., 2020, 2021, 2022, 2023) are used to evaluate IR systems.", "6.2 Question Answering\n\nQuestion Answering datasets for retrieval evaluation fall into two primary categories based on complexity. Single-hop QA benchmarks include Natural Questions (NQ) (Kwiatkowski et al., 2019), TriviaQA (Joshi et al., 2017), SQuAD (Rajpurkar et al., 2016), and PopQA (Mallen et al., 2023) test the RAG system's ability to retrieve and utilize information to generate the final answer. Multi-hop QA benchmarks comprise 2WikiMultiHopQA (Ho et al., 2020) (questions requiring evidence from exactly two Wikipedia articles via bridging/comparison), HotpotQA (Yang et al., 2018), and MuSiQue (Trivedi et al., 2022), which features compositionally complex questions constructed from interconnected sub-questions. Due to the dependency on intermediate answers, such questions cannot be answered through isolated single-step retrieval. Therefore, this design naturally motivates the RAG systems to decompose the original query and new queries based on the intermediate answers help in the retrieval of better context.", "6.3 Fact Checking/Verification\n\nThese datasets assess models' ability to verify claims against retrieved evidence. FEVER (Fact Extraction and VERification) (Thorne et al., 2018) is an open-domain fact-checking benchmark that requires retrieval over a large collection of Wikipedia articles and training NLI models to classify claims as supported, refuted, or not having enough information based on Wikipedia evidence. While FEVER deals with simple claims, HoVeR (Jiang et al., 2020) proposes a fact-checking benchmark with claims that require multi-hop reasoning and multi-turn retrieval. QuanTemp (V et al., 2024) is the first to propose a large open-domain benchmark for fact-checking numerical claims. It comprises claims that require interleaving claim decomposition and retrieval based on the verification output of sub-claims. This relates to query-level feedback and generation-time feedback. AveriTeC (Schlichtkrull et al., 2023) is a real-world claim verification dataset that includes diverse claim types with supporting or refuting evidence gathered through web search, making it particularly valuable for evaluating RAG systems on authentic fact-checking scenarios that require verification against potentially noisy or contradictory web sources.", "6.4 Complex Reasoning\n\nEmerging benchmarks introduce novel relevance criteria requiring awareness of reasoning structure. BRIGHT (SU et al., 2025) defines document relevance not by topical alignment but by whether passages contain logical constructs (deductive steps, analogies, constraints) necessary to derive answers. This challenges lexical retrievers that lack inference-awareness. However, reasoningaugmented retrieval methods, such as CoT (Chainof-Thought), have shown performance gains. Further, there exist reasoning/agentic tasks like Deep Research (Wu et al., 2025), GPQA (Rein et al., 2023), MATH500 (Cobbe et al., 2021), which involve access to the web search engine, have been used in the RAG setting (Li et al., 2025a; Wei et al., 2025). However, due to the absence of the cor- pus, it is hard to evaluate the retrieval performance. Recent benchmarks like BrowseComp-Plus (Chen et al., 2025b) provide a curated corpus for Deep Research tasks and enable the evaluation of retrieval performance.", "6.5 Evaluation Metrics\n\nEvaluating retrieval systems and RetrievalAugmented Generation (RAG) pipelines is critical for ensuring the accuracy, relevance, and reliability of generated responses. Retrieval evaluation typically focuses on metrics such as recall@k, precision, mean reciprocal rank (MRR), and hit rate, which assess how effectively the system retrieves pertinent documents or passages given a query. In contrast, RAG evaluation is more holistic, combining retrieval quality with generation fidelity and coherence. Common approaches include measuring answer correctness using exact match (EM) or F1 score, assessing faithfulness to retrieved evidence to detect hallucinations, and evaluating relevance and fluency through human or automated scoring (e.g., BLEU, ROUGE, or BERTScore). Recent frameworks like RAGAS (ExplodingGradients, 2024), ARES (Saad-Falcon et al., 2024), and CRUX (Ju et al., 2025) also emphasize end-to-end evaluation, where the interplay between retrieval accuracy and generation quality is analyzed to identify bottlenecks-such as irrelevant documents leading to incorrect answers-making comprehensive evaluation essential for diagnosing and improving RAG system performance.", "Challenges . The current RAG evaluation methods mainly focus on the retrieval and final answer performance. However, Reasoning RAG systems are highly dependent on intermediate reasoning steps and retrieval rounds. Therefore, it is also important to consider additional evaluation dimensions such as computational cost, efficiency, or number of retrieval rounds."]}
{"type": "entry", "index": 3, "question": "When does knowledge-graph RAG outperform dense retrievers on long-tail entity queries?", "answer": "Knowledge-graph RAG tends to outperform pure dense retrievers when the query is about a specific but rarely-mentioned (long\u2011tail) entity and the answer depends on how that entity is related to others, rather than on surface\u2011level wording.\n\nIn dense retrieval, passages are ranked by overall semantic similarity to the query text, which can pull in many \u201clook\u2011alike\u201d sentences that mention similar words but refer to different people, companies, or events. For example, in a financial KG, a dense retriever might return both \u201cSteve Jobs quits Apple\u201d and \u201cDavid Peter leaves Starbucks\u201d for a query like \u201cwho quit Apple,\u201d because both talk about someone leaving a company [3]. A knowledge graph that explicitly encodes entities and relations can restrict the search to the subgraph around the specific company node (Apple) and its outgoing \u201cquit/left\u201d relations, so only passages actually about quitting Apple are retrieved [3]. This is exactly the kind of precision you need for long\u2011tail entities that don\u2019t appear in many contexts.\n\nIndustrial systems that build custom KGs for domains like LinkedIn customer QA report substantially better retrieval metrics (MRR, Recall, NDCG) and downstream answer quality (BLEU, METEOR, ROUGE) than a baseline semantic search system, indicating advantages on specialized, often long\u2011tail user questions where entity structure matters [3].\n\nGraph-based RAG frameworks like GraphRAG and related methods show that organizing content into entity communities and subgraphs makes it easier to pull the right subgraph for complex, relation\u2011heavy queries instead of scanning many loosely related text chunks [3][4]. This helps especially when the query is vague or underspecified in wording but well-defined in terms of underlying entities and relationships\u2014exactly the setting where dense retrievers struggle with noise and near\u2011matches [3].", "tags": ["hybrid_vs_kg", "long_tail_entities"], "full_faithfulness": 1.0, "full_answer_relevancy": 0.9611225119000001, "seed_faithfulness": 1.0, "seed_answer_relevancy": 0.9660236389, "context_relevance": 0.755545002991404, "context_relevance_error": null, "context_relevance_reason": "The context passages provide substantial information about knowledge-graph RAG methods and their advantages over dense retrievers, particularly in handling long-tail entity queries. The mention of specific frameworks like GraphRAG and their ability to structure knowledge effectively addresses the user's question. However, while the context is relevant, it lacks direct comparisons or specific instances where knowledge-graph RAG outperforms dense retrievers, which would strengthen the response further.", "structure_citation": 0.7724855666803808, "structure_error": null, "structure_reason": "The Actual Output effectively discusses the advantages of knowledge-graph RAG over dense retrievers for long-tail entity queries, providing specific examples and insights. It includes citations [3] and [4], which correspond to relevant snippets in the Context. However, while most claims are well-supported, there are instances where the connection to the cited snippets could be clearer, particularly regarding the empirical metrics mentioned. This slight lack of clarity in supporting all claims fully prevents a perfect score.", "hallucination_score": 0.8688877288547415, "hallucination_error": null, "hallucination_reason": "The response clearly articulates when knowledge-graph RAG outperforms dense retrievers, specifically highlighting its effectiveness with long-tail entity queries. It accurately describes the limitations of dense retrieval methods and provides relevant examples, such as the distinction between retrieving similar sentences versus specific entity relationships. The claims made are well-supported by the context, particularly regarding the advantages of knowledge graphs in enhancing retrieval precision. However, while the response is strong, it could benefit from a more explicit connection to the specific metrics mentioned in the context, such as MRR and Recall, to fully align with the evaluation steps.", "contexts_used": ["Related Work\n\nRAG has emerged as a critical framework for enhancing LLMs by integrating external knowledge. Early approaches such as REALM (Guu et al. 2020) and DPR (Karpukhin et al. 2020) focus on encoding large text corpora into dense embeddings. In recent years, GraphRAG has become a new direction because it can structure fragmented knowledge. RAPTOR (Sarthi et al. 2024) and Microsoft's GraphRAG (Edge et al. 2025) both use hierarchical clustering: RAPTOR constructs recursive trees with multi-level summarization, and GraphRAG applies community detection with LLM-generated synopses, to support coarse-tofine retrieval and high-coverage responses. DALK (Li et al. 2024) and KGP (Wang et al. 2024) introduce dynamic KG construction and traversal agents, using LLMs to build domain-specific graphs and self-aware retrieval policies, to inject structural context while reducing noise. GFMRAG (Luo et al. 2025), G-Retriever (He et al. 2024), and LightRAG (Guo et al. 2025) combine graph neural encoders with specialized retrieval objectives: a querydependent GNN trained in two stages for multi-hop generalizability, a Prize Collecting Steiner Tree formulation to reduce hallucination and improve scalability, and a dual-level graph-augmented index for efficient, incrementally updatable lookup, respectively enabling accurate, scalable reasoning over graphs. HippoRAG (Guti\u00b4 errez et al. 2024), inspired by hippocampal memory processes, leverages Personalized PageRank for single-step multi-hop retrieval, delivering state-of-the-art efficiency and performance on both path following and path finding QA tasks. HippoRAG2 (Guti\u00b4 errez et al. 2025) further optimizes knowledge graph refinement and deeper passage integration. More details regarding related work are provided in the Appendix A.", "Retriever Module (Dense Passage Retrieval).\n\nThe retriever's job is to efficiently identify which pieces of text in a large corpus are relevant to the input query x . Modern RAG implementations typically use dense retrievers like Dense Passage Retrieval (DPR) [45] in lieu of traditional keyword search. In DPR, a bi-encoder architecture is employed: a question encoder E q ( x ) maps the query x to a d -dimensional vector, and a passage encoder E p ( d ) maps each candidate document (or passage) d in the corpus to a d -dimensional vector in the same space. Relevance is assessed via a similarity score, usually the dot product of these vectors:\n\nAt query time, the retriever computes v q = E q ( x ) and then finds the topK documents whose vector E p ( d ) has highest inner product with v q . This search for maximum inner product can be implemented efficiently using Approximate Nearest Neighbor techniques (e.g., FAISS) to handle millions of documents in sub-linear time. The output of the retriever is the set Z = { z 1 , . . . , z K } of top-ranked documents and potentially their similarity scores. One can view the retriever as defining a probability distribution P ret ( z | x ) over documents z in the corpus, such that:\n\nwith the normalization \u2211 d \u2208C exp( E q ( x ) \u22a4 E p ( d )) (in practice approximated by summing over retrieved candidates rather than all of C ). In other words, the retriever assigns higher probability (or score)", "to documents whose embedding is most similar to the query's embedding. The DPR retriever is usually first trained on pairs of questions and relevant passages to ensure E q and E p produce representations that maximize dot-products for true Q-A pairs and minimize them for irrelevant pairs. The training objective is often a contrastive loss : for a given question q with a gold relevant passage p + and a set of negative passages { p -1 , . . . , p -N } , the encoder is trained to maximize sim ( q, p + ) while minimizing sim ( q, p -) for negatives. This can be formulated as a cross-entropy loss treating the positive passage as the correct class among one positive and N negatives:\n\nwhich encourages E q and E p to embed true pairs closer together than any negative pair [45]. After training, the retriever can generalize to new queries: it embeds the query and efficiently finds the nearest neighbor passages in the index. This retrieval step is crucial because it narrows down the evidence from potentially billions of tokens to a manageable subset that the generator will actually consider.", "Generator Module (Conditional Seq2Seq Model). The generator in a RAG pipeline is typically a sequence-to-sequence language model that produces the final answer or output text, given the input query and retrieved documents. Formally, the generator defines a conditional distribution P gen ( y | x, Z ) over output sequences, where Z = { z 1 , . . . , z K } are the retrieved passages. The generator is often initialized from a pre-trained transformer-based seq2seq model (such as BART or T5) to leverage rich language generation capabilities [52]. During generation, the model is provided with the question (or prompt) as well as the content of the retrieved documents. There are multiple ways to feed the retrieved context to the generator:\n\nIn an early fusion approach, one can concatenate the query x with the text of all retrieved passages into a single extended input sequence (perhaps with special separators) and have the seq2seq model attend to all of it at once. This is straightforward: the generator effectively treats the combined text as context and learns to pick out the relevant bits when producing the answer. However, this can be difficult if K is large, as the input may become very long.", "In the late fusion approach adopted by RAG [52], the generator considers one retrieved document at a time as a context and then marginalizes over the document choices (as in Eq. 1). Specifically, the RAG-Sequence variant fixes a single document z i as context for generating the entire output and computes P ( y | x ) by summing the probabilities P ( y | x, z i ) weighted by the retriever's confidence P ( z i | x ) . Another variant, RAG-Token, allows the generator to switch between documents at the token level, effectively marginalizing over the document choice for each generated token [52]. In both cases, the generator P gen ( y | x, z ) itself works like a standard seq2seq model: it factorizes over the output tokens y 1 , . . . , y T as \u220f T t =1 P \u03b8 ( y t | x, z, y <t ) , i.e. it generates one token at a time, attending to the input query x and the content of z (or multiple z 's if early fusion). The generator's architecture typically uses an encoder-decoder Transformer: the encoder encodes the combination of x and z , and the decoder produces y autoregressively.", "Because the generator is conditioning on retrieved evidence, it tends to produce outputs that are supported by that evidence. For example, if the query asks for a specific factual answer, the generator can copy or rephrase the needed information from one of the retrieved passages. This is in contrast to a vanilla language model which would attempt to rely on parametric knowledge (which might be outdated or incomplete). In sum, the generator module in RAG is responsible for fluent and coherent text generation, but crucially it is grounded by the retrieval context, which guides it toward accurate content.", "2 Related Works\n\nSpare and Dense Retriever RAG methods: Hybrid approaches, such as the \"Blended RAG\" framework proposed by Sawarkar et al. [12], demonstrate that combining multiple retrievers can improve the retrieval of relevant passages for clearly defined queries. This framework integrates six distinct retrievers-including traditional sparse methods (e.g., BM25) and dense semantic search techniques-selected based on their performance across benchmark datasets like Natural Questions (NQ) [13], TREC-COVID [14], SqUAD [15], and HotPotQA [16].", "A key strength of this hybrid search technique is its ability to return relevant passages, aligning well with Research Goal 3 for well-specified queries. However, these methods do not fully address Research Goal 3 when handling vaguer questions. In such cases, the hybrid approach can retrieve extraneous or unnecessary information, thus failing to eliminate the \"needle in a haystack\" problem. This shortcoming is evident in evaluations on the CoQA dataset [17], where retrieval accuracies ranged only from 45% to 49%. The reliance on document metadata during retrieval-a common feature in these systems-further complicates matters, as such metadata is often minimal in real-world scenarios (e.g., limited to document names and creation dates), thereby increasing both the creation time of the information store (contradicting Research Research Goal 1) and the risk of returning irrelevant passages (contradicting Research Research Goal 3) leading to longer contexts (contradicting Research Goal 2).", "Figure 3: Results of Priyanka Mandikal et al. on 2 example queries [18].\n\nQuery: Do fibroblasts from CF patients grow at a\n\nQuery: What is the prognosis for patients who have\n\nnormal rate?\n\nhad an episode of respiratory failure?\n\nMethod P@1 P@3 P@5 P@10 VSR 33.33 SPECTERZ Hybrid 66.67 Sample Retrieval This paper analyzes the cell cycle of cystic\n\nThe diagnosis, management and prognosis of 17 infants\n\nfibrosis-derived fibroblasts and statistically\n\nwith cystic fibrosis (CF) and severe respiratory disease\n\ndemonstrates their failure support normal\n\nwere reviewed for the\n\nperiod 1968 to 1972.\n\nThe clinical\n\npopulation doubling time in vitro and indicates that this\n\ncourse these infants was characterized by is due largely\n\nto a failure of the\n\ncystic fibrosis-derived\n\nfailure to thrive and\n\ncells to enter the cell cycle in normal numbers.\n\nWe have malnutrition Despite vigorous therapy including no explanation for significant difference antibiotics there was\n\n60% mortality:\n\nA delay in the\n\nbehavior of these cells from patients with cystic fibrosis\n\ndiagnosis of CF from the onset of respiratory symptoms\n\nvis-a-vis either disease control fibroblasts, with a\n\nmean of six weeks was considered an important\n\nsave to conclude that proceeds from an intrinsic\n\nfactor affecting survival. This data supports the need for\n\nproperty of the cells themselves.", "developing a reliable screen test for CF at birth.", "In contrast, recent work by Mandikal et al. [18] has advanced a hybrid retrieval model that integrates a sparse TF-IDF retriever with a dense retriever based on fine-tuned SPECTER2 embeddings [19]. Their approach employs a weighted scoring mechanism, controlled by a hyperparameter \u03bb , to balance the contributions of the two retrieval methods. Empirical evaluations on a medical database concerning Cystic Fibrosis indicate that while dense retrieval alone underperforms, incorporating approximately 20% of the sparse score (i.e., setting \u03bb = 0 . 2 for the sparse component) significantly improves document retrieval accuracies across different retrieval ranks [Figure 3]. Crucially, this method indexes documents solely based on their content, thereby avoiding the pitfalls of metadata dependency. This contributes to reducing information store creation times (Research Goal 1) and partially mitigates the retrieval of unnecessary information (Research Goal 3), although it does not entirely resolve the challenge of handling vaguer queries.", "In summary, while hybrid retrieval approaches represent a significant advancement by returning more relevant passages for well-defined queries complying with Research Goal 3. However they remain imperfect in filtering out unnecessary information for vaguer questions which can lead to longer contexts. This limitation underscores the need for further refinement to minimize the \"needle in a haystack\" problem in large-scale, real-world applications and make a pipeline which is compliant with Research Goal 2 as well.", "Reranker RAG methods: To address the limitation which Hybrid-Search methods have in adhering to Research Goal 2 for vaguer questions, Reranker methods have been developed to reduce the size and improve the quality of the final context.", "Figure 4: A high level view of what a RAG pipeline with a reranker looks like.\n\nIst Stage 2nd Stage Knowledge Embedding-based LLM-based Docs Corpus retrieval reranking\n\nUsually this is a process called 2-step retrieval where the first step involves retrieving documents such as through sparse or dense retriever techniques and then a second stage where an LLM based reranker comes into play to decide which of the documents retrieved are most relevant to the query and sorts them in order [Figure 4] before passing it to an LLM for generation.", "Prior to the formal conceptualization of Retrieval-Augmented Generation (RAG), information retrieval research explored the use of document rerankers. For example, Noguiera et al.'s 2020 study, Document Ranking with a Pretrained Sequence-to-Sequence Model [20], proposed employing a sequence-to-sequence model as a document reranker. This work evaluated multiple fine-tuned T5 models of varying sizes, as well as a fine-tuned BERT model, on documents initially retrieved using the sparse BM25 keyword method. The incorporation of these rerankers led to improvements in retrieval performance across the Robust04, Core17, and Core18 datasets. However, a critical limitation of this framework was its reliance on large models, leaving unresolved whether the observed performance gains were due to the reranking strategy or simply the use of more powerful models.", "Subsequent advancements in RAG have integrated BERT-based rerankers into new pipelines. Michael Glass et al.'s Re 2 G framework [21] embeds a BERT-based reranker-similar to that of Noguiera et al.-within a RAG system. This integration has shown notable improvements in metrics such as Recall (for 5 documents), overall recall, precision, and accuracy on several datasets, as indicated by performance on the KILT leaderboard. Nevertheless, a significant drawback of this approach is the necessity to host an additional model, which directly conflicts with Goal 4: alleviating the monetary and computational costs associated with deploying full-scale LLMs.", "Further refinement in reranking approaches is demonstrated by Xuegang Ma et al., who fine-tuned a LLaMA-2-7B model for reranking in their RankLLaMA system [22]. Although RankLLaMA achieves superior reranking performance, it again relies on a large model comparable to LLaMA-2-7B. Such reliance on resource-intensive models undermines the overarching objective of Goal 4, as the additional computational and financial burden associated with hosting these rerankers contradicts the need for efficiency and cost reduction.", "In summary, while reranker models can mitigate the 'needle in a haystack' problem (Goal 2) by filtering out least relevant information, they are not a sustainable solution. Their requirement for additional large-scale models significantly increases computational and monetary costs, thereby violating Goal 4. Consequently, alternative strategies that achieve comparable retrieval performance without incurring such overhead are essential for the advancement of practical and efficient RAG systems.", "Knowledge Graph RAG methods: Knowledge graph-based solutions have been proposed as a promising alternative for enhancing the RAG process. The core idea behind employing knowledge graphs is to address semantic search challenges by organizing data according to intrinsic relationships. For example, although not originally applied within a RAG context, Dawei Cheng et al.'s 2020 work on constructing knowledge graphs for a financial price prediction tool [23] illustrates how grouping semantically related data into distinct sub-networks enables a query-such as 'who quit Apple'-to retrieve only the pertinent subset of information, rather than returning all semantically similar sentences (e.g., both 'Steve Jobs quits Apple' and 'David Peter leaves Starbucks'). This targeted retrieval helps mitigate the \"needle in a haystack\" problem (Goal 2) and increases the rate of retrieving relevant passages (Goal 3).", "Building on these insights, recent research has directly integrated knowledge graphs within RAG frameworks. For instance, Zhentao Xu et al. [24] developed a system for LinkedIn's customer question answering service by first", "creating a custom knowledge graph and then retrieving related nodes. The retrieved information is subsequently refined using GPT-4 to produce coherent responses. When compared with a baseline semantic search system, this pipeline significantly improved retrieval metrics-including Mean Reciprocal Rank, Recall, and Normalized Discounted Cumulative Gain-as well as evaluation measures such as BLEU, METEOR, and ROUGE, thereby supporting Goals 2 and 3.", "However, despite these promising advances, knowledge graph-based approaches fall short of meeting Goal 1. The process of constructing and training knowledge graphs is computationally intensive and time-consuming, which directly conflicts with the objective of reducing information store creation times.", "| | Model | EM | Fl Score | Accuracy | Hallucination |\n|---:|:--------------|-----:|:-----------|:-----------|:----------------|\n| 0 | Human | 63 | | | |\n| 1 | MHQA-GRN | 33.2 | | | |\n| 2 | Embedding-RAG | 28 | 37 | 46 | 30 |\n| 3 | KG-RAG | 19 | | 32 | 15 |\n\nFigure 5: Results comparing knowledge graph framework with semantic search methods [25].", "For example, Diego Sanmart\u00edn's 2024 work [25] demonstrated a Knowledge Graph RAG system that leverages LLMs to extract entity-relationship triples from unstructured text and employs a 'Chain of Explorations' retrieval strategy. While this approach shows potential for handling vague queries, the high training times required to build and maintain the knowledge graph significantly increase the overall overhead. Moreover, the compressed storage format-using entity-relationship triples-can lead to a loss of detailed contextual information, thereby limiting the system's ability to fully expand upon relevant topics during retrieval hence resulting in even lower EM, F1, Accuracy scores with only an improvement in Hallucination compared to a standard Semantic Search method [Figure 5].", "In summary, while knowledge graph-based solutions offer clear benefits in terms of improving semantic search and enhancing the retrieval of relevant passages (addressing Goals 2 and 3), they do not currently satisfy Goal 1 due to their high training times and associated computational costs. Addressing these inefficiencies is essential for realizing the full potential of knowledge graphs within efficient and practical RAG frameworks.", "Combining Knowledge Graphs and Vector based Search methods: The limitations inherent in a purely knowledge graph-based RAG framework suggest that combining the strengths of knowledge graphs with traditional semantic search methods may yield superior results. Bhaskarjit Sarmah et al. [26] propose such an integrated approach through their HybridRAG framework. This framework concatenates outputs from a knowledge graph query-implemented using Microsoft's GraphRAG [27] with content indexed by extracting entity relationships in chunk sizes of 1024-with results from a semantic search, which similarly employs chunk sizes of 1024 and the text-embedding-ada-002 model. The aggregated context is then provided to GPT-3.5 to generate the final answer. The dataset for this study was constructed by web scraping earnings call documents of companies listed on the NIFTY 50, and evaluation was performed using the RAGAS evaluation platform [28].", "The HybridRAG framework demonstrated improvements in metrics such as Faithfulness, Answer Relevancy Score, and Context Recall. These enhancements support Goal 2 by reducing the \"needle in a haystack\" problem and Goal 3 by increasing the rate of retrieving relevant passages for answering questions. However, the framework underperformed in Context Precision when compared to standalone implementations of both knowledge graph-based and vector database-based RAG systems. This shortcoming indicates that the current integration does not fully leverage the advantages of traditional vector search techniques, particularly hybrid search techniques, which is critical for fully meeting Goal 3.", "6 Conclusion\n\nThis paper introduces the K 2 RAG framework, which addresses key limitations of traditional RAG methods. By utilizing a summarized corpus, we achieved an average reduction of 93% in training times. When combined with quantized models and an advanced retrieval process, K 2 RAG outperformed all tested naive RAG methods in terms of answer accuracy, resource efficiency, and execution speed, even with more complex steps. As a result, K 2 RAG offers a more accurate and lightweight RAG pipeline. While the framework demonstrated a slight improvement in mean similarity and a higher Q 3 value, it primarily excelled in answering certain question types. For future work, we recommend testing K 2 RAG on additional datasets to assess its performance on a wider range of topics, and enhancing knowledge graph retrieval techniques to further improve answer accuracy across all question types by identifying a broader set of relevant topics.", "Knowledge Graph Based Retrieval-Augmented Generation\n\nTo better capture the relational nature of information, KGbased RAG has emerged as a prominent research direction. By representing knowledge as a graph of entities and relations, these methods aim to provide a more structured and semantically rich context for the LLM (Peng et al. 2024). Early approaches in this domain focused on leveraging graph structures for improved retrieval. For instance, GraphRAG (Edge et al. 2024) organizes documents into community-based KGs to preserve local context, while other methods like FastGraphRAG utilize graph-centrality metrics such as PageRank (Page et al. 1999) to prioritize more important nodes during retrieval. This subgraph retrieval approach has also proven effective in industrial applications like customer service, where KGs are constructed from historical support tickets to provide structured context (Xu et al. 2024). These methods marked a significant step forward by imposing a macro-structure onto the knowledge base, moving beyond disconnected text chunks.", "Recognizing the need for more fine-grained control and abstraction, subsequent works have explored more sophisticated hierarchical structures. HiRAG, the current stateof-the-art, clusters entities to form multi-level summaries (Huang et al. 2025a), while LightRAG (Guo et al. 2024) proposes a dual-level framework to balance global and local information retrieval. While these hierarchical methods have progressively improved retrieval quality, a critical gap persists in how the constructed graph structures are leveraged at query time. The retrieval process is often decoupled from the indexing structure; for instance, an initial search may be performed over a 'flattened' list of all nodes, rather than being directly guided by the indexed community or hierarchical relations. This decoupling means the rich structural information is primarily used for post-retrieval context expansion, rather than for guiding the initial, crucial step of identifying relevant information. This can limit performance on complex queries where the relations between entities are paramount, highlighting the need for a new paradigm where the retrieval process is natively co-designed with the knowledge structure."]}
{"type": "entry", "index": 4, "question": "What retrieval settings help reduce needle-in-a-haystack issues for medical QA?", "answer": "To reduce \u201cneedle\u2011in\u2011a\u2011haystack\u201d problems in medical QA, the retrieval setup should focus on bringing back a small, very focused set of highly relevant items instead of large, noisy context.\n\n1. Control context size and granularity\n- Use abstracts or snippets rather than full articles; long, unfocused context diluted model performance in biomedical QA, so reranking down to a smaller, high\u2011quality subset was necessary for good answers [1].\n- Limit the number of documents passed to the model (e.g., rerank from thousands down to a few hundred or less) to avoid disorienting the model with too much text [1].\n\n2. Use multi\u2011stage retrieval\n- Start with high\u2011recall lexical retrieval (e.g., BM25 over PubMed) to get a broad set of candidates (up to thousands) [1].\n- Apply a strong semantic reranker to pick only the top slice for RAG (e.g., top 300) [1].\n- If evidence is missing, fall back to broader web\u2011or multi\u2011source search over trusted medical sites to fill gaps, rather than over\u2011stuffing the first pass with everything [1].\n\n3. Improve query quality\n- Use an LLM to rewrite or expand queries (synonyms, related terms, less rare jargon) when initial recall is poor, boosting the chance that the key paper is retrieved without over\u2011expanding every query by default [1].\n\n4. Use structured, corpus\u2011aware retrieval\n- Retrieve from structured medical knowledge sources such as knowledge graphs and prepend only the most relevant subgraph, which consistently improved accuracy across several medical QA benchmarks [3].\n- In multimodal/heterogeneous setups, use corpus\u2011specific retrieval for different knowledge sources so each corpus is searched with methods tuned to its content, instead of one generic retriever over everything [2].", "tags": ["retrieval_tuning"], "full_faithfulness": 1.0, "full_answer_relevancy": 0.9135841092, "seed_faithfulness": 0.8636363636000001, "seed_answer_relevancy": 0.9273029459000001, "context_relevance": 0.8976322813389481, "context_relevance_error": null, "context_relevance_reason": "The context passages provide comprehensive insights into retrieval settings that can mitigate needle-in-a-haystack issues in medical QA. Specifically, the discussion on the integration of LLMs for information retrieval, the use of semantic reranking, and the multi-stage retrieval approach directly addresses the user's question. The emphasis on maintaining relevance while expanding query coverage and the mention of ensemble methods highlight effective strategies for improving retrieval accuracy in complex biomedical queries. However, while the information is highly relevant, a more explicit connection to specific 'needle-in-a-haystack' challenges could enhance clarity.", "structure_citation": 0.7600086201045874, "structure_error": null, "structure_reason": "The Actual Output effectively addresses the question about retrieval settings for medical QA, providing detailed strategies that align with the context provided. It includes multiple claims supported by [n] markers, specifically [1], [2], and [3], which correspond to relevant snippets in the context. However, while most claims are well-supported, there is a minor inconsistency with the citation of [3], which does not directly support the claim made about structured retrieval. This slight mismatch prevents a perfect score.", "hallucination_score": 0.8893376167284271, "hallucination_error": null, "hallucination_reason": "The response effectively addresses the input question by outlining specific retrieval settings that can mitigate 'needle-in-a-haystack' issues in medical QA. It provides detailed strategies such as controlling context size, using multi-stage retrieval, improving query quality, and employing structured retrieval methods, all of which align well with the context provided. The output supports the claims made in the input and is consistent with the context, demonstrating a strong understanding of the challenges in biomedical QA. However, a minor shortcoming is the lack of explicit references to the context's findings, which could have further strengthened the connection.", "contexts_used": ["Abstract\n\nBiomedical question answering (QA) poses significant challenges due to the need for precise interpretation of specialized knowledge drawn from a vast, complex, and rapidly evolving corpus. In this work, we explore how large language models (LLMs) can be used for information retrieval (IR), and an ensemble of zero-shot models can accomplish state-of-the-art performance on a domain-specific Yes/No QA task. Evaluating our approach on the BioASQ challenge tasks, we show that ensembles can outperform individual LLMs and in some cases rival or surpass domain-tuned systems - all while preserving generalizability and avoiding the need for costly fine-tuning or labeled data. Our method aggregates outputs from multiple LLM variants, including models from Anthropic and Google, to synthesize more accurate and robust answers. Moreover, our investigation highlights a relationship between context length and performance: while expanded contexts are meant to provide valuable evidence, they simultaneously risk information dilution and model disorientation. These findings emphasize IR as a critical foundation in Retrieval-Augmented Generation (RAG) approaches for biomedical QA systems. Precise, focused retrieval remains essential for ensuring LLMs operate within relevant information boundaries when generating answers from retrieved documents. Our results establish that ensemble-based zero-shot approaches, when paired with effective RAG pipelines, constitute a practical and scalable alternative to domain-tuned systems for biomedical question answering.", "3. Our methodology\n\n3.1. Information Retrieval Pipeline\n\nTo support high-quality RAG for Phase A+, we developed an IR pipeline that integrates traditional lexical search with LLM-based query generation and semantic reranking (Fig. 1).", "If the initial query returns fewer than five documents, we invoke Gemini 2.5 Pro Preview (05-06) to automatically revise the query. The model is prompted to enhance retrieval recall by enabling approximate matching and omitting overly rare or domain-specific terms. This refinement step is done to improve the query coverage while maintaining relevance. Our experiments have shown that this process is required in less than 5% of the queries in the BioASQ 13 test set.", "We index all PubMed article titles and abstracts in an Elasticsearch instance, using BM25 retrieval as the ranking function. For each input question, we use Gemini 2.0 Flash to generate a structured Elasticsearch query that captures the semantic intent of the question using synonyms, related terms, and full boolean query string syntax rules supported by Elasticsearch. This query is validated using regular expressions and then is used to retrieve up to 10,000 documents.", "Following document retrieval, we apply a semantic reranking model (Google semantic-ranker-default004) to reduce the number of candidate documents [11]. This model re-scores the initially retrieved documents based on semantic similarity to the original question, allowing us to select the top 300 most relevant documents. This reranked subset is used for downstream RAG-based QA, since despite really long context supported by modern Transformer architectures [12, 13], we could not get adequate QA results on full article abstracts without this step.", "Finally, we have added additional IR searches to handle the cases where a QA step does not return a response based on the evidence retrieved from Elasticsearch. We have observed that Elasticsearch context might not provide sufficient evidence for QA in 3-7% of test cases for Phase A+, depending on the batch. An automated process is used to expand IR sources to address these cases. First, we are using a Google search restricted to PubMed sources to attempt to find new matches. If that fails, we extend our sources to include Home of the Office of Health Promotion and Disease Prevention, WebMD,\n\nThis multi-stage retrieval approach, combining LLM-generated queries, a traditional BM25 search, and semantic reranking, enables flexible, high-recall, and high-precision document selection tailored to complex biomedical queries.", "Figure 1: IR process\n\nPubMed corpus in Elasticsearch Query Generation (Gemini 2.0 Flash) Query Valida- tion and IR (BM25, \u2264 10,000 docs) Results < Refinement 2.5 Pro) Reranking (semantic- reranker-4) Top 300 Articles for RAG No Yes Refine\n\nHealthline, and Wikipedia. This ensures that we have an answer candidate for all questions in Phase A+ test sets.", "3.2. Question Answering Pipeline\n\nWe adopt a unified, zero-shot QA framework for both Phase A+ and Phase B of the challenge. While the core QA procedure remains consistent across phases, Phase A+ incorporates an additional IR step to verify the presence of candidate answers within relevant documents (described at the end of Section 3.1). This ensures that selected documents contain sufficient information to support answer generation.", "To generate candidate answers, we leverage several large language models (LLMs): Gemini 2.0 Flash, Gemini 2.5 Flash Preview (2025-04-17), and Claude 3.7 Sonnet (2025-02-19). Prompts are adjusted using examples derived from the BioASQ 11 test set, improving the response structure and quality.", "The system uses zero-shot prompting, tailored to the question type: Yes/No, Factoid, or List. We experiment with multiple types of input context: (1) IR-derived results from Phase A+, (2) curated snippets provided in Phase B, and (3) full abstracts of articles selected during Phase B. This allows us to examine the influence of context granularity on answer accuracy and completeness.", "To consolidate candidate answers, we perform a secondary synthesis step using Gemini 2.0 Flash. This model is prompted to resolve any contradictions, select the most precise and specific answer components, and integrate complementary information into a single, unified response. As part of this step, the model also returns a confidence score estimating the reliability of the synthesized answer. If the score is below a predefined threshold (0.5, determined empirically), the synthesis is re-run with reduced sampling temperature (from 0.1 to 0.0) to improve determinism. This synthesis process is evaluated using the BioASQ 12 dataset to ensure consistency with benchmark standards.", "Table 1\n\nResults of our runs on BioASQ 13 Phase A+, Yes/No questions.", "| | Batch | System | Accuracy | Ranking |\n|---:|:--------|:------------------|-----------:|----------:|\n| 0 | 3 | Extractive | 0.73 | 41 |\n| 1 | | (last) | 0.23 | 58 |\n| 2 | 4 | Extractive | 0.92 | 1 |\n| 3 | | Simple truncation | 0.88 | 11 |\n| 4 | | Kmeans | 0.65 | 67 |\n| 5 | | (last) | 0.65 | 67 |\n\nTable 2\n\nResults of our runs on BioASQ 13 Phase A+, Factoid questions.", "| | Batch | System | MRR | Ranking |\n|---:|:--------|:------------------|------:|----------:|\n| 0 | 3 | Extractive | 0.14 | 41 |\n| 1 | | (last) | 0.05 | 47 |\n| 2 | 4 | Extractive | 0.43 | 17 |\n| 3 | | Simple truncation | 0.29 | 51 |\n| 4 | | Kmeans | 0.05 | 62 |\n| 5 | | (last) | 0.05 | 62 |", "2 Related Work\n\nMedical Report Retrieval for Generation. Existing Medical MMRAG approaches primarily utilize the medical images to retrieve relevant reports (He et al. 2024; Sun et al. 2025; Xia et al. 2024, 2025). For instance, FactMM-RAG (Sun et al. 2025) enhances report generation by incorporating high-quality reference reports. Similarly, RULE (Xia et al. 2024) and MMed-RAG (Xia et al. 2025) integrate reference reports and employ preference fine-tuning to improve model utilization of retrieved reports. Although these approaches improve the factual accuracy of responses, they neglect the retrieval of medical documents, which are crucial for Med-LVLM's reliable inference.", "Medical Document Retrieval for Generation. Acknowledging the limitations of report-only retrieval, recent studies have increasingly emphasized medical documents as knowledge sources (Choi et al. 2025; Shaaban et al. 2025; Wu et al. 2025; Hamza et al. 2025). Among them, MKGF (Wu et al. 2025) and K-LLaVA (Hamza et al. 2025) both employ multimodal retrievers to fetch documents from the database, aiming to mitigate hallucination issues in language models. ChatCAD+ (Zhao et al. 2024b) and MIRA (Wang et al. 2025) utilize a zero-shot query rewriting module for retrieval. Nevertheless, these retrieval methods overlook the substantial content differences among various corpora, lacking corpus-specific retrieval mechanisms.", "6 Conclusion\n\nThis work addresses the critical challenges of effective retrieval and multi-aspect alignment for heterogeneous knowledge in the Medical MMRAG field. MedAtlas provides a rich, multi-source knowledge base for medical multimodal tasks. The HeteroRAG framework enables precise report retrieval and multi-corpus retrieval, followed by aligning heterogeneous retrieval results through Heterogeneous Knowledge Preference Tuning. Extensive experiments demonstrate that our framework achieves state-of-the-art performance across multiple medical VQA and report generation benchmarks. Our work paves the way for effectively integrating multi-source medical knowledge, advancing the reliability and applicability of Med-LVLMs in clinical scenarios.", "2 Results\n\n2.3 Quality Assessment of Extracted Relationship Triples\n\nFor automated evaluation, two state-of-the-art LLMs, GPT-4.1 [74] and DeepSeek-v3 [75], were employed. A random subset comprising 1% of the abstracts (n = 34,725), resulting in 83,438 extracted triples, was selected for evaluation. Each abstract and its corresponding triples were formatted into structured prompts and independently assessed by both models according to a standardized four-tier rubric: Correct (3.0), Likely Correct (2.0), Likely Incorrect (1.0), and Incorrect (0.0) (the specific evaluation prompt is illustrated in Extended Data Figure 3 a ). Triples receiving scores of \u2265 2 . 0 were deemed valid. The evaluation outcomes are presented in Figure 3 a and b , illustrating the proportion of valid triples across relation types for GPT-4.1 and DeepSeek-v3, respectively. Both models demonstrated high overall accuracy, with 85.44% and 88.10% of triples rated as valid by\n\n2 https://pubmed.ncbi.nlm.nih.gov/", "GPT-4.1 and DeepSeek-v3, respectively. For most relation types, validity was approximately 90%, except for Negative Correlate, which exhibited slightly lower agreement. These findings underscore the high precision of the Extractor Agent across diverse biomedical relation types and support its utility for downstream analyses.", "In parallel, a manual evaluation was conducted to further validate extraction accuracy. Three domain experts with doctoral-level training in artificial intelligence and medicine independently reviewed a randomly selected subset of 400 abstracts, comprising 1,060 extracted triples. Each abstract and its associated triples were evaluated using the same standardized scoring rubric. Triples receiving scores of \u2265 2.0 were considered valid. As shown in Figure 3 c , all three reviewers demonstrated high consistency, with overall validity rates exceeding 86% across assessors. The close concordance between manual and automated evaluations further substantiates the robustness of the Extractor Agent in accurately capturing biomedical relationships, providing strong support for the application of the extracted knowledge in large-scale medical analyses.", "To further validate the reliability of the LLM-based assessments, we used three expert annotations as reference standards to evaluate GPT-4.1 and DeepSeek-v3 on the same subset of 400 abstracts, respectively. As shown in Figure 3 d -f , both models exhibited strong concordance with expert evaluations, achieving precision, recall, and F1 scores of approximately 95% across metrics. These results further corroborate the accuracy of the automated scoring framework and its alignment with expert judgment.", "Finally, inter-rater agreement was assessed across all evaluators-including three human experts and two LLMs-by computing pairwise Cohen's kappa coefficients on a shared evaluation subset (Figure 3 g ) [82]. Most pairwise comparisons (80%) yielded kappa values exceeding 0.6, indicating substantial agreement-an accepted threshold for reliable concordance in domains involving subjective judgment, including medicine, psychology, and natural language processing [83]. The coefficients between expert 1 and expert 2 (0.5663), and between expert 2 and expert 3 (0.5446), fell slightly below this threshold but still reflected moderate agreement, closely approaching the substantial range. These findings demonstrate strong inter-rater reliability across both human and automated evaluators, underscoring the robustness and reproducibility of the evaluation framework.", "2.4 Evaluating Downstream Utility in Medical Question Answering\n\nWe evaluated the downstream utility of our constructed KG as a RAG information source across seven multiplechoice medical QA datasets. These included four widely used benchmarks [76]-MMLU-Med, MedQA-US, PubMedQA*, and BioASQ-Y/N-spanning a broad spectrum of clinical and biomedical reasoning tasks. To further assess diagnostic reasoning under varying complexity, we introduce MedDDx, a newly developed benchmark suite focused on differential diagnosis [77]. Questions are stratified into three levels-MedDDx-Basic, MedDDxIntermediate, and MedDDx-Expert-based on the variance in semantic similarity among answer choices. All MedDDx subsets were designed to reduce training data leakage and more closely reflect authentic clinical reasoning. Detailed dataset statistics are shown in Figure 4 a . We systematically evaluated five state-of-the-art LLMs to measure the impact of KG-based retrieval. Each model was tested in a zero-shot setting under two conditions: (1) direct answering using internal knowledge alone, and (2) RAG, with relevant KG subgraphs prepended as external context. The models-GPT-4-turbo, GPT-3.5-turbo (OpenAI) [78], DeepSeek-v3 (DeepSeek) [75], Qwen-Max, and Qwen-Plus (Qwen) [79]-span diverse architectures and training regimes, representing both proprietary and open-source systems. All models were accessed via publicly available APIs without additional fine-tuning. Version details and access endpoints are summarized in Figure 4 b .", "Figures 4 c -i present model performance across the seven medical QA datasets using radar plots, each depicting the five LLMs under both direct answering (w/o RAG) and RAG conditions (w/ RAG). Notably, the background shading in the radar plots is lighter for the MedDDx suite (Figure 4 g -i ) than for the four widely used benchmarks (Figure 4 c -f ), reflecting the overall lower accuracy of all models on these recently introduced and semantically more challenging datasets. This contrast highlights the greater complexity and reduced risk of training data leakage inherent to the MedDDx design. Across all datasets, RAG with our KG consistently outperformed direct answering. The most substantial improvements were observed in tasks requiring deeper clinical reasoning, such as MedQA-US and the MedDDx suite. For example, on MedQA-US, GPT-3.5-turbo improved from 0.5986 to 0.6834 (+8.5 percentage points), and Qwen-Max from 0.7306 to 0.7636. On MedDDx-Expert, RAG yielded absolute gains of up to +8.6 points for GPT-3.5-turbo and +5.7 points for Qwen-Max. Even in knowledge-intensive but semantically simpler tasks such as MMLU-Med and BioASQ-Y/N, RAG offered modest yet consistent benefits. On MMLU-Med, GPT-4-turbo improved from 0.8724 to 0.9054, while DeepSeek-v3 achieved the highest score overall at 0.9183 with KG support. In BioASQ-Y/N, RAG further enhanced already strong performance, with four models exceeding 0.85 accuracy following augmentation. Notably, several models performed better on MedDDx-Expert than on MedDDx-Basic, despite the former being constructed with higher semantic complexity. This counterintuitive trend may be related to differences in distractor framing, where Expert-level distractors-", "a", "Figure 4: Overview of evaluation datasets, model configurations, and performance across medical QA tasks. a . Dataset statistics for the seven medical QA benchmarks used in this study. The benchmark suite includes four widely adopted datasets [76] (MMLU-Med, MedQA-US, PubMedQA*, and BioASQ-Y/N) and three newly developed differential diagnosis datasets [77] (MedDDx-Basic, MedDDx-Intermediate, and MedDDx-Expert). For each dataset, we report the number of multiple-choice questions and the corresponding answer option formats. b . Configuration of the five LLMs evaluated: GPT-4-turbo, GPT-3.5-turbo (OpenAI) [78], DeepSeek-v3 (DeepSeek) [75], Qwen-Max, and Qwen-Plus (Qwen) [79]. All models were accessed through public APIs in their zero-shot settings without fine-tuning. The specific version identifiers and access platforms are indicated. c -i . Model performance across the seven QA datasets, shown as radar plots. Each chart compares zero-shot accuracy for five LLMs under two conditions: direct answering without retrieval (w/o RAG) and RAG with our KG (w/ RAG). Across all datasets, RAG with our KG consistently outperformed direct answering.\n\nDatasets Size Options MMLU-Med 1,089 A/B/C/D MedQA-US 1,273 PubMedQA* Yes/No/Maybe BioASQ-Y/N Yes/No MedDDx-Basic MedDDx-Intermediate 1,041 MedDDx-Expert Provider Model Version Accessed URL OpenAI GPT-4-turbo\n\nhttps://platform.openai.com/docs/models/gpt-4-turbo\n\nGPT-3.5-turbo\n\nhttps://platform.openai.com/docs/models/gpt-3.5-turbo\n\nDeepSeek\nDeepSeek-v3", "https://huggingface.co/deepseek-ai/DeepSeek-V3\n\nQwen\nQwen-Max\n\nhttps://www.alibabacloud.com/help/en/model-\n\nstudio/what-is-qwen-llm Qwen-Plus b BioASQ-YIN w/o RAG RAG 0.9054 0.8130 0.5780 0.8625 0.5660 0,5720 0.5520 0.7401 0.7880 0.4940 0.831 0.5300 0.8953 0.8834 0.9183 0.8036 h wlo RAG 0.5197 0.5437 0,5714 0.5207 0.5347 0.4890 0,4265 506- 0.3685 0.4204 0,.4688 0.5020 0,4720 0.5259 0.4990 0.5043 0.5592 0,5878 0.8935 0.8576 7855| 0.8398 DeepSe -Max Seek-v3 0,5135 ) 5673 0.5469 0.4700", "Figure 5: Case study of tocilizumab for literature-based discovery and drug repurposing within the KG. a . Known association between tocilizumab and rheumatoid arthritis, supported by multiple publications, with the earliest reported date defined by the first extracted supporting paper. b . Two multi-hop reasoning paths linking tocilizumab to COVID-19 via intermediate genes FGB and TNF. The inferred Treat relation (red arrow) was derived solely from earlier literature, while later studies validated this prediction (green arrow). The temporal order of evidence highlights the KG's capacity to anticipate therapeutic connections prior to their recognition in the literature.\n\ntociliz-\numab\n\nIdentifier:\n\nMESH:C502936\n\nEntity Type:\n\nChemical\n\nTerminology:\n\nNCBI MeSH\nPage Link\n\n: meshb.nlm.nih.gov/record/ui?ui=C502936\n\nTreat Arthritis Rheum atoid MESH:D001172 Disease\n\n: meshb.nlm.nih.gov/record/ui?ui=D001172\n\nConfidence\n: 0.999999925\nPubMed IDs\n\n:\n\n26374404,27958380,29146040,30859494,308\n\n88472,32844216,35713462,36688476\n\nEarliest Reported Date\n: 2016-07-01\n\nmeshb.nlm.nih.gov/record/ui?ui=C502936\n\nFGB Gene Terminology NCBI Gene\n\n: www.ncbi.nlm.nih.gov/gene/2244\n\nCOVID -19 Identifier : MESH:D000086382 : NCBI MeSH\n\nmeshb.nlm.nih.gov/record/ui?ui=D000086382\n\nTNF"]}
{"type": "entry", "index": 5, "question": "How does Reciprocal Rank Fusion compare to weighted hybrid scoring on arXiv-style corpora?", "answer": "The provided documents do not report any direct comparison between Reciprocal Rank Fusion (RRF) and a weighted hybrid scoring scheme on arXiv-style corpora, so we cannot say which performs better under those conditions.\n\nFrom the documents, we only know the following about RRF:\n\n1. RRF is a rank-based fusion method that combines the rankings from multiple retrievers. For a document d, its fused score is computed as the sum over retrievers of 1 / (k + rank_i(d)), where rank_i(d) is the position of d in retriever i\u2019s ranked list and k is a constant that dampens the influence of very high ranks [2]. This means RRF emphasizes documents that are consistently ranked near the top across methods.\n\n2. RRF is used to fuse dense and sparse retrieval results in a mixed retriever for harm-reduction RAG, helping integrate semantic and keyword-based matches [2].\n\n3. It is also used as a baseline hybrid retrieval method that combines dense and sparse search results in a large GraphRAG-style system [4].\n\n4. In a medical multimodal RAG framework, Reciprocal Rank Fusion is used to combine image-to-text and text-to-text retrieval results, before comparing against a more specialized retriever, showing that the specialized method can outperform this simple fusion, but without any link to arXiv-style corpora [3].\n\nNone of the documents define or evaluate a \u2018weighted hybrid scoring\u2019 scheme, nor do they discuss retrieval experiments on arXiv or arXiv-like scientific-paper corpora. Therefore, a data-driven comparison between RRF and weighted hybrid scoring on arXiv-style corpora is not available in the supplied material.", "tags": ["hybrid_vs_kg", "rrf_hybrid"], "full_faithfulness": 0.9444444444000001, "full_answer_relevancy": 0.0, "seed_faithfulness": 0.7000000000000001, "seed_answer_relevancy": 0.0, "context_relevance": 0.2556171844453696, "context_relevance_error": null, "context_relevance_reason": "The context passages provided do not directly address the user's question about the comparison between Reciprocal Rank Fusion and weighted hybrid scoring on arXiv-style corpora. While some passages mention Reciprocal Rank Fusion, they lack specific information or analysis related to weighted hybrid scoring, making them largely irrelevant to the user's inquiry.", "structure_citation": 0.5576464375676288, "structure_error": null, "structure_reason": "The response correctly identifies that there are no direct comparisons between Reciprocal Rank Fusion (RRF) and weighted hybrid scoring in the provided documents, which aligns with the evaluation steps. However, while it includes claims supported by snippets (e.g., the description of RRF), it fails to utilize the [n] markers effectively, as it does not cite the specific snippets corresponding to the claims made. This lack of citation for the claims results in a moderate score.", "hallucination_score": 0.6101977113787885, "hallucination_error": null, "hallucination_reason": "The response accurately identifies that there is no direct comparison between Reciprocal Rank Fusion (RRF) and weighted hybrid scoring on arXiv-style corpora, which aligns with the context provided. However, while it elaborates on RRF's characteristics and applications, it lacks specific details about weighted hybrid scoring, which diminishes the completeness of the answer. The absence of relevant information about weighted hybrid scoring limits the overall effectiveness of the response.", "contexts_used": ["Conclusion\n\nThe observed increase in query length suggests that the current training scheme, constrained by limited samples and a single epoch, may inadvertently favor verbose queries. Future work could explore larger-scale training or refined reward mechanisms to balance query conciseness with retrieval effectiveness. Additionally, enhancing the alignment between rewriters and semantic retrievers through advanced reward policies or training strategies could unlock further performance gains.", "In conclusion, RL-QR offers a scalable, annotation-free solution for query optimization, with broad applicability to diverse retrievers and databases. Its demonstrated success on industrial in-house data underscores its potential to transform real-world retrieval systems, paving the way for more efficient and versatile RAG applications.", "Chan, C.-M.; Xu, C.; Yuan, R.; Luo, H.; Xue, W.; Guo, Y.; and Fu, J. 2024. Rq-rag: Learning to refine queries for retrieval augmented generation. arXiv preprint arXiv:2404.00610 .", "Chen, J.; Xiao, S.; Zhang, P.; Luo, K.; Lian, D.; and Liu, Z. 2024. BGE M3-Embedding: Multi-Lingual, MultiFunctionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation. arXiv:2402.03216.", "Comanici, G.; Bieber, E.; Schaekermann, M.; Pasupat, I.; Sachdeva, N.; Dhillon, I.; Blistein, M.; Ram, O.; Zhang, D.; Rosen, E.; et al. 2025. Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multimodality, Long Context, and Next Generation Agentic Capabilities. arXiv preprint arXiv:2507.06261 .", "Cormack, G. V.; Clarke, C. L.; and Buettcher, S. 2009. Reciprocal rank fusion outperforms condorcet and individual rank learning methods. In Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval , 758-759.", "Edge, D.; Trinh, H.; Cheng, N.; Bradley, J.; Chao, A.; Mody, A.; Truitt, S.; Metropolitansky, D.; Ness, R. O.; and Larson, J. 2024. From local to global: A graph rag approach to query-focused summarization. arXiv preprint arXiv:2404.16130 .", "Faysse, M.; Sibille, H.; Wu, T.; Omrani, B.; Viaud, G.; Hudelot, C.; and Colombo, P. 2024. Colpali: Efficient document retrieval with vision language models. In The Thirteenth International Conference on Learning Representations .", "Feng, H.; Wei, S.; Fei, X.; Shi, W.; Han, Y.; Liao, L.; Lu, J.; Wu, B.; Liu, Q.; Lin, C.; et al. 2025. Dolphin: Document image parsing via heterogeneous anchor prompting. arXiv preprint arXiv:2505.14059 .", "Fix, E. 1985. Discriminatory analysis: nonparametric discrimination, consistency properties , volume 1. USAF school of Aviation Medicine.", "Hurst, A.; Lerer, A.; Goucher, A. P.; Perelman, A.; Ramesh, A.; Clark, A.; Ostrow, A.; Welihinda, A.; Hayes, A.; Radford, A.; et al. 2024. Gpt-4o system card. arXiv preprint arXiv:2410.21276 .", "Answer Requirements:\n\nStart with the precise numerical value and units.\n\nInclude route of administration if specified.\n\nInclude frequency/timing context if mentioned.", "Explain significance of the dosage (e.g., threshold, therapeutic, etc.)\n\nAvoid referencing 'the text' or 'the source'\n\nKnowledge Content: Here is the knowledge content to use: knowledge\n\nFormat your response as:\n\n'question': 'Question text', 'answer': 'Numerical value + units + explanation from source',\n\n'context': 'The exact sentences of context information you used from the source'\n\nTable 2: System instruction used for safety boundary check tasks.", "(i) You are tasked with answering a yes/no question related to harm reduction practices based on reliable information.\n\n(ii) You must respond in exactly this format:\n\nYes/No , followed by a brief explanation of the substantial reason(s) behind.", "Knowledge Retrieval : We implemented a mixed retrieval method using dense and sparse retrievers. Specifically, we leverage dense retriever (using Sentence-Transformers (Reimers and Gurevych, 2019)) to identify semantically related harm reduction information. We then implement sparse retriever BM25 (Robertson et al., 2009) using precise term-matching to accurately retrieve specific substance names and dosage information that may not be optimally captured through dense retriever alone.", "Knowledge Reranking : To effectively integrate retrieved knowledge, we employ the Reciprocal Rank Fusion (RRF) method, which combines the results from both retrieval methods using a rank-based fusion: score ( d ) = \u2211 i 1 k + rank i ( d ) , where d denotes the target document and rank i ( d ) denotes document d 's rank in the i -th retrieval method, and k is a constant that mitigates the impact of high rankings.", "The RAG scheme integrates with LLMs when providing harm reduction information. Its model response R is formulated as:\n\nwhere K denotes the retrieved knowledge.", "5 Experiments\n5.2 Main Results\n\nMed-LVLMs: Figure 3 shows that HeteroRAG, with a 7B parameter size, outperforms most advanced Med-LVLMs, which contain 4 to 5 times more parameters across multiple datasets. This indicates that the proposed framework advances the medical multimodal capabilities of existing MedLVLMs to a higher level.", "5.3 Effectiveness of Retrieved Knowledge\n\nWe conduct ablation studies to evaluate the contribution of knowledge sources as shown in Table 3. The 'Original' and 'SFT' settings represent the performance of the original Lingshu-7B and Lingshu-7B after SFT on the original training set, which does not include reports and documents. The other configurations examine HeteroRAG's performance when either reports or documents are removed. The results show that retrieved knowledge significantly improves Med-LVLM's performance compared to the Original baseline. The performance improvements from super-\n\n| | Models | Rad. | Oph. | Pat. |\n|---:|:------------|:-------|:-------|:-------|\n| 0 | BiomedCLIP | 30.20 | 13.45 | 28.85 |\n| 1 | PMC-CLIP | 30.05 | 19.80 | 23.35 |\n| 2 | PubMedCLIP* | 13.35 | - | - |\n| 3 | MM-Retinal* | - | 4.65 | - |\n| 4 | QuiltNet* | - | - | 39.65 |\n| 5 | FactMM-RAG* | 44.25 | - | - |\n| 6 | RULE* | 31.80 | 18.90 | - |\n| 7 | MMed-RAG* | 31.80 | 18.90 | 30.20 |\n| 8 | ModCLIPs* | 79.40 | 47.55 | 77.35 |\n\nTable 4: Image-to-text recall@5 of different retrievers. The asterisks (*) denote the modality-specific retrievers.", "vised fine-tuning alone are insufficient to compensate for the absence of knowledge. When reports or documents are excluded, the performance degradation confirms that both sources are important for HeteroRAG's knowledgeintensive inference. Furthermore, all five corpora contribute to Med-LVLM's capacity.", "5.4 Effectiveness of ModCLIPs\n\nWe evaluate ModCLIPs against other medical retrievers on image-to-text report retrieval tasks, as shown in Table 4. Generalist retrievers include BiomedCLIP and PMCCLIP (Lin et al. 2023). Modality-specific retrievers include PubMedCLIP (Eslami, Meinel, and De Melo 2023), MMRetinal (Wu et al. 2024b), QuiltNet (Ikezogwo et al. 2023), FactMM-RAG, RULE and MMed-RAG. Using the test set described in Section 4.1 with recall@5 as our evaluation metric, our experiments demonstrate that ModCLIPs consistently outperform competing methods across all three modalities. This superior performance can be attributed to", "| | Methods | OMVQA-Rad | OMVQA-Oph | Quilt-VQA |\n|---:|:----------------|------------:|------------:|------------:|\n| 0 | CLIP | 76.83 | 84.42 | 63.85 |\n| 1 | MQG w/o DPO w/o | 82.08 | 91.25 | 69.97 |\n| 2 | | 81.08 | 88.17 | 65.6 |\n| 3 | SFT | 78.75 | 86 | 64.14 |\n\nTable 5: Performance of HeteroRAG under two ablation settings: replacing MQG with CLIP-based retrieval, and removing the training stages of MQG.", "Figure 4: Accuracy and disalignment metrics of Lingshu-7B trained with different methods and data.\n\nOriginal SFT HKPT w/o CMA w/o KU w/o KR Accuracy (\u2191) MD (\u2193) KUD (\u2193) KID (\u2193)\n\ntwo key advantages: (1) Single-modality training yields significantly better modality-specific understanding compared to mixed-modality approaches, and (2) our training data offers more comprehensive coverage and greater diversity within each modality.", "5.5 Effectiveness of MQG\n\nWe further investigate the effectiveness of MQG in Table 5. First, the MQG in HeteroRAG is replaced with a CLIP retrieval module. Specifically, for each medical visual question, the ModCLIPs are employed to retrieve documents through both image-to-text and text-to-text retrieval. The two retrieval results are combined using Reciprocal Rank Fusion. We also ablate the DPO and SFT training stages of the MQG. The number of retrieved documents remains consistent. Our experiments demonstrate that MQG retrieves more relevant documents compared to standard CLIP methods. This improvement can be attributed to better alignment of MQG and each corpus's characteristics. Furthermore, both the SFT and DPO training stages prove essential in developing MQG.", "5.6 Alignment Effectiveness of HKPT\n\nTo evaluate the alignment effectiveness of HKPT, we introduce three additional metrics besides answer accuracy: Modality Disalignment (MD), Knowledge Usage Disalignment (KUD), and Knowledge Interference Disalignment (KID). MD corresponds to CMA in Section 4.3, KUD corresponds to KU, and KID corresponds to KR. MD measures\n\n| | Models | OMVQA-Rad | OMVQA-Oph | Quilt-VQA |\n|---:|:---------------|------------:|------------:|------------:|\n| 0 | LLaVA-Med-7B | 53.67 | 56.83 | 66.18 |\n| 1 | + HeteroRAG | 60.17 | 71.42 | 69.39 |\n| 2 | HuatuoGPT-V-7B | 72.08 | 81.83 | 66.18 |\n| 3 | + HeteroRAG | 78.17 | 84.5 | 71.43 |\n| 4 | Lingshu-7B | 74.92 | 80.83 | 49.27 |\n| 5 | + HeteroRAG | 82.08 | 91.25 | 69.97 |\n\nTable 6: Model performance when the HeteroRAG framework is applied to different Med-LVLMs.", "the proportion that the Med-LVLM correctly answers with the irrelevant image among cases where it correctly answers with the original image. KUD measures the proportion that the Med-LVLM succeeds when any retrieval source (report, document, or report+document) is introduced among cases where it fails without retrieval. KID measures the proportion that the Med-LVLM fails when any retrieval source is introduced among cases where it succeeds without retrieval.", "Figure 4 shows the average metrics on OMVQA-Rad, OMVQA-Oph, and Quilt-VQA. The 'SFT' method refers to SFT using the training dataset with documents and reports added. The results demonstrate that HKPT improves overall accuracy compared to both the original and SFT models. Moreover, all three types of disalignment are significantly reduced by the HKPT method. We further conduct ablation studies on each type of preference pair in HKPT, including CMA, KU, and KR. The results confirm that each component effectively enhances the corresponding alignment capability as expected.", "5.7 Compatibility Analysis\n\nTo analyze the compatibility of the HeteroRAG framework with different Med-LVLMs, we apply it to LLaVA-Med7B and HuatuoGPT-V-7B besides Lingshu-7B. Specifically, the ModCLIPs and MQG in HRM are kept unchanged, as they are universal across different downstream readers. The HKPT process is performed separately for each MedLVLM. Results in Table 6 show that HeteroRAG brings consistent improvements over all Med-LVLMs. This indicates that HeteroRAG can be transferred to diverse Med-LVLMs.", "2 Preliminaries\n\nLet C = { \ud835\udc50 1 , \ud835\udc50 2 , . . . , \ud835\udc50 \ud835\udc36 } be an index of textual passages (i.e. we would refer to them as chunks interchangeably) and q be a natural language question that is provided as an input to our system. Retrieving items from C relevant to q can be achieved by using a base retrieval function \u210e \ud835\udc58 base ( q , C ) \u2286 C for returning a ranked list of items from C , in descending order according to a particular retrieval score.", "In the context of this challenge, given an input query q \u2032 , a dense retriever on top of C : \u210e \ud835\udc58 dense ( q \u2032 , C ) can be implemented using the provided Pinecone index. Similarly, a sparse retrieval step: \u210e \ud835\udc58 sparse ( q \u2032 , C ) can be achieved using the provided OpenSearch instance. A baseline retrieval step can be also implemented as a hybrid combination of passages coming from a dense and sparse retriever. Each hybrid retrieval search step returns the top\ud835\udc58 items from an index of interest by aggregating the results of dense and sparse retrieval using Reciprocal Rank Fusion (RRF) [1], as follows:\n\nwhere \u210e \ud835\udc58 dense , \u210e \ud835\udc58 sparse \u2286 C are functions returning sets of items from C , in descending order according to score dense and scoresparse respectively.", "6 Evaluation of RAG Systems\n\nThe evaluation of RAG systems is multifaceted, as performance depends not only on the generative model but also on the quality of the retrieval pipeline. A robust evaluation framework must assess retrieval accuracy, answer quality, factuality, latency, and scalability. This section provides a structured overview of RAG evaluation criteria, benchmarks, and the impact of architectural design choices. Summary of this section is in Table Table 1", "6.1 Evaluation Dimensions and Metrics\n\nRAG performance is commonly assessed along the following dimensions:\n\nRetrieval Accuracy. Key metrics include:\n\nRecall@ k - Measures the proportion of queries where a relevant document appears among the topk retrieved results.", "Mean Reciprocal Rank (MRR) - Captures the average inverse rank of the first relevant document, rewarding early placement.", "Mean Average Precision (MAP) -Evaluates the quality of ranked retrieval across relevant items.", "6.2 Benchmarks and Datasets\n\nSeveral benchmarks are widely used to evaluate RAG systems:\n\nNatural Questions (NQ) , TriviaQA , and WebQuestions for open-domain QA [52].", "FEVER and AveriTeC for fact-checking, emphasizing verifiability.\n\nKILT benchmark suite integrates QA, dialog, slot filling, and entity linking over Wikipedia.", "BEIR evaluates retrieval across 31 zero-shot tasks in domains like biomedicine and finance.\n\nMTRAG targets multi-turn conversations requiring sequential retrieval and reasoning.", "TREC RAG Track (2024-) defines unified evaluation of retrieval, generation, and support quality over MS MARCO with metrics like nugget recall and citation coverage [71].", "6.3 Retrieval-Augmented Generation Assessment System\n\nRAGAS (Retrieval-Augmented Generation Assessment System) is an evaluation framework specifically designed for assessing and improving the factuality and grounding of RAG systems. Unlike conventional metrics that measure superficial linguistic overlap, RAGAS emphasizes the alignment between generated content and retrieved documents, providing explicit signals regarding factual correctness and attribution quality. By systematically measuring how well the generated outputs are supported by the retrieved evidence, RAGAS helps identify and penalize hallucinations-instances where the model generates plausible but unsupported statements. Consequently, employing RAGAS during model training or iterative fine-tuning guides RAG systems toward producing outputs firmly grounded in verifiable sources, substantially improving factual accuracy and reducing the incidence of hallucinated information.", "| | Category | Metric/Tool | Description |\n|---:|:-------------------|:---------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 0 | Retrieval Accuracy | Recall@ k MRR MAP | Proportion of queries with a relevant doc in top- k results Average inverse rank of the first relevant document Mean precision across all relevant retrieved documents |\n| 1 | Generation Quality | Exact Match (EM), F1 BLEU, ROUGE Faithfulness Rate | Measures overlap with ground-truth answers N-gram based overlap metrics for generation Consistency with retrieved context (manual/automated) |\n| 2 | Efficiency | Latency (retrieval/generation) Memory/Compute Usage | Time required for each processing step Resource demands for model operation |\n| 3 | Scalability | Index Size vs. Accuracy Adaptability | Impact of increasing corpus size on performance Ability to incorporate new data without retraining |\n| 4 | Benchmarks | Natural Questions, TriviaQA FEVER, AveriTeC KILT BEIR MTRAG TREC RAG Track | Open-domain QA evaluation Fact-checking datasets Multi-task benchmark over Wikipedia 31-task zero-shot retrieval benchmark Multi-turn reasoning benchmark Unified eval. of retrieval, generation, citation |\n| 5 | Tooling | RAGAS | Evaluates factual consistency and grounding |", "Table 1: Evaluation Dimensions, Metrics, Benchmarks, and Tools for RAG Systems", "B. Results\n\nThe results of the experiments are summarized in Tables I and II, which presents the performance metrics of TechnicalEmbeddings compared to several baseline models. The allmpnet-base-v2 model [15], generates high-quality sentence embeddings through fine-tuning BER and testing various pooling strategies. All-MiniLM-L6-v2 [16] employs multihead self-attention relationship distillation to reduce parameters and improve efficiency. The BGE series [17], including bge-small-en and bge-base-en, effectively extracts and cleans semantically relevant text pairs from large Chinese web corpora, aiding weak supervision training. The results clearly illustrate the effectiveness of the TechnicalEmbeddings framework in improving technical question answering performance across two distinct datasets. In Table I, which evaluates the Rust-Docs-QA dataset, TechnicalEmbeddings achieves a Mean Average Precision (MAP) of 0.2238 and a Mean Reciprocal Rank (MRR) of 0.2249. These scores represent a notable enhancement over the baseline models, particularly the all-mpnet-base-v2, which records an MAP of 0.1734 and an MRR of 0.1817. The ability of Technical-Embeddings to achieve higher precision and recall scores -0.0785 and 0.3364, respectively -demonstrates its effectiveness in retrieving relevant documents, thereby addressing user needs more effectively.", "| | Model | MAP | MRR | Precision | Recall |\n|---:|:---------------------|-------:|-------:|------------:|---------:|\n| 0 | all-mpnet-base-v2 | 0.1734 | 0.1817 | 0.0745 | 0.2745 |\n| 1 | all-MiniLM-L6-v2 | 0.1892 | 0.1897 | 0.0765 | 0.3235 |\n| 2 | bge-small-en | 0.2096 | 0.2196 | 0.0784 | 0.3039 |\n| 3 | bge-base-en | 0.1942 | 0.2039 | 0.0773 | 0.3186 |\n| 4 | Technical-Embeddings | 0.2238 | 0.2249 | 0.0785 | 0.3364 |\n\nTABLE I PERFORMANCE OF DIFFERENT RETRIEVAL MODELS ON T H E RUSTDOCS-QA DATASET.", "Moving to Table II, which presents the performance metrics on the RAG-EDA dataset, we observe that TechnicalEmbeddings matches the best baseline model, bge-small-en, with an MAP and MRR score of 0.6926. This alignment suggests that our framework produces results on par with", "leading models in the field. Furthermore, TechnicalEmbeddings outperforms bge-small-en in terms of Recall, achieving a score of 0.8111 compared to bge-small-en's 0.8000. Such consistency across multiple performance metrics underscores the robustness and reliability of our model in retrieving pertinent technical content, which is essential for users navigating complex documentation.", "|---:|:---------------------|-------:|-------:|------------:|---------:|\n| 0 | all-mpnet-base-v2 | 0.6 | 0.6 | 0.623 | 0.6889 |\n| 1 | all-MiniLM-L6-v2 | 0.5648 | 0.5648 | 0.5824 | 0.6333 |\n| 2 | bge-small-en | 0.6926 | 0.6926 | 0.7201 | 0.8 |\n| 3 | bge-base-en | 0.6352 | 0.6352 | 0.6686 | 0.7667 |\n| 4 | Technical-Embeddings | 0.6926 | 0.6926 | 0.723 | 0.8111 |\n\nTABLE II PERFORMANCE OF DIFFERENT RETRIEVAL MODELS ON T H E RAGEDA DATASET", "The comparative analysis of these results highlights the advantages of incorporating synthetic query generation contextual summary, and prompt tuning into the TechnicalEmbeddings framework. The significant improvements in MAP, MRR, precision, and recall indicate that our approach effectively captures the nuances of technical language. By simulating real-world user interactions through synthetic queries, the model is better equipped to understand diverse query types and structures, which contributes to its superior performance in technical question answering."]}
{"type": "entry", "index": 6, "question": "What\u2019s the impact of chunk size and overlap on faithfulness for academic PDFs?", "answer": "From the available evidence, chunk size clearly affects faithfulness, while the impact of overlap is not directly measured.\n\nIn the K\u00b2RAG study, they evaluated different chunk sizes on a real-world financial report (Lyft 2021) and measured answer faithfulness and relevancy. As chunk size decreased from very large to moderately sized chunks, faithfulness improved sharply: 2048 tokens \u2192 80.37 faithfulness, 1024 \u2192 94.26, 512 \u2192 97.59, 256 \u2192 97.22, and then dropped slightly at 128 \u2192 95.74 [1]. Relevancy showed a similar pattern, peaking around 256\u2013512 tokens [1].\n\nThis suggests there is a \u201csweet spot\u201d around 256\u2013512 tokens per chunk for academic-style PDFs: \n- Too large chunks (e.g., 2048 tokens) mix many topics, so the model pulls in extra, loosely related content and becomes less faithful to the specific question. \n- Too small chunks (e.g., 128 tokens) may miss supporting context, so the model has to fill gaps, again hurting faithfulness. \n- Mid-sized chunks (256\u2013512) are large enough to contain a complete argument or section, but small enough to stay tightly focused, which helps faithful, on-point answers.\n\nOn overlap, K\u00b2RAG defines a general chunking formula with a configurable overlap O and reports using overlaps like 20 tokens for vector stores and 100 for the knowledge-graph index, but it does not provide an ablation or faithfulness scores for different overlap values [1]. So we cannot say, from these documents alone, how overlap quantitatively impacts faithfulness, only that some overlap is used to keep continuity across chunks.", "tags": ["benchmarks_eval", "retrieval_tuning"], "full_faithfulness": 1.0, "full_answer_relevancy": 0.0, "seed_faithfulness": 0.9615384615, "seed_answer_relevancy": 0.0, "context_relevance": 0.698562281953917, "context_relevance_error": null, "context_relevance_reason": "The context passages provide substantial information about the impact of chunk size and overlap on faithfulness, particularly in the section discussing the chunking strategy and its effects on retrieval accuracy and faithfulness metrics. Specific details, such as the configurations used for chunk sizes and overlaps, and their correlation with faithfulness and relevancy metrics, directly address the user's question. However, some passages are less relevant, which slightly detracts from the overall coherence of the response.", "structure_citation": 0.7948482283021601, "structure_error": null, "structure_reason": "The Actual Output effectively discusses the impact of chunk size on faithfulness, supported by specific data from the K\u00b2RAG study, which corresponds to the provided Context. It includes citations for the claims made, particularly regarding the faithfulness scores associated with different chunk sizes. However, while it mentions overlap, it does not provide specific faithfulness scores or detailed analysis for overlap, which slightly detracts from its completeness in addressing the question.", "hallucination_score": 0.8328297735363401, "hallucination_error": null, "hallucination_reason": "The response effectively addresses the impact of chunk size on faithfulness, providing specific data from the K\u00b2RAG study that supports the claim. It clearly outlines how different chunk sizes affect faithfulness and relevancy, aligning well with the input question. However, while it mentions that the impact of overlap is not directly measured, it could have provided more context or examples regarding the implications of overlap on faithfulness, which would strengthen the overall response.", "contexts_used": ["3 Methodology:\n\n3.1 K 2 RAG Framework\n\nTo meet the research goals, the K 2 RAG framework implements four techniques:\n\n1. Knowledge Graph: K 2 RAG employs a knowledge graph component to organize and interconnect topics within the corpus [Figure 8(a)]. Unlike traditional chunking approaches, which struggle to balance the trade-off", "between providing sufficient context and avoiding irrelevant information, the knowledge graph addresses this issue by structuring content into interconnected nodes which enables knowledge-rich and more context-aware retrieval capable of handling complex or vague queries that vector-based search methods often struggle with.", "2. Hybrid Search: To minimize false positive embeddings-where semantically similar but irrelevant chunks are retrievedK 2 RAG integrates a hybrid retriever [Figure 8(b)]. Inspired by Mandikal et al.'s paper [18], we weight dense vector and sparse vector retrieval at an optimized 80%/20% ratio. This hybrid method significantly improves retrieval accuracy, helping reduce noise by ensuring the most relevant content is retrieved.", "3. Summarization: To combat the \"Needle-in-Haystack\" issue, K 2 RAG incorporates summarization at multiple stages in the pipeline [Figure 8 (B) and (E)]. To speed up indexing, documents are summarized before being indexed into vector and knowledge graph stores. Additionally retrieved content is summarized at query time to further refine the context provided to the LLM. By summarizing at both indexing and retrieval, we ensure concise, high-quality input for generation.", "4. Lightweight Models: For VRAM resources efficiency, K 2 RAG leverages a lightweight Longformer-based text summarizer [29], along with quantized LLMs to produce a low VRAM and resource-efficient pipeline without compromising output quality.", "3.1.1 Corpus Summarization Process\n\nTo address research goal 1, we decrease training times of the K 2 RAG framework compared to naive RAG frameworks by summarizing the training corpus to reduce its size while preserving as much information as possible. Hence a Longformer based model [29] fine tuned for long text summarization is used as it's self-attention mechanism is highly optimized for capturing and processing large chunks of texts compared to those seen in traditional transformer models. This helps speed up training times of information store components used in K 2 RAG . To create our summarized corpus we iterate over each article in the corpus and then summarize it using the loaded in Longformer summarizer model. Then these results are stored into a CSV format from which then they can be loaded in for training the RAG pipeline [Algorithm 1] [Figure 6].", "3.1.2 Indexing\n\nThe chunking strategy to generate a set of chunks C d from a single document or text d is defined as:\n\nFigure 6: How to generate summaries of corpus.\n\nSummarise Document Load corpus For each Longformer Encoder Decoder Model Store into list Repeat Export to CSV", "where S is the desired chunk size, O is the desired chunk overlap, Nt d is the total number of tokens in the document d . The inner set denotes a chunk constructed from tokens x (( i -1) \u2217 ( S -O ))+1 to x (( i -1) \u2217 ( S -O ))+ S and L t is the limit of tokens to process to ensure the number of chunks of size S and overlap O are maximized.", "We then apply the chunking strategy defined in (1) across all documents in the summarized corpus cor sum and take the union of chunks to give us the final set of chunks C to pass to the knowledge graph and vector stores for indexing:\n\nThe configuration used to train all information retrievers is S = 256 , O = 20 when chunking for the dense and sparse vector stores indexing, and S = 300 , O = 100 when chunking for knowledge graph indexing.", "This was decided in accordance with Xiaohua Wang et al.'s paper for vector stores indexing which showed 512 and 256 token chunks achieved the best results in Faithfulness and Relevancy metrics [9] [Figure 7]. Using the chunk size less than 512 is used to help promote in smaller contexts generated to pass to the generator LLM to address the \"Needle-in-Haystack\" challenge [8] by ensuring enough information is contained within the chunks retrieved and minimising noisy information.", "| | Chunk Size | lyft_2021.Average Faithfulness | lyft_2021.Average Relevancy |\n|---:|-------------:|---------------------------------:|------------------------------:|\n| 0 | 2048 | 80.37 | 91.11 |\n| 1 | 1024 | 94.26 | 95.56 |\n| 2 | 512 | 97.59 | 97.41 |\n| 3 | 256 | 97.22 | 97.78 |\n| 4 | 128 | 95.74 | 97.22 |\n\nFigure 7: Chunk sizes and their effects on Answer Faithfulness and Answer Relevancy.", "3.1.3 Retrieval and Generation\n\nFor all pipelines the following system prompt was used for generation of the final answer:", "| | 0 |\n|---:|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 0 | Answer Generation System Prompt |\n| 1 | Additional Information: context from retrieval Instruction: You are a smart LLM who gives an answer to the question in as little words as possible using the additional information provided above. Question: question Short Answer: |\n\nWe establish the top k chunks C q retrieved from the dense and sparse vector databases for a question q is a hybrid retriever [Figure 8 (b)] defined as:\n\nwhere S is the set of scores computed for each chunk in C (2) using a scoring function scr of the question q , chunk c and a weighting parameter \u03bb :", "Where e represents the dense embeddings model as a function and z represents the sparse embedding function.", "The first step in the K 2 RAG pipeline [Figure 8] is to use a knowledge graph trained on summarized data to generate an answer containing the identified topics relevant to the query [Figure 8(A)]. Then a Longformer model is used to summarize the results obtained from the knowledge graph [Figure 8(B)]. Over the knowledge graph results summary, we split it into chunks of S = 128 and O = 10 following our text chunking strategy (1) . This is so that each chunk contains information about a clear topic needed to help answer the question from which for each chunk we use a quantized LLM to create a sub-question out of it [Figure 8(C)] with the following specialized prompt:", "3.2 Implemented Naive RAG Pipelines\n\nWe implemented 4 common naive RAG pipelines;\n\nSemantic - Retrieving only from a Dense Vector Database.\n\nKeyword - Retrieving only from a Sparse Vector Database.", "Hybrid - Retrieving from both Dense and Sparse Vector Database.\n\nKnowledge Graph (KG) - Retrieving only from a Knowledge Graph.", "3.2.1 Indexing\n\nThe indexing process for each information store is the same as in K 2 RAG but with chunks C created with the unsummarized corpus cor unsum instead:", "3.2.2 Retrieval and Generation\n\nAll na\u00efve retrieval-augmented pipelines in our framework follow a two-step process: (1) retrieval of information relevant to the input question, and (2) passage of the retrieved information to an unquantized large language model (LLM) for answer generation. In the case of our Semantic, Keyword, and Hybrid pipelines, we use a hybrid retrieval mechanism that combines semantic similarity and keyword matching. The weighting parameter \u03bb controls the balance between the two retrieval strategies: a value of \u03bb = 1 corresponds to purely semantic retrieval [Figure 9(S)], \u03bb = 0 corresponds to purely keyword-based retrieval [Figure 9(K)], and \u03bb = 0 . 8 gives higher preference to semantic retrieval while still incorporating keyword-based signals [Figure 9(H)].", "For the naive Knowledge Graph (KG) retrieval pipeline, the retrieval process differs significantly. Instead of directly retrieving passages from the unsummarized corpus through vector search, we retrieve structured information from a", "Figure 8: K 2 RAG framework overview. Component (a): Knowledge Graph. Component (b): Hybrid Retriever containing Dense and Sparse retrievers. Component (c): Embeddings Model. Step (A): Knowledge Graph results. Step (B): Summarise Knowledge Graph results. Step (C): Generating sub-questions from chunks. Step (D): Generating sub-answers for sub-questions. Step (E): Generate the final answer from sub-answers and Knowledge Graph Results Summary.\n\nA Retrieve from Knowledge Graph B Results C Generate sub-questions from Results Summary D Answer Each Sub-question and Summarise Answer Embeddings Quantised LLM Hybrid Vector Retriever E\n\nGive final answer with\n\nsub-answers and Knowledge Graph User Question Final Answer SUMMARY\n\nknowledge graph that has been constructed using the unsummarized corpus and a quantized LLM. In this setup, the same quantized LLM is employed both for generating the knowledge graph and for querying it at retrieval time.", "Model Size: Large vs. Mid-size\n\nTo study the role of scale, we compare: (1) Qwen3-235B-A22B (Yang et al. 2025), the flagship model with 235 billion total parameters and 22 billion active, optimized for long-context reasoning, and (2) Qwen3-30B-A3B (Yang et al. 2025), a 30B-parameter variant with 3B active, designed for faster inference and reduced hardware costs.", "Figure 4: Overall performance of different systems across regions. (Subject to aesthetic changes.)\n\nAnswer Quality Faithfulness Recall@30 0.0 0.2 0.4 0.6 0.8 1.0 1.2 Score 69.6% N/A 68.2% 97.0% 9.5% 88.0% 96.9% 93.2% Overall Performance General LLMs Vanilla RAG GridCodex\n\n| | Model | Answer Quality | Faithfulness |\n|---:|:----------|-----------------:|---------------:|\n| 0 | 235B-A22B | 0.852 | 0.968 |\n| 1 | 30B-A3B | 0.74 | 0.995 |\n\nTable 3: Model size impact (Netherlands dataset).", "As shown in Table 3, scaling from 30B to 235B parameters improves answer quality by 11.2%. The larger model demonstrates stronger multi-hop reasoning, cross-clause referencing, and ambiguity resolution due to greater representational capacity and expert routing. However, this gain comes at a cost: faithfulness drops slightly (0.968 vs. 0.995), as the 235B model integrates retrieved context more aggressively, occasionally introducing inferred connections not explicitly present in the documents. This highlights a key trade-off-larger models enable richer reasoning but may lean toward speculative integration, whereas smaller models remain more conservative and strictly grounded in retrieved evidence.", "3 Experiments\n3.1 Experimental Setup\n\nWe split DentalQA into training and test sets in a 4:1 ratio while preserving each subfield's proportions, and report all results on the held-out test set. MCQperformance is measured by Accuracy; MAQ by Accuracy, Precision, Recall and F1; and OEQ and DEF by BERTScore F1 (Zhang et al., 2019). We conduct our experiments on multiple popular LLMs. For general LLMs, we select DeepSeek-V3, DeepSeek-R1, GPT-4o, GPT-4o-mini, LLaMA-3.23B-Instruct, LLaMA-3.1-8B-Instruct (Grattafiori et al., 2024) and Qwen-2.5-1.5/3B/7B/14B/32BInstruct (Qwen et al., 2025). For medical LLMs, we select BioMistral-7B (Labrak et al., 2024), HuatuoGPT2-7B (Chen et al., 2024) and LLaMA3-8B-UltraMedical (Zhang et al., 2024). We evaluate in a zero-shot setting using task-specific prompt templates (Appendix A.3). Experiments are conducted on eight NVIDIA RTX 3090 GPUs.", "3.2 Domain Adaptation on Qwen2.5-3B\n\nTo enhance dentistry-specific knowledge and capabilities, we adopt three adaptation strategies based on Qwen-2.5-3B-Instruct. (a) Supervised FineTuning (SFT): Full-model fine-tuning on the DentalQA training split for four epochs with a learning rate of 1e-6 and batch size 16 using bfloat16 precision. (b) Retrieval-Augmented Generation (RAG): At inference, retrieve the top-5 most relevant passages from DentalCorpus via FAISS with bge-m3 embeddings and prepend them to the prompt (Appendix A.3). (c) SFT + RAG: Combine the above supervised fine-tuning with retrieval augmentation during inference.", "3.3 Results\n\nThe main results are presented in Table 1, where we report the performance of 14 LLMs and our domain adaptation results.", "Overall Trends. Performance varies markedly by language. On DentalBench-ZH, DeepSeek-R1 achieves state-of-the-art accuracy on both MCQ", "| | Model | DentalQA-ZH.MCQ.ACC | DentalQA-ZH..ACC | DentalQA-ZH.MAQ.P | DentalQA-ZH.MAQ.R | DentalQA-ZH..F1 | DentalQA-ZH.OEQ.BERTScore | DentalQA-ZH.DEF.BERTScore | DentalQA-EN.MCQ.ACC | DentalQA-EN.OEQ.BERTScore | DentalQA-EN.DEF.BERTScore |\n|---:|:--------------------------------|:--------------------------------|:--------------------------------|:--------------------------------|:--------------------------------|:--------------------------------|:--------------------------------|:--------------------------------|:--------------------------------|:--------------------------------|:--------------------------------|\n| 0 | General LLMs | | | | | | | | | | |\n| 1 | GPT-4o | 64.86 | 37.30 | 87.75 | 81.74 | 84.63 | 27.23 | 21.60 | 73.98 | 31.28 | 29.21 |\n| 2 | GPT-4o-mini | 51.65 | 29.73 | 81.36 | 87.37 | 84.26 | 26.48 | 19.50 | 60.59 | 34.55 | 29.42 |\n| 3 | Deepseek-V3 | 69.28 | 41.35 | 87.22 | 86.23 | 86.72 | 27.79 | 17.78 | 68.28 | 27.65 | 25.73 |\n| 4 | Deepseek-R1 | 76.06 | 43.51 | 88.64 | 86.68 | 87.65 | 26.77 | 15.81 | 60.04 | 20.91 | 18.58 |\n| 5 | Llama-3.2-3B | 38.22 | 7.30 | 72.24 | 65.01 | 68.44 | 19.88 | 15.13 | 48.96 | 28.13 | 26.13 |\n| 6 | Llama-3.1-8B | 40.80 | 10.27 | 77.45 | 67.49 | 72.13 | 16.69 | 4.96 | 55.60 | 25.31 | 21.75 |\n| 7 | Qwen2.5-1.5B | 45.58 | 13.24 | 76.67 | 77.55 | 77.11 | 21.74 | 8.57 | 38.09 | 26.10 | 21.59 |\n| 8 | Qwen2.5-3B | 48.63 | 19.19 | 77.70 | 80.37 | 79.01 | 20.89 | 11.16 | 41.77 | 34.48 | 29.62 |\n| 9 | Qwen2.5-7B | 60.29 | 26.22 | 83.08 | 79.22 | 81.11 | 26.37 | 11.59 | 49.23 | 26.28 | 22.12 |\n| 10 | Qwen2.5-14B | 66.48 | 33.51 | 84.05 | 85.01 | 84.53 | 25.47 | 12.69 | 50.49 | 26.68 | 21.93 |\n| 11 | Qwen2.5-32B | 70.86 | 39.46 | 85.50 | 86.15 | 85.82 | 26.02 | 11.65 | 58.34 | 26.59 | 22.78 |\n| 12 | Medical LLMs | | | | | | | | | | |\n| 13 | BioMistral-7B | 25.44 | 5.68 | 76.33 | 47.43 | 58.51 | 14.48 | 14.06 | 34.96 | 34.50 | 29.55 |\n| 14 | HuatuoGPT2-7B | 22.51 | 6.22 | 74.50 | 67.54 | 70.85 | 25.38 | 21.04 | 25.47 | 15.50 | 16.55 |\n| 15 | Llama-3-8B-UltraMedical | 30.32 | 11.08 | 72.86 | 81.52 | 76.95 | 18.74 | 9.18 | 46.10 | 26.76 | 24.80 |\n| 16 | Domain Adaptation on Qwen2.5-3B | Domain Adaptation on Qwen2.5-3B | Domain Adaptation on Qwen2.5-3B | Domain Adaptation on Qwen2.5-3B | Domain Adaptation on Qwen2.5-3B | Domain Adaptation on Qwen2.5-3B | Domain Adaptation on Qwen2.5-3B | Domain Adaptation on Qwen2.5-3B | Domain Adaptation on Qwen2.5-3B | Domain Adaptation on Qwen2.5-3B | Domain Adaptation on Qwen2.5-3B |\n| 17 | Qwen2.5-3B | 48.63 | 19.19 | 77.70 | 80.37 | 79.01 | 20.89 | 11.16 | 41.77 | 34.48 | 29.62 |\n| 18 | w . SFT | 54.58 | 25.60 | 75.57 | 93.24 | 83.48 | 22.42 | 15.29 | 47.90 | 37.74 | 30.79 |\n| 19 | w . RAG | 54.45 | 21.35 | 74.88 | 91.17 | 82.22 | 30.18 | 22.13 | 48.74 | 36.47 | 30.04 |\n| 20 | w . SFT+RAG | 60.06 | 29.07 | 77.30 | 93.46 | 84.62 | 30.06 | 20.85 | 52.15 | 37.68 | 29.65 |", "Table 1: Overall Performance on DentalQA. We use Accuracy (ACC), Precistion (P), Recall (R), F1, and BERTScore F1 (BERTScore) as our metrics. Bold indicates the best result, and underline indicates the second best.", "and MAQ, with DeepSeek-V3 and Qwen2.5-32B close behind. Conversely, on DentalBench-EN, GPT-4o leads across these tasks. In both languages, however, open-ended tasks (OEQ and DEF) trail far behind MCQ and MAQ, underscoring enduring challenges in domain-specific generative reasoning and terminology.", "General Models vs. Medical Models. Although medical LLMs perform relatively well on OEQ and DEF, they fall markedly short of general-purpose models on MCQ and MAQ. For example, Llama3.1-8B consistently outperforms its medical counterpart across all multiple-choice tasks, suggesting that medical tuning may insufficiently capture dentistry-specific factual knowledge.", "Impact of Model Scale. In the Qwen-2.5 family, scaling improves MCQ and MAQ notably but yields limited gains on OEQ and DEF, suggesting factual recall benefits more from model size than generative reasoning does.", "Domain Adaptation. Both SFT and RAG improve MCQ and MAQ, but RAG shows a larger impact on open-ended tasks (e.g., OEQ-ZH BERTScore: +9.29 vs. +1.53). Combining both yields additive gains-especially on MCQ and MAQ (+11.43 and\n\n+9.88). For OEQ/DEF, SFT+RAG offers clear benefit over SFT alone in Chinese, while in English the effect is less consistent, indicating language sensitivity in retrieval effectiveness.", "6. Analysis\n\n6.1. Case Study: Bridging Parametric and Non-Parametric Methods\n\nMemory Decoder fundamentally learns to compress the knowledge stored in large non-parametric datastores into a compact parametric model, combining the memorization capabilities of retrieval methods with the efficiency and generalization of parametric approaches. To validate this hypothesis, we conducted case studies on WikiText-103 examining how different methods assign probabilities to specific tokens.", "As shown in Table 5, Memory Decoder exhibits two crucial capabilities:\n\nLong-tail Knowledge: For factual information like \"Jacobi\" and \"1906\", Memory Decoder assigns dramatically higher probabilities than the base model (68.94% vs. 0.12% and 98.65% vs. 1.57%), successfully capturing the memorization benefits of non-parametric methods.", "Semantic Coherence: For function words and logical continuations like \"on\" and \"C\", Memory Decoder maintains probabilities closer to the base model rather than following kNN-LM's lower probabilities, demonstrating its ability to preserve coherent language modeling capabilities that pure retrieval methods sacrifice.", "| | Long-tail Knowledge Learning.Context (target token underlined) | Long-tail Knowledge Learning.MemDec | Long-tail Knowledge Learning.kNN | Long-tail Knowledge Learning.Base LM |\n|---:|:---------------------------------------------------------------------------------------------------------------|:--------------------------------------|:-----------------------------------|:---------------------------------------|\n| 0 | he starred alongside actors Mark Strong and Derek Jacobi | 68.94% | 9.39% | 0.12% |\n| 1 | The launch of HMS Dreadnought in 1906 by the Royal Navy raised the stakes | 98.65% | 40.62% | 1.57% |\n| 2 | Semantic Coherence and Reasoning | Semantic Coherence and Reasoning | Semantic Coherence and Reasoning | Semantic Coherence and Reasoning |\n| 3 | Context (target token underlined) | MemDec | kNN | Base LM |\n| 4 | In 2000 Boulter had a guest-starring role on the television series The Bill | 40.11% | 8.07% | 45.51% |\n| 5 | ...three tank squadrons for special overseas operations, known as 'A', 'B' and ' C ' Special Service Squadrons | 50.10% | 10.76% | 63.04% |", "Table 5 | Probability assignments for specific tokens by different methods. Orange section : Memory Decoder excels at capturing long-tail factual knowledge, assigning dramatically higher probabilities than the base model. Cyan section : For semantic coherence, Memory Decoder intelligently balances between kNN-LM and base model probabilities, preserving linguistic fluency.", "| | | GPT2-small | GPT2-medium | GPT2-large | GPT2-xl | Avg |\n|---:|:-----------------------|-------------:|--------------:|-------------:|----------:|------:|\n| 0 | Base | 24.89 | 18.29 | 15.8 | 14.39 | 18.34 |\n| 1 | DAPT | 14.76 | 12.78 | 11.1 | 10.16 | 12.2 |\n| 2 | + MemDec-small (124M) | 13.36 | 12.25 | 11.53 | 10.93 | 12.01 |\n| 3 | + MemDec-medium (345M) | 12.08 | 11.59 | 10.92 | 10.43 | 11.26 |\n| 4 | + MemDec-large (774M) | 11.67 | 11.23 | 10.83 | 10.28 | 11 |\n\nTable 6 | Performance comparison with different Memory Decoder sizes. Even small Memory Decoders achieve competitive performance with full-parameter DAPT while maintaining plug-and-play capability.", "These observations confirm that Memory Decoder occupies a unique position: it enhances memorization of domain-specific and long-tail knowledge like non-parametric methods, while maintaining the generalization and reasoning capabilities inherent to parametric models.", "6.2. Impact of Memory Decoder Size\n\nTable 6 examines how Memory Decoder size affects performance across the GPT2 family. As Memory Decoder size increases, performance consistently improves across all base models, with the large variant achieving the best average perplexity. These results validate that Memory Decoder provides an efficient alternative to full model fine-tuning: practitioners can choose the decoder size based on their computational constraints while maintaining the crucial advantage of preserving the original model's capabilities.", "6.3. Ablation on the pre-training objective\n\nWe compare Memory Decoder against logit interpolation with a DAPT model. Table 7 shows Memory Decoder consistently outperforms DAPT interpolation across all GPT2 scales, with an average gain of", "| | Base Model | Baseline PPL | + DAPT-small | + MemDec-small |\n|---:|:-------------|---------------:|---------------:|:-----------------|\n| 0 | GPT2-small | 24.89 | 15.95 | 13.36 (-2.59) |\n| 1 | GPT2-medium | 18.29 | 14.26 | 12.25 (-2.01) |\n| 2 | GPT2-large | 15.8 | 13.13 | 11.53 (-1.60) |\n| 3 | GPT2-xl | 14.39 | 12.3 | 10.93 (-1.37) |\n| 4 | Average | 18.34 | 13.91 | 12.01 (-1.90) |\n\nTable 7 | Memory Decoder vs. DAPT model interpolation on WikiText-103. Both use 124M parameter models with different training objectives.", "1.90 perplexity points. Notably, the gains persist even at larger scales, confirming that our hybrid pre-training objective provides complementary value beyond what standard language modeling objectives can achieve, regardless of model size."]}
{"type": "entry", "index": 7, "question": "How do retrieval-augmented summarizers handle conflicting evidence across papers?", "answer": "Retrieval-augmented summarizers face two main kinds of conflict: disagreements among retrieved papers, and disagreements between those papers and the model\u2019s own internal knowledge.\n\nOne strategy is to add a dedicated summarization or aggregation step that explicitly filters and reconciles evidence. In the multi-agent counterspeech framework, retrieved static (curated knowledge base) and dynamic (web) evidence are first combined, then passed to a Summarization Agent whose job is to \u201cfilter and distill the most relevant details,\u201d remove redundancy, and \u201cresolve conflicting information,\u201d so that only the \u201cmost reliable and pertinent content is retained\u201d before the final response is generated [1]. In practice, this means the summarizer is prompted to compare studies, downplay outliers, and produce a single, coherent summary rather than echoing all sides.\n\nAnother strategy is to model which context to trust. Conflict-Aware Retrieval-Augmented Generation (CARE) builds a \u201ccontext assessor\u201d that encodes external documents into special memory-like embeddings so the model can distinguish more reliable from less reliable context when it conflicts with what it already \u201cknows\u201d [2]. This improves accuracy on QA and fact-checking tasks, suggesting the summarizer can selectively weight or discount conflicting papers instead of blindly averaging them [2].\n\nFinally, dedicated benchmarks such as MAGIC, ECON, and WikiContradict are used to test whether models can even detect and locate conflicts between pieces of retrieved evidence [3]. Results show that current LLMs still struggle, especially with complex, multi-hop conflicts [3], which means that, in practice, retrieval-augmented summarizers may miss or mishandle subtle disagreements unless they use explicit conflict-detection and resolution mechanisms like the ones above.", "tags": ["summarization_conflict"], "full_faithfulness": 0.7142857143, "full_answer_relevancy": 0.9246572655, "seed_faithfulness": 0.619047619, "seed_answer_relevancy": 0.9244687016, "context_relevance": 0.845464467646735, "context_relevance_error": null, "context_relevance_reason": "The context passages collectively provide substantial information on how retrieval-augmented summarizers, particularly the CARE framework, handle conflicting evidence. They detail the mechanisms of evidence retrieval, summarization, and the evaluation of conflicting information, directly addressing the user's question about managing conflicting evidence across papers. However, while the passages are relevant, they could be more explicitly connected to the specific processes of summarization in the context of conflicting evidence.", "structure_citation": 0.8033955696342335, "structure_error": null, "structure_reason": "The Actual Output effectively discusses strategies for handling conflicting evidence in retrieval-augmented summarizers, referencing specific methods and frameworks. It includes citations [1], [2], and [3], which correspond to relevant snippets in the Context. However, while the claims are generally supported by the cited snippets, there is a slight lack of clarity in how some claims directly relate to the citations, which prevents a perfect score.", "hallucination_score": 0.8531209373373756, "hallucination_error": null, "hallucination_reason": "The response effectively addresses the input question by outlining how retrieval-augmented summarizers manage conflicting evidence through various strategies, such as dedicated summarization steps and context assessment. It aligns well with the context provided, particularly in discussing the Summarization Agent's role in filtering and reconciling evidence. However, while it covers the main strategies, it could have included more specific examples or results from the benchmarks mentioned to further strengthen its claims.", "contexts_used": ["3 Multi-Agent Retrieval-Augmented Framework\n\n3.1 Generation\n\nThe counterspeech generation process involves leveraging LLMs to create effective, informative, and polite responses that counter health misinformation. The primary objective is to generate responses that not only provide factual corrections but also address misinformation in a respectful and engaging manner. Our proposed multi-agent system uses specialized agents that work in sequence to retrieve, filter, and process evidence, thereby optimizing the RAG pipeline (Figure 1).", "Information Retrieval To incorporate static and dynamic evidence, two separate agents are employed to retrieve evidence: one is a Static Retrieval Agent and another one is a Dynamic Retrieval Agent . These Agents collect information based on the health misinformation posts. The Static Retrieval Agent conducts an inquiry within a structured local knowledge base by employing a dual approach that integrates vector-based semantic search with keywordbased exact matching (Yuan et al., 2024; Sawarkar et al., 2024), ensuring that only valid medical knowledge is included in this process. In parallel, the Dynamic Retrieval Agent performs real-time web searches using the DuckDuckGo API, introducing the latest health misinformation corrections from authoritative sources. The two sources of knowledge are combined to produce the final evidence.", "Evidence Summarization To prevent information overload from combined evidence, we introduce a Summarization Agent to filter and distill the most relevant details combined. This step is crucial for eliminating redundancy and resolving conflicting information, ensuring that only the most reliable and pertinent content is retained for counterspeech generation. Specifically, we prompt the Summarization Agent to condense evidence into a clear and refined summary.", "Counterspeech Generation We integrate distilled evidence from the Summarized Agent into generation by prompting Counterspeech Generation Agent to produce evidence-based counterspeech.", "Counterspeech Refinement To further enhance accessibility and clarity, we employ the Refinement Agent to reframe and polish counterspeech, ensuring that the generated counterspeech is not only well-supported by evidence but also contextually relevant and trustworthy.", "3.2 Evaluation\n\nWe employ a comprehensive evaluation framework based on four key criteria: politeness, relevance, informativeness, and factual accuracy. These criteria ensure that the responses are engaging, respectful, factually sound, and effective in addressing health misinformation.", "(a) Politeness: Politeness in responses is essential in communication, which helps to foster user engagement and build a supportive health community (Shan et al., 2022). A polite counterspeech helps avoid potential backlash and is more likely to be accepted by users (Perez et al., 2025), as it encourages a respectful tone (Song et al., 2025a; Yue et al., 2024; He et al., 2023). To evaluate politeness, we utilize the Multilingual Politeness Classification Model (Srinivasan & Choi, 2022) to assess whether a response adheres to polite discourse norms.", "(b) Relevance: Relevant responses ensure the response directly addresses the specific health misinformation claim (Yue et al., 2024; He et al., 2023). We assess relevance using the BM25 relevance scoring algorithm (Robertson et al., 2009), which measures how well the generated response aligns with the original health misinformation content.", "(c) Informativeness: Informativeness reflects the amount of useful, relevant, and evidencebased content shared in the response. A highly informative counterspeech response not only corrects health misinformation but also provides actionable insights and explanations that improve user understanding and support informed decision-making (Chung et al., 2024). To measure informativeness, we employ LLM as an evaluator (Li et al., 2024b) with detailed instructions, assessing if the response provides valuable information (Hong et al., 2025).", "(d) Factual Accuracy: Factual accuracy demonstrates the reliability and trustworthiness of the generated response by ensuring that the information is correct and supported by evidence (Zhou et al., 2024). In counterspeech generation, presenting scientifically accurate facts can effectively correct misinformation and maintain user trust (Yue et al., 2024). We employ LLM evaluation (Li et al., 2024b) to check how accurate the generated answers are by comparing them to the gathered evidence. The model rates how accurate a response is by checking if it correctly presents scientific facts and does not include any false information.", "This paper presents ConflictAware Retrieval-Augmented Generation (CARE), a novel approach that enhances large language models (LLMs) by addressing context-memory conflict\u2014issues arising when retrieved context contradicts the model's internal knowledge. CARE includes a context assessor that encodes external contexts into memory embeddings that help the model discern reliable from unreliable information. The framework improves the LLM's accuracy on question-answering and fact-checking tasks, achieving an average performance gain of 5.0% over existing methods. The study categorizes existing solutions into adaptive retrieval, decoding strategies, and robust training, highlighting how CARE addresses knowledge conflicts more effectively while preserving the general capabilities of LLMs.", "4 Experimental Setups\n\nWith the MAGIC benchmark as a foundation, we conduct experiments to examine how LLMs handle inter-context knowledge conflicts. We evaluate various open-source and proprietary LLMs without task-specific training, instead prompting them to identify potential contradictions. In the following, we outline the LLMs, datasets, prompting strategies, and metrics used in our experiments.", "LLMs We use 5 LLMs: Llama 3.1 70B Instruct (Dubey et al., 2024), o1 (OpenAI, 2024b), Mixtral8x7B Instruct (MistralAI, 2023), Claude 3.5 Haiku (Anthropic, 2024), GPT-4o-mini (OpenAI, 2024a).", "Datasets Alongside MAGIC, we employ two existing benchmarks to highlight its strengths.\n\nECON (Jiayang et al., 2024): A dataset created by introducing evidence conflicts through two methods-answer conflicts and factoid conflicts-highlighting contradictions in supporting evidence. It contains 168 data instances.", "WikiContradict (Hou et al., 2024): A humanannotated QA benchmark utilizing Wikipedia's contradiction tags to capture real-world knowledge conflicts. It categorizes contradictions into explicit and implicit types. After deduplication, it comprises 103 data samples.", "MAGIC : It is constructed atop KGs, where conflicts are systematically induced from the un-", "| | Models / Datasets | ECON | WikiContradict | MAGIC |\n|---:|:--------------------|-------:|-----------------:|--------:|\n| 0 | Mixtral 8x7B | 46.43 | 52.43 | 37.92 |\n| 1 | Llama 3.1 70B | 81.41 | 78.79 | 72.86 |\n| 2 | Claude 3.5 Haiku | 83.33 | 61.17 | 60.28 |\n| 3 | GPT-4o-mini | 88.1 | 82.52 | 83.61 |\n| 4 | o1 | 74.4 | 74.76 | 68.06 |\n| 5 | Average | 74.73 | 69.93 | 64.54 |\n\nTable 2: ID scores (%) on three KC detection datasets. Lower scores indicate higher task complexity. The lowest value in each row is shown in bold .", "derlying relational structure. It encompasses both single-hop and multi-hop contradictions, with the number of conflicts dynamically varying across context pairs. In total, MAGIC consists of 1,080 carefully curated examples, with comprehensive statistics presented in Table 1. Remarkably, the scale of MAGIC surpasses that of existing benchmarks by a significant margin, offering a richer and more challenging resource for evaluating inter-context conflict detection.", "Prompting strategy Prior work (Jiayang et al., 2024; Hou et al., 2024) typically frames the task as a binary classification problem, relying on minimal prompts (see Appendix B). In contrast, we introduce a stepwise prompting strategy (refer to Figure 17 for details) designed to more fully probe the capabilities of LLMs for inter-context conflict detection.", "Metrics To account for the stochasticity of LLMs, all models perform three independent inference runs. We use two metrics for fine-grained evaluation, with scores averaged across all instances in each dataset. These metrics are manually computed by participating researchers, as automatic methods-such as LLM-as-a-judge-remain insufficiently reliable for this task. 13\n\nIdentification ( ID ) score: If a model fails to detect a conflict in any of the three attempts, it receives a score of 0; otherwise, it receives 1.", "Localization ( LOC ) score: We further evaluate LLMs' performance on conflict localization. A full score (1) is awarded only if all conflicting locations are correctly identified; otherwise, the score is 0. Note that this fine-grained evaluation has not been considered in previous work.", "13 If they were, further investigation into knowledge conflict detection would be unnecessary. See Appendix H for details.", "|---:|:--------------------|-------:|-----------------:|--------:|\n| 0 | Mixtral 8x7B | 35.71 | 40.78 | 17.4 |\n| 1 | Llama 3.1 70B | 54.49 | 51.52 | 37.92 |\n| 2 | Claude 3.5 Haiku | 66.07 | 52.43 | 42.5 |\n| 3 | GPT-4o-mini | 63.69 | 68.93 | 55 |\n| 4 | o1 | 64.88 | 65.48 | 49.72 |\n| 5 | Average | 57.09 | 55.74 | 40.51 |\n\nTable 3: LOC scores (%) on three KC detection datasets. Lower scores indicate higher complexity. We observe that models struggle more with pinpointing the exact location of a conflict than with detecting its presence. The lowest value in each row is shown in bold .", "7 Conclusion\n\nWe propose a KG-based benchmark, MAGIC, for inter-context knowledge conflict detection with greater diversity and complexity. Experimental results reveal the strengths and limitations of LLMs in handling knowledge conflicts. Despite recent progress, LLMs continue to struggle with conflict detection in complex cases, e.g., those requiring multi-hop reasoning. As a future direction, we aim to develop an optimized method to help models overcome these limitations.", "Ethics Statement\n\nWe release MAGIC, a benchmark dataset for intercontext knowledge conflict detection. The dataset is constructed from Wikidata5M, a publicly available resource under permissive licenses, and contains no personally identifiable information. While we carefully filtered and validated the generated contexts, we cannot fully guarantee the absence of biased or potentially offensive content. MAGIC is released solely for scientific research, with the aim of fostering the development of systems that can reliably handle conflicting information and support safe deployment in real-world applications."]}
{"type": "entry", "index": 8, "question": "When should we prefer sparse BM25 over dense embeddings for acronym-heavy queries?", "answer": "Prefer sparse BM25 over dense embeddings when the meaning of the query hinges on the exact character sequence\u2014like acronyms, codes, or very specific names\u2014rather than on broader semantics.\n\nSparse BM25 works by matching exact terms and weighting them by how rare and informative they are in the corpus [2][3]. This makes it stronger when:\n\n1. The key signal is a short token or acronym\n   - For example, music questions where performance depends on correctly matching entity names like specific artists or albums see a clear factual advantage from BM25 over dense embeddings [5]. Acronyms behave similarly: they are short, often opaque strings that dense semantic models may not interpret well, but BM25 will still match exactly.\n\n2. You need high precision on rare, domain\u2011specific strings\n   - In harm\u2011reduction RAG, sparse BM25 is explicitly used to accurately retrieve specific substance names and dosage information that dense retrieval may miss [4]. Acronym\u2011heavy clinical or technical queries (e.g., drug codes, protocol IDs, radiation plan labels) have the same property: rare, compact identifiers that must be matched exactly.\n\n3. There is little surrounding context\n   - Dense retrieval shines when it can use semantic similarity (e.g., \u2018financial earnings\u2019 vs. \u2018quarterly revenue\u2019 [1]). If a query is mostly acronyms with minimal natural language, there is less semantic signal to exploit, so BM25\u2019s exact matching becomes more reliable.\n\nIn practice, many systems combine both: dense embeddings for semantic coverage and BM25 to catch exact names, acronyms, and numbers that dense models may underweight [2][4][5]. For acronym\u2011heavy queries, you\u2019d typically give BM25 at least equal or higher weight, or use it as the primary first\u2011stage retriever.", "tags": ["long_tail_entities", "bm25_sparse"], "full_faithfulness": 1.0, "full_answer_relevancy": 0.9545066822, "seed_faithfulness": 1.0, "seed_answer_relevancy": 0.9543717850000001, "context_relevance": 0.2479565814036521, "context_relevance_error": null, "context_relevance_reason": "The context passages provide some information about Retrieval-Augmented Generation (RAG) and the use of dense and sparse retrieval methods, including BM25. However, they do not directly address the specific question of when to prefer sparse BM25 over dense embeddings for acronym-heavy queries. The relevance of the context is limited, as it lacks a focused discussion on the nuances of acronym-heavy queries and the comparative advantages of sparse versus dense methods in that specific context.", "structure_citation": 0.7605607065839883, "structure_error": null, "structure_reason": "The Actual Output effectively addresses the question about when to prefer sparse BM25 over dense embeddings for acronym-heavy queries, providing clear reasoning and examples. It includes multiple claims supported by citations [2][3][4][5], which correspond to snippets in the Context. However, while the claims are generally consistent with the context, there is a minor issue with the citation for the example of music questions, as it does not directly match a specific snippet in the provided Context. This slight inconsistency prevents a perfect score.", "hallucination_score": 0.7916426850595131, "hallucination_error": null, "hallucination_reason": "The response accurately identifies when to prefer sparse BM25 over dense embeddings, particularly for acronym-heavy queries, and provides relevant examples that align with the context. However, it lacks direct citations from the context to support its claims, which slightly undermines its overall credibility.", "contexts_used": ["3 Foundations of RAG\n\n3.1 Definition and Key Concepts\n\nRetrieval-Augmented Generation (RAG) : RAG is a framework that combines a neural text retrieval module with a text generation module to improve the quality of generated responses in knowledge-intensive tasks. Formally, a RAG model augments a sequence-to-sequence (seq2seq) generator with access to an external text corpus (non-parametric memory) via a retriever [52, 45]. Given an input query x , the retriever R selects a small subset of relevant documents Z = { z 1 , z 2 , . . . , z K } from a large corpus C (with K \u226a|C| ) [45]. The generator then conditions on both the query x and the retrieved documents Z to produce an output y (such as an answer or a descriptive text). Formally, the RAG model can be viewed as a latent variable generative model that defines a probability distribution over outputs y by marginalizing over the retrieved documents z i :", "where P ret ( z i | x ) is the probability of retrieving document z i given query x (the retriever's output distribution), and P gen ( y | x, z i ) is the generator's conditional probability of producing y given x and a particular retrieved document z i . In practice, P ret ( z i | x ) is typically non-zero only for the topK retrieved items, providing a tractable approximation to the full sum over the corpus [52]. The retriever R itself can be defined as a function R ( x, C ) \u2192 Z that takes a query and returns a small subset Z of corpus C (with | Z | = K \u226a |C| ) likely to contain information relevant to x [45]. By design, RAG models maintain two kinds of memory : a parametric memory (the knowledge encoded in the generator's weights) and a non-parametric memory (the external text corpus accessed via retrieval) [52]. A standard RAG architecture is illustrated in Figure 1 below. A key distinction between RAG and pure large language model (LLM) generation is the use of this external non-parametric knowledge source at inference time. Traditional LLM-based generation relies solely on the model's internal parameters for knowledge, which can", "lead to hallucinations and factual inaccuracies when the model's training data does not adequately cover the query's topic [52]. In contrast, RAG explicitly grounds the generation of retrieved documents that serve as up-to-date evidence, enabling the model to generate content supported by those documents. This retrieval step means that RAG's outputs can be more accurate and factually correct compared to generation from a standalone LLM, especially for knowledge-intensive queries. Empirically, [52] demonstrates that a RAG model generates more specific and factual responses than a parametric-only generator, since the retrieved text provides verified information that the generator can incorporate. Another benefit is that the knowledge in a RAG system can be easily updated by modifying the document index (or corpus) without retraining the generator, addressing the stiffness of LLMs that have fixed knowledge up to their training cutoff date. In summary, RAG introduces a modular architecture where a retrieval component supplies relevant context 'just in time' for the generator, marrying the strengths of Information Retrieval (IR) with those of large-scale generation.\n\nFigure 1: Illustration of a RAG Architecture.\n\nChunking Indexing Documents Chunks Vectorize Embedding Model Search Node 1 User Query Node 2 Node 3 Retrieve Relevant Generate Contexts Prompt Response LLM Augment", "Chunking, Embedding, and (Re)ranking :A typical RAG pipeline consists of four stages: chunking, embedding, (re)ranking, and generation. First, chunking is applied to the knowledge source: large documents are segmented into smaller, self-contained pieces (e.g., paragraphs or passages) for indexing. Using fine-grained text chunks as retrieval units improves the chance that a query will surface a highly relevant fragment, rather than an entire lengthy document [52]. For example, open-domain QA systems split Wikipedia articles into passage chunks to enable pinpoint retrieval of answer-containing segments [45, 52]. The chunk size is typically tuned to balance context completeness and specificity chunks must be large enough to contain useful context, yet small enough to match queries narrowly and fit within model context windows. Next, each chunk is embedded into a high-dimensional vector representation that encodes its semantic content. This is usually done with a transformer-based bi-encoder that produces dense vector embeddings of text [45]. The embeddings serve as keys in a vector index (or vector database) that supports efficient nearest-neighbor search. At query time, the user's query is likewise embedded into the same vector space, and the system performs similarity search to retrieve the most relevant chunk vectors. In contrast to sparse keyword search, dense embeddings enable semantic matching: a query about 'financial earnings' can retrieve a chunk about 'quarterly revenue' even if exact words differ [45]. Modern RAG implementations often combine dense retrieval with lightweight filtering or hybrid search (e.g., BM25 + embeddings) to improve recall for difficult queries. The result of the initial retrieval stage is a candidate set of topk chunks that are potentially relevant to", "the query. To further improve precision, an optional re-ranking step is applied on the retrieved candidates before generation. The topk chunks from the first stage may contain some irrelevant or only tangentially related items, since embedding similarity is a coarse proxy for relevance. A re-ranker model (typically a cross-encoder transformer that jointly encodes query and document) evaluates each retrieved chunk in the context of the query and produces a refined relevance score [63]. By re-scoring and sorting the candidates, the re-ranker ensures that the most pertinent chunks (for example, those actually containing the answer to a question) are ranked highest. This two-stage retrieval process - a fast dense retriever followed by a more accurate but expensive re-ranker - has been shown to significantly boost retrieval performance on knowledge-intensive benchmarks. For instance, neural cross-attention re-rankers achieve substantially higher accuracy than single-stage retrievers alone [63]. In practice, re-ranking is crucial in high-stakes applications (e.g., legal or medical QA), where one must maximize the likelihood that the top context passages truly address the user's query. After re-ranking, the top N (e.g. 3-5) chunks are selected as the final context passages for the generative model. In the generation stage, the LLM produces an answer or response conditioned on the retrieved external chunks. Typically, a sequence-to-sequence model (such as T5 or BART) is used so that the retrieved text can be prepended or incorporated into the model's input along with the user query [52, 73]. During training, the model learns to copy or attend to the relevant facts from the retrieved documents and integrate them into a coherent output. This approach allows the generator to cite up-to-date, specific information beyond its parametric knowledge. For example, [52] show that a RAG model (BART-based) can accurately answer open-domain questions by retrieving and conditioning on Wikipedia text, dramatically reducing hallucinations compared to a standalone LLM. The generated output can also include references to source documents, providing traceability for the facts used. Retrieval augmentation thus serves as a 'live memory' for the LLM: it supplies factual grounding from an external knowledge base while the language model creates fluent and contextually relevant text. Notably, recent large-scale studies have demonstrated that even very large models benefit from retrieval augmentation. For instance, the RETRO model augments a 7.5-billion-parameter transformer with a database of trillions of tokens, yielding improved perplexity and factual accuracy by looking up passages during generation [10]. In summary, chunking, embedding, re-ranking, and generation work in concert in RAG systems to leverage external knowledge - the retrieval components identify and prioritize relevant information, and the generation component uses that information to produce answers that are both informative and grounded in source data. This modular design has become a foundation for building more reliable and explainable AI assistants in knowledge-intensive domains.", "3.2 Technical Components of RAG\n\nA RAG system is composed of two primary components - a retriever module and a generator module -along with a strategy for fusing their outputs. We break down these components and the underlying mechanics as follows.", "3.3 Historical Context\n\nThe evolution of RAG builds upon earlier developments in open-domain question answering (QA) and neural information retrieval. Traditional open-domain QA systems were typically pipeline-based, consisting of a retrieval step followed by a reading or extraction step [12]. For example, [12] introduced the DrQA system, which first used a TF-IDF or BM25 retriever to select Wikipedia articles and then fed those to a machine reader model to extract answers. This established the value of retrieving relevant text from a large corpus as an essential first step in answering open-domain questions. However, in such pipeline approaches the retriever was not integrated into the learning of the reader, and the system could not adjust retrieval based on the end task's needs. Subsequent research sought to bridge this gap by jointly learning retrieval and answering. Notably, the concept of using learned dense representations for retrieval emerged as a powerful alternative to traditional sparse retrieval. Early milestones in this direction include latent retrieval models: [48] proposed the ORQA model, which treats retrieval as a latent variable problem and pre-trains a neural retriever on an unsupervised 'inverse cloze task' before jointly fine-tuning it with a reader on QA. Around the same time, [26] introduced REALM, a retrieval-augmented language model pre-training method that incorporated a differentiable retriever into the pre-training of a masked language model. REALM demonstrated that pre-training a model to retrieve and reason over Wikipedia could significantly improve open-domain QA, highlighting the benefit of coupling a language model with a learned retrieval mechanism [26]. These efforts were focused primarily on question answering (often extractive), but they laid important groundwork for retrieval-augmented generation by showing that retrieval and neural text generation can be trained in tandem.", "In parallel, the idea of combining external knowledge with neural networks has roots in earlier memory-augmented models . For instance, Memory Networks [95] and subsequent variants allowed neural networks to read from an external memory of facts and use that information to answer questions or generate responses. These models (e.g., [95, 87]) demonstrated the feasibility of non-parametric memory for reasoning, albeit on smaller-scale knowledge bases or synthetic tasks. While memory network architectures were often task-specific and required the memory to be relatively small or structured, they presaged the RAG approach by emphasizing that not all knowledge needs to be baked into model parameters-some can be looked up as needed. Another line of work in dialogue systems also integrated retrieval into generation: the Wizard of Wikipedia project [18] is a prime example, where a conversational agent retrieves relevant Wikipedia sentences and conditions a generative dialogue model on those sentences to produce knowledgeable responses. This retrieval-based dialogue system (published in 2019) demonstrated improved factuality and depth in conversational responses, reflecting the general trend that augmenting generators with retrieved context yields more informative and correct outputs.", "These developments converged in 2020 with the formalization of Retrieval-Augmented Generation by [52], who unified the retriever-reader architecture with seq2seq generation in an end-to-end framework. The RAG model of [52] was a culmination of insights from open-domain QA and neural IR: it used a dense passage retriever [45] to fetch text chunks from Wikipedia and a powerful seq2seq generator (BART) to produce answers or summaries, training both components jointly. By marginalizing over multiple retrieved documents (as in Eq. 1), the RAG system could leverage several pieces of evidence and was shown to outperform both parametric-only models and earlier retrieve-and-read pipelines on knowledge-intensive tasks like open QA [52]. The introduction of RAG in 2020 is considered a key milestone because it generalized retrieval-augmented architectures beyond QA to any generative task requiring external knowledge. It also spurred a new line of research into knowledge-enhanced text generation , influencing subsequent models that further refined retrieval modules, document ranking, and fusion techniques for even better performance.", "Execution Flow of a RAG System. To summarize the interactions of these components, we outline the step-by-step execution flow of a typical RAG system processing a query:\n\nQuery Encoding: Given an input query x (e.g., a question in natural language), the retriever's query encoder E q first encodes x into a vector representation v q . This vector captures the semantic meaning of the query in a dense", "embedding space.", "generation components were not trained jointly, and the end-to-end approach was limited to extractive answers. In 2018, work shifted toward tighter integration between retrieval and reading. [92] proposed R 3 ( Reinforced Reader-Ranker ), adding a neural ranker to score retrieved passages by answer likelihood. The system then learned ranker-reader synergy via reinforcement learning, boosting open-domain QA accuracy by better filtering relevant evidence. Meanwhile, neural IR methods gained traction. [48] introduced Latent Retrieval in their Open-Retrieval QA (ORQA) framework, training a dense retriever and a reader end-to-end with only question-answer supervision. Specifically, the retriever embeddings were pretrained on an inverse cloze task and then adapted to select evidence documents that help the QA model answer correctly. This concept of dense retrieval , which outperformed sparse BM25 by up to 19% in exact-match QA scores, would become the foundation of subsequent RAG models. Still, these early systems were limited mostly to extractive QA, without a unified end-to-end training for generative outputs.", "Related Knowledge:\n\nFor nasopharyngeal carcinoma (NPC), the\n\n#3: Propose the best plan among all iterations.\n\nclinical criteria should be followed\n\nFormat:\n\nIter 5.", "As shown in Fig. 2, the left side of the template provides contextual input, including the objective function structure, patient-specific planning data and relevant clinical knowledge retrieved via the RAG module. This integrated context ensures that the model's reasoning remains aligned with protocol-defined criteria and patient-specific conditions. The right side of the template defines a standardized question-and-answer format, which instructs the model to perform weight adjustment, assess plan acceptability, and select the optimal plan among all iterations. Each response adheres to a fixed schema designed for seamless parsing by the downstream optimization engine.\n\nFigure 3: The framework of RAG-enhanced prompt generation.\n\nEmbedding & Chunking Indexing Professional Textbooks\n\nPractice Guidelines;\n\nVector Database Semantic Vector Clinical Reports Knowledge Base Retrieval & ReRank Embedding Query Vector Search Evaluate current plan and adjust\n\nweights.\n\nRelated Knowledge User Original Prompt No change\n\nEvaluate current plan and\n\nAugment\n0.01\n0.02\n\nadjust weights based on\n\n1.3 <clinical knowledge Wndl Answer Large Language Model Augmented Prompt", "To compensate for the domain knowledge limitations of the base LLM, a RAG module was incorporated into the proposed framework to dynamically inject SIBT-specific clinical knowledge during prompt generation and decision making. As illustrated in Fig. 3, the RAG system consists of two key components: a structured clinical knowledge base and a hybrid retrieval pipeline. During each iteration, the system queries relevant protocol and case-specific knowledge, which is then incorporated into the prompt to guide the LLM in evaluating the current plan and recommending clinically informed weight adjustments.", "The knowledge base was constructed from professional textbooks, practice guidelines, and clinical reports relevant to SIBT. Source documents in PDF and plain text formats were parsed and segmented using a two-stage splitting strategy. First, an embedding-guided semantic chunking approach preserved contextual coherence; subsequently, content-aware secondary splitting optimized chunk size for both retrieval accuracy and generative performance.", "For knowledge retrieval, a hybrid retriever combining dense and sparse retrieval techniques was adopted. Dense retrieval utilized semantic embeddings generated by the BGE-Small-Zh-v1.5 model [5] and stored in a Chroma-powered vector database for efficient similarity search. Given a query q , the dense similarity score is calculated via cosine similarity between the query embedding v q and document embeddings v i :\n\nFor sparse retrieval, the BM25 algorithm [26] was utilized to complement the dense retrieval results. The BM25 score is computed as:\n\nwhere t denotes a term in the query q , and f ( t, D ) is the frequency of term t in document D . The parameter | D | indicates the length of document D , while avgdl denotes the average document length across the entire corpus. The term IDF ( t ) reflects the inverse document frequency of term t , which down-weights common terms and emphasizes more informative ones. The hyperparameter k 1 controls the scaling of term frequency contribution, while b adjusts for document length normalization. In our experiments, k 1 = 1 . 2 , b = 0 . 75 .", "The final retrieval score was obtained by weighted fusion of dense and sparse scores. To further refine retrieval precision, a cross-encoder reranking model [22] was applied to re-score candidate documents based on query-document relevance pairs. Only documents with final scores exceeding a predefined threshold are selected for retrieval and passed to the language model for subsequent reasoning.", "During the weight adjustment process in each iteration, the RAG module queries and retrieves knowledge specific to the tumor site and OARs involved in the current case, enabling clinically informed evaluations and adaptive weight recommendations.", "2 Preliminaries\n2.1 Retrieval System\n\nThe core objective of a retrieval system is to identify and rank a subset of documents ( d 1 , d 2 , ..., d k ) from a large corpus C based on their estimated relevance to a query q . Classical retrieval approaches, such as BM25 (Robertson et al., 1995), rely on exact term matching and produce sparse relevance scores. In contrast, dense retrieval methods employ neural encoders to project queries and documents into a shared embedding space, enabling semantic similarity matching (Karpukhin et al., 2020). Since first-stage retrievers often produce noisy candidates, modern pipelines incorporate a second-stage re-ranking step using more expressive models. This includes LLM-based rankers (Pradeep et al., 2023b;\n\nMa et al., 2024; Sun et al., 2023) and reasoningaugmented models such as ReasonIR (Shao et al., 2025), Rank-1 (Weller et al., 2025), and RankR1 (Zhuang et al., 2025), which refine the initial rankings by modeling deeper interactions between the query and candidate documents.", "2.2 Retrieval-Augmented Generation\n\nRetrieval-Augmented Generation (RAG) (Lewis et al., 2020b) is a hybrid paradigm that enhances the generative capabilities of large language models (LLMs) by incorporating non-parametric external knowledge during inference. This design mitigates well-documented limitations of standalone LLMs, including hallucinations, confident but incorrect outputs, and inability to reflect up-to-date or domain-specific information due to static pretraining (Hurst et al., 2024; Grattafiori et al., 2024; Yang et al., 2025a).", "RAG introduces an explicit retrieval step: for a query q , a retriever selects a set of topk documents { d 1 , . . . , d k } from an external corpus. A generator G then conditions on both q and the retrieved context to produce the output a = G ( q, d 1 , . . . , d k ) where G is typically an encoderdecoder or decoder-only LLM fine-tuned to integrate retrieved evidence into its generation process.", "2.3 Challenges in RAG\n\nA central challenge in RAG is that generation quality is tightly coupled with retrieval quality. If relevant (i.e., answer-containing) documents are not retrieved, or if irrelevant context is included, the generator is prone to producing incorrect or misleading outputs (Cuconasu et al., 2024, 2025; Liu et al., 2024).", "Consequently, improving the topk retrieval results is crucial. This can be viewed both as a selection problem (how to retrieve the most relevant documents) and a filtering problem (how to suppress distracting or noisy context). To this end, several methods have been proposed that incorporate various forms of feedback , ranging from simple lexical overlap to more sophisticated agentic or reasoning-based signals, to guide and refine the retrieval process.", "In this survey, we systematically categorize these feedback mechanisms and analyze their effectiveness across different components of the RAG pipeline. We focus on how feedback is acquired, represented, and integrated into retrieval, with the", "Figure 1: Illustration of feedback signals across the RAG pipeline. Feedback can modify the query (e.g., rewriting), the retrieved pool (e.g., ranker-based expansion), or the generation loop (e.g., retrieval triggers based on uncertainty).\n\nRetrieval Reranker results Generation Query Encoder representation new results Ranker Relevance Dynamic Understanding Generated Output Feedback levels Query Level Retrieval Level Generation Time Query Understanding Ranker Relevance Dynamic Retrieval - Query Expansion - Query Rewriting\n\n- Query Vector Adaption\n\n- Query Decomposition\n- Result refinement\n\n- When to retrieve\n\n- What (new query) to retrieve\n\naim of providing a comprehensive taxonomy and highlighting open research challenges.", "Answer Requirements:\n\nStart with the precise numerical value and units.\n\nInclude route of administration if specified.\n\nInclude frequency/timing context if mentioned.", "Explain significance of the dosage (e.g., threshold, therapeutic, etc.)\n\nAvoid referencing 'the text' or 'the source'\n\nKnowledge Content: Here is the knowledge content to use: knowledge\n\nFormat your response as:\n\n'question': 'Question text', 'answer': 'Numerical value + units + explanation from source',\n\n'context': 'The exact sentences of context information you used from the source'\n\nTable 2: System instruction used for safety boundary check tasks.", "(i) You are tasked with answering a yes/no question related to harm reduction practices based on reliable information.\n\n(ii) You must respond in exactly this format:\n\nYes/No , followed by a brief explanation of the substantial reason(s) behind.", "Knowledge Retrieval : We implemented a mixed retrieval method using dense and sparse retrievers. Specifically, we leverage dense retriever (using Sentence-Transformers (Reimers and Gurevych, 2019)) to identify semantically related harm reduction information. We then implement sparse retriever BM25 (Robertson et al., 2009) using precise term-matching to accurately retrieve specific substance names and dosage information that may not be optimally captured through dense retriever alone.", "Knowledge Reranking : To effectively integrate retrieved knowledge, we employ the Reciprocal Rank Fusion (RRF) method, which combines the results from both retrieval methods using a rank-based fusion: score ( d ) = \u2211 i 1 k + rank i ( d ) , where d denotes the target document and rank i ( d ) denotes document d 's rank in the i -th retrieval method, and k is a constant that mitigates the impact of high rankings.", "The RAG scheme integrates with LLMs when providing harm reduction information. Its model response R is formulated as:\n\nwhere K denotes the retrieved knowledge.", "6. RESULT\n6.1 In-domain Performance\n\nQA Fine-tuning Comparing QA fine-tuning with zeroshot performance, factual performance improved by 1.0%, but contextual performance decreased by 5.5%. This sug- gests that while QA fine-tuning is effective in helping the model retain information from the training data, it may also reduce the overall inference capability.", "RAGInference Byutilizing RAG inference without additional training, we were able to address the low factual performance that was an issue with previous LLMs. It demonstrated a 14.6% higher factual performance compared to GPT-4o [35]. Contextual performance improved by 3.6% compared to zero-shot, but was still 4.2% lower than GPT4o.", "RAG Fine-tuning The model fine-tuned on the RAGstyle dataset showed improvements in both types of questions. Compared to RAG inference, factual performance improved by 0.4%, and contextual performance improved by 3.2%. This demonstrates that by learning to leverage context, the model not only improves its memory of information present in the training data but also enhances its overall contextual understanding ability. It exhibited a remarkable 15.0% higher factual performance compared to GPT-4o, and only 1.0% lower contextual performance, which is nearly equivalent. Considering factors such as the model size, amount of training data, and the extent of training, this is an exceptionally high performance.", "| | Embedding | Passage Size | Factual | Contextual |\n|---:|:-------------------|:-------------------|----------:|-------------:|\n| 0 | Gold (Upper Bound) | Gold (Upper Bound) | 97.8 | 97 |\n| 1 | | 512 | 82 | 88.8 |\n| 2 | BM25 [16] | 256 | 82.8 | 88 |\n| 3 | | 128 | 82.2 | 89 |\n| 4 | | 512 | 46.6 | 81 |\n| 5 | Contriever [19] | 256 | 55.6 | 84.2 |\n| 6 | | 128 | 58.2 | 86.6 |\n| 7 | | 512 | 41.2 | 79.6 |\n| 8 | CLAP [20] | 256 | 41 | 84 |\n| 9 | | 128 | 41.8 | 84 |\n\nTable 4 : Llama 3.1 8B Instruct [34] RAG performance on ArtistMus , by different passage size and embeddings.", "6.2 Out-of-domain Performance\n\nTo validate the effectiveness of the MusT-RAG in out-ofdomain scenarios, we conducted experiments by changing the benchmark from ArtistMus to TrustMus [13], using the same framework with in-domain evaluation. The results are presented in Table 3.", "Zero-shot Baselines A similar trend was observed in the zero-shot evaluation for the in-domain setting. MuLLaMA [36] and ChatMusician [12] performed worse than the random baseline (25%), which is due to incorrect answers being counted when the models failed to follow instructions. Given that the overall zero-shot performance closely aligns with the factual scores from the in-domain evaluation, we infer that TrustMus mostly consists of factual questions. The Llama 3.1 8B Instruct [34] model scored 17.2% lower than GPT-4o [35].", "QA Fine-tuning The QA fine-tuned model showed a 3.8% decrease in performance compared to zero-shot, which can be attributed to the fact that models trained on artist data tend to forget information about out-of-domain topics, such as Instrument and Genre .", "RAG Inference RAG inference led to an 5.0% performance improvement over zero-shot, demonstrating that MusT-RAG framework is also helpful for out-of-domain data, such as The Grove Dictionary Online [15], which is the basis for the TrustMus benchmark.", "RAG Fine-tuning The RAG fine-tuned model showed a 0.7% improvement over RAG inference, even with the same artist data used for QA fine-tuning. This supports the fact that the RAG fine-tuning method, which incorporates context, enhances the model's robustness in contextual understanding, even for out-of-domain data.", "6.3 Ablation Study: Retriever Configurations\n\nTable 4 shows the results of the RAG inference for the Llama 3.1 8B Instruct [34] with various passage sizes and embeddings, evaluated under the same total computation budget for fair comparison. The performance on contextual questions tended to improve as the passage size decreased across all embedding models. In contrast, for\n\nFigure 2 : RAG performance and retrieval time for Wikipedia Corpus [8] and MusWikiDB.\n\nWikipedia Corpus\nMusWikiDB\n\nPerformance & Retrieval Time\n\n85.4 0.0384 0.04 79.5 0.03 0.02 0.01 0.0038 0.00 Performance Retrieval Time", "factual questions, only Contriever [19] showed clear improvements with shorter passages, while BM25 [16] and CLAP [20] showed little to no change in performance across different passage lengths. For factual questions, there was a significant performance gap between BM25 and the other two dense embeddings. This is likely because ArtistMus places high importance on music entities such as artist and albums. Overall, the best performance was achieved using BM25 with a passage size of 128. When compared to the gold context, the factual performance was 15.6% lower, and the contextual performance was 8.0% lower. In Figure 2, we compare the RAG inference performance using the Wikipedia corpus [8] and MusWikiDB. The results show that MusWikiDB achieves a 10x faster retrieval speed and 5.9% higher performance."]}
{"type": "entry", "index": 9, "question": "What are effective reranker thresholds to balance recall vs precision in RAG?", "answer": "Across the provided references, reranker thresholds are chosen empirically to trade off recall (keeping more possibly relevant passages) against precision (keeping only very likely relevant passages).\n\nOne approach is a **dynamic threshold** based on the score distribution for each query. A multimodal RAG system reranks paragraph chunks with a cross-encoder that outputs scores in [0,1], then applies a statistics-based cutoff using the Median Absolute Deviation (MAD) with two key parameters: a minimum score \u03c4 = 0.1 and a scaling factor \u03bb = 1.5 [1].\n- \u03c4 = 0.1 is a hard floor: anything below this is always discarded, which removes clearly irrelevant noise but keeps most plausible candidates (higher recall) [1].\n- \u03bb controls how aggressively you filter around the typical scores: lowering \u03bb enforces stricter filtering (higher precision, lower recall); raising \u03bb relaxes filtering (higher recall, more noise) [1]. This lets the threshold adapt per query instead of using a single global cutoff.\n\nIn another setting, question\u2013chunk pairs are kept only if their relevance score exceeds 0.4, chosen to \"balance precision and recall\" when constructing supervision data [3]. This is a **fixed threshold** and is relatively stricter than \u03c4 = 0.1, favoring precision for training data quality [3].\n\nMore generally, work on RAG stresses that RAG pipelines \"often require extensive tuning to balance recall and precision effectively\" and that reranking alone gives only marginal gains in some mixture-of-knowledge scenarios [4][5]. This implies there is no universally optimal threshold: you typically\n- start with a relatively low floor (\u22480.1) when you care about recall,\n- move toward stricter values (\u22480.3\u20130.4+) when you need cleaner, more precise contexts,\n- and prefer dynamic, distribution-aware cutoffs when reranker score distributions are long-tailed [1][3][4].", "tags": ["rerankers", "retrieval_tuning"], "full_faithfulness": 1.0, "full_answer_relevancy": 0.9137848540000001, "seed_faithfulness": 0.5, "seed_answer_relevancy": 0.0, "context_relevance": 0.7152459630588602, "context_relevance_error": null, "context_relevance_reason": "The context passages provide substantial information related to reranking thresholds and balancing recall versus precision in RAG systems. Specifically, the discussion on dynamic thresholding and the use of Median Absolute Deviation (MAD) to optimize retrieval precision directly addresses the user's question. However, while the passages are relevant, they could be more explicit in detailing effective thresholds, which slightly limits the overall alignment with the user's inquiry.", "structure_citation": 0.7714556563793394, "structure_error": null, "structure_reason": "The Actual Output effectively discusses reranker thresholds and their impact on recall and precision, referencing specific parameters like \u03c4 = 0.1 and \u03bb = 1.5. However, while it includes citations [1], [3], [4], and [5], not all claims are directly supported by the provided Context snippets, particularly regarding the empirical choices and their implications. This leads to a slight deduction in the score due to some unsupported claims and the need for clearer citation alignment.", "hallucination_score": 0.8440682302800292, "hallucination_error": null, "hallucination_reason": "The response effectively discusses empirical approaches to setting reranker thresholds, aligning well with the input question about balancing recall and precision in RAG. It provides specific details on dynamic and fixed thresholds, including the use of Median Absolute Deviation (MAD) and the parameters \u03c4 and \u03bb, which are relevant to the context. However, while the output is mostly supported by the context, it could benefit from a clearer connection to the specific challenges mentioned in the context regarding latency and query handling in multimodal systems.", "contexts_used": ["3 Methodology\n\n3.1 Lightweight Query Routing\n\nIn real-world multimodal QA systems, especially those deployed on latency-critical platforms such as smart glasses, efficient query routing is essential. Not all user questions warrant the same computational treatment: some can be answered directly from visual input, while others require external or time-sensitive knowledge. Applying a full RAG pipeline to every query introduces unnecessary latency and resource usage. Prior work has shown that when response latency exceeds 7-10 seconds, users report feeling significantly more tense, tired, frustrated, and sluggish, all of which contribute to a worse subjective user experience [1]. These findings are particularly relevant to the CRAG-MM challenge, which enforces a strict 10-second response limit per turn. Moreover, CRAG-MM includes a diverse range of question types-from simple image-based recognition to temporal queries, making adaptive query handling even more critical.", "To address this, we design a lightweight query routing module as the first stage of our pipeline, enabling early classification of queries to avoid unnecessary computation. We employ a compact instruction-tuned model, LLaMA-3.2-1B-Instruct 1 [5], as a dedicated router. This smaller model is computationally efficient and can semantically classify queries with minimal overhead. A key design consideration is the well-documented tendency of large language models to generate hallucinated or outdated answers when queried about facts or recent events beyond their training cutoff. To mitigate these risks and optimize downstream decisions, our\n\n1 https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct", "For each query \ud835\udc5e \ud835\udc56 , we first generate a query-aware image summary \ud835\udc60 \ud835\udc56 using LLaMA-3.2-11B-Vision-Instruct 2 [5], an instructiontuned vision-language model capable of producing grounded and context-sensitive descriptions:\n\nwhere \ud835\udc3c \ud835\udc56 is the input image and \ud835\udc53 denotes the vision language model. The prompt of summarization is provided in Appendix A.2 We then form the retrieval query by concatenating the textual question and the summary:\n\nInitial Recall. Depending on the routing decision \ud835\udc52 , we perform either text-based or image-based retrieval with a unified search pipeline \ud835\udc45 provided by organizer:", "where \ud835\udc3e \ud835\udc5f\ud835\udc52\ud835\udc50\ud835\udc4e\ud835\udc59\ud835\udc59 is top K recall and \ud835\udc3e \ud835\udc5f\ud835\udc52\ud835\udc50\ud835\udc4e\ud835\udc59\ud835\udc59 = 10 in our implementation. Each retrieved item \ud835\udc51 \ud835\udc56 \ud835\udc57 contains various structured attributes. The search pipeline was carried out using two modality-specific embedding models: bge-large-en-v1.5 3 for text-based queries and clip-vit-large-patch14-336 4 [8] for image-based queries. We specifically extract the description , caption , and summary fields from the mock knowledge graph (KG) and web snippets. These contents are then normalized into individual paragraphs by splitting on newlines. The resulting paragraph-level chunks are scored independently during reranking. Chunking is essential, as the retrieved fields may span entire documents, often exceeding the reranker's maximum input sequence length. In practice, we found paragraphlevel granularity to be well-suited for most modern reranker models.", "RerankingwithDynamicThresholding. Toimprove retrieval precision, we rerank the normalized paragraph chunks using bgereranker-v2-m3 5 [2], a state-of-the-art multilingual cross-encoder. Each chunked paragraph from \ud835\udc51 \ud835\udc56 \ud835\udc57 is scored against the expanded query \ud835\udc5e \u2032 \ud835\udc56 as follows:\n\nwhere \ud835\udc53 rank denotes the reranker and \ud835\udc60\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc52 \u2208 [ 0 , 1 ] is the predicted relevance score. Top-ranked chunks are retained for generation based on a dynamic threshold.", "A significant challenge in RAG is handling the noisy, long-tailed nature of retrieved documents. To address this, we move beyond conventional fixed-threshold filtering and utilize a principled, statisticsbased dynamic cutoff. We leverage the Median Absolute Deviation (MAD) to suppress noisy evidence while retaining salient chunks. MAD is a robust statistic widely used to measure the variability of a distribution, particularly effective under skewed or heavy-tailed distributions where traditional metrics like standard deviation may be unreliable. Given that reranker scores are often unevenly distributed with a small number of highly relevant chunks and many low-scoring distractors, MAD offers a principled and stable way to estimate a threshold without being overly\n\n2 https://huggingface.co/meta-llama/Llama-3.2-11B-Vision-Instruct\n\n3 https://huggingface.co/BAAI/bge-large-en-v1.5\n\n4 https://huggingface.co/openai/clip-vit-large-patch14-336\n\n5 https://huggingface.co/BAAI/bge-reranker-v2-m3\n\nsensitive to outliers. Let ScoreSet \ud835\udc56 = { score \ud835\udc56 \ud835\udc57 | 1 \u2264 \ud835\udc57 \u2264 \ud835\udc3e } and TopScores \ud835\udc56 = Top 10 ( ScoreSet \ud835\udc56 ) . Then\n\nwhere \ud835\udf0f = 0 . 1 and \ud835\udf06 = 1 . 5 are chosen in our setting. \ud835\udf0f is the minimum reranker score threshold and \ud835\udf06 controls the strictness of filtering by scaling the MAD, a higher \ud835\udf06 allows more relaxed filtering, while a lower \ud835\udf06 enforces stricter selection. We keep\n\nContext Construction. The final RAG context is created by concatenating filtered snippets:", "Here, \ud835\udc36 \ud835\udc56 denotes the final textual context for query \ud835\udc5e \ud835\udc56 . Each \ud835\udc51 \ud835\udc56\ud835\udc5a \u2208 \u02dc \ud835\udc37 \ud835\udc56 is a paragraph-level snippet selected after reranking. The index \ud835\udc5a refers to the position of the snippet within the filtered set \u02dc \ud835\udc37 \ud835\udc56 . We prepend each snippet with a tag [Info \ud835\udc5a ].", "3.3 Dual-Path Generation\n\nThe primary task of the VLM is to then generate an accurate and reliable answer for each user query \ud835\udc5e \ud835\udc56 . Not all queries have relevant contexts in the knowledge graph, and in some cases, the reranker may assign low relevance scores to all retrieved snippets, resulting in a minimal or empty context \ud835\udc36 \ud835\udc56 . Additionally, the VLM's prior knowledge may still yield correct answers even when relevant external context is unavailable. To balance grounding in retrieved evidence with the model's prior capabilities, we adopt a dual-path generation strategy. Specifically, for each query \ud835\udc5e \ud835\udc56 , we generate two responses: one using retrieval-augmented generation (RAG) and one without context (non-RAG):\n\nwhere \ud835\udc53 shares the same vision-language model with the summary process. The RAG answer \ud835\udc4e \u2032 \ud835\udc56 is conditioned on the retrieved and filtered context \ud835\udc36 \ud835\udc56 , while the non-RAG answer \ud835\udc4e \ud835\udc56 is generated without any retrieval and relies on the VLM's prior knowledge only.", "Self-consistency. This dual-path setup enables us to later assess the consistency between the two outputs and identify cases where retrieved contexts improve factual grounding. To this end, we adapt the self-consistency check from [9], with modifications tailored to our dual-path generation setup. After generating answers with and without retrieved context, we evaluate their consistency using an additional verification step. Specifically, we pass the query \ud835\udc5e \ud835\udc56 , the retrieved contexts \ud835\udc36 \ud835\udc56 (if any), the RAG answer \ud835\udc4e \u2032 \ud835\udc56 , and the non-RAG answer \ud835\udc4e \ud835\udc56 into the VLM for comparison.", "The motivation follows the self-consistency principle proposed by [9]: LLMs (thus VLMs) (1) are prone to hallucinations without grounding on retrieved contexts, and (2) may produce multiple plausible answers for complex queries. Since relevant context from the knowledge graph is not guaranteed for every query, self-consistency helps ensure that the model produces stable, consistent responses when prompted multiple times. In cases where a relevant context is found, agreement between the RAG and non-RAG answers indicates consistency and strengthens confidence in the generated", "output by showing that it is both grounded and aligned with the LLM's prior knowledge.\n\nTo perform this check, the VLM is prompted as an impartial judge tasked with determining whether the two answers are consistent with each other and with the provided context and image. It is instructed to respond with a binary decision: yes or no . Full prompt details are provided in Appendix A.3.", "Two-Stage Retrieval and Reranking\n\nOur pipeline adopts a two-stage retrieval strategy to effectively balance recall and precision when handling decomposed subquestions and complex multihop queries.", "Stage 1: AQG-based Retrieval. For each decomposed single-hop query q i , we embed it using the E5 encoder and perform dense retrieval over the AQG embedding index constructed from all answerable questions. Each AQG is linked to its source document chunk d j , enabling us to retrieve relevant evidence indirectly via semantically aligned subquestions.", "We sort AQGs by their similarity to the query, and collect the corresponding source chunks. Since multiple AQGs may point to the same document, this process yields a highquality document candidate pool with natural deduplication.", "Stage 2: Original-Question Reranking. We then rerank the deduplicated document set using a cross-encoder that scores the relevance of each candidate chunk d j with respect to the original multihop question q . This reranking filters out contextually irrelevant documents that may have been retrieved via valid but tangential subquestions. The final top-ranked documents are passed to the generator for answer synthesis.", "ii) The Dataset of DPO Processing\n\nIn this study, we used a large language model (LLM) to extract salient information from each question and its corresponding answer, identifying key concepts to guide relevance assessment. We then computed a semantic relevance score between each question and the candidate text chunks retrieved by the Retrieval-Augmented Generation (RAG) system. This scoring was based on the degree of alignment between the extracted key points and the retrieved content. To ensure high-quality", "training data, we filtered out low-relevance pairs and retained only those question-chunk pairs whose relevance scores exceeded a predefined threshold of 0.4. This threshold was empirically chosen to balance precision and recall in identifying contextually appropriate retrievals for use in subsequent model training.\n\nFigure 8 | Data Synthesis Process for DPO\n\nqwen-110b Query & of Answer Relevant Score RAG Chunks point", "LLMs in QA\n\nThe advent of LLMs has steered in a transformative era in NLP, particularly within the domain of QA. These models, pre-trained on massive corpora of diverse text, exhibit sophisticated capabilities in both natural language understanding and generation. Their proficiency in producing coherent, contextually relevant, and human-like responses to a broad spectrum of prompts makes them exceptionally well-suited for QA tasks, where delivering precise and informative answers is paramount. Recent advancements by models such as BERT [57] and ChatGPT [58], have significantly propelled the field forward. LLMs have demonstrated strong performance in open-domain QA scenarios-such as commonsense reasoning[20]-owing to their extensive embedded knowledge of the world. Moreover, their ability to comprehend and articulate responses to abstract or contextually nuanced queries and reasoning tasks [22] underscores their utility in addressing complex QA challenges that require deep semantic understanding. Despite their strengths, LLMs also pose challenges: they can exhibit contextual ambiguity or overconfidence in their outputs ('hallucinations')[21], and their substantial computational and memory requirements complicate deployment in resource-constrained environments.\n\nRAG, fine tuning in QA", "LLMs also face problems when it comes to domain specific QA or tasks where they are needed to recall factual information accurately instead of just probabilistically generating whatever comes next. Research has also explored different prompting techniques, like chain-of-thought prompting[24], and sampling based methods[23] to reduce hallucinations. Contemporary research increasingly explores strategies such as fine-tuning and retrieval augmentation to enhance LLM-based QA systems. Fine-tuning on domain-specific corpora (e.g., BioBERT for biomedical text [17], SciBERT for scientific text [18]) has been shown to sharpen model focus, reducing irrelevant or generic responses in specialized settings such as medical or legal QA. Retrieval-augmented architectures such as RAG [19] combine LLMs with external knowledge bases, to try to further mitigate issues of factual inaccuracy and enable real-time incorporation of new information. Building on RAG's ability to bridge parametric and non-parametric knowledge, many modern QA pipelines introduce a lightweight re-ranking step [25] to sift through the retrieved contexts and promote passages that are most relevant to the query. However, RAG still faces several challenges. One key issue lies in the retrieval step itself-if the retriever fails to fetch relevant documents, the generator is left to hallucinate or provide incomplete answers. Moreover, integrating noisy or loosely relevant contexts can degrade response quality rather than enhance it, especially in high-stakes domains where precision is critical. RAG pipelines are also sensitive to the quality and domain alignment of the underlying knowledge base, and they often require extensive tuning to balance recall and precision effectively.", "2 Experiments\n\n2.2 RQ1: Effectiveness of RAG under Mixture-of-Knowledge Scenarios.\n\n1.64 1.81 0.62 0.57 0.85 0.45 1.13 0.91 (e) Qwen-3-32B Pubmed Wiki Pes2o C4 Github Math SE Book Arxiv CC SimpleQA SciQ ARC-C MMLU MMLU-Pro CSBench 0.72 16.28 3.14 0.90 1.20 0.88 1.46 4.35 1.10 1.40 0.30 2.00 0.50 1.90 1.70 0.60 2.60 0.17 0.09 0.00 0.26 0.34 0.44 0.25 0.37 0.22 0.39 0.20 0.43 0.68 0.96 0.49 0.64 0.48 0.52 0.82 0.79 0.74 0.40 0.51 0.28 (f) GPT-4o\n\nand mid-sized models, while the gains diminish as model size increases. For the strongest models, retrieval offers marginal or even slightly negative gains, suggesting that larger models are increasingly able to internalize domain knowledge without the need for external retrieval. These results highlight the generalizability of our findings and demonstrate that the impact of retrieval augmentation is robust across different knowledge domains.", "2.3 RQ 2: Instance-level Analysis of Multiple Retrieval Sources\n\nTo investigate whether different retrieval sources provide unique advantages, we conduct an instancelevel study (Figure 2) measuring the proportion of queries that can be solved only by retrieving from a specific corpus-compared to using all sources or no retrieval at all. Our results show that a significant fraction of cases (e.g., 8%-39% for Llama-3.18B) depend exclusively on retrieval from particular corpora. Crucially, no single retrieval source consistently outperforms others across all query types , highlighting the need for dynamic, query-specific routing to the most relevant corpus to maximize RAG performance in heterogeneous, real-world knowledge environments.", "2.4 RQ 3: Effectiveness of Reranking\n\nOne possible reason for the limited improvement in RAG performance is the inherent limitations of", "Figure 3: Performance with rerank on retrieval effectiveness across different datasets and models.\n\nPubmedWiki Pes2o GithubMath Book Arxiv All llama-3.2-3b llama-3.1-8b qwen3-4b qwen3-8b qwen3-32b gpt-4o-mini gpt-4o -0.63 7.11 -0.36\n\n-0.57 -0.23 -0.49 -0.07 -0.58\n\n2.04 8.49 3.02 31.90 4.42 9.05 3.93 4.32 3.67 4.94 3.19 12.8934.09 -0.82 3.50 0.21\n\n-0.70 -0.60 -0.70 -0.48 -0.75\n\n0.77 -0.85 2.93 0.14\n\n-0.76 -0.62 -0.69 -0.45 -0.76\n\n0.69\n3.81\n1.99\n\n-0.71 -0.13 -0.78 -0.68 -0.75 -0.58 -0.81\n\n0.27\n2.53\n-0.46\n\n-0.33 -0.01 -0.39 -0.29 -0.36 -0.26 -0.32\n\n1.89\n-0.24\n0.23\n\n-0.27 -0.24 -0.27 -0.24 -0.25 -0.26 -0.23 -0.17\n\n(a) SimpleQA -0.22 -0.10 0.24 -0.26 0.11 -0.20 -0.32 0.41 -0.01 0.06 0.03 -0.14 0.46 -0.15 0.07 -0.09 0.19 -0.11 -0.19 0.18\n\n-0.25 -0.13 -0.21 -0.01 -0.23 -0.06 -0.20 -0.08 -0.29 -0.02\n\n-0.32 -0.15 -0.24 -0.10 -0.28 -0.11 -0.27 -0.11 -0.35 -0.07 -0.01\n\n-0.20 -0.15 -0.25 -0.10 -0.19 -0.14 -0.18 -0.14 -0.24 -0.09 -0.05\n\n-0.14 -0.13 -0.18 -0.07 -0.13 -0.09 -0.13 -0.10 -0.13 -0.08 -0.04\n\n(b) SciQ", "-0.32 -0.15 -0.24 -0.10 -0.28 -0.11 -0.27 -0.11 -0.35 -0.07 -0.01\n\n-0.20 -0.15 -0.25 -0.10 -0.19 -0.14 -0.18 -0.14 -0.24 -0.09 -0.05\n\n-0.14 -0.13 -0.18 -0.07 -0.13 -0.09 -0.13 -0.10 -0.13 -0.08 -0.04\n\n(c) ARC-C -0.00 0.05 -0.03 0.04 -0.05 0.10 0.16 0.01 0.02 0.15 -0.04 -0.00 -0.02 -0.04 -0.02 -0.05 -0.06 -0.04 -0.01 -0.02\n\n-0.03 -0.01 -0.04 -0.00 -0.05\n\n-0.06 -0.02 -0.03 -0.02 -0.04 -0.02 -0.05 -0.02 -0.05 -0.01\n\n-0.03 -0.02 -0.02 -0.02 -0.03 -0.02 -0.03 -0.02 -0.04 -0.02 -0.01\n\n-0.01 -0.02 -0.02 -0.01 -0.01 -0.01 -0.01 -0.01 -0.01 -0.01 -0.01\n\n(d) MMLU -0.06 -0.00 -0.04 -0.07 0.12 0.31 -0.04\n\n-0.08 -0.02 -0.04 -0.02 -0.05 -0.02 -0.04 -0.01 -0.06 -0.01\n\n-0.06 -0.02 -0.05 -0.03 -0.08 -0.02 -0.07 -0.02 -0.09 -0.01", "-0.02 -0.01 -0.02 -0.02 -0.01 -0.00 -0.01 -0.01 -0.02 -0.01\n\n(e) MMLU-Pro\n0.08\n0.13\n\n-0.12 -0.01 -0.04 -0.01 -0.06\n\n-0.05 -0.01 -0.09 -0.03\n\n-0.14 -0.06 -0.09 -0.05 -0.07 -0.05 -0.08 -0.07 -0.11 -0.04 -0.03\n\n-0.10 -0.06 -0.09 -0.06 -0.06 -0.04 -0.04 -0.04 -0.09 -0.04 -0.05\n\n-0.03 -0.01 -0.02 -0.02 -0.02 -0.01 -0.00 -0.00 -0.01 -0.01\n\n-0.02 -0.01 -0.02\n\n-0.02 -0.01 -0.01 -0.00 -0.01 -0.00 -0.00\n\n(f) CSBench\n\nFigure 4: Performance comparison of routing strategies across MMLU and MMLU-Pro datasets.\n\nmmlu_pro 0.0 0.2 0.4 0.6 0.8 Accuracy no retrieval all sources qwen3-4b (plain) qwen3-8b (plain) qwen3-32b (plain) qwen3-4b (cot) qwen3-8b (cot) qwen3-32b (cot) Upper Bound", "retrievers. To investigate this, inspired by prior work demonstrating benefits from adding reranking to the RAG pipeline (Shao et al., 2024; Yu et al., 2024), we apply reranking to the top-30 retrieved results. As shown in Figure 3, reranking yields only marginal gains across datasets. This suggests that improving retrieval quality through reranking is insufficient in mixture-of-knowledge scenarios. The retriever's limited capacity and restricted access to relevant knowledge highlight the need for deeper integration between knowledge sources, retrieval mechanisms, and generative models.", "2.5 RQ 4: Evaluating LLMs as Routers for Mixture-of-Knowledge Retrieval\n\nPrevious analyses highlight the need for adaptive retrieval mechanisms that dynamically route queries to the most relevant corpus based on topic context. Here, we investigate whether current LLMs can effectively perform query routing. Figure 4 evaluates LLMs from the Qwen-3 series (4B, 8B, and 32B parameters) as 'routers' that select among heterogeneous knowledge sources at inference time. We compare plain prompting versus chain-of-thought prompting on the MMLU and MMLU-Pro datasets, which cover diverse general and professional domains.", "2.5 RQ 4: Evaluating LLMs as Routers for Mixture-of-Knowledge Retrieval\n\nSpecifically, we benchmark: (1) no-retrieval baselines; (2) static retrieval from all corpora ('all sources'); (3) LLM-prompted routing variants (plain and chain-of-thought); and (4) an oracle router upper bound. Surprisingly, neither prompting strategy consistently outperforms static retrieval. In fact, both routing approaches often underperform compared to simply retrieving from all sources, and occasionally fall below no-retrieval baselines. Chain-of-thought prompting provides only marginal improvements, while scaling model size yields negligible or even negative returns.", "2.5 RQ 4: Evaluating LLMs as Routers for Mixture-of-Knowledge Retrieval\n\nWe attribute this failure to two main factors: (1) Inaccurate relevance estimation: Without dedicated training, LLMs struggle to reliably identify which corpus holds the needed information, especially amid overlapping or stylistically diverse", "2.5 RQ 4: Evaluating LLMs as Routers for Mixture-of-Knowledge Retrieval\n\ncorpora. Minor routing errors propagate to poor retrieval quality. (2) Training-inference mismatch: LLM training typically lacks explicit multi-source comparison tasks, limiting the effectiveness of prompt-based routing as a meta-reasoning problem. Future directions should focus on learned routing modules trained with supervision or reinforcement learning, or on tightly integrated RAG systems that jointly optimize source selection and generation.", "3 Method\n\n3.3 RAG-based Pseudo-label Generation\n\nThis section presents RAG-based pseudo-label generation in three steps: feature extraction, storage optimization, and retrieval-based generation.", "3.3.1 Feature Extraction. To construct the vector database, we use the DINOv2 [36] Small model as the feature extractor ( FE ). DINOv2 Small is chosen for its efficient performance and relatively small parameter count, offering results comparable to ResNet50 [16], but with superior image representations due to its self-supervised training approach. An alternative, such as using a ResNet-based feature extractor, would generate a 7 \u00d7 7 grid of tokens, resulting in only 49 token-mask pairs per image, which has less image representation. Moreover, using ResNet could yield inaccurate downsampled masks, negatively impacting performance. This is why we avoided using a pyramid-style ViT backbone [30, 43, 44].", "For each image in the training set, we extract feature vectors from the final layer of the feature extractor and pair them with corresponding downsampled mask regions, yielding vector-mask pairs. The input images are resized to 224 \u00d7 224, and with a patch size of 14, the extracted feature vectors form a 16 \u00d7 16 grid. As a result, each image generates 256 feature vectors, each corresponding to a resized 16 \u00d7 16 mask. For simplicity and resource efficiency, we omit the use of class tokens. The feature vectors and corresponding mask values are formally represented as follows:\n\nHere, I denotes the input image, and \ud835\udc61 \ud835\udc56 represents the \ud835\udc56 -th token (patch) of the image. Each \ud835\udc63 \ud835\udc56 is the feature vector corresponding to the token \ud835\udc61 \ud835\udc56 , extracted by the feature extractor. The value \ud835\udc5a \ud835\udc56 is the corresponding mask value for the token \ud835\udc61 \ud835\udc56 , which lies within the range [ 0 , 1 ] due to the downsampling process. Finally, \ud835\udc41 is the total number of tokens (or patches) in the image.", "3.3.2 Optimizing Storage. Despite the resource efficiency of DINOv2 Small, the size of the resulting vector database still remains substantial. With 4040 images, and each image generating 256 vector-mask pairs, the total number of vector-mask pairs can be calculated as: 4040 \u00d7 256 \u2248 10 . 3424 \u00d7 10 7 \u2248 103.424 million . The scale of this dataset presents challenges, particularly in terms of retrieval time and the computational resources required for subsequent processing. To mitigate these issues, we apply unsupervised clustering-specifically the KMeans algorithm to compress the database while retaining its representative capacity for segmentation tasks. The clustering process is formalized as follows:", "Figure 2: Architectural overview of the proposed RAG-SEG, consisting of two stages: (1) RAG system construction via feature extractor and vector database indexing; and (2) inference through retrieval-based mask generation and refinement with SAM2.\n\nFeature Extractor ( DINOv2-small ) Segmentation Model (SAM2) Build RAG System Inference Phase RAG-SEG Pipeline \u2744 Data Flow Process Flow\n\nPost process & Prompt Flow\n\nFrozen\nTrain Images\nVector Database\n\n(token vector, mask value)\n\nDownsample Ground Truth Test Image Prediction Retrieval Generation Reshape Upsample Initial Prediction First RAG token vector mask value Second SEG\n\nwhere D denotes the original set of vector-mask pairs, C represents the clustered set, and \ud835\udc3e is the number of clusters. Experimental results indicate that setting \ud835\udc3e = 4096 achieves a favorable trade-off between storage reduction and segmentation accuracy.", "This clustering approach significantly reduces storage requirements and enhances retrieval efficiency, enabling faster access to relevant information (vector-mask pairs). The theoretical basis for this method lies in the observed similarity between adjacent patches and their corresponding masks within the same image, as well as the similarity across different images within the same task.", "3.3.3 Retrieval and Generation. Given a query image, we extract token-wise features and retrieve the top\ud835\udc58 most similar vector-mask pairs from the database using a similarity function \ud835\udc53 (e.g., L2, Inner Product (IP), or Cosine). In practice, IP yields the best results.", "Each stored vector v \ud835\udc57 is assigned a mask value \ud835\udc5a \ud835\udc57 \u2208 [ 0 , 1 ] . For each query token q \ud835\udc56 , its pseudo-label \u02c6 \ud835\udc5a \ud835\udc56 is computed by averaging the mask values of the top\ud835\udc58 retrieved vectors from the database:\n\nEmpirically, using \ud835\udc58 = 1 (i.e., nearest neighbor) achieves the best segmentation performance while simplifying computation.", "Thefinal segmentation mask \u02c6 \ud835\udc40 \ud835\udc5e is formed by aggregating pseudolabels across all token positions:\n\nThis retrieval-based generation produces high-quality pseudo-labels without requiring model fine-tuning, leveraging external knowledge encoded in the feature database.", "3.4 SAM-based Refinement\n\nWhile RAG-based segmentation provides robust coarse localization by exploiting recurring patterns such as occlusion and texture (see Figure 4), its output masks often lack saturation and fine structural detail. Traditional post-processing methods, e.g., Conditional Random Fields (CRF) [25], are ineffective due to the similar appearance statistics of foreground and background in camouflaged scenes. Conversely, off-the-shelf SAM shows limited efficacy in camouflaged object detection without task-specific training. Nonetheless, SAM's prompt-driven design enables integration of detailed information into segmentation masks. Leveraging these complementary strengths, we employ SAM2 as a refinement module, guided by preliminary RAG-generated masks.", "Post-Processing Optimization. To enhance mask prompts for SAM2, we evaluate various thresholding strategies on initial RAG outputs. Experiments indicate a threshold of 0.3 optimally balances segmentation quality and computational efficiency, effectively suppressing noise while preserving structural details to boost SAM2 refinement. Additionally, we explore point prompts to further improve refinement (see Appendix)."]}
{"type": "entry", "index": 10, "question": "why is everyone saying RAG doesn\u2019t scale? how are people fixing that?", "answer": "People say \u201cRAG doesn\u2019t scale\u201d for a few related reasons:\n\n1. **Compute and cost blow up**\n   - Naive RAG often uses full 16\u2011bit large models for generation, which demand a lot of memory and make inference slow and expensive as traffic grows or you support many domains.[1]\n   - Some retrieval granularities (like token\u2011level retrieval) give great accuracy on rare or out\u2011of\u2011domain cases but introduce heavy computation overhead during inference.[6]\n   - Building and updating large knowledge stores (e.g., knowledge graphs, big vector indices) can take a long time, causing downtime whenever you refresh content.[1]\n\n2. **Quality doesn\u2019t improve proportionally**\n   - Larger base models see *diminishing returns* from retrieval: retrieval helps small models more, while big models often internalize much of the knowledge already.[4]\n   - Retrieval is often noisy: similarity search can pull in semantically similar but irrelevant chunks, leading to \u201cneedle\u2011in\u2011a\u2011haystack\u201d failures and retrieval hallucinations.[1][3][4]\n   - Standard tricks like reranking and query routing only bring limited gains in real\u2011world settings.[4]\n\nHow people are trying to fix it:\n\n1. **Cheaper, more efficient generators**\n   - Use quantized models so the generator is lighter and mainly rewrites already\u2011retrieved info, instead of doing everything itself.[1]\n\n2. **Better retrieval and embeddings**\n   - Combine multiple embedding models and select answers based on confidence (Confident RAG) to improve robustness over vanilla RAG.[2]\n   - Multi\u2011agent setups like AC\u2011RAG use a \u201cDetector\u201d to challenge a \u201cResolver,\u201d iteratively refining retrieval and reducing hallucinations.[3]\n\n3. **Smarter generators for RAG**\n   - Instruction\u2011tune models specifically for RAG skills: filtering relevant info, combining passages, and doing RAG\u2011specific reasoning with a \u201cthink before answering\u201d process (HIRAG).[5]\n\n4. **Alternatives to heavy retrieval**\n   - Use learned memory modules (e.g., Memory Decoder) that provide differentiable, full\u2011dataset access without expensive per\u2011query retrieval and custom datastores.[6]", "tags": ["scaling_cost"], "full_faithfulness": 1.0, "full_answer_relevancy": 0.9610566209, "seed_faithfulness": 0.9032258065000001, "seed_answer_relevancy": 0.9598293226000001, "context_relevance": 0.8347132867456765, "context_relevance_error": null, "context_relevance_reason": "The context passages provide substantial information related to the scalability issues of RAG, addressing the user's question about why RAG doesn't scale and how people are fixing that. Specific details about naive RAG implementations, challenges with memory demands, and proposed solutions like quantized models and the K 2 RAG framework are directly relevant. However, while the context is informative, it could be more explicitly connected to the user's inquiry about current fixes, which slightly limits the overall clarity of the response.", "structure_citation": 0.6026161323896672, "structure_error": null, "structure_reason": "The Actual Output includes several claims supported by citations, but there are issues with citation matching. For instance, claims about compute and cost blow-up are supported by citation [1], which is present in the Context, but the claim about retrieval being noisy is cited with [1], [3], and [4], where [1] and [4] are relevant, but [3] does not match the context provided. Additionally, some claims lack citations altogether, such as the assertion about instruction-tuning models for RAG skills. Overall, while there is a good attempt to support claims with citations, the presence of unmatched markers and unsupported claims affects the overall score.", "hallucination_score": 0.85, "hallucination_error": null, "hallucination_reason": "The response effectively identifies and explains the reasons behind the claim that RAG doesn't scale, addressing key issues such as computational costs and diminishing returns on larger models. It also outlines various strategies being implemented to mitigate these challenges, which aligns well with the input's inquiry. However, while the output is detailed and informative, it could benefit from a more explicit connection to the context provided, particularly regarding the limitations of quantized models and their impact on performance, which could enhance the overall accuracy of the claims made.", "contexts_used": ["1 Introduction\n\ndifferent setbacks associated with the LLM fine-tuning process such as improving the final efficacy as well as reducing computational resources.", "Fine-tuning LLMs is usually done on full LLMs and is generally not advised to fine-tune on or convert pre-finetuned models into quantized models due to potential loss of knowledge retention and accuracy which can be attributed to lower precision of the model's weights. Even though existing techniques like QLoRA have shown results which indicate minimal performance degradation upon the resulting fine-tuned quantized models which are based on their Guanaco models, this is not applicable to all LLMs [3]. For example an empirical study [4] evaluated different LLAMA3 quantizations and fine-tune quantizations has shown a significant reduction in LLAMA3-8B scores on the MMLU benchmarks when compared to the standard full 16-bit model and a QLoRA fine-tune-quantization to 4bits. This is because although all LLMs follow the basic transformer architecture there are still differences in model designs which can impact the quality of fine-tunes and hence even previously considered efficient methods such as QLoRA face their limitations with newer models [4].", "Since 2020 the concept of RAG has since evolved to include many different implementations and approaches to performing information retrieval to enhance LLM answering capabilities on new datasets [Figure 1]. This highlights how the benefits gained from RAG are being researched in depth to further optimize this process.", "Answer Accuracy: Naive RAG implementations often struggle with the \"Needle-in-a-Haystack\" problem [8], where the generator model fails to provide accurate answers due to an inability to extract relevant information from irrelevant or excessively long contexts. This issue typically arises from poorly optimized chunk sizes and retrieval processes [9]. Optimizing chunk size is critical and challenging, as smaller chunks risk losing essential context, while larger chunks may introduce irrelevant information and unnecessarily increase context length [Figure 2]. Optimizing the retrieval process to maximize the relevance of retrieved information blocks is crucial. This ensures that irrelevant data, which could act as noise and obscure the correct information, does not compromise the generator LLM's ability to accurately", "Figure 2: Graph showing how LLMs prefer extracting information from top or bottom of context to answer a question [7].\n\n20 Total Retrieved Documents (~4K tokens)\n\n5th lOth 15th 2Oth\n\nPosition of Document with the Answer\n\n3.5-turbo-0613\n\ngpt-3.5-turbo-0613 (closed-book)\n\ngpt-\n\nanswer the question. Another limitation of naive RAG systems is that embeddings which are similar but not actually relevant to answer the question might be retrieved [10] as the original sentences might be semantically similar or carry a similar meaning but might not be right in helping answer the question in that context.", "Scalability: Naive implementations of Retrieval-Augmented Generation (RAG) often rely on 16-bit floating-point large language models (LLMs) for the generation component. However, this approach introduces significant scalability challenges due to the increased memory demands required to host the LLM as well as longer inference times due to using a higher precision number type. To enable more efficient scaling, it is crucial to integrate methods or techniques that reduce the memory footprint and inference times of generator models. Quantized models offer more scalable solutions due to less computational requirements, hence when developing RAG systems we should aim to use quantized LLMs for more cost effective deployment as compared to a full fine-tuned LLM whose performance might be good but is more expensive to deploy due to higher memory requirements. A quantized LLM's role in the RAG pipeline itself should be minimal and for means of rewriting retrieved information into a presentable fashion for the end users to interpret.", "Additionally, in naive RAG implementations, information store components, such as Knowledge Graphs, face lengthy training times for their creation and updating. This creates scalability challenges, as it increases the downtime required to update the information store with new knowledge in a production environment. Hence an effective data pre-processing step should be introduced to reduce the training corpus size and hence training times while preserving as much information as possible.", "Solution: We propose the K 2 RAG framework which addresses the following 4 research goals based on the aforementioned answer accuracy and scalability issues characteristic of naive RAG implementations:\n\nGoal 1. Reduce information store such as Knowledge Graph and Spare and Dense vector database creation times.", "Goal 2. Reduce chances of LLMs suffering from \"Needle in Haystack\" Problem.\n\nGoal 3. Increase rate of retrieving relevant passages for answering the question.", "Goal 4. Alleviate time and computational cost associated with full LLMs.\n\nK 2 RAG incorporates various different concepts covered in the literature in a novel way to improve the performance and offer a more resource efficient and accurate RAG pipeline compared to naive RAG methods outlined in the literature.", "Abstract\n\nRecently, as Large Language Models (LLMs) have fundamentally impacted various fields, the methods for incorporating up-to-date information into LLMs or adding external knowledge to construct domainspecific models have garnered wide attention. Retrieval-Augmented Generation (RAG), serving as an inference-time scaling method, is notable for its low cost and minimal effort for parameter tuning. However, due to heterogeneous training data and model architecture, the variant embedding models used in RAG exhibit different benefits across various areas, often leading to different similarity calculation results and, consequently, varying response quality from LLMs. To address this problem, we propose and examine two approaches to enhance RAG by combining the benefits of multiple embedding models, named Mixture-Embedding RAG and Confident RAG. Mixture-Embedding RAG simply sorts and selects retrievals from multiple embedding models based on standardized similarity; however, it does not outperform vanilla RAG. In contrast, Confident RAG generates responses multiple times using different embedding models and then selects the responses with the highest confidence level, demonstrating average improvements of approximately 10% and 5% over vanilla LLMs and RAG, respectively. The consistent results across different LLMs and embedding models indicate that Confident RAG is an efficient plug-andplay approach for various domains. We will release our code upon publication.", "\u2217 Corresponding Author: Shiting Chen (u3011355@connect.hku.hk), Jinsong Chen (jinsong.chen@live.com)", "ENHANCING RETRIEVAL AUGMENTATION VIA ADVERSARIAL COLLABORATION\n\nLetian Zhang \u22c6 \u2020\n\nGuanghao Meng \u22c6 \u2020\n\nXudong Ren \u2020 Yiming Wang \u2021 Shu-Tao Xia \u2020 \u2020 Tsinghua University\n\n\u2021 Huawei Technologies Ltd.", "Retrieval-augmented Generation (RAG) is a prevalent approach for domain-specific LLMs, yet it is often plagued by 'Retrieval Hallucinations'-a phenomenon where fine-tuned models fail to recognize and act upon poor-quality retrieved documents, thus undermining performance. To address this, we propose the Adversarial Collaboration RAG (AC-RAG) framework. AC-RAG employs two heterogeneous agents: a generalist Detector that identifies knowledge gaps, and a domain-specialized Resolver that provides precise solutions. Guided by a moderator, these agents engage in an adversarial collaboration, where the Detector's persistent questioning challenges the Resolver's expertise. This dynamic process allows for iterative problem dissection and refined knowledge retrieval. Extensive experiments show that AC-RAG significantly improves retrieval accuracy and outperforms state-ofthe-art RAG methods across various vertical domains. 1\n\nIndex Terms -Retrieval-Augmented Generation (RAG), Adversarial Collaboration, Multi-Agent Systems", "This study evaluates the effectiveness of Retrieval-Augmented Generation (RAG) systems across diverse knowledge sources and tasks. Utilizing MASSIVEDS, a large-scale datastore, the authors found that retrieval mainly benefits smaller models while larger models exhibit diminishing returns. The analysis shows no single retrieval source consistently outperforms others, highlighting the necessity for adaptive retrieval strategies. Furthermore, efforts such as reranking and query routing have limited improvement on overall performance. Key challenges include inaccurate relevance estimation and training-inference mismatch, suggesting a need for improved integration of retrieval mechanisms with language models. The findings indicate a critical gap in deploying RAG systems effectively in real-world scenarios, particularly for larger models that increasingly internalize domain knowledge without retrieval support.", "Abstract\n\nRetrieval-augmented generation (RAG) has become a fundamental paradigm for addressing the challenges faced by large language models in handling real-time information and domainspecific problems. Traditional RAG systems primarily rely on the in-context learning (ICL) capabilities of the large language model itself. Still, in-depth research on the specific capabilities needed by the RAG generation model is lacking, leading to challenges with inconsistent document quality and retrieval system imperfections. Even the limited studies that fine-tune RAG generative models often lack a granular focus on RAG task or a deeper utilization of chain-of-thought processes . To address this, we propose that RAG models should possess three progressively hierarchical abilities (1) Filtering: the ability to select relevant information; (2) Combination: the ability to combine semantic information across paragraphs; and (3) RAGspecific reasoning: the ability to further process external knowledge using internal knowledge. Thus, we introduce our new RAG instruction fine-tuning method, Hierarchical-Thought Instruction-Tuning Retrieval-Augmented Generation (HIRAG) incorporates a \"think before answering\" strategy. This method enhances the model's open-book examination capability by utilizing multi-level progressive chainof-thought. Experiments show that the HIRAG training strategy significantly improves the model's performance on datasets such as RGB, PopQA, MuSiQue, HotpotQA, and PubmedQA.", "7. Related Work\n\nRetrieval-Augmented Generation Retrieval-Augmented Generation (RAG) enhances language models by incorporating knowledge from external sources, with retrieval granularity ranging from documents [Chen et al., 2017] to passages [Guu et al., 2020, Izacard et al., 2023, Lewis et al., 2020] to tokens [He et al., 2021, Khandelwal et al., 2019, Min et al., 2022, Yogatama et al., 2021]. Tokenlevel retrieval achieves superior performance for rare patterns and out-of-domain scenarios but introduces substantial computation overhead during inference. While non-differentiable retrieval mechanisms prevent end-to-end optimization and memory token approaches [Chevalier et al., 2023] enable differentiable access but are limited to local contexts, Memory Decoder provides both differentiable optimization and full-dataset knowledge access without expensive retrieval operations or model-specific datastores.", "Domain Adaptation Domain adaptation techniques have evolved from domain-specific pre-training (SciBERT [Beltagy et al., 2019], BioBERT [Lee et al., 2020], ClinicalBERT [Huang et al., 2019]) to parameter-efficient methods like LoRA [Hu et al., 2022] and adapters [Diao et al., 2021, 2023, Wang et al., 2020]. However, these approaches require model-specific modifications, preventing generalization across architectures. Memory Decoder addresses this limitation by providing a domain-specific memory module that enhances multiple frozen language models without parameter modifications, enabling cross-model adaptation within tokenizer families and efficient cross-tokenizer transfer with minimal additional training."]}
{"type": "entry", "index": 11, "question": "are there ways to make rag more accurate without retraining the llm?", "answer": "Yes. The papers describe several ways to make RAG more accurate that all work without retraining the underlying LLM:\n\n1. Improve the knowledge base\n- Curate a focused, high\u2011quality corpus that exactly matches your domain (e.g., regulatory texts, curriculum documents, domain manuals). This lets the model ground its answers in the right material without changing its weights [2][3][6].\n- Keep the corpus fresh: you can add or update documents as regulations, curricula, or guidelines change, with no LLM retraining needed [3][4][6].\n\n2. Improve chunking and indexing\n- Split documents into sensible passages so each chunk contains a coherent piece of information the model can actually use [1][2][3].\n- Use appropriate embedding models for your domain (e.g., math or domain\u2011specific embeddings, or even audio\u2011text joint embeddings in music) to get more semantically accurate retrieval [1][5].\n\n3. Improve retrieval strategies\n- Move from \u201cvanilla RAG\u201d to enhanced retrieval: the GridCodex work shows that improved retrieval can reach higher domain adaptability and strong handling of long documents, still without fine\u2011tuning the LLM [2].\n- Use multi\u2011stage query refinement: explain terms, expand the user\u2019s question, and inject context before the final retrieval step to better match complex regulations or long texts [2].\n- Support multi\u2011hop retrieval over long, layered documents (e.g., via frameworks like RAPTOR) so the system can pull together scattered rules or conditions [2].\n- Use confidence\u2011based methods such as Confident RAG, where multiple retrieval rounds with different embeddings are run and the LLM answers only when it is sufficiently confident; this significantly improves accuracy over vanilla RAG and plain LLMs [5].\n\nAll of these keep the LLM frozen and work purely by improving what and how you retrieve.", "tags": ["scaling_cost"], "full_faithfulness": 0.9333333333, "full_answer_relevancy": 0.9607122439, "seed_faithfulness": 1.0, "seed_answer_relevancy": 0.9607194649, "context_relevance": 0.960070179862368, "context_relevance_error": null, "context_relevance_reason": "The context passages provide comprehensive information about Retrieval-Augmented Generation (RAG) and its advantages, specifically addressing the user's question about improving RAG accuracy without retraining the LLM. The passages explain how RAG allows for dynamic retrieval of external knowledge during inference, enhancing factual accuracy and adaptability, which directly aligns with the user's inquiry.", "structure_citation": 0.7593565731215988, "structure_error": null, "structure_reason": "The Actual Output effectively addresses the question about improving RAG accuracy without retraining the LLM, providing multiple strategies supported by citations. However, while most claims are backed by the Context, some citations ([3], [4], [5], [6]) do not have direct matches in the provided Context, which slightly undermines the overall support for the claims made.", "hallucination_score": 0.9345076751000908, "hallucination_error": null, "hallucination_reason": "The response accurately addresses the input question about improving RAG accuracy without retraining the LLM by providing multiple specific strategies, such as enhancing the knowledge base, chunking, indexing, and retrieval strategies. Each suggestion is well-supported by relevant details from the context, demonstrating a strong understanding of RAG principles. The only minor shortcoming is the lack of explicit citations for some claims, which could enhance the credibility of the suggestions.", "contexts_used": ["3. RETRIEVAL AUGMENTED GENERATION\n\n3.1 RAG Framework\n\nRetrieval-Augmented Generation (RAG) enhances the capabilities of LLMs by combining their generative abilities with access to external knowledge. Instead of relying solely on parametric memory, RAG retrieves relevant passages from an external database during inference time to ground responses in factual context.", "3.1.1 Indexing\n\nThe first step in RAG is constructing a searchable knowledge database. This involves segmenting a large corpus into fixed-size text passages (chunking), followed by representing each passage using an embedding models. Various embedding models can be used for indexing:\n\nSparse Embeddings [16, 17] use term frequency-based scoring to match exact keywords, offering fast and interpretable retrieval for large-scale datasets.", "Dense Embeddings [17-19] map questions and documents into a shared vector space, enabling semantic matching beyond keyword overlap.", "Audio-Text Joint Embeddings [20-24] extend this further by jointly embedding text with the audio modality. By leveraging contrastive learning between audio and text, they can serve as more domain-specialized text embedding models for music-related tasks.", "3.1.2 Retrieval\n\nFormally, the retriever R is defined as a function:", "where q is the input question, D is the entire database of text passages, and c \u2282 D is the filtered context consisting of the topk passages, such that | c | = k \u226a | D | . Each passage 1 p \u2208 D is scored based on its similarity to the input question using cosine similarity between their embeddings:\n\nHere, E ( \u00b7 ) denotes an embedding function that maps both questions and passages into a shared vector space. The retriever ranks all passages in D by their similarity scores and selects the topk passages to form c , which serve as the external context for the generation step.", "3.1.3 Generation\n\nThe retrieved context c is provided to a generator LLM, which produces an output sequence using next-token prediction. Each token x i is generated conditioned on the input query q , the retrieved context c , and the previously generated tokens x <i :\n\nThis structure enables the model to dynamically incorporate external knowledge during inference, improving factual accuracy and adaptability without retraining.", "3.2 RAG vs. Fine-tuning\n\nLLMs often struggle with specialized tasks such as MQA due to limited exposure to domain-specific knowledge during pretraining. To address this, two primary domain adaptation strategies are commonly used: fine-tuning and RAG. Fine-tuning is akin to a closed-book exam: the model internalizes domain knowledge during training and must rely solely on that knowledge at inference. While effective for learning structured formats or stylistic patterns [26, 27], it is resource-intensive and inflexible when adapting to new or frequently changing knowledge. In contrast, RAG is like an open-book exam: the model dynamically retrieves relevant information from an external knowledge source during inference. This enables LLMs to access up-todate and specialized information without retraining. Prior studies [28,29] show that RAG improves factual accuracy, mitigates hallucinations, and provides greater transparency by allowing source verification. It is also more scalable and economically efficient, as it does not require updating model parameters [27]. These benefits are especially useful in rapidly evolving domains like music, where new artists, compositions, and styles continuously emerge.", "3.3 RAG with Fine-tuning\n\nWhile fine-tuning typically relies on question-answer pairs, it does not always emphasize learning to extract relevant information from the context provided alongside the question. In standard fine-tuning, the model is trained to\n\n1 A passage refers to a portion of a document relevant to a query [25].", "directly map a question to its answer without fully leveraging any external context that might be available. As a result, the model may struggle to utilize background information effectively, especially when answering questions that require specialized or up-to-date knowledge.", "To address this limitation, we adopt a RAG-style finetuning approach using a dataset consisting of (context, question, answer) triples. Unlike standard QA fine-tuning, which relies solely on the question, our method introduces an external relevant passage p for the input question q . This enables the model to learn how to incorporate relevant contextual information during answer generation. Both approaches share the same next-token prediction objective, but differ in the input they condition on. In standard finetuning, the model is trained as follows:\n\nwhere the model predicts each answer token x i based only on the question and the previously generated tokens. In contrast, RAG-style fine-tuning conditions the generation not only on the question but also on the relevant passages as context:\n\nwhere c is the relevant passage retrieved from an external corpus. By incorporating c as an additional context, the model is encouraged to utilize external knowledge when generating answers. This strategy improves the model's ability to ground its responses in retrieved evidence, leading to more accurate and contextually appropriate answers. During RAG fine-tuning, we used gold passages with high relevance to the answers, ensuring the model learns to effectively utilize contextual information.", "Introduction\n\nIn the accelerating transition toward renewable energy, electricity has become the central medium linking diverse sustainable energy sources with end users, placing unprecedented demands on the robustness and efficiency of power grids (Gri 2016; Ren 2017; Ntomaris et al. 2014). Grid codes-the rules and regulations that govern the reliable operation of power systems-specify critical technical requirements for integrating renewable energy resources, thereby safeguarding grid stability and security. Yet, these codes differ significantly across countries and regions (Ullah et al. 2025), rendering grid code compliance a complex but indispensable challenge for electricity infrastructure providers seeking international expansion.", "Conventional grid code interpretation relies heavily on human expertise, as illustrated in Figure 1. Grid codes are often written with dense, region-specific terminology, technical jargon, and implicit assumptions that demand deep domain knowledge. To navigate these complexities, electricity providers typically consult local specialists or external agencies, which significantly increases operational costs and project timelines. Human interpretation, however, is prone to inconsistency, stemming from ambiguities in the documents or simple human error. The challenge intensifies with lengthy, multi-layered regulations, where manual\n\nFigure 1: Conventional human grid code interpretation workflow. Specialists manually read and verify raw grid code files to answer user queries, which is time-consuming and prone to errors.\n\nUser query What are overvoltage protection\n\nrequirements?\n\nHuman\ninterpretation\n\u2026\n\nRaw gride code files\n\nProtection point: 1.1 pu\n\nProtection time: 60 mins\n\nDays/hours of searching & prone to errors & unscalable\n\nreview becomes prohibitively time-consuming and costly. This dilemma is further compounded for companies operating across multiple jurisdictions, where the limited capacity of specialists creates a critical bottleneck. These challenges underscore the urgent need for scalable, automated solutions that can efficiently process large volumes of regulatory text, deliver reliable compliance interpretation, and reduce the risk of errors.", "The rapid advancement of large language models (LLMs) (OpenAI 2023; Int 2024; Minaee et al. 2025) offers an intriguing solution for grid code reasoning and compliance. With extraordinary text comprehension ability, LLMs excel at text understanding and question answering tasks. However, lack of specialized domain knowledge and susceptibility to hallucinations (Xu, Jain, and Kankanhalli 2025; Huang et al. 2025) challenge their enterprise and industrial applications. Table 1 have summarized mainstream LLM domain adaptation methods. While finetuning with domain-specific datasets can significantly mitigate some of these issues (J et al. 2024; Jeong 2024), it requires high-quality datasets, which are difficult to obtain in energy industry due to the data confidentiality and cumbersome work of data washing and formatting. Additionally, substantial computation resource and LLM engineering experience required can hinder the finetuning of LLMs (Hu et al. 2021; Chen et al. 2024), especially in mid-size or small enterprises and conventional", "| | Method | Domain Adaptability | Deployment Cost | Data Requirement | Long Doc Handling |\n|---:|:-------------------|:----------------------|:------------------|:-------------------------|:--------------------|\n| 0 | General LLM | \u2717 Low | \u2713 Low | \u2717 None (pretrained only) | \u2717 Poor |\n| 1 | Fine-Tuning | \u2713 High | \u2717 High | \u2717 Requires labeled data | \u2717 Moderate |\n| 2 | RAG | \u2713 Medium | \u2713 Medium | \u2713 Unlabeled documents | \u2717 Varies |\n| 3 | Improved Retrieval | \u2713 High | \u2713 Medium | \u2713 Unlabeled documents | \u2713 Strong |\n\nTable 1: Comparison of mainstream approaches for industry-specific LLM adaptations.\n\nindustries, such as energy and power grids.", "Recently, retrieval-augmented generation (RAG) has emerged as a cost-effective alternative to fine-tuning, offering higher accuracy and more flexible deployment for enterprise applications (Lewis et al. 2021; Fan et al. 2024). By enabling large language models (LLMs) to seamlessly access external knowledge bases during inference, RAG reduces hallucinations and improves factual consistency without retraining. Constructing such knowledge bases is straightforward: raw documents are split into chunks, embedded, and stored in vector databases-an approach that can be fully deployed locally without extensive data cleaning. As a result, RAGprovides a private, scalable, and low-overhead solution for adapting LLMs to industrial use cases (Balaguer et al. 2024).", "In this paper, we demonstrate a RAG-driven AI framework for grid code interpretation. We build external knowledge bases with grid code documents and orchestrate them with high performance open-source LLMs, such as DeepSeek (Guo et al. 2025; Liu et al. 2024) and Qwen3 (Yang et al. 2025), which demonstrate strong reasoning ability and scalability. Our system achieves answer quality up to 88%, as validated by experts from Southern Grid and other external authorities. We believe our framework also holds potential for broader applications beyond compliance interpretation-such as proactively identifying potential regulatory violations and generating power grid simulation configurations. These capabilities could significantly accelerate regulatory workflows and reduce compliance risk in electricity industry.", "In this paper, we introduce a RAG-driven AI framework for grid code interpretation, leveraging domain-specific knowledge bases constructed from regulatory documents and orchestrated with high-performance open-source LLMs such as DeepSeek (Guo et al. 2025; Liu et al. 2024) and Qwen3 (Yang et al. 2025). These models exhibit strong reasoning capability and scalability, enabling reliable compliance interpretation. Expert validation with Southern Grid and external authorities shows that our system achieves answer quality of up to 88%. Beyond compliance interpretation, our framework also holds promise for proactive regulatory monitoring, such as detecting potential violations and generating simulation-ready grid configurations, thereby accelerating regulatory workflows and mitigating compliance risks in the electricity sector.\n\nOur contributions are as follows:\n\nFirst RAG-driven framework for power grid code reasoning and compliance. We present the first RAGbased agent tailored for automated reasoning and compliance verification in grid codes, addressing domain-\n\nspecific challenges such as technical jargon, implicit assumptions, and regulatory ambiguity.", "Optimized multi-stage query refinement and enhanced retrieval. We design a multi-stage refinement pipeline-incorporating term explanation and context injection-that improves query understanding and retrieval precision. This is further integrated with the RAPTOR framework (Sarthi et al. 2024) to support robust, multihop retrieval across lengthy regulatory documents.", "Demonstrated accuracy and real-world applicability. Extensive evaluation shows substantial improvements over baseline LLM and RAG methods in both retrieval accuracy and compliance reasoning, enabling trustworthy, explainable outputs for real-world deployments.", "2 Related Work\n\n2.1 The Evolution of Automated Question Generation (AQG) in Educational NLP\n\nAutomated Question Generation (AQG) has been a long-standing goal in Natural Language Processing (NLP) for education. Early approaches often relied on rule-based systems and templates, which were rigid and difficult to scale. The advent of neural networks and, more recently, large language models (LLMs) has revolutionized the field, enabling the generation of fluent and complex questions from source text.[4] Recent studies highlight that while LLMs are", "2.1 The Evolution of Automated Question Generation (AQG) in Educational NLP\n\npowerful generators, the quality of their output for specialized educational purposes is highly dependent on the context provided and the prompting strategy employed.[5] The challenge has shifted from merely generating a syntactically correct question to generating one that accurately tests a specific concept or cognitive skill, such as conceptual reasoning rather than simple factual recall.[5]", "2.2 Grounding Generative Models: The Rise of Retrieval-Augmented Generation (RAG)\n\nRetrieval-Augmented Generation (RAG) has emerged as the foremost technique for addressing the 'grounding problem' of LLMs.[9] First proposed by Lewis et al. (2020), the RAG architecture combines the strengths of a parametric memory (the pre-trained LLM) with a non-parametric memory (an external, retrievable knowledge source).[10] In this hybrid model, the LLM does not rely solely on its internal, static knowledge. Instead, given a prompt, it first retrieves relevant documents from the external source and then uses this retrieved context to inform and constrain its generated output. This approach has been shown to significantly reduce hallucinations and improve the factual accuracy of LLM responses. In educational technology, RAG is particularly valuable. It allows for the creation of AI systems that can provide factually consistent, curriculum-aligned answers and content.[9] Furthermore, because the knowledge base is external, it can be easily updated with new or corrected information (e.g., a revised syllabus or new textbook) without the need for costly and time-consuming LLM retraining.[9] This makes RAG a practical and adaptable solution for developing dynamic educational tools.[14]", "2.3 Generative AI for Low-Resource Languages: Gaps and Opportunities\n\nThe development of GenAI has been heavily skewed towards high-resource languages like English, creating a 'digital language divide'.[11] Languages with smaller digital footprints, such as Bahasa Melayu, face significant challenges, including data scarcity, lack of culturally-aware benchmarks, and poor representation in the massive datasets used to train foundational models.[6] Consequently, even state-of-the-art LLMs often exhibit weaker performance in these languages.[6] While NLP toolkits and resources for Bahasa Melayu exist, they are often general-purpose and may not be sufficient for highly specialized, domain-specific tasks like generating questions that adhere to the nuances of the Malaysian national curriculum.[12] This study positions RAG as a pragmatic strategy to bridge this gap. By forcing the model to ground its output in a curated, local-language corpus (the official curriculum documents), RAG can effectively leverage the power of a global LLM for a local, specialized task, circumventing the need for a massive, domain-specific training dataset.", "2.4 Automated Evaluation of Generated Content: Beyond Lexical Overlap\n\nEvaluating the quality of generated text is a complex challenge. Traditional metrics like ROUGE and BLEU, which rely on lexical overlap, are inadequate for assessing semantic meaning and are thus poorly suited for evaluating educational questions.[18] Semantic Textual Similarity (STS) offers a more robust alternative. STS measures the degree of semantic relatedness between two texts, typically by calculating the cosine similarity between their vector embeddings.[19] This allows for an assessment of whether a generated question aligns thematically with a specific learning objective, even if they do not share the exact same words.[21] However, STS is not without its limitations. Research has shown that standard STS can be ambiguous and may not always correlate well with performance on a specific downstream task.[19] The similarity score depends on the aspect of interest, which can be subjective.[22] This critique motivates the need for a second, complementary evaluation metric that is more functional and task-oriented. This study introduces a novel RAG-based Question-Answering (RAG-QA) validation method to address this gap. This method provides a direct, functional test of a question's contextual validity by assessing whether it can be answered using the core curriculum document as a knowledge source, thereby offering a stricter and more objective measure of curriculum relevance.", "6 Discussion\n\nA key strength of our approach is its robustness to regulatory updates and corpus expansion. Unlike supervised fine-tuning or RLHF, which embed knowledge directly into model parameters and require costly retraining when new information emerges, our RAG pipeline decouples knowledge from the model. The LLM remains fixed while the retriever dynamically accesses an external corpus that can be updated at any time. As new editions of the ITU Radio Regulations are released or additional documents become relevant, they can simply be indexed without retraining, preserving system validity as the regulatory landscape evolves. This modular design also scales naturally to larger corpora, since retrieval performance depends on embedding quality and index structure rather than model size.", "Beyond its technical architecture, our work also contributes the first dedicated evaluation dataset for radio regulations, which can serve as a reusable benchmark for future research. By providing standardized, validated multiple-choice questions derived directly from authoritative sources, this dataset enables systematic comparison of retrieval and generation methods in a legally sensitive domain where no prior benchmark existed. It can support the development of domain-specific models, guide fine-tuning or retrieval design, and foster reproducibility by offering a stable testbed for future studies.", "4 Experiment\n4.1 Experiment Setup\n\nexperiment. These included vanilla LLM, vanilla RAG with different embedding models, mixture-embedding RAG with various combinations of embedding models, and the combined results of confident RAG method.", "Dataset : Gsm8k (Cobbe et al., 2021) is a dataset of approximately 8,500 high-quality, linguistically diverse grade school math word problems designed to support question answering on basic mathematical tasks. These problems typically require 2 to 8 steps of reasoning, mainly involving elementary arithmetic operations, and are solvable by middle school students. The solutions are provided in natural language, emphasizing sequential reasoning rather than complex concepts or variables. The train dataset of Gsm8k was used as the retrieved courpus for RAG, while the first 500 items from the test dataset were used as user questions.", "Corpus : Two types of corpora were selected for retrieval: (1) Mathematics textbooks from OpenStax 1 covering domains such as calculus, algebra and trigonometry. The content was segmented into sub-section. (2) Math QA items from the Gsm8k train dataset. One textbook sub-section and three QA items will be retrieved according to the similarity based on their similarity to the user-provided math question each time.", "Embedding models : We chose four embedding models for encoding, including: allMiniLM-L6-v2 (Face), ModernBERT-large (Warner et al., 2024), MathBERT (Peng et al., 2021) and stsb-roberta-large (Reimers and Gurevych, 2019) models.", "LLMs : To validate our proposed method and ensure repeatability, we selected three LLMs, following (Shao et al., 2025): Qwen2.5Math-7B (Yang et al., 2024), Llama-3.1-8B (Grattafiori et al., 2024) and OLMo-2-11247B (OLMo et al., 2024).", "4.2.1 Vanilla LLM and RAG\n\nTable 1 presents a comparison of three different LLMs with and without RAG using different embedding models. All models achieve significant improvements in accuracy after applying RAG method, with an average improvement of\n\n1 https://openstax.org/subjects/math\n\n5%. The accuracy of Qwen2.5-Math-7B increases from 75.2% to 80.5%, Llama-3.1-8B from 16.6% to 21.3%, and OLMo-2-1124-7B from 21.0% to 26.0%. This demonstrates that incorporating RAG methods can effectively enhance overall model performance on the task.", "4.2.2 Results of Mixture-Embedding RAG method\n\nRegarding the mixture-embedding RAG method (see Table 2), the average accuracies of the three LLMs did not perform well, compared to vanilla RAG. Llama-3.1-8B and OLMo-2-11247B demonstrated similar accuracy as vanilla RAG, with the differences ranging from -0.2% to 0.5%. Surprisingly, Qwen2.5-Math-7B showed a decrease of 5.5% compared to vanilla RAG.", "4.2.3 Results of Confident RAG method\n\nWhile using different embedding models for RAG in multiple rounds, the results of Confident RAG method are shown in Table 3. Four main findings were obtained:\n\nThe accuracy of Confident RAG method surpasses that of both the vanilla RAG (with an average improvement ranging from 3.2% to 4.9%) and vanilla LLM (with an average improvement ranging from 8.1% to 9.9%).", "For each LLM, after applying the Confident RAG method, the accuracy improved by nearly 10% when using the best confidence metric compared to the vanilla LLM.", "Among all confidence metrics, self certainty and distributional perplexity demonstrated the best performance, with average improvements of 9.9% and 9.7%, respectively, over the vanilla LLM. These two metrics also performed well across different LLMs. For instance, there was an increase of 9.1% for Qwen2.5-Math-7B using self certainty as the confidence metric, an increase of 10.4% for Llama-3.1-8B with both metrics, and an increase of 12.3% for OLMo-2-1124-7B using distributional perplexity.", "Regarding the optimal number of embedding models (N) for multi-rounds, no evidence suggests that a larger n yields better accuracy. In our experiments, the accuracy when N=3 was always larger than when N=2. Meanwhile, the accuracy for N=3 and N=4 was similar, with a maximum difference of 1% across the three LLMs. Considering factors such as time cost, GPU capacity, and other constrains, this finding suggests that", "2 Related Works\n\n2.1 Weight Tuning of Treatment Planning\n\nTraditionally, weight tuning in SIBT has been carried out using protocol-based methods [44]. This approach simulates the iterative decision-making process of human planners by starting with predefined clinical objectives and iteratively adjusting these objectives in a flowchart-like manner based on plan quality feedback. In the context of high-dose-rate (HDR) brachytherapy, some researchers have formulated weight tuning as a multi-objective optimization problem, applying Bayesian optimization to identify Pareto-optimal combinations of penalty weights [11]. Other studies have explored the use of reinforcement learning (RL) for automated weight tuning [23] [29]; however, the design of appropriate reward functions and action spaces remains a significant challenge, limiting the practical effectiveness of RL-based approaches. In the domain of external beam radiotherapy, LLMs have demonstrated the ability to emulate planner behavior in adjusting optimization weights with a limited number of clinical planning examples, including text, DVH tables, and dose distribution images [36] [17]. Despite promising early results, such approaches remain largely unexplored in brachytherapy and face challenges in generalizing across diverse cases due to limited domain knowledge integration. Moreover, most existing LLM-based planning methods rely on cloud-hosted APIs, which raises concerns regarding data security, network dependency, and regulatory compliance, thereby limiting their suitability for clinical deployment.", "2.2 Large Language Models\n\nRecent advances in large language models, such as DeepSeek-R1 [8], Gemini [32], GPT-4 [1], LLaMA [7], and QWen [41], have demonstrated remarkable capabilities in instruction following [45], contextual learning [28], and chain-of-thought reasoning [38]. Trained on massive general-domain corpora, these models exhibit emergent behaviors such as in-context learning and few-shot generalization, allowing for integration into downstream workflows with minimal task-specific supervision. However, due to their reliance on publicly available internet data, general-purpose LLMs often underperform in specialized domains such as healthcare, finance, or law, where domain-specific terminology and reasoning are critical.", "To address this, domain-adapted LLMs have been developed through pretraining or fine-tuning on specialized datasets. For instance, in the medical field, models such as BioGPT [18], Med-PaLM [30], and LLaVA-Med [14] integrate structured biomedical knowledge and clinical corpora to improve performance on tasks such as clinical question answering [30], radiology report generation [31], and image-guided diagnosis [16]. While effective, these approaches demand significant computational resources, large volumes of annotated data, and frequent retraining to remain current. This limits their scalability and flexibility in practical clinical settings.", "To overcome these limitations and maintain adaptability without incurring high retraining costs, our proposed approach utilizes a general-purpose LLM that dynamically accesses external domain knowledge through retrieval-augmented generation (RAG). Instead of embedding domain knowledge into model parameters, RAG retrieves relevant external information during inference to supplement generation, offering improved transparency and up-to-date knowledge access. This hybrid approach has shown promise in medical question answering and decision support [16] [43] [2] [37] [24], and forms the foundation of our strategy for enabling general-purpose LLMs to reason over clinical data without domain-specific fine-tuning."]}
{"type": "entry", "index": 12, "question": "fine-tuning vs RAG which does better when?", "answer": "Fine\u2011tuning and RAG each shine in different situations.\n\nWhen fine\u2011tuning does better\n- Stable, slow\u2011changing knowledge: Fine\u2011tuning is like preparing for a closed\u2011book exam. The model \u201cstudies\u201d domain data during training and must rely on what it has memorized later [1]. This works well when the core knowledge doesn\u2019t change often (for example, music theory rules, fixed formats, or writing styles) and when you want the model to reliably follow certain structures or tones [1].\n- Strong control over style/format: Fine\u2011tuning is especially effective for getting consistent answer formats or stylistic patterns [1].\n- Small, focused tasks: If the domain is narrow and not huge (e.g., a specific style of Q&A), training the model directly can be efficient.\n\nWhen RAG does better\n- Rapidly changing or growing knowledge: RAG is like an open\u2011book exam: the model looks up external documents at answer time instead of memorizing everything [1]. This is better when facts keep changing or new items keep appearing (e.g., new artists, songs, or legal/computer\u2011science updates) [1][3][4].\n- Need for transparency and source checking: Because RAG fetches and shows supporting passages, users can verify answers and reduce hallucinations [1]. Many works focus on improving retrieval and context handling for exactly this reason [2][3][4][5][6].\n- Scalability and cost: Updating knowledge just means updating the database, not retraining the whole model, which is cheaper and more scalable than repeatedly fine\u2011tuning [1][4]. Quantized (smaller, cheaper) models can work well as the generator in a RAG pipeline [4].\n\nBest of both: fine\u2011tuned RAG\nSeveral methods fine\u2011tune models specifically to better use retrieved context and ignore distractors, improving factual accuracy and multi\u2011hop reasoning [1][2][3][5][6]. This hybrid is often best when you can afford some training and need both strong reasoning and up\u2011to\u2011date knowledge.", "tags": ["_untagged"], "full_faithfulness": 0.92, "full_answer_relevancy": 0.9480341961000001, "seed_faithfulness": 0.8148148148000001, "seed_answer_relevancy": 0.9499916411, "context_relevance": 0.9654479559913597, "context_relevance_error": null, "context_relevance_reason": "The context passages provide a comprehensive comparison between fine-tuning and Retrieval-Augmented Generation (RAG), directly addressing the user's question about which method performs better and under what circumstances. The detailed explanation of RAG's advantages, such as its ability to dynamically retrieve information and adapt to new knowledge, contrasts effectively with the limitations of fine-tuning, which is described as resource-intensive and inflexible. This alignment with the evaluation steps demonstrates a strong relevance to the user's inquiry.", "structure_citation": 0.7080860216416297, "structure_error": null, "structure_reason": "The Actual Output effectively discusses the strengths of fine-tuning and RAG in various contexts, with multiple claims supported by citations. However, while most citations correspond to relevant snippets in the Context, there are instances where the claims made are not fully substantiated by the provided snippets, particularly regarding the specifics of RAG's advantages. This leads to a slight deduction in points for unsupported claims.", "hallucination_score": 0.8947052090001819, "hallucination_error": null, "hallucination_reason": "The response accurately describes the strengths of fine-tuning and RAG in various contexts, aligning well with the provided context. It effectively explains when fine-tuning is preferable, such as for stable knowledge and stylistic control, and when RAG is advantageous, like in rapidly changing domains. However, while most claims are supported, the mention of 'fine-tuned RAG' could be clearer in its distinction from standard fine-tuning and RAG, which slightly detracts from the overall clarity.", "contexts_used": ["3. RETRIEVAL AUGMENTED GENERATION\n\n3.1 RAG Framework\n\nRetrieval-Augmented Generation (RAG) enhances the capabilities of LLMs by combining their generative abilities with access to external knowledge. Instead of relying solely on parametric memory, RAG retrieves relevant passages from an external database during inference time to ground responses in factual context.", "3.1.1 Indexing\n\nThe first step in RAG is constructing a searchable knowledge database. This involves segmenting a large corpus into fixed-size text passages (chunking), followed by representing each passage using an embedding models. Various embedding models can be used for indexing:\n\nSparse Embeddings [16, 17] use term frequency-based scoring to match exact keywords, offering fast and interpretable retrieval for large-scale datasets.", "Dense Embeddings [17-19] map questions and documents into a shared vector space, enabling semantic matching beyond keyword overlap.", "Audio-Text Joint Embeddings [20-24] extend this further by jointly embedding text with the audio modality. By leveraging contrastive learning between audio and text, they can serve as more domain-specialized text embedding models for music-related tasks.", "3.1.2 Retrieval\n\nFormally, the retriever R is defined as a function:", "where q is the input question, D is the entire database of text passages, and c \u2282 D is the filtered context consisting of the topk passages, such that | c | = k \u226a | D | . Each passage 1 p \u2208 D is scored based on its similarity to the input question using cosine similarity between their embeddings:\n\nHere, E ( \u00b7 ) denotes an embedding function that maps both questions and passages into a shared vector space. The retriever ranks all passages in D by their similarity scores and selects the topk passages to form c , which serve as the external context for the generation step.", "3.1.3 Generation\n\nThe retrieved context c is provided to a generator LLM, which produces an output sequence using next-token prediction. Each token x i is generated conditioned on the input query q , the retrieved context c , and the previously generated tokens x <i :\n\nThis structure enables the model to dynamically incorporate external knowledge during inference, improving factual accuracy and adaptability without retraining.", "3.2 RAG vs. Fine-tuning\n\nLLMs often struggle with specialized tasks such as MQA due to limited exposure to domain-specific knowledge during pretraining. To address this, two primary domain adaptation strategies are commonly used: fine-tuning and RAG. Fine-tuning is akin to a closed-book exam: the model internalizes domain knowledge during training and must rely solely on that knowledge at inference. While effective for learning structured formats or stylistic patterns [26, 27], it is resource-intensive and inflexible when adapting to new or frequently changing knowledge. In contrast, RAG is like an open-book exam: the model dynamically retrieves relevant information from an external knowledge source during inference. This enables LLMs to access up-todate and specialized information without retraining. Prior studies [28,29] show that RAG improves factual accuracy, mitigates hallucinations, and provides greater transparency by allowing source verification. It is also more scalable and economically efficient, as it does not require updating model parameters [27]. These benefits are especially useful in rapidly evolving domains like music, where new artists, compositions, and styles continuously emerge.", "3.3 RAG with Fine-tuning\n\nWhile fine-tuning typically relies on question-answer pairs, it does not always emphasize learning to extract relevant information from the context provided alongside the question. In standard fine-tuning, the model is trained to\n\n1 A passage refers to a portion of a document relevant to a query [25].", "directly map a question to its answer without fully leveraging any external context that might be available. As a result, the model may struggle to utilize background information effectively, especially when answering questions that require specialized or up-to-date knowledge.", "To address this limitation, we adopt a RAG-style finetuning approach using a dataset consisting of (context, question, answer) triples. Unlike standard QA fine-tuning, which relies solely on the question, our method introduces an external relevant passage p for the input question q . This enables the model to learn how to incorporate relevant contextual information during answer generation. Both approaches share the same next-token prediction objective, but differ in the input they condition on. In standard finetuning, the model is trained as follows:\n\nwhere the model predicts each answer token x i based only on the question and the previously generated tokens. In contrast, RAG-style fine-tuning conditions the generation not only on the question but also on the relevant passages as context:\n\nwhere c is the relevant passage retrieved from an external corpus. By incorporating c as an additional context, the model is encouraged to utilize external knowledge when generating answers. This strategy improves the model's ability to ground its responses in retrieved evidence, leading to more accurate and contextually appropriate answers. During RAG fine-tuning, we used gold passages with high relevance to the answers, ensuring the model learns to effectively utilize contextual information.", "Main Results\n\nTable 1 presents the results of our approach and the baselines on four RAG tasks of open-domain QA. It can be observed that:\n\nFirst, our proposed method outperforms all other baselines in terms of average QA performance. Whether in the zero-shot setting or with lightweight fine-tuning, BEE-RAG consistently achieves the best results. This highlights the effectiveness of our key motivation: the reduction of context entropy and dynamic attention allocation on documents, which proves to be applicable to both single-hop and multihop datasets.", "Second, our method demonstrates its ability to handle more challenging datasets, assisting LLMs in effectively addressing complex RAG tasks. On relatively more diffi-", "| | Settings | NQ.EM | NQ.F1 | TriviaQA.EM | TriviaQA.F1 | HotpotQA.EM | HotpotQA.F1 | 2WikiQA.EM | 2WikiQA.F1 | Average.EM | Average.F1 |\n|---:|:------------------------|:------------|:------------|:--------------|:--------------|:--------------|:--------------|:-------------|:-------------|:-------------|:-------------|\n| 0 | Qwen-2.5-7B | Qwen-2.5-7B | Qwen-2.5-7B | Qwen-2.5-7B | Qwen-2.5-7B | Qwen-2.5-7B | Qwen-2.5-7B | Qwen-2.5-7B | Qwen-2.5-7B | Qwen-2.5-7B | Qwen-2.5-7B |\n| 1 | Vanilla RAG | 26.84 | 42.03 | 43.05 | 53.41 | 24.54 | 35.16 | 20.16 | 27.16 | 28.65 | 39.44 |\n| 2 | Chain-of-Thought | 26.86 | 40.20 | 39.59 | 51.78 | 26.20 | 35.88 | 22.93 | 26.58 | 28.89 | 38.61 |\n| 3 | Chain-of-Note | 26.50 | 41.75 | 41.20 | 52.03 | 26.74 | 35.95 | 19.93 | 26.58 | 28.59 | 39.08 |\n| 4 | PCW | 23.87 | 38.21 | 40.88 | 51.50 | 24.72 | 34.63 | 23.11 | 27.19 | 28.15 | 37.88 |\n| 5 | Position-Engineering | 25.76 | 38.61 | 41.38 | 52.30 | 27.77 | 36.56 | 22.69 | 26.96 | 29.40 | 38.60 |\n| 6 | Self-Critic | 24.27 | 35.36 | 40.74 | 52.04 | 25.11 | 35.35 | 22.00 | 27.51 | 28.03 | 37.57 |\n| 7 | Multiply-Attention | 26.18 | 41.43 | 42.92 | 53.62 | 27.13 | 34.94 | 18.87 | 24.02 | 28.78 | 38.50 |\n| 8 | Zero-BEE (ours) | 29.03 | 43.15 | 44.32 | 55.93 | 27.86 | 37.15 | 24.47 | 29.61 | 31.42 \u2020 | 41.46 \u2020 |\n| 9 | LoRA (2.06M) | 37.62 | 47.93 | 47.98 | 56.32 | 28.44 | 38.92 | 20.76 | 26.30 | 33.70 | 42.37 |\n| 10 | Prefix-Tuning (2.07M) | 36.75 | 46.33 | 48.21 | 56.32 | 31.19 | 40.15 | 23.69 | 29.19 | 34.96 | 43.00 |\n| 11 | Light-BEE (ours, 2.06M) | 38.71 | 50.11 | 49.39 | 56.73 | 35.95 | 44.81 | 28.32 | 31.73 | 38.09 \u2020 | 45.84 \u2020 |\n| 12 | LLaMA-3-8B | LLaMA-3-8B | LLaMA-3-8B | LLaMA-3-8B | LLaMA-3-8B | LLaMA-3-8B | LLaMA-3-8B | LLaMA-3-8B | LLaMA-3-8B | LLaMA-3-8B | LLaMA-3-8B |\n| 13 | Vanilla RAG | 27.13 | 37.77 | 41.17 | 55.46 | 23.07 | 34.22 | 14.20 | 23.74 | 26.39 | 37.80 |\n| 14 | Chain-of-Thought | 29.10 | 38.60 | 45.31 | 56.96 | 22.74 | 30.23 | 14.50 | 25.58 | 27.91 | 37.84 |\n| 15 | Chain-of-Note | 28.46 | 37.83 | 41.49 | 51.22 | 21.34 | 28.99 | 19.87 | 29.50 | 27.79 | 36.89 |\n| 16 | PCW | 25.71 | 35.47 | 38.72 | 48.99 | 17.14 | 25.92 | 17.88 | 26.65 | 24.86 | 34.26 |\n| 17 | Position-Engineering | 25.98 | 35.72 | 41.01 | 54.21 | 23.24 | 30.62 | 16.08 | 26.61 | 26.58 | 36.79 |\n| 18 | Self-Critic | 20.86 | 27.06 | 42.61 | 51.06 | 23.77 | 31.21 | 15.50 | 25.94 | 25.69 | 33.82 |\n| 19 | Multiply-Attention | 27.08 | 36.33 | 43.40 | 55.05 | 21.27 | 28.24 | 11.87 | 23.74 | 25.91 | 35.84 |\n| 20 | Zero-BEE (ours) | 30.50 | 38.96 | 45.67 | 57.04 | 23.84 | 34.29 | 20.67 | 30.56 | 30.17 \u2020 | 40.21 \u2020 |\n| 21 | LoRA (2.62M) | 35.39 | 48.68 | 53.65 | 58.55 | 33.15 | 42.15 | 18.91 | 28.67 | 35.27 | 44.51 |\n| 22 | Prefix-Tuning (2.63M) | 36.39 | 48.25 | 48.75 | 56.48 | 32.30 | 42.50 | 16.33 | 25.13 | 33.44 | 43.09 |\n| 23 | Light-BEE (ours, 2.62M) | 38.42 | 49.23 | 53.96 | 58.67 | 38.06 | 44.72 | 24.70 | 31.89 | 38.79 \u2020 | 46.13 \u2020 |", "Table 1: Main results of BEE-RAG on four RAG tasks with different LLM backbones for both zero-shot inference and lightweight fine-tuning settings. Zero-BEE denotes the zero-shot inference version of BEE-RAG, while Light-BEE denotes the parameter-efficient (lightweight) fine-tuning version. All results are averaged over 8 runs; \u2020 indicates statistical significance with p < 0 . 05 .", "cult multi-hop datasets, BEE-RAG shows significant performance improvements. Notably, on the latest 2WikiMultihopQA dataset, our approach achieves an approximately 5% improvement over vanilla RAG, further highlighting the effectiveness of our approach.", "Third, our parametric-efficient fine-tuning method demonstrates better generalization performance with few parameters, emphasizing the effectiveness of the proposed adaptive balancing entropy factor learning approach. The result shows that whether on trained or zero-shot datasets, our method outperforms the others. Notably, the performance of the other two methods may degrade when transferred to multi-hop tasks, revealing their limitations in handling challenging RAG tasks.", "Experitmental Setups\n\nDatasets. We evaluate DPS on five widely used question answering benchmarks covering both multi-hop and domain-specific reasoning. HotpotQA (Yang et al. 2018) requires concatenating evidence from two supporting Wikipedia paragraphs to answer complex questions. MuSiQue (Trivedi et al. 2022) similarly provides multihop queries with paragraph-level annotations, but emphasizes compositional reasoning chains. 2WikiMQA (Ho et al. 2020) comprises factoid questions that may span one or two Wikipedia articles. To evaluate long context and high-level query, we include Legal (Qian et al. 2024), a collection of legal-domain questions with specialized terminology, and CS (Qian et al. 2024), which focuses on computer science and programming knowledge.", "Baselines. We compare against popular pre-trained models in zero-shot mode (GPT-4o, Gemini-1.5-flash, Qwen2.5-72B, and Llama3.1-70B) to measure the benefit of retrieval augmentation. In the fine-tuned RAG category, we include DPA-RAG (Dong et al. 2024) and RankRAG (Yu et al. 2024), both of which jointly optimize retriever and generator components using passage-level supervision and contrastive or distillation objectives. For reranker-augmented methods, we evaluated seven standard rerankers-BGE-reranker-v2-m3(Chen et al. 2024a), Qwen3-reranker-8B(Zhang et al. 2025b), MonoT5(Nogueira et al. 2020b), RankingGPT (Zhang et al. 2023), RankLlama (Ma et al. 2024), RankZephyr(Pradeep, Sharifymoghaddam, and Lin 2023b) and RankVicuna (Pradeep, Sharifymoghaddam, and Lin 2023a)-that apply a pretrained reranking model over fast retriever candidates before feeding the top subset into an LLM.", "Evaluation. Compared to previous reranking models that focus on retrieval-based metrics, we place greater emphasis on whether the retrieved passages can improve the quality of the generated content. All methods are assessed using token-level F1 and Exact Match (EM) metrics on multi-hop QA dataset. For the CS and Legal datasets, the ground-truth consists of long-form answers, so only F1 is used as the evaluation metric. The experiments are conducted on a single NVIDIA A100 GPU with a fixed test batch size and temperature. Each configuration is run three times with different random seeds. During evaluation, for each dataset, we vary the top-k parameter in baseline rerankers from 1 to 8 to measure performance across different evidence sizes, and find K=5 is the optimal settings for most rerankers on five datasets.", "Implementation. We fine-tune DPS on Qwen2.5-7B using LoRA adapters on 4 NVIDIA A100 GPUs. We set training epoch I = 1 and use the DeepSpeed ZeRO strategy during training, with learning rates of 1e -4 and a warm ratio of 0.05. To ensure fairness, we use the same vector retrieval method to retrieve 30 documents from the identical corpus, serving as the candidate set for both the reranker and DPS. We use one of the most advanced text-encoding models, BGE-M3 (Chen et al. 2024b), as the fast retriever across all reranker methods. We use Qwen2.5-7B as the generator (backbone LLMs) for all the baselines in main results. The detail implementation of baseliens and our source code can be found in the Appendix.", "1 Introduction\n\nMotivation: Customizing LLMs to be more knowledgeable on specific areas has been a popular task to accomplish in recent years. A technique known as fine-tuning has been proposed and popularized through various different methods of fine-tuning coming up over the past few years such as FLAN [1], LoRA [2] and QLoRA [3] which all aim to address\n\n\u2217 Citation : Authors. Title. Pages.... DOI:000000/11111.\n\nAbdelkarim Erradi Qatar University erradi@qu.edu.qa", "different setbacks associated with the LLM fine-tuning process such as improving the final efficacy as well as reducing computational resources.", "Fine-tuning LLMs is usually done on full LLMs and is generally not advised to fine-tune on or convert pre-finetuned models into quantized models due to potential loss of knowledge retention and accuracy which can be attributed to lower precision of the model's weights. Even though existing techniques like QLoRA have shown results which indicate minimal performance degradation upon the resulting fine-tuned quantized models which are based on their Guanaco models, this is not applicable to all LLMs [3]. For example an empirical study [4] evaluated different LLAMA3 quantizations and fine-tune quantizations has shown a significant reduction in LLAMA3-8B scores on the MMLU benchmarks when compared to the standard full 16-bit model and a QLoRA fine-tune-quantization to 4bits. This is because although all LLMs follow the basic transformer architecture there are still differences in model designs which can impact the quality of fine-tunes and hence even previously considered efficient methods such as QLoRA face their limitations with newer models [4].", "Since 2020 the concept of RAG has since evolved to include many different implementations and approaches to performing information retrieval to enhance LLM answering capabilities on new datasets [Figure 1]. This highlights how the benefits gained from RAG are being researched in depth to further optimize this process.", "Answer Accuracy: Naive RAG implementations often struggle with the \"Needle-in-a-Haystack\" problem [8], where the generator model fails to provide accurate answers due to an inability to extract relevant information from irrelevant or excessively long contexts. This issue typically arises from poorly optimized chunk sizes and retrieval processes [9]. Optimizing chunk size is critical and challenging, as smaller chunks risk losing essential context, while larger chunks may introduce irrelevant information and unnecessarily increase context length [Figure 2]. Optimizing the retrieval process to maximize the relevance of retrieved information blocks is crucial. This ensures that irrelevant data, which could act as noise and obscure the correct information, does not compromise the generator LLM's ability to accurately", "Figure 2: Graph showing how LLMs prefer extracting information from top or bottom of context to answer a question [7].\n\n20 Total Retrieved Documents (~4K tokens)\n\n5th lOth 15th 2Oth\n\nPosition of Document with the Answer\n\n3.5-turbo-0613\n\ngpt-3.5-turbo-0613 (closed-book)\n\ngpt-\n\nanswer the question. Another limitation of naive RAG systems is that embeddings which are similar but not actually relevant to answer the question might be retrieved [10] as the original sentences might be semantically similar or carry a similar meaning but might not be right in helping answer the question in that context.", "Scalability: Naive implementations of Retrieval-Augmented Generation (RAG) often rely on 16-bit floating-point large language models (LLMs) for the generation component. However, this approach introduces significant scalability challenges due to the increased memory demands required to host the LLM as well as longer inference times due to using a higher precision number type. To enable more efficient scaling, it is crucial to integrate methods or techniques that reduce the memory footprint and inference times of generator models. Quantized models offer more scalable solutions due to less computational requirements, hence when developing RAG systems we should aim to use quantized LLMs for more cost effective deployment as compared to a full fine-tuned LLM whose performance might be good but is more expensive to deploy due to higher memory requirements. A quantized LLM's role in the RAG pipeline itself should be minimal and for means of rewriting retrieved information into a presentable fashion for the end users to interpret.", "Additionally, in naive RAG implementations, information store components, such as Knowledge Graphs, face lengthy training times for their creation and updating. This creates scalability challenges, as it increases the downtime required to update the information store with new knowledge in a production environment. Hence an effective data pre-processing step should be introduced to reduce the training corpus size and hence training times while preserving as much information as possible.", "Solution: We propose the K 2 RAG framework which addresses the following 4 research goals based on the aforementioned answer accuracy and scalability issues characteristic of naive RAG implementations:\n\nGoal 1. Reduce information store such as Knowledge Graph and Spare and Dense vector database creation times.", "Goal 2. Reduce chances of LLMs suffering from \"Needle in Haystack\" Problem.\n\nGoal 3. Increase rate of retrieving relevant passages for answering the question.", "This paper introduces PrismRAG, a novel fine-tuning framework aimed at enhancing retrieval-augmented generation (RAG) for question answering. The method focuses on training language models to manage distracting passages and improve reasoning capabilities. PrismRAG reduces hallucinations and maintains factuality across various benchmarks. The method incorporates synthetic distractors in the training data and emphasizes dynamic chain-of-thought generation. Experimental results show a 5.4% improvement in average factual accuracy compared to existing solutions, outperforming state-of-the-art methods in multiple scenarios.", "2 Related Work\n\nWhich metric to optimize. Multi-hop QA involves two sub-tasks: retrieving relevant documents, and then answering the question based on the documents. Some methods report document retrievalspecific metrics such as recall [Hsu et al., 2024] whereas others report final answer metrics such as exact match [Jin et al., 2025]. Typically, a model is trained to optimize a particular metric (such as recall) and also evaluated on the same metric. For robustness, in this work we train on the recall metric and test on all metrics, including final answer metrics.", "Prompting-based RAG approaches. With the recent advancements in the capabilities of large API-based LMs, some works explored prompting to call external search/retrievers at inference. Toolformer[Schick et al., 2023] uses a self-supervised objective to train an external model that decides to call tools (like Bing and Google search engines). ReAct [Yao et al., 2023] is another powerful prompting technique that allows the model to structure its outputs as thoughts, actions and observations, yielding significant improvements in the ability of LLMs to interact with external environments. Trivedi et al. [2022a] proposed IRCoT, another prompting strategy that alternates between chain-of-thought [Wei et al., 2022] steps and gathering evidence through retrievals. By using the intermediate traces, the IRCoT is able to decide what to retrieve by issuing the right search queries. Iter-RetGen [Shao et al., 2023] improves evidence gathering in multi-hop scenarios by combining", "retrieval and generation iteratively, such that a model's response is incorporated in the reasoning trace. However, both IRCoT [Trivedi et al., 2022a] and Iter-RetGen [Shao et al., 2023] rely on a fixed or predefined number of retrieval loops at inference, offering limited control over latency.", "Finetuning-based techniques. A prevalent method for multi-hop QA using small LMs is supervised finetuning using reasoning traces from a large LM such as GPT-4 [Asai et al., 2023, Chan et al., 2024]. Other methods are trained to predict the next query to be retrieved [Chan et al., 2024]. Methods that scale the test-time compute that infer using multiple trajectories have also been proposed [Wang et al., 2025]. Recently, reinforcement learning-based techniques have been proposed that develop a reward based on outputting the ground-truth answer [Jin et al., 2025]. However, none of the techniques focus on efficiency of the solution. In fact, in Search-R1, the goal of RL is to increase the number of searches. Instead, we use RL to decrease the average number of searches done by our model.", "Traditional RAG approaches. Early work in grounding generation with real world documents focused on end-to-end differentiable encoder-decoder pipeline REALM [Guu et al., 2020], which augments Masked-Language Modeling (MLM) with a latent retriever model, backpropagating through retrieval to learn both retriever and generator jointly. However, this approach incurs significant computational cost and has only been shown to work with relatively smaller models like T5 [Raffel et al., 2020]. Building on this, Lewis et al. [2020] proposed a general finetuning strategy, RAG-Token which demonstrated that join-training outperforms fixed dense retrieval and BM25.", "RL-based Retrieval Augmented Generation. Recently, framing search query as an RL problem has received attention. LeReT [Hsu et al., 2024] performs preference optimization using diverse few shot prompts leveraging hundred-thousands of ground truth annotated documents. However, LeReT utilizes a fixed amount of compute per instance during inference and cannot be readily generalized to variable-hop scenarios. Similarly, concurrent works, Jin et al. [2025] and Chen et al. [2025] propose end-to-end RL-based optimization that only leverages the final answer annotation. These methods show that RL can effectively be used to teach the search query generator model to issue more search queries for multi-hop problems without considering latency . Our two-stage RL framework, by contrast, first explores without RL to maximize recall and then learns to stop at test time using RL."]}
{"type": "entry", "index": 13, "question": "does long context windows now removed the need for RAG?", "answer": "No. Current evidence says long context windows reduce some pressure but do not remove the need for RAG.\n\n1. Cost and efficiency: Studies comparing long-context models with RAG show that while larger windows can take in more text, RAG is still more cost\u2011efficient and better at surfacing explicit citations and evidence [1]. Clinical work over long electronic health records finds that RAG can match or nearly match the performance of using the model\u2019s full 128k\u2011token context, but with a fraction of the tokens required [5].\n\n2. Context limits and degradation: Even long\u2011context models are still bounded, and performance often degrades when important information is buried in very long inputs (\u201clost in the middle\u201d) [4][5]. RAG helps by selecting only the most relevant passages instead of dumping everything into the window [5].\n\n3. Up\u2011to\u2011date, specialized, and rigorous knowledge: RAG has become a standard way to keep LLMs current and factual after training, especially in domains like general web knowledge and legal or clinical settings [1][2][5]. Retrieval lets you update the external index or augmented knowledge without retraining the base model [1][2].\n\n4. Complex reasoning over long narratives: For tasks like understanding long stories or case histories, simply extending context is not enough; specialized retrieval frameworks (e.g., ComoRAG) that iteratively fetch and integrate evidence outperform static long\u2011context reading [4].\n\nOverall, long context windows and RAG are complementary: long context helps when you truly must read a big continuous span, but RAG remains crucial for efficiency, accuracy, recency, and interpretability in real applications [1][4][5].", "tags": ["_untagged"], "full_faithfulness": 1.0, "full_answer_relevancy": 0.9171863724, "seed_faithfulness": 0.7647058824, "seed_answer_relevancy": 0.9306210904000001, "context_relevance": 0.8223341454804428, "context_relevance_error": null, "context_relevance_reason": "The context passages collectively address the user's question about whether long context windows eliminate the need for retrieval-augmented generation (RAG). Several passages discuss the relationship between long context LLMs and RAG, highlighting that while long context windows can absorb more text, RAG remains more cost-efficient and effective in certain scenarios. This directly relates to the user's inquiry, providing a nuanced view of the ongoing debate and the complementary roles of both approaches. However, some passages could be more directly focused on the implications of long context windows specifically negating the need for RAG, which slightly detracts from the overall clarity of the answer.", "structure_citation": 0.638365213356834, "structure_error": null, "structure_reason": "The Actual Output includes claims supported by citations, but there are issues with citation matching. While citations [1], [4], and [5] are referenced, not all claims are directly supported by the provided Context snippets. For instance, the claim about RAG being more cost-efficient is not explicitly backed by the Context, which affects the overall support for the claims made. Additionally, the citation [2] is mentioned but not present in the Context, leading to a deduction in points.", "hallucination_score": 0.8479810506137866, "hallucination_error": null, "hallucination_reason": "The response accurately addresses the question by stating that long context windows do not eliminate the need for RAG, supported by multiple claims regarding cost efficiency, context limits, and the necessity of RAG for up-to-date knowledge. However, while the claims are well-supported by the context, the response could have explicitly mentioned any potential limitations or counterarguments regarding the effectiveness of long context windows in certain scenarios, which would have provided a more balanced view.", "contexts_used": ["4 Year-by-Year Progress in RAG\n\n4.2 Major Milestones (2020-2024)\n\nperformance while using 25 \u00d7 fewer parameters, underscoring retrieval's efficiency gains.", "Google's ATLAS [35] unifies retrieval and generation during pre-training . Using the unsupervised Contriever dense retriever [37] and a Fusion-in-Decoder reader, ATLAS achieves new state-of-the-art results on Natural Questions and TriviaQA and, in few-shot mode, outperforms much larger PaLM-540B by 3 EM on NQ with only 64 examples. These results highlight that high-quality retrieval plus multi-document fusion can outperform sheer parameter count.", "Beyond QA, RAG became core to knowledge-grounded dialogue . Meta's BlenderBot 3 [80] couples a 175B LLaMA-style generator with live internet search and long-term memory, reducing hallucinations and increasing user engagement in open-domain conversation. BlenderBot 3's deployment study shows that users prefer retrieval-grounded responses and that continual online learning can keep the retriever index current. Retrieval quality itself improved through unsupervised contrastive training. Contriever [37] dispenses with labelled (question, passage) pairs, yet surpasses BM25 and even DPR on BEIR benchmarks, making high-recall indexes available for any corpus. Such retrievers power ATLAS and other 2022 RAG systems, demonstrating that scalable training data is no longer a bottleneck.", "Finally, 2022 work extended RAG to fact-checking, summarization, and few-shot learning . ATLAS gains 5F1 on FEVER, indicating that retrieved evidence helps verdict generation. Early studies in retrieval-augmented summarization show improved factual consistency by grounding summaries in external documents. Few-shot evaluations reveal that retrieval narrows the data gap: ATLAS and RETRO deliver strong accuracy with under 100 task examples, whereas closed-book baselines require orders of magnitude more data.", "Outlook By the end of 2022, RAG had broadened from open-domain QA into a general recipe for knowledge-intensive NLP. Parameter-efficient hybrids like RETRO and ATLAS challenge the notion that bigger models alone yield better knowledge; instead, high-quality retrieval and multi-document reasoning emerge as key levers. Open challenges include faster retrieval over trillion-token corpora, differentiable multi-hop reasoning, and robust evaluation of evidential faithfulness, but 2022 firmly established retrieval-augmented generation as a premier path toward up-to-date, factual, and data-efficient language models.", "2023 -RAG Meets LLMs. By 2023, mainstream LLM-based applications (e.g., ChatGPT with plugins, Bing Chat, enterprise chatbots) widely incorporated retrieval [27]. This retrieve-then-generate paradigm was used to mitigate hallucinations and update factual knowledge post-training. The debate emerged as to whether long-context LLMs (with tens of thousands of tokens) could negate the need for retrieval systems. Studies like [54] showed that while large context windows can absorb more text, RAG remains more cost-efficient and better at exposing citations. Hybrid approaches also arose: letting a model choose between retrieving or just using a long context. Overall, RAG became a cornerstone for credible LLM deployments needing up-to-date knowledge and interpretability. SOme of the areas that received attention in 2023 are discussed below:\n\nScale and few-shot learning: Atlas[36], an 11B-parameter retrieval-augmented model, achieved 42.4% accuracy on Natural Questions with only 64 training examples, outperforming a 540B closed-book model by 3%. Atlas also set new few-shot records on TriviaQA and FEVER (gains of +3-5%), matching 540B-scale performance on multi-task benchmarks. Crucially, Atlas's dense document index can be easily updated with new text, demonstrating updatable knowledge.", "Adaptive retrieval: Self-RAG[5] trains a single language model to generate special 'reflection' tokens that trigger on-demand retrieval and self-critique. In experiments, 7B and 13B Self-RAG models substantially outperformed ChatGPT and a RAG-augmented Llama-2-chat baseline on open-domain QA, reasoning, and fact-verification tasks, yielding much higher factual accuracy and citation precision.", "Knowledge-grounded dialogue: Retrieval augments dialogue systems to improve consistency and informativeness. Kumari et al.[47] incorporate retrieved persona and context snippets in long conversation modeling, showing that adding relevant knowledge improves response quality. Similarly, Kang et al.[43] propose SURGE , which retrieves relevant subgraphs from a knowledge graph and uses them to bias the response generation. SURGE produces more coherent, factual responses grounded in the retrieved", "knowledge.", "Summarization and explanation: RAG has been applied to summarization and explanation tasks. By retrieving source documents or evidence passages, RAG-augmented summarizers produce more accurate and detailed summaries than closed-book models. Likewise, in fact-checking pipelines, retrieving evidence before verification leads to more reliable verdicts and explanations. These applications extend RAG's grounding advantages beyond QA to a broader range of generative tasks.", "4.3 2025 - The Current Direction\n\nThe community is exploring how to marry graph knowledge with text retrieval. A comprehensive survey formalised the GraphRAG paradigm and mapped design choices for graph-aware retrievers and generators [29]. A companion study compared vanilla RAG and GraphRAG across QA and summarisation, showing complementary strengths and proposing hybrid fusion strategies [28].", "Abstract\n\nConventional RAG is considered one of the most effective methods for addressing model knowledge insufficiency and hallucination, particularly in the judicial domain that requires high levels of knowledge rigor, logical consistency, and content integrity. However, the conventional RAG method only injects retrieved documents directly into the model's context, which severely constrains models due to their limited context windows and introduces additional computational overhead through excessively long contexts, thereby disrupting models' attention and degrading performance on downstream tasks. Moreover, many existing benchmarks lack expert annotation and focus solely on individual downstream tasks while real-world legal scenarios consist of multiple mixed legal tasks, indicating conventional benchmarks' inadequacy for reflecting models' true capabilities. To address these limitations, we propose PL-CA, which introduces a parametric RAG (P-RAG) framework to perform data augmentation on corpus knowledge and encode this legal knowledge into parametric vectors, and then integrates this parametric knowledge into the LLM's feed-forward networks (FFN) via LoRA, thereby alleviating models' context pressure. Additionally, we also construct a multi-task legal dataset comprising more than 2000 training and test instances, which are all expertannotated and manually verified. We conduct our experiments on our dataset, and the experimental results demonstrate that our method reduces the overhead associated with excessively long contexts while maintaining competitive performance on downstream tasks compared to conventional RAG. Our code and dataset are provided in the appendix.", "Related Work\n\nRecent efforts have aimed to improve RAG performance under long contexts (Laban et al. 2024). PCW (Ratner et al. 2023) restricts attention within fixed-size windows to reduce attention dilution. CEPE (Yen, Gao, and Chen 2024) uses a lightweight encoder to process inputs chunk by chunk, enabling better use of long contexts via cross-attention. RAFT (Zhang et al. 2024b) and Chain-of-Notes (Yu et al. 2023) extend the context capacity of models through instruction tuning on long-input data. However, these methods either yield limited performance gains or incur high training costs. Some works explore the effect of attention distribution (Wang et al. 2023). Su (2021) adjust attention scaling by context length to match short-context behavior, but this fails to highlight key segments and may miss important information.", "Our BEE-RAG introduces an attention reallocation mechanism. It reduces attention entropy gap with short contexts and guides focus to key segments. This improves longcontext RAG performance.", "1 Introduction\n\nThe core challenge of long-context narrative comprehension lies not merely in connecting discrete pieces of evidence, a task more naturally defined as multi-hop Question Answering (QA), but in performing a dynamic cognitive synthesis to grasp necessary background and content progression (Xu et al. 2024a). Unlike multi-hop QA (Yang et al. 2018), which seeks a static path through fixed facts, narrative comprehension requires emulating a human reader: continuously building and revising a global mental model of the\n\n* These authors contributed equally.\n\n\u2020 Project lead. Correspondence to: < liyanlxu@tencent.com >\n\nFigure 1: Comparison of RAG reasoning paradigms.\n\n(c) ComoRAG\n(b) Multi-step RAG\nWhy Snape Kill\n\nDumbledore Albus?\n\nStateless Reasoning Stateful Reasoning Fragmented Evidence Lack of Fusion Memory-Organized Stateful Comprehension Contradictory Evidence Motive Unclear\n\nAn act of loyalty\n\nnot betrayal Snape Bully Harry Kills Albus Protect (a) Single-step RAG Shallow/Superficial Understanding\n\nSnape was a loyal\n\nDeath Eater The Half-Blood Prince where Snape Coherent Context Formed Apparent Contradiction Causally Incomplete Event Unbreakable Vow Context-Grounded Exploratory Probing Dynamic Memory Workspace Knowledge Consolidation Consolida1on New Acquisition Acqulsition", "plot, characters, and their evolving motivations (JohnsonLaird 1983). The complexity of this process is well exemplified by a classic narrative question 'Why did Snape kill Dumbledore?' from the Harry Potter series. Answering this requires weaving a complete web of evidence from disparate clues spanning multiple books-Dumbledore's terminal illness, the Unbreakable Vow, and Snape's deeply concealed loyalty. The true significance of these clues is only fully reconciled in hindsight. This capability is what we term stateful reasoning : it demands more than linking static evidence; it requires maintaining a dynamic memory of the narrative, one that is constantly updated as new revelations emerge. Long-context LLMs have demonstrated promising performance on benchmarks such as the 'Needle in a Haystack' test (Eisenschlos, Yogatama, and Al-Rfou 2023) in recent years. However, their capacity to process long narratives (200k+ tokens) remains limited by finite context windows. Furthermore, as the input length increases, these models are prone to the 'lost in the middle' problem (Liu et al. 2024), which raises perplexity and impairs generation quality. This limitation is particularly pronounced in narrative compre- hension tasks which require stateful reasoning. As a result, retrieval-augmented generation (RAG) (Lewis et al. 2020) has emerged as an important strategy for tackling long context comprehension with LLMs.", "However, existing RAG methods still struggle to effectively address this challenge. Advanced single-step retrieval remains limited by its static index. This includes methods such as RAPTOR (Sarthi et al. 2024), which clusters and summarizes text chunks to retrieve at different levels of details; HippoRAGv2 (Guti\u00b4 errez et al. 2025), which mimics the human hippocampus by building a knowledge graph to achieve multi-hop reasoning in a single retrieval step. Nonetheless, single-step methods rely on one-shot static retrieval, which may lead to shallow comprehension. For example, in Figure 1(a), the evidence about Snape can mislead the model into making a false inference.", "As a remedy, multi-step retrieval methods offer a more promising direction, such as IRCoT (Trivedi et al. 2023), which interleaves the retrieval process with Chain-ofThought reasoning (Wei et al. 2022); Self-RAG (Asai et al. 2024), which trains a model to adaptively retrieve and reflect on evidence; and MemoRAG (Qian et al. 2025), which uses a dual-system architecture to generate clues from compressed global context. These methods all target to obtain richer context through iterative retrieval. However, their retrieval steps are typically independent, which lack coherent reasoning throughout explicit narrative progression, featuring fragmented evidence with a stateless comprehension. As illustrated in Figure 1(b), due to a lack of dynamic memory, multi-step retrieval fails to integrate contradictory evidence such as 'Snape protects/bullies Harry' and cannot understand the evolution of his actions, ultimately unable to yield the correct answer.", "In this work, we seek inspiration from the function of Prefrontal Cortex (PFC) in human brains, which employs a sophisticated reasoning process called Metacognitive Regulation (Fernandez-Duque, Baird, and Posner 2000). This process is not a single action but a dynamic interplay between new evidence acquisition , driven by goal-directed memory probes (Dobbins and Han 2006; Miller and Constantinidis 2024), and subsequent knowledge consolidation . During consolidation, new findings are integrated with past information to construct an evolving, coherent narrative. This iterative cycle allows the PFC to continuously assess its understanding and revise its strategy, providing a direct cognitive blueprint for our framework's stateful reasoning approach.", "We introduce ComoRAG, a cognitive-inspired, memoryorganized RAG framework, imitating the human Prefrontal Cortex (PFC) for achieving true stateful reasoning. At its core is a dynamic cognitive loop operating on a memory workspace, which actively probes and integrates new evidence to build a coherent narrative comprehension.", "This process, as illustrated in Figure 1(c), is a closed loop of evolving reasoning states. Faced with a complex query like 'Why did Snape kill Dumbledore?' , the system's memory state evolves from an initial 'causally incomplete event' ( Snape kills Albus ), to an 'apparent contradiction' upon finding contradictory information ( Snape protects Harry ), and ultimately to a logically consistent coherent context through deeper exploration and evidence fusion. Only in this final, complete cognitive state can ComoRAG perform the correct stateful reasoning, deriving the profound insight that it was 'an act of loyalty, not betrayal' .", "This cognitively-inspired design yields substantial improvements across four challenging long-context narrative benchmarks. ComoRAG is shown to consistently outperform all categories of strong baselines across each dataset. Our analysis reveals several key findings. First, these gains stem directly from the cognitive loop, which transforms a static knowledge base into a dynamic reasoning engine; for instance, accuracy on EN.MC jumps from a static-retrieval baseline of 64.6% to 72.9%, with performance efficiently converging in around 2-3 cycles. Second, our framework excels on narrative queries that require global understanding of plot progression, achieving up to a 19% relative F1 improvement on these challenging question types where others falter. Finally, our framework demonstrates remarkable modularity and generalizability. Its core loop can be flexibly integrated to existing RAG methods such as RAPTOR, which directly yields a 21% relative accuracy gain). Also, switching to a stronger model as the backbone LLM agents can upgrade reasoning in the entire cognitive loop, attaining accuracy from 72.93% to 78.17%. These results collectively validate that ComoRAG provides a principled, cognitivelyinspired new paradigm for retrieval-based long narrative comprehension towards stateful reasoning.", "Abstract\n\nElectronic health records (EHRs) are long, noisy, and often redundant, posing a major challenge for the clinicians who must navigate them. Large language models (LLMs) offer a promising solution for extracting and reasoning over this unstructured text, but the length of clinical notes often exceeds even stateof-the-art models' extended context windows. Retrieval-augmented generation (RAG) offers an alternative by retrieving task-relevant passages from across the entire EHR, potentially reducing the amount of required input tokens. In this work, we propose three clinical tasks designed to be replicable across health systems with minimal effort: 1) extracting imaging procedures, 2) generating timelines of antibiotic use, and 3) identifying key diagnoses. Using EHRs from actual hospitalized patients, we test three state-of-the-art LLMs with varying amounts of provided context, using either targeted text retrieval or the most recent clinical notes. We find that RAG closely matches or exceeds the performance of using recent notes, and approaches the performance of using the models' full context while requiring drastically fewer input tokens. Our results suggest that RAG remains a competitive and efficient approach even as newer models become capable of handling increasingly longer amounts of text.", "1 Introduction\n\nElectronic health records (EHRs) contain comprehensive documentation of patient care, including critical information for diagnosis and treatment planning. However, the volume of clinical notes has exploded in recent years, driven in part by copypaste practices, templated documentation, and regulatory pressures-a phenomenon often referred to as 'note bloat'. For example, nearly 1 in 5 patients arrive at the emergency department with a chart the size of Moby Dick (over 200K words) (Patterson et al., 2024). As a result of this, clinicians must navigate increasingly lengthy and redundant records to locate key information. Large language models (LLMs) can potentially alleviate this burden by assisting clinicians in quickly extracting information and reasoning over EHR, and have demonstrated promising capabilities in clinical summarization (Van Veen et al., 2024) and question answering (Singhal et al., 2025). However, the sheer volume of clinical documentation can exceed most LLMs' context window size. A practical approach is to provide the most recent notes, which may suffice for some tasks but risks omitting crucial information buried in earlier documentation.", "Retrieval-augmented generation (RAG) has emerged as a prominent solution to using LLMs on long documents by retrieving only the most relevant text passages for a given task. Rather than processing entire patient charts, RAG systems can selectively extract pertinent clinical information to answer specific questions. This approach can potentially reduce computational costs, improve accuracy through elimination of noise, and mitigate the 'lost-in-the-middle' effect (Liu et al., 2024), where model performance degrades when relevant information is buried within lengthy contexts.", "However, there has been limited empirical evaluation on the accuracy and token efficiency of this retrieval approach for tasks that require longitudinal reasoning over real-world EHR data. One barrier is the scarcity of large, annotated clinical datasets due to legal and ethical constraints regarding patient privacy. While the MIMIC datasets (Johnson et al., 2016) have been further annotated for benchmarking a variety of natural language processing tasks, including question-answering, this data is restricted to the patients' ICU stay, as opposed to the full hospital course, limiting their potential for testing realistic use cases that stretch the token limitations of LLMs' processing abilities.\n\nTo address these gaps, we define three tasks that", "Figure 1: Retrieval-augmented generation pipeline for clinical question answering over EHR.\n\nChunking Vancomycin Patient vitals: [ Assembled 10/02 10/07 Prompt 2. Ceftriaxone Past Medical Embed query\n\nYou are a medical AI\n\npresent History: [ ] and chunks assistant tasked with Clinical\n\nanalyzing EHR.\n\nSubjective: [ ] Notes Assessment: [ Plan: [] Assessment: [] Select\n\nPlan:\n\nWhat antibiotics is the\n\nchunks\n\npatient taking?\n\nby LLM cosine What antibiotics has Generation Query similarity\n\nthe patient had and\n\nwhen?", "reflect different clinical reasoning demands and can be replicated in other health systems without labor-intensive manual annotation:\n\nImaging Procedures : Produce a list of imaging procedures (including modality, date, and anatomical location) that occurred during a hospitalization from the raw clinical notes.", "Antibiotic Timelines : Generate the timelines of therapeutic antibiotic use for patients with a severe infection.\n\nDiagnosis generation : Identify the key diagnoses relevant to the hospitalization.", "The Imaging Procedures task is a straightforward extractive task, requiring the model to identify the imaging procedures that occurred on different days across the course of the hospital stay. The Antibiotic Timelines task requires not only identifying the antibiotics the patient was on and when they were discontinued, but also incorporating medical reasoning to determine what those antibiotics were administered for. The final task, Diagnosis Generation requires the most medical reasoning-the model is asked not just to list the diagnoses that were mentioned, but determine which required active management and impacted the care plan.", "These tasks allow us to investigate the following questions: Given a limited token budget, to what extent can targeted retrieval of information from the full hospital stay improve efficiency and performance over simply providing an LLM with the most recent notes? Does utilizing the extremely long context windows of state-of-the-art models provide any further benefit?", "Using EHR data from an academically-affiliated US hospital, we evaluate three LLMs on these tasks using varying amounts of clinical context, including up to the models' full context window of 128K tokens.", "Our findings suggest that while RAG can provide substantial efficiency improvements over comparable amounts of recent clinical note tokens, this effect is highly task-dependent. However, in all three tasks, we found RAG to achieve near-parity to using the full context window with a fraction of the tokens, indicating that retrieval remains a competitive approach even as newer model architectures continue to extend context windows."]}
{"type": "entry", "index": 14, "question": "give me innovations in RAG from the papers anything unconventional?", "answer": "Here are some unconventional innovations in RAG from the papers:\n\n1) Test\u2011time feedback and adaptive retrieval\n- Retrieval is no longer a one\u2011shot step: systems adjust queries, rankers, and retrieval frequency based on feedback from the model\u2019s own uncertainty, hallucination detectors, or consistency checks across languages and models [1].\n- Examples: triggering retrieval only when token probabilities drop (FLARE, DRAGIN), when hallucinations are detected (DRAD), or when cross\u2011lingual / multi\u2011model answers disagree (Rowen) [1].\n- Corrective RAG and similar systems classify retrieved docs as correct/incorrect/ambiguous and rewrite queries or trigger new retrieval rounds accordingly [1].\n\n2) Self\u2011triggered, reasoning\u2011driven retrieval (\"agentic RAG\")\n- Models are prompted or trained to plan: decompose questions, decide when to search, and refine queries as they reason (ReAct, SelfAsk, SMR, Searchain) [1].\n- SMR even defines actions like Refine, Rerank, Stop to avoid redundant or off\u2011track retrieval and to stop once retrieval quality is \u201cgood enough\u201d [1].\n- MCTS\u2011RAG applies Monte Carlo Tree Search to explore alternative retrieval paths, an unusual use of game\u2011style search inside RAG [1].\n\n3) Hierarchical \"think\u2011before\u2011answering\" training\n- HIRAG fine\u2011tunes RAG models to develop three explicit abilities: filtering relevant info, combining paragraphs, and doing RAG\u2011specific reasoning that blends external and internal knowledge, all via multi\u2011level chain\u2011of\u2011thought before answering [2].\n\n4) RL for frugal retrieval\n- FrugalRAG and related work use reinforcement learning not to retrieve more, but to learn when to stop searching and minimize the number of retrieval calls while maintaining answer quality [3].\n\nThese directions treat retrieval as a controlled, reasoned process\u2014often with explicit search or RL\u2014rather than a fixed pre\u2011generation step.", "tags": ["_untagged"], "full_faithfulness": 0.7222222222, "full_answer_relevancy": 0.9295696469, "seed_faithfulness": 0.7826086957, "seed_answer_relevancy": 0.9416073104, "context_relevance": 0.8046374666968552, "context_relevance_error": null, "context_relevance_reason": "The context passages provide substantial information about innovations in Retrieval-Augmented Generation (RAG), including unconventional approaches like adaptive RAG and self-triggered retrieval methods. Specific examples such as DRAD and MCTS-RAG illustrate novel techniques that enhance retrieval efficiency and reasoning capabilities. However, while the passages are relevant, they could be more explicitly tied to the user's request for 'unconventional' innovations, as some details focus on traditional methods and challenges rather than solely on innovative aspects.", "structure_citation": 0.79749778146611, "structure_error": null, "structure_reason": "The Actual Output presents several unconventional innovations in RAG and includes claims supported by citations marked with [1], [2], and [3]. However, while the claims are generally aligned with the context provided, there are instances where the citations do not directly correspond to specific snippets in the context, particularly regarding the details of the innovations. This leads to a slight deduction in points for potential unsupported claims, despite the overall relevance and coherence of the response.", "hallucination_score": 0.8492585409602414, "hallucination_error": null, "hallucination_reason": "The response identifies several unconventional innovations in Retrieval-Augmented Generation (RAG) that align well with the context provided. It accurately describes concepts like test-time feedback, self-triggered retrieval, and hierarchical training, which are supported by the context discussing adaptive retrieval strategies and the importance of retrieval quality. However, while the claims are mostly supported, the response could benefit from clearer citations or references to specific studies mentioned in the context to strengthen its credibility.", "contexts_used": ["2 Preliminaries\n2.1 Retrieval System\n\nThe core objective of a retrieval system is to identify and rank a subset of documents ( d 1 , d 2 , ..., d k ) from a large corpus C based on their estimated relevance to a query q . Classical retrieval approaches, such as BM25 (Robertson et al., 1995), rely on exact term matching and produce sparse relevance scores. In contrast, dense retrieval methods employ neural encoders to project queries and documents into a shared embedding space, enabling semantic similarity matching (Karpukhin et al., 2020). Since first-stage retrievers often produce noisy candidates, modern pipelines incorporate a second-stage re-ranking step using more expressive models. This includes LLM-based rankers (Pradeep et al., 2023b;\n\nMa et al., 2024; Sun et al., 2023) and reasoningaugmented models such as ReasonIR (Shao et al., 2025), Rank-1 (Weller et al., 2025), and RankR1 (Zhuang et al., 2025), which refine the initial rankings by modeling deeper interactions between the query and candidate documents.", "2.2 Retrieval-Augmented Generation\n\nRetrieval-Augmented Generation (RAG) (Lewis et al., 2020b) is a hybrid paradigm that enhances the generative capabilities of large language models (LLMs) by incorporating non-parametric external knowledge during inference. This design mitigates well-documented limitations of standalone LLMs, including hallucinations, confident but incorrect outputs, and inability to reflect up-to-date or domain-specific information due to static pretraining (Hurst et al., 2024; Grattafiori et al., 2024; Yang et al., 2025a).", "RAG introduces an explicit retrieval step: for a query q , a retriever selects a set of topk documents { d 1 , . . . , d k } from an external corpus. A generator G then conditions on both q and the retrieved context to produce the output a = G ( q, d 1 , . . . , d k ) where G is typically an encoderdecoder or decoder-only LLM fine-tuned to integrate retrieved evidence into its generation process.", "2.3 Challenges in RAG\n\nA central challenge in RAG is that generation quality is tightly coupled with retrieval quality. If relevant (i.e., answer-containing) documents are not retrieved, or if irrelevant context is included, the generator is prone to producing incorrect or misleading outputs (Cuconasu et al., 2024, 2025; Liu et al., 2024).", "Consequently, improving the topk retrieval results is crucial. This can be viewed both as a selection problem (how to retrieve the most relevant documents) and a filtering problem (how to suppress distracting or noisy context). To this end, several methods have been proposed that incorporate various forms of feedback , ranging from simple lexical overlap to more sophisticated agentic or reasoning-based signals, to guide and refine the retrieval process.", "In this survey, we systematically categorize these feedback mechanisms and analyze their effectiveness across different components of the RAG pipeline. We focus on how feedback is acquired, represented, and integrated into retrieval, with the", "Figure 1: Illustration of feedback signals across the RAG pipeline. Feedback can modify the query (e.g., rewriting), the retrieved pool (e.g., ranker-based expansion), or the generation loop (e.g., retrieval triggers based on uncertainty).\n\nRetrieval Reranker results Generation Query Encoder representation new results Ranker Relevance Dynamic Understanding Generated Output Feedback levels Query Level Retrieval Level Generation Time Query Understanding Ranker Relevance Dynamic Retrieval - Query Expansion - Query Rewriting\n\n- Query Vector Adaption\n\n- Query Decomposition\n- Result refinement\n\n- When to retrieve\n\n- What (new query) to retrieve\n\naim of providing a comprehensive taxonomy and highlighting open research challenges.", "5 Generation-time feedback\n\nRAG systems face two fundamental challenges: determining when to retrieve external knowledge, since not all queries benefit from it, and how to retrieve relevant content effectively (Su et al., 2024b). Classical RAG pipelines rigidly follow a fixed sequence of retrieval, optionally ranking, followed by generation, limiting their ability to adapt to the context or task. To address these limitations, recent work has introduced adaptive RAG , where the retrieval strategy is dynamically adjusted according to the query, the model feedback, or the complexity of the task. We categorize this emerging line of work into three main classes.", "5.1 Rule-Based and Discriminative Approaches\n\nIn-Context RALM (Retrieval-Augmented Language Model) (Ram et al., 2023) proposes retrieving relevant context documents during inference at fixed intervals (every s tokens, known as the retrieval stride), using the last l tokens of the input as the retrieval query. In a similar spirit, IRCoT (Interleaving Retrieval in a CoT) (Trivedi et al., 2023) dynamically retrieves documents if the CoT (Wei et al., 2022) step has not provided the answer. At first, it uses the original question to retrieve the context and then uses the last generated CoT sentence as a query for subsequent retrieval. However, both of these methods retrieve the context regardless of whether the LLM needs external context or not. Hence, the unnecessary retrieval steps add additional latency cost during answer generation. Also, the noisy retrieved context can lead to a wrong answer. CtRLA (Huanshuo et al., 2025) devises a latent space probing-based approach for making decisions regarding retrieval timings for adaptive retrieval augmented generation. The authors extract latent vectors that represent abstract concepts like honesty and confidence and use these dimensions to steer retrieval and control LLM behavior, leading to better performance and robust answers. To overcome the over-retrieval limitation of rule-based dynamic RAG methods, retrieval-ondemand approaches have been proposed. These methods trigger retrieval only when the LLM needs it, based on either external feedback (Section 5.2)or the LLM's own assessment (Section 5.3).", "5.2 Retrieval-on-Demand via Feedback Signals\n\nThe feedback signals can come from different sources, including the answer uncertainty, the model's internal states, or context faithfulness and sufficiency. SKR (Wang et al., 2023b) asks LLM itself if additional information is needed to answer the query. If yes, then the retrieval round is triggered; otherwise, the answer is generated from the", "LLM's internal knowledge. However, the judgment is solely based on LLM, and without context, they try to be overconfident (Xiong et al., 2024). FLARE (Jiang et al., 2023) retrieves the documents only if the token probability is below a predefined threshold and uses the last generated sentence as a query for retrieval (excluding the uncertain tokens) and generates the response until the next uncertain token or completion is done. However, these uncertain tokens are not equally important to trigger a retrieval round. Based on this, DRAD (Su et al., 2024a) uses an external module for hallucination detection on entities in the generated answer; if the answer contains hallucination, the retrieval is triggered. The last generated sentence (without a hallucinated entity) is used as a query for retrieval. However, the choice of the new query for retrieval relies on heuristic strategies. Since the model's information needs may extend beyond the last sentence or CoT, it could require context from a broader span of the generation to effectively build confidence. Based on this motivation, DRAGIN (Su et al., 2024b), similar to FLARE, also considers the token probabilities as a criterion of the retrieval round but does not consider the uncertain tokens as a part of the new query. Further, it also reformulates the query using the keywords based on the model's internal attention weights and reasoning. SeaKR (Yao et al., 2025) computes the self-aware uncertainty using internal states of the LLM. If the uncertainty is above a threshold, then a retrieval round is triggered.", "Other types of works, like Rowen (Ding et al., 2024), consider the LLM's answer consistency as feedback. Rowen considers answer consistency across languages of the same question with semantically similar variations, and the consistency over answers generated by different LLMs. If the total consistency is below a predefined threshold, then the retrieval round is triggered. However, similar to SUNAR (V et al., 2025), the consistency can be toward wrong answers.", "However, these approaches consider all queries equally complex and might end up with noisy context retrieval and hence a wrong answer. Adaptive RAG (Jeong et al., 2024) uses a query routing mechanism that predicts whether the query needs retrieval or not. Further, it also decides on the number of retrieval rounds based on query complexity. However, it assumes that the retrieved context is relevant to the query without assessing its relevancy or sufficiency. Towards the relevancy, CRAG", "(Corrective RAG) (Yan et al., 2024) evaluates the relevance scores using a fine-tuned model, and classifies the retrieved document into correct, incorrect, and ambiguous. If the context is not correct, then a rewritten query is issued to the web search engine. Similar fashion, SIM-RAG (Yang et al., 2025b) focuses on the context sufficiency angle, and trains a lightweight critic model that provides feedback if the retrieved context is sufficient to generate the answer. If the information is not sufficient, then a new query is formulated using the original query and the already retrieved context, and a retrieval round is triggered. Further CoV-RAG (He et al., 2024) identifies errors, including reference and answer correctness, and truthfulness, and then scores them using a trained verifier. Based on the scores, either provide a final or rewrite the query and do a further retrieval round.", "5.3 Self-Triggered Retrieval via Reasoning\n\nIn this section, we discuss works where LLM autonomously makes the decision on when to retrieve and how to retrieve through query decomposition or planning-based approaches without external triggers. These approaches are also termed Reasoning RAG or Agentic RAG . These approaches can be divided into mainly two categories: first, where the instructions for query decomposition, when to retrieve, and what to retrieve are provided in the prompt along with few-shot examples; second, where the language models are trained to decide by themselves whether to decompose the query, when to retrieve, and what to retrieve.", "Prompt-Based Methods. DeComP (Khot et al., 2023) divides a task into granular sub-tasks and delegates them to different components through actions. However, DeComP only acts as a trigger for when to retrieve and employs a BM25 retriever for getting relevant documents in a single shot. It does not subsequently generate reasoning steps to improve retrieval, thus not providing much indication as to how to retrieve. ReAct (Yao et al., 2022) interleaves the generation of verbal reasoning traces with actions that interact with the external environment. The verbal reasoning traces act as indicators of how to retrieve, and the actions themselves serve as triggers (when to retrieve). Similarly, SelfAsk (Press et al., 2023) proposes to decompose the original complex query into simpler sub-questions iteratively interleaved by a retrieval step and intermediate answer generation. At each step, the LLM makes a decision to generate a follow-up question if more information is needed, or it may generate the final answer. Authors observed that this approach helped cover diverse aspects of complex queries and improved search and downstream answering performance.", "However, these approaches do not have provision for correction of the entire reasoning trajectory, and an intermediate error may cause cascading failures. Searchain (Xu et al., 2024) proposes to mitigate this by constructing a global reasoning chain first, where each node comprises a retrievaloriented query, an answer from LLM to the query, and a flag indicating if additional knowledge is needed to arrive at a better answer. SMR (State Machine Reasoning) (Lee et al., 2025) identifies the issues of the CoT-based query decomposition and retrieval methods like ReAct (Yao et al., 2022), where the CoT might result in redundant reasoning (new queries that result in the retrieval of the same documents) and misguided reasoning (new query diverges from the user's intent). To address these limitations, SMR proposes three actions: Refine, Rerank, and Stop. Action Refine updates the query using the feedback from the already retrieved documents, and a retrieval round is triggered. Then the retrieved documents are ranked according to the old query to make sure only the relevant information is used to answer. Finally, the Stop action is called to stop the reasoning if a sufficient retrieval quality is achieved, which helps in token efficiency and prevents overthinking.", "MCTS-RAG (Hu et al., 2025) combines Monte Carlo Tree Search (MCTS) with RetrievalAugmented Generation (RAG) to improve reasoning and retrieval in language models. It guides the search for relevant information using MCTS to explore promising retrieval paths, enhancing answer accuracy. However, it is computationally expensive due to the iterative tree search process and may struggle with highly noisy or irrelevant documents. Search-O1 (Li et al., 2025a) proposes an agentic search workflow for reasoning augmented retrieval by letting the Large Reasoning Models (LRMs) like O1 decide autonomously when to retrieve by detecting the presence of salient uncertain words in their output. Additionally, they augment the workflow with a reason-in-documents step, where LRMs analyze the documents in depth to remove noise and reduce redundancy before employing them to generate the final answer.", "Abstract\n\nRetrieval-augmented generation (RAG) has become a fundamental paradigm for addressing the challenges faced by large language models in handling real-time information and domainspecific problems. Traditional RAG systems primarily rely on the in-context learning (ICL) capabilities of the large language model itself. Still, in-depth research on the specific capabilities needed by the RAG generation model is lacking, leading to challenges with inconsistent document quality and retrieval system imperfections. Even the limited studies that fine-tune RAG generative models often lack a granular focus on RAG task or a deeper utilization of chain-of-thought processes . To address this, we propose that RAG models should possess three progressively hierarchical abilities (1) Filtering: the ability to select relevant information; (2) Combination: the ability to combine semantic information across paragraphs; and (3) RAGspecific reasoning: the ability to further process external knowledge using internal knowledge. Thus, we introduce our new RAG instruction fine-tuning method, Hierarchical-Thought Instruction-Tuning Retrieval-Augmented Generation (HIRAG) incorporates a \"think before answering\" strategy. This method enhances the model's open-book examination capability by utilizing multi-level progressive chainof-thought. Experiments show that the HIRAG training strategy significantly improves the model's performance on datasets such as RGB, PopQA, MuSiQue, HotpotQA, and PubmedQA.", "2 Related Work\n\nWhich metric to optimize. Multi-hop QA involves two sub-tasks: retrieving relevant documents, and then answering the question based on the documents. Some methods report document retrievalspecific metrics such as recall [Hsu et al., 2024] whereas others report final answer metrics such as exact match [Jin et al., 2025]. Typically, a model is trained to optimize a particular metric (such as recall) and also evaluated on the same metric. For robustness, in this work we train on the recall metric and test on all metrics, including final answer metrics.", "Prompting-based RAG approaches. With the recent advancements in the capabilities of large API-based LMs, some works explored prompting to call external search/retrievers at inference. Toolformer[Schick et al., 2023] uses a self-supervised objective to train an external model that decides to call tools (like Bing and Google search engines). ReAct [Yao et al., 2023] is another powerful prompting technique that allows the model to structure its outputs as thoughts, actions and observations, yielding significant improvements in the ability of LLMs to interact with external environments. Trivedi et al. [2022a] proposed IRCoT, another prompting strategy that alternates between chain-of-thought [Wei et al., 2022] steps and gathering evidence through retrievals. By using the intermediate traces, the IRCoT is able to decide what to retrieve by issuing the right search queries. Iter-RetGen [Shao et al., 2023] improves evidence gathering in multi-hop scenarios by combining", "retrieval and generation iteratively, such that a model's response is incorporated in the reasoning trace. However, both IRCoT [Trivedi et al., 2022a] and Iter-RetGen [Shao et al., 2023] rely on a fixed or predefined number of retrieval loops at inference, offering limited control over latency.", "Finetuning-based techniques. A prevalent method for multi-hop QA using small LMs is supervised finetuning using reasoning traces from a large LM such as GPT-4 [Asai et al., 2023, Chan et al., 2024]. Other methods are trained to predict the next query to be retrieved [Chan et al., 2024]. Methods that scale the test-time compute that infer using multiple trajectories have also been proposed [Wang et al., 2025]. Recently, reinforcement learning-based techniques have been proposed that develop a reward based on outputting the ground-truth answer [Jin et al., 2025]. However, none of the techniques focus on efficiency of the solution. In fact, in Search-R1, the goal of RL is to increase the number of searches. Instead, we use RL to decrease the average number of searches done by our model.", "Traditional RAG approaches. Early work in grounding generation with real world documents focused on end-to-end differentiable encoder-decoder pipeline REALM [Guu et al., 2020], which augments Masked-Language Modeling (MLM) with a latent retriever model, backpropagating through retrieval to learn both retriever and generator jointly. However, this approach incurs significant computational cost and has only been shown to work with relatively smaller models like T5 [Raffel et al., 2020]. Building on this, Lewis et al. [2020] proposed a general finetuning strategy, RAG-Token which demonstrated that join-training outperforms fixed dense retrieval and BM25.", "RL-based Retrieval Augmented Generation. Recently, framing search query as an RL problem has received attention. LeReT [Hsu et al., 2024] performs preference optimization using diverse few shot prompts leveraging hundred-thousands of ground truth annotated documents. However, LeReT utilizes a fixed amount of compute per instance during inference and cannot be readily generalized to variable-hop scenarios. Similarly, concurrent works, Jin et al. [2025] and Chen et al. [2025] propose end-to-end RL-based optimization that only leverages the final answer annotation. These methods show that RL can effectively be used to teach the search query generator model to issue more search queries for multi-hop problems without considering latency . Our two-stage RL framework, by contrast, first explores without RL to maximize recall and then learns to stop at test time using RL."]}
{"type": "entry", "index": 15, "question": "any results in these papers on fine-tuning an embedder for RAG?", "answer": "Among these papers, only one clearly reports fine-tuning an embedding model specifically to improve RAG.\n\nIn the Generalized RL for Retriever-Specific Query Rewriter work, the text-based RAG system uses a dense encoder called ixi-DocEmbedding as its semantic retriever. This encoder is *further trained* (fine-tuned) on Korean query\u2013document pairs following the BGE-M3 recipe, using contrastive learning (InfoNCE), self-distillation, model merging, and iterative hard-negative mining [4]. The training data mix includes:\n- Curated open-source Korean QA/query\u2013doc datasets\n- High-quality synthetic query\u2013document pairs generated by LLMs\n- Real user interaction logs from deployed RAG systems [4]\n\nThe authors state that, after this fine-tuning, the encoder reaches performance comparable to state-of-the-art Korean embedding models and yields \u201csubstantial improvements on internal RAG evaluation benchmarks\u201d when used as the semantic retriever [4]. So this is a concrete example of fine-tuning an embedder for RAG-style retrieval.\n\nThe other papers focus on different aspects:\n- MusT-RAG varies off-the-shelf retrievers (BM25, Contriever, CLAP) and chunk sizes, and fine-tunes the *generator* for RAG, but does not fine-tune the embedding models themselves [1].\n- BEE-RAG introduces parameter-efficient fine-tuning (LoRA, Prefix-Tuning, Light-BEE) on the *LLM* to handle context better, not on the retriever encoder [2].\n- FrugalRAG and FIRESPARQL discuss fine-tuning for reasoning or SPARQL generation and show that RAG quality can help or hurt, but they do not describe embedding-model fine-tuning [3][5].", "tags": ["_untagged"], "full_faithfulness": 0.8421052632, "full_answer_relevancy": 0.9416858508, "seed_faithfulness": 0.6315789474, "seed_answer_relevancy": 0.9382531749, "context_relevance": 0.8605202072153324, "context_relevance_error": null, "context_relevance_reason": "The context passages provide relevant information about fine-tuning methods for RAG, specifically discussing improvements in factual and contextual performance. The passages directly address the user's inquiry about results related to fine-tuning an embedder for RAG, particularly highlighting performance metrics and comparisons with other models. However, while the information is mostly relevant, some passages could be seen as tangential, which slightly detracts from the overall alignment with the user's question.", "structure_citation": 0.625997493636447, "structure_error": null, "structure_reason": "The Actual Output includes claims supported by citations, specifically referencing the fine-tuning of the ixi-DocEmbedding model [4]. However, while it correctly identifies the relevant papers, it fails to provide sufficient citations for all claims made, particularly regarding the other papers mentioned. For instance, the claims about MusT-RAG [1], BEE-RAG [2], and others are not directly supported by the provided Context snippets, leading to some unsupported assertions. This results in a deduction of points for missing citations and unsupported claims.", "hallucination_score": 0.8515405883036127, "hallucination_error": null, "hallucination_reason": "The response accurately identifies a specific paper that reports fine-tuning an embedding model for RAG, supporting the claim made in the input. It provides detailed information about the methods used in the fine-tuning process and the results achieved, which aligns well with the context provided. However, while it correctly notes that other papers focus on different aspects, it could have more explicitly stated any unsupported claims or inaccuracies in those papers to strengthen the evaluation.", "contexts_used": ["6. RESULT\n6.1 In-domain Performance\n\nQA Fine-tuning Comparing QA fine-tuning with zeroshot performance, factual performance improved by 1.0%, but contextual performance decreased by 5.5%. This sug- gests that while QA fine-tuning is effective in helping the model retain information from the training data, it may also reduce the overall inference capability.", "RAGInference Byutilizing RAG inference without additional training, we were able to address the low factual performance that was an issue with previous LLMs. It demonstrated a 14.6% higher factual performance compared to GPT-4o [35]. Contextual performance improved by 3.6% compared to zero-shot, but was still 4.2% lower than GPT4o.", "RAG Fine-tuning The model fine-tuned on the RAGstyle dataset showed improvements in both types of questions. Compared to RAG inference, factual performance improved by 0.4%, and contextual performance improved by 3.2%. This demonstrates that by learning to leverage context, the model not only improves its memory of information present in the training data but also enhances its overall contextual understanding ability. It exhibited a remarkable 15.0% higher factual performance compared to GPT-4o, and only 1.0% lower contextual performance, which is nearly equivalent. Considering factors such as the model size, amount of training data, and the extent of training, this is an exceptionally high performance.", "| | Embedding | Passage Size | Factual | Contextual |\n|---:|:-------------------|:-------------------|----------:|-------------:|\n| 0 | Gold (Upper Bound) | Gold (Upper Bound) | 97.8 | 97 |\n| 1 | | 512 | 82 | 88.8 |\n| 2 | BM25 [16] | 256 | 82.8 | 88 |\n| 3 | | 128 | 82.2 | 89 |\n| 4 | | 512 | 46.6 | 81 |\n| 5 | Contriever [19] | 256 | 55.6 | 84.2 |\n| 6 | | 128 | 58.2 | 86.6 |\n| 7 | | 512 | 41.2 | 79.6 |\n| 8 | CLAP [20] | 256 | 41 | 84 |\n| 9 | | 128 | 41.8 | 84 |\n\nTable 4 : Llama 3.1 8B Instruct [34] RAG performance on ArtistMus , by different passage size and embeddings.", "6.2 Out-of-domain Performance\n\nTo validate the effectiveness of the MusT-RAG in out-ofdomain scenarios, we conducted experiments by changing the benchmark from ArtistMus to TrustMus [13], using the same framework with in-domain evaluation. The results are presented in Table 3.", "Zero-shot Baselines A similar trend was observed in the zero-shot evaluation for the in-domain setting. MuLLaMA [36] and ChatMusician [12] performed worse than the random baseline (25%), which is due to incorrect answers being counted when the models failed to follow instructions. Given that the overall zero-shot performance closely aligns with the factual scores from the in-domain evaluation, we infer that TrustMus mostly consists of factual questions. The Llama 3.1 8B Instruct [34] model scored 17.2% lower than GPT-4o [35].", "QA Fine-tuning The QA fine-tuned model showed a 3.8% decrease in performance compared to zero-shot, which can be attributed to the fact that models trained on artist data tend to forget information about out-of-domain topics, such as Instrument and Genre .", "RAG Inference RAG inference led to an 5.0% performance improvement over zero-shot, demonstrating that MusT-RAG framework is also helpful for out-of-domain data, such as The Grove Dictionary Online [15], which is the basis for the TrustMus benchmark.", "RAG Fine-tuning The RAG fine-tuned model showed a 0.7% improvement over RAG inference, even with the same artist data used for QA fine-tuning. This supports the fact that the RAG fine-tuning method, which incorporates context, enhances the model's robustness in contextual understanding, even for out-of-domain data.", "6.3 Ablation Study: Retriever Configurations\n\nTable 4 shows the results of the RAG inference for the Llama 3.1 8B Instruct [34] with various passage sizes and embeddings, evaluated under the same total computation budget for fair comparison. The performance on contextual questions tended to improve as the passage size decreased across all embedding models. In contrast, for\n\nFigure 2 : RAG performance and retrieval time for Wikipedia Corpus [8] and MusWikiDB.\n\nWikipedia Corpus\nMusWikiDB\n\nPerformance & Retrieval Time\n\n85.4 0.0384 0.04 79.5 0.03 0.02 0.01 0.0038 0.00 Performance Retrieval Time", "factual questions, only Contriever [19] showed clear improvements with shorter passages, while BM25 [16] and CLAP [20] showed little to no change in performance across different passage lengths. For factual questions, there was a significant performance gap between BM25 and the other two dense embeddings. This is likely because ArtistMus places high importance on music entities such as artist and albums. Overall, the best performance was achieved using BM25 with a passage size of 128. When compared to the gold context, the factual performance was 15.6% lower, and the contextual performance was 8.0% lower. In Figure 2, we compare the RAG inference performance using the Wikipedia corpus [8] and MusWikiDB. The results show that MusWikiDB achieves a 10x faster retrieval speed and 5.9% higher performance.", "Experimental Settings\n\nDatasets We conduct experiments on both single-hop and multi-hop question-answering (QA) tasks. Specifically, we evaluate our approach on the Natural Questions (NQ) (Kwiatkowski et al. 2019), TriviaQA (Joshi et al. 2017), HotpotQA (Yang et al. 2018), and 2WikiMultihopQA (2WikiQA) (Ho et al. 2020) datasets.", "Metrics We use Exact Match ( EM ) (Lee, Chang, and Toutanova 2019) and F1 (Karpukhin et al. 2020) to assess the QA accuracy of retrieval-augmented LLMs, which are commonly adopted in QA evaluation. EM checks if predicted answers exactly match the ground truth, while F1 measures their overlap in precision and recall.", "Implementation Details We select LLaMA-3-8B (Dubey et al. 2024) and Qwen-2.5-7B (Yang et al. 2024) as base LLM. For the zero-shot setting (Section ), we derive the theoretical values for \u00b5 and \u03c3 2 based on Equation 9, and perform a search around these values to determine their optimal configuration. For the adaptive balancing entropy factor learning, we set d r = 8 . We conduct the experiments on 8 NVIDIA A100 40GB GPUs, with a learning rate of 5e-4, a cosine learning rate scheduler, and a batch size of 16. To demonstrate the robustness of our training approach, we train the LLMs for no more than 500 steps solely on the NQdataset and subsequently validate its performance across multiple datasets.", "Baselines We introduce two types of baselines for comparison with our approach, including zero-shot methods and lightweight fine-tuning methods.", "Zero-shot methods. We select several prompt-based strategies that enhance generation performance in RAG scenarios for comparison, including prompt-based methods (Chain-of-Thought (Wei et al. 2022), Chain-of-Note (Yu et al. 2023), Self-Critic (Asai et al. 2023)), architecturedriven methods (PCW (Ratner et al. 2023), PositionEngineering (He et al. 2024)), and attention reweighting method (Multiply-Attention (Chiang and Cholak 2022)).", "Parametric-efficient fine-tuning methods: We choose two commonly used PEFT approaches in RAG, including LoRA (Hu et al. 2022) and Prefix-Tuning (Li and Liang 2021) for comparison with our parameter-efficient training strategy.", "Main Results\n\nTable 1 presents the results of our approach and the baselines on four RAG tasks of open-domain QA. It can be observed that:\n\nFirst, our proposed method outperforms all other baselines in terms of average QA performance. Whether in the zero-shot setting or with lightweight fine-tuning, BEE-RAG consistently achieves the best results. This highlights the effectiveness of our key motivation: the reduction of context entropy and dynamic attention allocation on documents, which proves to be applicable to both single-hop and multihop datasets.", "Second, our method demonstrates its ability to handle more challenging datasets, assisting LLMs in effectively addressing complex RAG tasks. On relatively more diffi-", "| | Settings | NQ.EM | NQ.F1 | TriviaQA.EM | TriviaQA.F1 | HotpotQA.EM | HotpotQA.F1 | 2WikiQA.EM | 2WikiQA.F1 | Average.EM | Average.F1 |\n|---:|:------------------------|:------------|:------------|:--------------|:--------------|:--------------|:--------------|:-------------|:-------------|:-------------|:-------------|\n| 0 | Qwen-2.5-7B | Qwen-2.5-7B | Qwen-2.5-7B | Qwen-2.5-7B | Qwen-2.5-7B | Qwen-2.5-7B | Qwen-2.5-7B | Qwen-2.5-7B | Qwen-2.5-7B | Qwen-2.5-7B | Qwen-2.5-7B |\n| 1 | Vanilla RAG | 26.84 | 42.03 | 43.05 | 53.41 | 24.54 | 35.16 | 20.16 | 27.16 | 28.65 | 39.44 |\n| 2 | Chain-of-Thought | 26.86 | 40.20 | 39.59 | 51.78 | 26.20 | 35.88 | 22.93 | 26.58 | 28.89 | 38.61 |\n| 3 | Chain-of-Note | 26.50 | 41.75 | 41.20 | 52.03 | 26.74 | 35.95 | 19.93 | 26.58 | 28.59 | 39.08 |\n| 4 | PCW | 23.87 | 38.21 | 40.88 | 51.50 | 24.72 | 34.63 | 23.11 | 27.19 | 28.15 | 37.88 |\n| 5 | Position-Engineering | 25.76 | 38.61 | 41.38 | 52.30 | 27.77 | 36.56 | 22.69 | 26.96 | 29.40 | 38.60 |\n| 6 | Self-Critic | 24.27 | 35.36 | 40.74 | 52.04 | 25.11 | 35.35 | 22.00 | 27.51 | 28.03 | 37.57 |\n| 7 | Multiply-Attention | 26.18 | 41.43 | 42.92 | 53.62 | 27.13 | 34.94 | 18.87 | 24.02 | 28.78 | 38.50 |\n| 8 | Zero-BEE (ours) | 29.03 | 43.15 | 44.32 | 55.93 | 27.86 | 37.15 | 24.47 | 29.61 | 31.42 \u2020 | 41.46 \u2020 |\n| 9 | LoRA (2.06M) | 37.62 | 47.93 | 47.98 | 56.32 | 28.44 | 38.92 | 20.76 | 26.30 | 33.70 | 42.37 |\n| 10 | Prefix-Tuning (2.07M) | 36.75 | 46.33 | 48.21 | 56.32 | 31.19 | 40.15 | 23.69 | 29.19 | 34.96 | 43.00 |\n| 11 | Light-BEE (ours, 2.06M) | 38.71 | 50.11 | 49.39 | 56.73 | 35.95 | 44.81 | 28.32 | 31.73 | 38.09 \u2020 | 45.84 \u2020 |\n| 12 | LLaMA-3-8B | LLaMA-3-8B | LLaMA-3-8B | LLaMA-3-8B | LLaMA-3-8B | LLaMA-3-8B | LLaMA-3-8B | LLaMA-3-8B | LLaMA-3-8B | LLaMA-3-8B | LLaMA-3-8B |\n| 13 | Vanilla RAG | 27.13 | 37.77 | 41.17 | 55.46 | 23.07 | 34.22 | 14.20 | 23.74 | 26.39 | 37.80 |\n| 14 | Chain-of-Thought | 29.10 | 38.60 | 45.31 | 56.96 | 22.74 | 30.23 | 14.50 | 25.58 | 27.91 | 37.84 |\n| 15 | Chain-of-Note | 28.46 | 37.83 | 41.49 | 51.22 | 21.34 | 28.99 | 19.87 | 29.50 | 27.79 | 36.89 |\n| 16 | PCW | 25.71 | 35.47 | 38.72 | 48.99 | 17.14 | 25.92 | 17.88 | 26.65 | 24.86 | 34.26 |\n| 17 | Position-Engineering | 25.98 | 35.72 | 41.01 | 54.21 | 23.24 | 30.62 | 16.08 | 26.61 | 26.58 | 36.79 |\n| 18 | Self-Critic | 20.86 | 27.06 | 42.61 | 51.06 | 23.77 | 31.21 | 15.50 | 25.94 | 25.69 | 33.82 |\n| 19 | Multiply-Attention | 27.08 | 36.33 | 43.40 | 55.05 | 21.27 | 28.24 | 11.87 | 23.74 | 25.91 | 35.84 |\n| 20 | Zero-BEE (ours) | 30.50 | 38.96 | 45.67 | 57.04 | 23.84 | 34.29 | 20.67 | 30.56 | 30.17 \u2020 | 40.21 \u2020 |\n| 21 | LoRA (2.62M) | 35.39 | 48.68 | 53.65 | 58.55 | 33.15 | 42.15 | 18.91 | 28.67 | 35.27 | 44.51 |\n| 22 | Prefix-Tuning (2.63M) | 36.39 | 48.25 | 48.75 | 56.48 | 32.30 | 42.50 | 16.33 | 25.13 | 33.44 | 43.09 |\n| 23 | Light-BEE (ours, 2.62M) | 38.42 | 49.23 | 53.96 | 58.67 | 38.06 | 44.72 | 24.70 | 31.89 | 38.79 \u2020 | 46.13 \u2020 |", "Table 1: Main results of BEE-RAG on four RAG tasks with different LLM backbones for both zero-shot inference and lightweight fine-tuning settings. Zero-BEE denotes the zero-shot inference version of BEE-RAG, while Light-BEE denotes the parameter-efficient (lightweight) fine-tuning version. All results are averaged over 8 runs; \u2020 indicates statistical significance with p < 0 . 05 .", "cult multi-hop datasets, BEE-RAG shows significant performance improvements. Notably, on the latest 2WikiMultihopQA dataset, our approach achieves an approximately 5% improvement over vanilla RAG, further highlighting the effectiveness of our approach.", "Third, our parametric-efficient fine-tuning method demonstrates better generalization performance with few parameters, emphasizing the effectiveness of the proposed adaptive balancing entropy factor learning approach. The result shows that whether on trained or zero-shot datasets, our method outperforms the others. Notably, the performance of the other two methods may degrade when transferred to multi-hop tasks, revealing their limitations in handling challenging RAG tasks.", "Abstract\n\nWe consider the problem of answering complex questions, given access to a large unstructured document corpus. The de facto approach to solving the problem is to leverage language models that (iteratively) retrieve and reason through the retrieved documents, until the model has sufficient information to generate an answer. Attempts at improving this approach focus on retrieval-augmented generation (RAG) metrics such as accuracy and recall and can be categorized into two types: (a) fine-tuning on large question answering (QA) datasets augmented with chain-ofthought traces, and (b) leveraging RL-based fine-tuning techniques that rely on question-document relevance signals. However, efficiency in the number of retrieval searches is an equally important metric, which has received less attention. In this work, we show that: (1) Large-scale fine-tuning is not needed to improve RAG metrics, contrary to popular claims in recent literature. Specifically, a standard ReAct pipeline with improved prompts can outperform state-of-the-art methods on benchmarks such as HotPotQA. (2) Supervised and RL-based fine-tuning can help RAG from the perspective of frugality, i.e., the latency due to number of searches at inference time. For example, we show that we can achieve competitive RAG metrics at nearly half the cost (in terms of number of searches) on popular RAG benchmarks, using the same base model, and at a small training cost (1000 examples).", "RAG System Implementation\n\nIn this experiment, we implemented two distinct RetrievalAugmented Generation (RAG) systems. The first is a multimodal RAG chatbot that generates responses using multimodal embeddings (derived from image-based documents)\n\ncombined with user queries. The second is a text-based RAG chatbot that utilizes parsed text chunks.", "For the multi-modal RAG system, we employed ColQwen2.5-3B (Faysse et al. 2024) for both document parsing and retrieval. We set the MAX_TOKEN limit for image encoding to 764, aligning it with the token size used for text encoding. To optimize multi-modal RAG performance, we pre-computed embeddings for image-based documents and performed retrieval without late interaction.", "For the text-based RAG system, we utilized an inhouse document parser, AI Parser , to convert documents (e.g., PDFs) into text chunks. We adopted three in-house retrievers: ixi-RAG lexical , ixi-RAG semantic , and ixi-RAG hybrid .", "ixi-RAG lexical is a traditional information retrieval system built upon an OpenSearch index and utilizing the BM25 scoring algorithm. This retriever operates by matching the exact tokens present in the user's query against the text chunks in the knowledge base. It excels at precision when the query contains specific terms, acronyms, or identifiers that are also present in the source documents, as its ranking is based on term frequency (TF) and inverse document frequency (IDF).", "ixi-RAG semantic is a modern retrieval system that operates on the principle of conceptual similarity rather than keyword matching. To this end, we utilized ixi-DocEmbedding , an embedding model specialized for Korean documents. The model was further trained on Korean query-document pairs using a domain-specific finetuning approach based on BGE-M3 (Chen et al. 2024). For training the retriever, we constructed a comprehensive corpus of Korean query-document pairs drawn from three sources: (1) carefully-curated open-source datasets, (2) high-quality synthetic pairs generated with large language models, and (3) production-grade interaction logs collected from deployed RAG systems. To enhance the learning signal, we employed iterative hard-negative mining and trained the encoder according to the BGE-M3 fine-tuning recipe, which couples InfoNCE contrastive objectives with selfdistillation and model-merging strategies. After training, we only use the dense retrieval as the semantic retriever. Empirically, the resulting model demonstrates performance on par with state-of-the-art open-source Korean encoders and achieves substantial improvements on internal RAG evaluation benchmarks. It converts both the user's query and the document chunks into high-dimensional vector embeddings. Retrieval is then performed by conducting a k-nearest neighbor (k-NN) (Fix 1985) search within this vector space, identifying documents whose embeddings are closest to the query's embedding. This approach allows the system to understand the user's intent and retrieve relevant information even if the phrasing is different and there is no direct keyword overlap.", "ixi-RAG hybrid combines both lexical and semantic approaches for improved retrieval. It is designed to leverage the complementary strengths of both retrieval paradigms. It first gathers a broad set of candidate documents by running lexical and semantic searches in parallel. The ranked lists from both retrievers are then fused using the Recip-", "| | Query Rewriter | Document Recognizer | Document Retriever | Train Data |\n|---:|:------------------|:----------------------|:---------------------|:---------------|\n| 0 | RL-QR multi-modal | ColQwen2.5-v0.2 | ColQwen2.5-v0.2 | D mm D tm D tm |\n| 1 | RL-QR lexical | AI Parser | ixi-RAG lexical | |\n| 2 | RL-QR semantic | AI Parser | ixi-RAG semantic | |\n| 3 | RL-QR hybrid | AI Parser | ixi-RAG hybrid | D tm |\n\nTable 2: RL-QR Training Scheme\n\nrocal Rank Fusion (RRF) algorithm (Cormack, Clarke, and Buettcher 2009). RRF calculates a new score for each document based on its position in the individual rankings, producing a single, more robustly ordered list that balances keyword relevance with semantic similarity. This method effectively captures both the precision of lexical search and the contextual understanding of semantic search to deliver a highly refined final ranking.", "6 Discussion\n\nWe discuss the key findings from the experiments and discuss limitations and future work.", "6.1 Discussion on the results\n\nOur experimental findings provide several key insights into the effectiveness of the FIRESPARQL framework and the performance of different strategies for SPARQL query generation with different epoch settings and different model sizes.", "Fine-Tuning Performance As shown in Table 1, fine-tuning LLMs on NLQ-SPARQL pairs leads to significant improvements over both the zero-shot baseline and the one-shot state-of-the-art methods. The highest performance is achieved by the fine-tuned LLaMA-3-8B-Instruct model trained for 15 epochs, attaining scores of 0.77 (BLEU-4), 0.91 (ROUGE-1), 0.86 (ROUGE-2), 0.90 (ROUGE-L), and 0.85 (RelaxedEM on all test cases). These results indicate that, across both syntactic-level (BLEU, ROUGE) and execution-level (RelaxedEM) evaluations, SPARQL queries generated by finetuned models are not only more accurate but also structurally well-formed and executable. This highlights the effectiveness of supervised adaptation for learning the ontology and structure of the underlying SKG during training.", "Model Size Impact As shown in Fig 3, LLaMA-3-8B-Instruct consistently outperforms LLaMA-3.2-3B-Instruct after fine-tuning across all evaluation metrics. This demonstrates that larger model capacity enhances the ability to internalize domain-specific patterns from training data, including the structure and semantics of the target SKG. Interestingly, the trend reverses in the one-shot setting: LLaMA-3.2-3B-Instruct performs better than the 8B variant on most metrics, except for RelaxedEM(success), as shown in Table 1. This performance gap might be attributed to the fact that LLaMA-3.2-3B-Instruct was released after LLaMA-3-8B-Instruct and incorporates pruning and distillation techniques, which were specifically applied to the 1B and 3B variants [1]. These techniques help to preserve performance while significantly improving efficiency, making the LLaMA-3.2-3B-Instruct model capable of strong instruction-following performance on resource-constrained devices. As a result, despite its smaller size, LLaMA-3.2-3B-Instruct may benefit from a more refined architecture and training strategies, allowing it to better leverage in-context examples in one-shot settings compared to the larger, but earlier, 8B model.\n\nBLEU-4 Avg ROUGE-L 0.875 0.70 0.825 0,65 0.800 0.60 0,750 0,725 Epochs RelaxedEM (success) RelaxedEM (all) Avg\n\nFig. 3: Average BLEU-4, ROUGE-L and RelaxedEM scores with different epochs on different fine-tuned models", "RAG Performance As shown in Table 1, the score of RelaxedEM(all) drops from 0.85 to 0.29 when incorporating RAG into the fine-tuned LLaMA-3-8BInstruct trained for 15 epochs, which does not lead to additional performance gains, and it even degrades performance. This decline can be attributed to the noisy or misaligned nature of the retrieved context-such as incorrect or irrelevant property suggestions from the ORKG-which may introduce confusion instead of providing useful guidance since we don't have a context checker to check if the context is relevant or not. Prior studies [10, 11] have similarly observed that low-quality RAG context can conflict with the knowledge already encoded in fine-tuned models, ultimately leading to reduced task performance.", "One-Shot Performance As shown in Table 1, the one-shot setting-using the most similar example from the training set-achieved strong performance, second only to the fine-tuned models. Specifically, the one-shot approach reached scores of 0.58 (BLEU-4), 0.81 (ROUGE-1), 0.73 (ROUGE-2), 0.78 (ROUGEL), and 0.40 (RelaxedEM on all test cases) on LLaMA-3.2-3B-Instruct model. These results suggest that in the absence of high-quality datasets for fine-tuning, one-shot learning offers a simple yet effective alternative for SPARQL query generation.", "Training Epoch Sensitivity As shown in Figure 3, the number of fine-tuning epochs has a significant impact on all metrics for both LLaMA-3.2-3B-Instruct and LLaMA-3-8B-Instruct models. First, both models start with high scores on all metrics at epoch 3 and then slightly decline during the early training phases (epochs 3-7), suggesting that the models may require sufficient training time to properly internalize the SKG-specific structures and semantics. Second, the training dynamics reveal an upward trend in performance from 7 epochs onward, with the best performance at 20 epochs for the 3B model and 15 epochs for the 8B model. This indicates that larger models tend to converge faster and exhibit stronger generalization early on, while smaller models require more epochs to achieve competitive performance.", "6.2 Error analysis on failed SPARQL queries\n\nWe further analyzed the generated SPARQL queries that either failed to execute or returned empty results, by comparing them with the corresponding ground truth queries. Under the best-performing configuration-using the fine-tuned LLaMA3-8B-Instruct model trained for 15 epochs-448 out of 513 generated SPARQL queries were executed successfully via QLever without any syntax errors and returned meaningful results. Meanwhile, 14 queries failed due to syntax errors, and 51 queries were executed successfully but returned empty results.", "To better understand the causes of failure, we examined the error messages for the 14 syntactically invalid queries and inspected the queries that returned empty results. Our analysis revealed that 11 out of the 14 syntactically invalid queries shared the same error message:", "Invalid SPARQL query: Variable ?metric is selected but not aggregated. All non-aggregated variables must be part of the GROUP BY clause. Note: The GROUP BY in this query is implicit because an aggregate expression was used in the SELECT clause.", "This indicates that these queries included aggregate functions (e.g., MAX(?value) ) in the SELECT clause but did not include the non-aggregated variables (e.g., ?metric , ?metric_lbl ) in a GROUP BY clause. In SPARQL 1.1, such usage is invalid unless all non-aggregated variables are explicitly grouped. This reflects a lack of adherence to SPARQL's syntax rules around aggregation and grouping.\n\nThe remaining 3 queries failed with the following error:\n\nInvalid SPARQL query: Token 'SELECT': mismatched input 'SELECT' expecting '}'."]}
{"type": "entry", "index": 16, "question": "rag pipeline noisy answers cause?", "answer": "Noisy answers in a RAG pipeline usually come from several points of failure working together:\n\n1) Bad or partially relevant retrieval\nIf the retriever pulls passages that are off-topic, ambiguous, or use different terminology than the question, the model is forced to reason over wrong or mixed signals and will answer incorrectly or irrelevantly [1]. Using too many passages can overwhelm the model with irrelevant context (\u201cneedle in a haystack\u201d), making it miss the key evidence [1][4].\n\n2) Over-retrieval and unconditional retrieval\nPipelines that always retrieve or retrieve repeatedly for every query can flood the context with unnecessary or noisy documents, which the model may latch onto and follow in the wrong direction [2]. Adaptive methods show that unneeded retrieval and noisy context are directly linked to wrong answers [2].\n\n3) Poor assessment of relevance/sufficiency\nSome systems assume that whatever was retrieved is good enough and don\u2019t check if it\u2019s actually relevant or sufficient, which means low-quality context goes straight into generation and misleads the model [2].\n\n4) Model\u2013context integration issues\nLLMs may underuse correct external evidence and instead rely on their internal (possibly outdated or wrong) knowledge, especially in deeper layers, which can override correct retrieved facts and cause hallucinations [1][3][5]. Contradictory or noisy passages further confuse this integration [1][3].\n\n5) Upstream knowledge or extraction errors\nWhen the external store (e.g., a knowledge graph) is built with imperfect extraction\u2014missing or erroneous triples from domain jargon or ambiguity\u2014the retriever can only supply flawed facts, which then produce noisy answers even if retrieval itself works well [6].\n\nAll of these factors can compound: slightly off retrieval plus overconfident model reasoning over noisy context often yields confident but incorrect answers.\n", "tags": ["_untagged"], "full_faithfulness": 1.0, "full_answer_relevancy": 0.9164118960000001, "seed_faithfulness": 0.9500000000000001, "seed_answer_relevancy": 0.9197988500000001, "context_relevance": 0.5655099083264827, "context_relevance_error": null, "context_relevance_reason": "The context passages provide relevant information about challenges in RAG, including issues related to noisy answers and retrieval quality. However, while some passages touch on the impact of retrieval on answer accuracy, they do not directly address the specific cause of noisy answers, which limits the overall relevance to the user's question.", "structure_citation": 0.5677622646444508, "structure_error": null, "structure_reason": "The Actual Output includes several claims supported by citations, but there are issues with citation accuracy. For instance, the citations [1], [2], and [3] are used multiple times, but not all claims are directly supported by the corresponding snippets in the Context. Additionally, some citations do not match the content of the Context, leading to unsupported claims. While the response is informative and addresses the question, the citation issues detract from its overall reliability.", "hallucination_score": 0.8155609756579316, "hallucination_error": null, "hallucination_reason": "The response effectively identifies multiple sources of noisy answers in a RAG pipeline, aligning well with the context provided. It discusses issues such as bad retrieval, over-retrieval, and model-context integration, which are all supported by the context's emphasis on retrieval quality and relevance. However, while the claims are mostly supported, the response could benefit from more explicit connections to specific context details, particularly regarding the challenges of integrating retrieved evidence with LLMs, which could enhance clarity and depth.", "contexts_used": ["7 Challenges of RAG\n\nThis section discusses the challenges of RAG, cases of manifestation of such challenges in the selected domain of RAG application, and outlines existing solutions and the way forward.", "7.1.1 Retrieval Quality and Relevance\n\nThe quality of retrieved documents significantly impacts the accuracy of RAG-generated answers. High recall and precision are critical since poor retrieval leads directly to incorrect or irrelevant answers [61]. Traditional methods like BM25 are limited, often missing relevant texts or returning noisy results [12]. Modern neural retrievers with dense embeddings improve performance but still face issues like vocabulary mismatches, ambiguous queries, and domain-specific terminology. Specialized domain tuning, such as using legal embeddings or medical synonym expansion, can help, but maintenance of these tailored retrievers remains challenging. Determining the optimal number of retrieved passages ( k ) is also complex. Too few passages limit evidence; too many overwhelm the model and introduce irrelevant context. Approaches like ranking retrieved passages or iterative query reformulation can improve retrieval precision, but add complexity and latency [34, 21].", "7.1.2 Latency and Efficiency\n\nRAG inherently increases computational complexity and latency compared to standalone LLMs due to retrieval overhead, vector searches, and expanded context processing. Techniques like approximate nearest neighbor indices (e.g., FAISS, HNSW), caching, model distillation, or lightweight retrievers can reduce latency at the expense of accuracy. Integrating retrieval efficiently with large language models (LLMs) and ensuring rapid responses in real-time scenarios (e.g., customer support) remains a significant challenge [3]. Interestingly, using retrieval can allow smaller models to match the performance of larger models without retrieval (e.g., RETRO, Atlas), reducing model size requirements but shifting complexity to maintaining external knowledge bases and infrastructure.", "7.1.3 Integration with Large Language Models\n\nIntegrating retrieved evidence effectively with LLMs is subtle. Models may ignore retrieved evidence, especially when internal model knowledge conflicts with external retrieved information, leading to a \"tug-of-war\" effect [41]. Multiple retrieved documents might create confusion or confirmation bias if they contradict each other. Limited input lengths in transformer-based LLMs exacerbate these integration challenges by forcing truncation or summarization, potentially omitting essential context. Fine-tuning models specifically for retrieval-augmented tasks often yields better integration than simple zero-shot prompting but introduces complexity, especially when using non-differentiable or API-based models that do not support custom training.", "7.2.1 Scalability and Infrastructure\n\nDeploying RAG at scale requires substantial engineering to maintain large knowledge corpora and efficient retrieval indices. Systems must handle millions or billions of documents, demanding significant computational resources, efficient indexing, distributed computing infrastructure, and cost management strategies [21]. Efficient indexing methods, caching, and multi-tier retrieval approaches (such as cascaded retrieval) become essential at scale, especially in large deployments like web search engines.", "7.2.2 Freshness and Knowledge Updates\n\nOne motivation for RAG is providing current information. However, continuously updating external knowledge bases and retrieval indices is challenging. Domains requiring real-time updates (e.g., finance, healthcare) demand sophisticated data pipelines for incremental updates, possibly frequent re-encoding of documents, and synchronization of retrieval indices. Delays in updates or inconsistencies between the LLM's internal knowledge and newly retrieved data can produce outdated or contradictory answers [61].", "7.2.3 Hallucination and Reliability\n\nWhile RAG reduces LLM hallucinations, it does not eliminate them completely. Models may fabricate or misattribute information if retrieval provides incomplete or partially contradictory context.", "Legal domain studies found that RAG significantly reduces hallucinations, but still generates errors at concerning rates [56]. Hallucinations also occur in citation generation, with models occasionally inventing nonexistent references. Strategies such as verifying outputs against retrieved sources or calibrating model confidence are needed, but no approach completely prevents hallucination.", "HIPAA), transparent data usage policies, and security testing are essential to protect privacy and prevent breaches.", "7.3.4 Accountability and Transparency\n\nRAG's use of sourced retrieval provides an", "7.2.4 Complex Pipeline and Maintenance\n\nadvantage for accountability, allowing users to trace AI-generated responses back to evidence. However, inaccurate citations or improper synthesis can mislead users. Ethical deployment involves clearly attributing evidence, providing explanations on request, managing user expectations, and clearly delineating accountability-especially when RAG informs critical decisions. Transparency and accountability require ongoing evaluation, oversight, and mechanisms for user feedback and correction [56].", "5 Generation-time feedback\n\nRAG systems face two fundamental challenges: determining when to retrieve external knowledge, since not all queries benefit from it, and how to retrieve relevant content effectively (Su et al., 2024b). Classical RAG pipelines rigidly follow a fixed sequence of retrieval, optionally ranking, followed by generation, limiting their ability to adapt to the context or task. To address these limitations, recent work has introduced adaptive RAG , where the retrieval strategy is dynamically adjusted according to the query, the model feedback, or the complexity of the task. We categorize this emerging line of work into three main classes.", "5.1 Rule-Based and Discriminative Approaches\n\nIn-Context RALM (Retrieval-Augmented Language Model) (Ram et al., 2023) proposes retrieving relevant context documents during inference at fixed intervals (every s tokens, known as the retrieval stride), using the last l tokens of the input as the retrieval query. In a similar spirit, IRCoT (Interleaving Retrieval in a CoT) (Trivedi et al., 2023) dynamically retrieves documents if the CoT (Wei et al., 2022) step has not provided the answer. At first, it uses the original question to retrieve the context and then uses the last generated CoT sentence as a query for subsequent retrieval. However, both of these methods retrieve the context regardless of whether the LLM needs external context or not. Hence, the unnecessary retrieval steps add additional latency cost during answer generation. Also, the noisy retrieved context can lead to a wrong answer. CtRLA (Huanshuo et al., 2025) devises a latent space probing-based approach for making decisions regarding retrieval timings for adaptive retrieval augmented generation. The authors extract latent vectors that represent abstract concepts like honesty and confidence and use these dimensions to steer retrieval and control LLM behavior, leading to better performance and robust answers. To overcome the over-retrieval limitation of rule-based dynamic RAG methods, retrieval-ondemand approaches have been proposed. These methods trigger retrieval only when the LLM needs it, based on either external feedback (Section 5.2)or the LLM's own assessment (Section 5.3).", "5.2 Retrieval-on-Demand via Feedback Signals\n\nThe feedback signals can come from different sources, including the answer uncertainty, the model's internal states, or context faithfulness and sufficiency. SKR (Wang et al., 2023b) asks LLM itself if additional information is needed to answer the query. If yes, then the retrieval round is triggered; otherwise, the answer is generated from the", "LLM's internal knowledge. However, the judgment is solely based on LLM, and without context, they try to be overconfident (Xiong et al., 2024). FLARE (Jiang et al., 2023) retrieves the documents only if the token probability is below a predefined threshold and uses the last generated sentence as a query for retrieval (excluding the uncertain tokens) and generates the response until the next uncertain token or completion is done. However, these uncertain tokens are not equally important to trigger a retrieval round. Based on this, DRAD (Su et al., 2024a) uses an external module for hallucination detection on entities in the generated answer; if the answer contains hallucination, the retrieval is triggered. The last generated sentence (without a hallucinated entity) is used as a query for retrieval. However, the choice of the new query for retrieval relies on heuristic strategies. Since the model's information needs may extend beyond the last sentence or CoT, it could require context from a broader span of the generation to effectively build confidence. Based on this motivation, DRAGIN (Su et al., 2024b), similar to FLARE, also considers the token probabilities as a criterion of the retrieval round but does not consider the uncertain tokens as a part of the new query. Further, it also reformulates the query using the keywords based on the model's internal attention weights and reasoning. SeaKR (Yao et al., 2025) computes the self-aware uncertainty using internal states of the LLM. If the uncertainty is above a threshold, then a retrieval round is triggered.", "Other types of works, like Rowen (Ding et al., 2024), consider the LLM's answer consistency as feedback. Rowen considers answer consistency across languages of the same question with semantically similar variations, and the consistency over answers generated by different LLMs. If the total consistency is below a predefined threshold, then the retrieval round is triggered. However, similar to SUNAR (V et al., 2025), the consistency can be toward wrong answers.", "However, these approaches consider all queries equally complex and might end up with noisy context retrieval and hence a wrong answer. Adaptive RAG (Jeong et al., 2024) uses a query routing mechanism that predicts whether the query needs retrieval or not. Further, it also decides on the number of retrieval rounds based on query complexity. However, it assumes that the retrieved context is relevant to the query without assessing its relevancy or sufficiency. Towards the relevancy, CRAG", "(Corrective RAG) (Yan et al., 2024) evaluates the relevance scores using a fine-tuned model, and classifies the retrieved document into correct, incorrect, and ambiguous. If the context is not correct, then a rewritten query is issued to the web search engine. Similar fashion, SIM-RAG (Yang et al., 2025b) focuses on the context sufficiency angle, and trains a lightweight critic model that provides feedback if the retrieved context is sufficient to generate the answer. If the information is not sufficient, then a new query is formulated using the original query and the already retrieved context, and a retrieval round is triggered. Further CoV-RAG (He et al., 2024) identifies errors, including reference and answer correctness, and truthfulness, and then scores them using a trained verifier. Based on the scores, either provide a final or rewrite the query and do a further retrieval round.", "5.3 Self-Triggered Retrieval via Reasoning\n\nIn this section, we discuss works where LLM autonomously makes the decision on when to retrieve and how to retrieve through query decomposition or planning-based approaches without external triggers. These approaches are also termed Reasoning RAG or Agentic RAG . These approaches can be divided into mainly two categories: first, where the instructions for query decomposition, when to retrieve, and what to retrieve are provided in the prompt along with few-shot examples; second, where the language models are trained to decide by themselves whether to decompose the query, when to retrieve, and what to retrieve.", "Prompt-Based Methods. DeComP (Khot et al., 2023) divides a task into granular sub-tasks and delegates them to different components through actions. However, DeComP only acts as a trigger for when to retrieve and employs a BM25 retriever for getting relevant documents in a single shot. It does not subsequently generate reasoning steps to improve retrieval, thus not providing much indication as to how to retrieve. ReAct (Yao et al., 2022) interleaves the generation of verbal reasoning traces with actions that interact with the external environment. The verbal reasoning traces act as indicators of how to retrieve, and the actions themselves serve as triggers (when to retrieve). Similarly, SelfAsk (Press et al., 2023) proposes to decompose the original complex query into simpler sub-questions iteratively interleaved by a retrieval step and intermediate answer generation. At each step, the LLM makes a decision to generate a follow-up question if more information is needed, or it may generate the final answer. Authors observed that this approach helped cover diverse aspects of complex queries and improved search and downstream answering performance.", "However, these approaches do not have provision for correction of the entire reasoning trajectory, and an intermediate error may cause cascading failures. Searchain (Xu et al., 2024) proposes to mitigate this by constructing a global reasoning chain first, where each node comprises a retrievaloriented query, an answer from LLM to the query, and a flag indicating if additional knowledge is needed to arrive at a better answer. SMR (State Machine Reasoning) (Lee et al., 2025) identifies the issues of the CoT-based query decomposition and retrieval methods like ReAct (Yao et al., 2022), where the CoT might result in redundant reasoning (new queries that result in the retrieval of the same documents) and misguided reasoning (new query diverges from the user's intent). To address these limitations, SMR proposes three actions: Refine, Rerank, and Stop. Action Refine updates the query using the feedback from the already retrieved documents, and a retrieval round is triggered. Then the retrieved documents are ranked according to the old query to make sure only the relevant information is used to answer. Finally, the Stop action is called to stop the reasoning if a sufficient retrieval quality is achieved, which helps in token efficiency and prevents overthinking.", "MCTS-RAG (Hu et al., 2025) combines Monte Carlo Tree Search (MCTS) with RetrievalAugmented Generation (RAG) to improve reasoning and retrieval in language models. It guides the search for relevant information using MCTS to explore promising retrieval paths, enhancing answer accuracy. However, it is computationally expensive due to the iterative tree search process and may struggle with highly noisy or irrelevant documents. Search-O1 (Li et al., 2025a) proposes an agentic search workflow for reasoning augmented retrieval by letting the Large Reasoning Models (LRMs) like O1 decide autonomously when to retrieve by detecting the presence of salient uncertain words in their output. Additionally, they augment the workflow with a reason-in-documents step, where LRMs analyze the documents in depth to remove noise and reduce redundancy before employing them to generate the final answer.", "1 Introductions\n\nRetrieval-Augmented Generation (RAG) empowers large language models (LLMs) by dynamically integrating external knowledge during inference, enabling precise adaptation to knowledge-intensive tasks and rapidly evolving domains [5, 18, 34]. As a cornerstone of context-aware generation, RAG has been widely deployed in real-world applications, including recommendation systems [16, 12, 26] and search engines [46, 58]. The broad applicability has spurred extensive optimization efforts on dynamic knowledge integration, including reranking strategies [61, 13] to prioritize relevance, adaptive retrieval mechanisms [1, 29] to minimize redundancy, and graph-based architectures [21, 15, 28] to model inter-document semantic relationships.\n\n\u2217 Work done during an internship at Tencent.\n\n\u2020 Corresponding author.", "Figure 1: Illustration for layer-wise behavior in LLMs for RAG. Given a query and retrieved documents with the correct answer ('Real Madrid'), shallow layers capture local context, middle layers focus on answer-relevant content, while deep layers may over-rely on internal knowledge and hallucinate (e.g., 'Barcelona'). Our proposal, LFD fuses middle-layer signals into the final output to preserve external knowledge and improve accuracy.\n\nShallow Layers\nMiddle Layers\nDeep Layers\n\nWho has more la liga titles real madrid or barcelona?\n\n\u2026Nine teams have been crowned champions, with\n\nReal Madrid\n\nwinning the title a\n\nrecord 33 times and Barcelona 25 times \u2026\n\nQuery\nRetrieved Document\n\n\u2026with Real Madrid winning the title a record 33 times and Barcelona 25 times \u2026\n\nShort-context Modeling\n\nFocus on Right Answer\n\nAnswer is barcelona\nWrong Answer\nLLMs\n\n\u2026with Real Madrid winning the title a record 33 times and Barcelona 25 times \u2026\n\n\u2026with Real Madrid winning the title a record 33 times and Barcelona 25 times \u2026\n\nInternal Knowledge Confound\n\nAnswer is Real Madrid\n\nRight Answer\nFusion", "Despite these advances, LLMs might underutilize accurate external contexts, disproportionately favoring internal parametric knowledge during generation [50, 40]. This overreliance risks propagating outdated information or hallucinations, undermining the trustworthiness of RAG systems. Surprisingly, recent studies reveal a paradoxical phenomenon: injecting noise-random documents or tokens-to retrieved contexts that already contain answer-relevant snippets can improve the generation accuracy [10, 49]. While this noise-injection approach is simple and effective, its underlying influence on LLM remains unclear. Furthermore, long contexts containing noise documents create computational overhead. Therefore, it is important to design more principled strategies that can achieve similar benefits without incurring excessive cost.", "This phenomenon enables more granular control and rigorous analysis of how LLMs integrate external knowledge. To investigate the underlying mechanisms, we study layer-wise external knowledge exploitation by measuring the divergence of ablating answer-determining context, i.e., the specific text segment within retrieved documents that directly supports the correct answer to a query. By intervening on injecting noise and measuring its impact on divergence patterns, we identify the relative importance of different layers in exploiting external knowledge. Empirically, we find that noise amplifies the contribution of answer-determining context in middle layers, highlighting their critical role in integrating long-range external information. To further support this observation, we compare attention distributions across heads and layers with versus without answer-determining context. The analysis shows that attention differences peak in middle layers but decline in later layers, signaling a transition from external knowledge reliance to internal parametric knowledge utilization. Following these observations, we propose a functional categorization of LLM layers: (1) shallow layers for short-context modeling, (2) intermediate layers for external knowledge integration, and (3) deeper layers for internal knowledge transformation. Therefore, when retrieved context already contains the correct answer, excessive dependence on internal knowledge in later layers introduces confounding effects, reducing generation accuracy, as visualized in Figure 1 (left).", "Based on this insight, we propose a simple decoding method, Layer Fusing Decoding (LFD), which enhances access to external factual knowledge without introducing additional noise overhead. The core idea is illustrated in the right panel of Figure 1. LFD fuses representations from the long-term context retrieval layer, where external knowledge is most effectively integrated, directly into the final decoding layer to maximize factual grounding. To identify the appropriate layer for fusion, we track the model's reliance on internal knowledge by measuring changes in hidden states across transformer feed-forward network (FFN) layers, where model knowledge is primarily stored [17, 11]. Specifically, we select the layer exhibiting minimal internal knowledge influence from the latter half of the model's layers. This criterion ensures that LFD captures the externally grounded signal before it is overridden by parametric knowledge in later layers. Importantly, LFD operates at inference time, requiring no post-hoc fine-tuning or architectural modifications, making it easily integrable into existing LLM pipelines. Finally, extensive empirical validation across diverse model architectures and datasets demonstrates that LFD delivers competitive performance relative to noise-based approaches, while incurring significantly lower computational overhead.", "5 Discussion\n\n5.1 Achieving Research Goal 1: Reduce information store creation and update times\n\nK 2 RAG effectively reduces the creation times for Knowledge Graph and Sparse and Dense vector databases, such as the Semantic and BM25-based Keyword databases. Performing text summarization on the training corpus led to an average reduction in training times of up to 93% , with only 25 minutes spent on summarization.", "5.1 Achieving Research Goal 1: Reduce information store creation and update times\n\nFor instance, the Semantic database creation, which originally took 4010 seconds (67 minutes) , was reduced to 441 seconds (7.35 minutes) when using the summarized corpus. Even when accounting for the summarization process, the total time was 32.25 minutes , significantly lower than training with the full corpus. Similarly, Knowledge Graph creation time dropped from nearly 18 hours to around 1 hour , thanks to the reuse of the summarized corpus. This approach not only saves time and resources but also reduces the cost of LLM-based Knowledge Graph creation, which is typically expensive to host.", "5.2 Achieving Research Goal 2: Mitigate the Needle-in-a-Haystack problem\n\nK 2 RAG effectively mitigates the \"Needle in a Haystack\" problem, which can hinder LLM performance when handling large contexts. By integrating the Longformer-based led-base-book-summary summarizer at key stages of the retrieval pipeline, the framework reduces context size while preserving essential information.", "5.2 Achieving Research Goal 2: Mitigate the Needle-in-a-Haystack problem\n\nK 2 RAG maintained answer quality and even outperformed naive pipelines on certain questions, as reflected in the higher accuracy and Q 3 quantile. This approach allows for improved LLM performance without a bloated context, preventing the model from missing critical details and enhancing the final answer quality.", "5.3 Achieving Research Goal 3: Improve retrieval accuracy\n\nThe K 2 RAG framework enhances passage retrieval through two key components. First, the knowledge component identifies relevant topics for answering the question. Based on these results, the system further explores each topic within the knowledge graph to extract more concrete and relevant chunks. This approach enables the development of more comprehensive answers. Similar to Research Goal 2, this strategy improves the quality of the final LLM-generated responses.", "5.4 Achieving Research Goal 4: Lower Temporal and Computational Overhead\n\nWhile rerankers enhance passage retrieval accuracy, they require an additional large model, increasing resource demands. Similarly, the naive Knowledge Graph RAG pipeline depends on two LLMs: Mistral-7B-FP16 (14GB VRAM) and Mistral-7B-Q4 (4.1GB VRAM).", "5.4 Achieving Research Goal 4: Lower Temporal and Computational Overhead\n\nIn contrast, the K 2 RAG framework streamlines the process by utilizing only Mistral-7B-Q4 (4.1GB VRAM) for querying the Knowledge Graph, question generation, and final answer generation, along with a small summarization model requiring just 648MB VRAM. This results in a total VRAM usage of 4.8GB, allowing for efficient RAG pipelines on smaller machines while significantly reducing computational and monetary costs.", "5.4 Achieving Research Goal 4: Lower Temporal and Computational Overhead\n\nMoreover, K 2 RAG achieves faster response times and superior answer quality compared to naive RAG pipelines. This is evident in its reduced question-answering times relative to the naive Knowledge Graph Search RAG, enabling K 2 RAG to process more queries in a span of time.", "5.5 Limitations\n\nWhile our RAG framework shows a slight mean accuracy advantage over naive methods, a mean similarity of 0.57, despite a high Q 3 value, indicates limited performance gains on most questions. This suggests it excels only at certain\n\n| | Pipeline | Memory Footprint |\n|---:|:-------------------------|:-------------------|\n| 0 | Semantic Search Pipeline | 14.3GB |\n| 1 | Keyword Search Pipeline | 14.3GB |\n| 2 | Hybrid Search Pipeline | 14.3GB |\n| 3 | Knowledge Graph Search | 18.1GB |\n| 4 | KeyKnowledgeRAG | 5GB |\n\nTable 4: Table of Memory Footprints", "question types, but the narrow scope restricts broader improvement. Reliance on broad knowledge graph searches may also reduce precision. A more focused retrieval strategy could yield more precise topic pinpointing and higher accuracy.", "2 Related Work\n\n2.1 Retrieval-Augmented Generation\n\nRetrieval-augmented generation (RAG) enhances large language models (LLMs) with a retrieval module that retrieves relevant passages from an external corpus given a query [5, 11, 20, 29, 31]. These retrieved documents are appended to the model's input context, enabling the LLM to access information beyond its parametric knowledge and improving performance across a wide range of tasks [3, 9, 11]. A range of studies has explored how to better integrate\n\nTo further investigate whether the benefits arise from improved robustness to noisy passages, we conduct experiments under two types of noisy retrieval scenarios: 1) Random Noise : We use the same four datasets as those of the general RAG scenarios. For each question, the model is provided with several randomly selected passages unrelated to the question. 2) Counterfactual Noise : A more challenging form of noise that is more likely to mislead the model. We use ConFiQA [2], where each question is paired with a counterfactual", "these retrieved passages into the generation process, including prompt-level augmentation [24, 25], embedding-level fusion [4, 8], and parametric-level adaptation [21, 23]. Our work proposes a new direction by injecting retrieved passages directly into the model's reasoning process, enabling a tighter coupling between external knowledge and step-by-step reasoning.", "2.2 Reasoning-Enhanced LLMs\n\nRecent advances in reasoning-augmented models, such as OpenAI's o1 [10], DeepSeek-R1 [6], and Qwen3 [27], have significantly expanded the capabilities of LLMs. These models explicitly generate intermediate reasoning steps before producing final answers, which enhances their ability to handle tasks requiring multi-step inference [12, 18, 32]. Such step-by-step reasoning not only enhances performance but also contributes to better interpretability and transparency in model generation (see Section 3 for detailed generation process). Recent studies have also begun to examine the reasoning process itself, investigating how to supervise or intervene during generation [1, 26, 30]. Wu et al. [26] observe that the attention during reasoning primarily focuses on internally generated tokens rather than the input context, and propose inserting instructions into the reasoning trajectory to better guide the model's thinking and enhance instruction-following capabilities. Our work introduces a new intervention method that incorporates retrieved passages into the reasoning process, enhancing the model's ability to recognize and resist noisy information.", "8 Discussion\n\nThroughout this work, we presented a multi-agent system that uses triplet-based knowledge graph construction and retrieval-augmented generation (RAG) to enable transparent, verifiable question-answering on a regulatory corpus. By delegating ingestion, triplet extraction, KG maintenance, and query orchestration to specialized agents, unstructured text becomes a structured data layer for precise retrieval. The synergy of KG and RAG provides high-confidence, explainable facts alongside fluent responses to the large language model, as Section 7 demonstrates through accurate section retrieval, factual correctness and navigational queries (Figure 3). Grounding answers with triplets reduces LLM hallucinations, and provenance links enable robust auditing.", "8.1 Challenges\n\nAn ontology-free approach facilitates rapid ingestion and incremental refinement but can lead to vocabulary fragmentation; canonicalization and entity resolution [GTHS14, SWLW14] help unify concepts, and advanced reasoning tasks may still benefit from partial or emergent schemas [RYM13]. Extraction quality directly affects the integrity of the KG, as domain-specific jargon or ambiguous references can produce missing or erroneous triples, and deeper inferences or temporal constraints may require additional rule-based or symbolic logic. Large-scale RAG pipelines also require careful optimization for embedding, indexing, and retrieval.", "8.2 Future Directions\n\nLooking ahead, we see multiple avenues for enhancing and extending the system: although our current pipeline supports factual lookups, more complex regulatory questions demand deeper logical reasoning or chaining of evidence, and integration with advanced reasoning LLMs can address multistep analysis and domain-specific inference needs. By including user feedback or expert annotations, we could iteratively refine triplet quality and reduce extraction errors. Active learning or weakly supervised methods may help identify ambiguous relationships, prompting relabeling or model retraining. Over time, such feedback loops would yield higher-precision knowledge graphs. Regulatory corpora often change rapidly (e.g., new guidelines, amendments). We aim to develop incremental update mechanisms that re-ingest altered documents and regenerate only those triples affected by the changes, minimizing downtime and ensuring continuous compliance coverage. Although we focus on health life science regulatory compliance , the underlying architecture of multi-agent ingestion, knowledge graph construction, and RAG QA-can be generalized to other domains with high stakes factual queries (e.g., clinical trials, financial regulations, or patent law). Tailoring the extraction logic and graph schema of each agent to domain-specific requirements would enable a larger impact."]}
{"type": "entry", "index": 17, "question": "my retrieval gets irrelevant stuff even though embeddings are good \u2014 normal?", "answer": "Yes, it\u2019s normal for retrieval to surface irrelevant chunks even when your embeddings are good.\n\nSeveral reasons show up repeatedly in RAG research:\n\n1. Query is sparse or vague\nUnderspecified or ambiguous queries often pull in low\u2011quality or off\u2011topic documents, even with strong dense retrievers.[2] This is especially visible for complex or unclear questions.\n\n2. Embeddings \u2260 perfect relevance\nModern neural retrievers with dense embeddings do improve over BM25, but they still struggle with vocabulary mismatches, ambiguity, and domain\u2011specific terms.[1] So you can have \u201cgood\u201d embeddings overall and still get noisy results on many queries.\n\n3. Well\u2011defined vs vague questions\nHybrid and dense methods work much better on clearly defined queries than on vague ones. For vaguer questions, they still retrieve lots of extraneous or unnecessary information\u2014the classic \u201cneedle in a haystack\u201d effect.[5]\n\n4. Too many retrieved passages\nChoosing k is tricky: too few misses evidence, too many bring in irrelevant context that confuses the model.[1]\n\nWhat people do about it:\n- Query rewriting / expansion or generating pseudo\u2011context and using dual\u2011path retrieval (separate searches for the original query and generated context) to improve relevance for hard queries.[2]\n- Hybrid retrieval (sparse + dense) and careful weighting of each signal.[5]\n- Adding a reranking stage that reorders or filters initially retrieved documents before they go to the LLM.[5]\n- Domain\u2011tuned embeddings or synonym expansion for specialized areas.[1]\n- Measuring retrieval quality explicitly with recall@k, precision, MRR, etc., to see where the bottleneck is.[6]\n\nSo some irrelevance is expected; the usual fix is layering better query handling, hybrid retrieval, and reranking on top of your base embeddings.", "tags": ["_untagged"], "full_faithfulness": 0.9583333333, "full_answer_relevancy": 0.9190100940000001, "seed_faithfulness": 0.8846153846, "seed_answer_relevancy": 0.9131667033, "context_relevance": 0.7516676571959261, "context_relevance_error": null, "context_relevance_reason": "The context passages provide relevant information about the challenges and intricacies of Retrieval-Augmented Generation (RAG), particularly regarding retrieval quality and relevance, which directly addresses the user's concern about receiving irrelevant results despite good embeddings. The passages discuss issues like vocabulary mismatches and the complexity of determining the optimal number of retrieved passages, which are pertinent to the user's question. However, while the response is mostly aligned with the evaluation steps, it could have included more direct solutions or strategies to mitigate the issue of irrelevant retrievals.", "structure_citation": 0.5598633553218438, "structure_error": null, "structure_reason": "The Actual Output includes several claims supported by citations, but there are issues with citation matching. For instance, the claim about vague queries leading to irrelevant documents is supported by [2], which corresponds to the context discussing retrieval quality and relevance. However, the citation [1] is used multiple times without a clear match in the context, and the claim about too many retrieved passages lacks a corresponding citation. This inconsistency in citation support and the presence of unmatched markers detracts from the overall reliability of the response.", "hallucination_score": 0.8728395591742999, "hallucination_error": null, "hallucination_reason": "The response accurately addresses the user's concern about retrieval yielding irrelevant results despite good embeddings, aligning well with the context provided. It identifies key factors such as vague queries and the limitations of embeddings, which are supported by the context. The mention of strategies to improve retrieval relevance further strengthens the response. However, while the output is comprehensive, it could have briefly acknowledged the potential for specific domain challenges, which would have made it even more aligned with the context.", "contexts_used": ["7 Challenges of RAG\n\nThis section discusses the challenges of RAG, cases of manifestation of such challenges in the selected domain of RAG application, and outlines existing solutions and the way forward.", "7.1.1 Retrieval Quality and Relevance\n\nThe quality of retrieved documents significantly impacts the accuracy of RAG-generated answers. High recall and precision are critical since poor retrieval leads directly to incorrect or irrelevant answers [61]. Traditional methods like BM25 are limited, often missing relevant texts or returning noisy results [12]. Modern neural retrievers with dense embeddings improve performance but still face issues like vocabulary mismatches, ambiguous queries, and domain-specific terminology. Specialized domain tuning, such as using legal embeddings or medical synonym expansion, can help, but maintenance of these tailored retrievers remains challenging. Determining the optimal number of retrieved passages ( k ) is also complex. Too few passages limit evidence; too many overwhelm the model and introduce irrelevant context. Approaches like ranking retrieved passages or iterative query reformulation can improve retrieval precision, but add complexity and latency [34, 21].", "7.1.2 Latency and Efficiency\n\nRAG inherently increases computational complexity and latency compared to standalone LLMs due to retrieval overhead, vector searches, and expanded context processing. Techniques like approximate nearest neighbor indices (e.g., FAISS, HNSW), caching, model distillation, or lightweight retrievers can reduce latency at the expense of accuracy. Integrating retrieval efficiently with large language models (LLMs) and ensuring rapid responses in real-time scenarios (e.g., customer support) remains a significant challenge [3]. Interestingly, using retrieval can allow smaller models to match the performance of larger models without retrieval (e.g., RETRO, Atlas), reducing model size requirements but shifting complexity to maintaining external knowledge bases and infrastructure.", "7.1.3 Integration with Large Language Models\n\nIntegrating retrieved evidence effectively with LLMs is subtle. Models may ignore retrieved evidence, especially when internal model knowledge conflicts with external retrieved information, leading to a \"tug-of-war\" effect [41]. Multiple retrieved documents might create confusion or confirmation bias if they contradict each other. Limited input lengths in transformer-based LLMs exacerbate these integration challenges by forcing truncation or summarization, potentially omitting essential context. Fine-tuning models specifically for retrieval-augmented tasks often yields better integration than simple zero-shot prompting but introduces complexity, especially when using non-differentiable or API-based models that do not support custom training.", "7.2.1 Scalability and Infrastructure\n\nDeploying RAG at scale requires substantial engineering to maintain large knowledge corpora and efficient retrieval indices. Systems must handle millions or billions of documents, demanding significant computational resources, efficient indexing, distributed computing infrastructure, and cost management strategies [21]. Efficient indexing methods, caching, and multi-tier retrieval approaches (such as cascaded retrieval) become essential at scale, especially in large deployments like web search engines.", "7.2.2 Freshness and Knowledge Updates\n\nOne motivation for RAG is providing current information. However, continuously updating external knowledge bases and retrieval indices is challenging. Domains requiring real-time updates (e.g., finance, healthcare) demand sophisticated data pipelines for incremental updates, possibly frequent re-encoding of documents, and synchronization of retrieval indices. Delays in updates or inconsistencies between the LLM's internal knowledge and newly retrieved data can produce outdated or contradictory answers [61].", "7.2.3 Hallucination and Reliability\n\nWhile RAG reduces LLM hallucinations, it does not eliminate them completely. Models may fabricate or misattribute information if retrieval provides incomplete or partially contradictory context.", "Legal domain studies found that RAG significantly reduces hallucinations, but still generates errors at concerning rates [56]. Hallucinations also occur in citation generation, with models occasionally inventing nonexistent references. Strategies such as verifying outputs against retrieved sources or calibrating model confidence are needed, but no approach completely prevents hallucination.", "HIPAA), transparent data usage policies, and security testing are essential to protect privacy and prevent breaches.", "7.3.4 Accountability and Transparency\n\nRAG's use of sourced retrieval provides an", "7.2.4 Complex Pipeline and Maintenance\n\nadvantage for accountability, allowing users to trace AI-generated responses back to evidence. However, inaccurate citations or improper synthesis can mislead users. Ethical deployment involves clearly attributing evidence, providing explanations on request, managing user expectations, and clearly delineating accountability-especially when RAG informs critical decisions. Transparency and accountability require ongoing evaluation, oversight, and mechanisms for user feedback and correction [56].", "Dual-path retrieval\n\nOne of the central challenges in RAG is handling sparse or underspecified queries, which often lead to the retrieval of ir- relevant or low-quality documents (Zhao et al. 2024; Singh et al. 2025). This issue becomes particularly pronounced when relying solely on the original query to perform retrieval (please refer to the appendix for further analysis). However, the previously generated pseudo-context can be used to enhance the retrieval. An intuitive solution might be to concatenate the query with LLM-generated pseudocontext to enrich the signal (Wang, Yang, and Wei 2023; Jagerman et al. 2023); however, such concatenation could be inefficient, as the query and pseudo-context serve different semantic roles and may not integrate cohesively. To address this, PAIRS adopts a dual-path retrieval (DPR) mechanism that treats the query and the self-generated context as complementary sources of information. Instead of combining them into a single retrieval signal, the system executes two parallel retrieval operations-one conditioned on the query and the other on the pseudo-context. Specifically, as shown in Fig. 2 (a), the system retrieves top n relevant documents using the embeddings of the query q = f ( q ) and the generated context p = f ( p ) , respectively, and the retrieved documents can be denoted as D 2 n = { d 1 , d 2 , ..., d 2 n } \u2282 D . This dual-path strategy enhances the relevance and diversity of the retrieved documents, which is especially beneficial for complex or ambiguous queries where either source alone may fall short.", "Figure 2: Illustration of (a) dual-path retrieval mechanism and (b) similarity calculation of retrievals.\n\nr q p d \ud835\udf03\n\n!\n\n\" # Retrieval embeddings\n\na.\n\nb.\n\nSimilarity calculation", "III. OVERVIEW\n\nUSER: What is the key pattern for Sambacry attack? ASSISTANT: \"Sambacry\" is not a word or term that is commonly used or recognized. It is not a term that is commonly associated with any specific field of study or industry. It is possible that this word or term was created or invented by the person who is using it in this chat. Without more information or context, it is difficult to provide a more comprehensive answer to your question.\n\nFigure 1: Gemma-3-4B (May-2025) lacks expertise.", "Generation (RAG), addressing scalability by eliminating manual rule creation and enabling rapid response within the first day. These rules then undergo rigorous validation through VNF testing against synthetic exploit traffic and benign traffic samples, which identifies and filters erroneous outputs before deployment. Finally, validated rules are propagated to ESGs, achieving compatibility by providing a unified enforcement layer that protects diverse connected devices across heterogeneous network environments.", "Challenges : While the baseline approach shows promise, several practical challenges hinder its implementation:\n\nLimited Vulnerability-Fixing Expertise : Existing LLMs [18], [11], [61], [36] are trained on generalpurpose datasets and lack specialized knowledge in niche domains like vulnerability remediation (Figure 1). While techniques like Retrieval Augmented Generation (RAG) can incorporate vulnerability-related context into prompts, they fail to internalize domain-specific expertise. This limitation prevents LLMs from reliably inferring accurate filter fixes for novel 1-day/n-day vulnerabilities\n\nGaps Between Language and Network : Current LLMs are designed and optimized for natural language interactions, as exemplified by training paradigms like Reinforcement Learning from Human Feedback (RLHF). This creates a significant semantic and structural disconnect when applied to network security. Their effectiveness in processing raw network-layer data (e.g., packets) or generating precise protocol-specific rules remains unproven, limiting their practical utility for real-world network enforcement tasks.", "LLM Hallucination and Non-Determinism : LLM outputs are inherently non-deterministic, often producing inconsistent or contradictory rules across repeated iterations. This instability introduces reliability risks, as identical inputs may generate divergent outputs-some of which could be erroneous or unsafe. Such unpredictability undermines trust in automated systems requiring consistent, repeatable results for critical tasks like vulnerability-fixing filtering.", "REFN's ideas : To address the above challenges, we propose REFN on top of the baseline approach, as shown in Figure 2. REFN is built on a simple yet powerful premise: training LLMs into dynamic, network-aware vulnerability fixing engines that can deliver fixes on day one. To achieve this, the system directly tackles the three core challenges through strategic design. The first idea is to close the expertise gap via knowledge distillation . Instead of relying on generic LLM knowledge, REFN distills historical vulnerability fixes (e.g., past CVEs, patches) into the model during training. This equips the LLM with an implicit playbook of remediation strategies, enabling it to infer fixes for new 1-day\n\nFigure 2: REFN's workflow.\n\nChallenge 1:\n\nLimited Vulnerability fixing expertise Challenge 3: LLM Hallucination & Non-determinism\n\nChallenge 2:\n\nGap between language and network Agentic-RAG-based Knowledge Distillation Router Agent Context Search Agents Knowledge Distillation Agents RL-From-VNF Pipeline Instruction Tuning VNF-GRPO VNF-Reward Function Online Agentic Validator Internet Edge Security Gateway Vulnerable Devices NToT Agent\n\nFuzzing & Trimming Agent\n\nRL training data REFN LLM VNF Fixes DP Feedbacks\n\nIdea 1:\n\nClose expertise\ngap via\ndistillation\n\nIdea 2:\n\nTranslate language to network actions via RL-From-VNF\n\nIdea 3:\n\nPunish hallucination via dataplane validation VNF Reward", "vulnerabilities by recognizing patterns from past fixes-even if the new vulnerability differs superficially. The second idea is to translate language to network actions via RL-from-VNF pipeline . REFN treats network filtering as a 'language' the LLM must learn. Through reinforcement fine-tuning (ReFT), the model is trained to enforce vulnerability-fixing filters on raw network packets (e.g., dropping malicious payloads) rather than generating text. Rewards are tied to real-world outcomes - blocking malicious packets while preserving legitimate packets. The third strategy is to punish hallucination via dataplane validation . Every LLM generated filter is validated as Virtualized Network Function (VNF) with both benign and malicious traffic. Filters that fail to block attacks or disrupt legitimate flows are punished via a VNF-based online reward function, creating a feedback loop that force out the LLM hallucination in the training stage.", "REFN's workflow : As illustrated in Figure 2, REFN transforms raw 1-day/n-day vulnerability context into reliable security enforcements on Edge Security Gateways (ESGs) through three core components:", "Agentic-RAG-Based Knowledge Distillation : This pipeline integrates agent-based systems, Retrieval-Augmented Generation (RAG), and knowledge distillation to transfer vulnerability-fixing expertise from powerful-but-expensive LLMs to specialized models (efficient for training fixes for new vulnerabilities). This architecture features three autonomous agents: a Router Agent directing queries, Context Search Agents retrieving vulnerability context, and Knowledge Distillation Agents extracting structured inference examples (e.g., filter rules). This automated pipeline ensures RL-optimized knowledge transfer while eliminating manual expertise bottlenecks (detailed in Section IV).\n\nRL-From-VNF Pipeline : Distilled knowledge is processed", "Figure 3: Common RAG (top) vs REFN's Agentic-RAGBased Knowledge Distillation (bottom).\n\nRetrieval Relevant Chunks LLM Vector stores embedding Descriptions Protocol Specifications Traces Query Prepare\n\nIssue 2: Unstructured Training Data\n\nIssue 1: Manual Context Preparation\n\nIssue 3: No examples\n\nRouter Agent RL Training Data Template Structured\n\nSolution 2: Structured Training Data\n\nSolution 1:\n\nQuery Template\nSolution 3: Examples\nFrom SOTA LLMs", "by the RL-From-VNF LLM training pipeline, which replaces human feedback with Virtualized Network Function (VNF) validation, which automatically generate rewards/penalties based on security enforcement outcomes. To improve training inefficiency, REFN presents the VNF-GRPO algorithm that integrates GRPO optimization, LoRA adapters and VNF. The pipeline's VNF-Based Reward Function evaluates each LLMgenerated filter on both benign and malicious traffic samples, training a lightweight network-aware LLM capable of translating textual vulnerability contexts into practical vulnerabilityfixing enforcements on the ESGs (detailed in Section V).", "Online Agentic Validator : To address the LLM hallucination and non-determinism challenge, REFN's Online Agentic Validator will validate the filters and generate quantitative rewards for the RL-From-VNF pipeline. Unlike coarse-grained network validation (e.g., binary block/no-block decisions), this validator employs fine-grained VNF-specific logic and can fuzz on LLM-generated filters to reward 'near-correct' outputs (key for RL training convergence). This approach provides granular feedback for Reinforcement Fine-Tuning (ReFT), balancing precision with training stability while reducing hallucinations. The design enables effective RL convergence and reliable exploit prevention (detailed in Section VI).", "2. Preliminaries and Related Work\n\n2.1. Retrieval-Augmented Generation (RAG)\n\nRetrieval-Augmented Generation (RAG) [12], [14], [15], [17], [20], [28] enhances the response quality of LLMs by incorporating relevant texts retrieved from an external knowledge database. A typical RAG system consists of a knowledge database D , a retriever, and an LLM. Given a user question q , the retriever selects relevant texts from D , which are then used by the LLM to generate a final answer. The inference process proceeds in two steps:\n\nStep I (Retrieving relevant knowledge): Initially, the retriever encodes the user question q through the question encoder, producing the question embedding E ( q ) . Subsequently, it employs a similarity metric to identify and retrieve the topK texts most closely related to the question embedding from the knowledge database. The embeddings for these texts are pre-calculated and maintained in a vector database using the question encoder.", "Step II (Knowledge-augmented generation): The topK texts retrieved as relevant are provided as context to the LLM together with the user question q . The LLM integrates this contextual data with the user question to produce the final response.", "2.2. Related Work\n\nPoisoning attacks to RAG: Recent studies have highlighted the growing threat of poisoning attacks on RAG systems [12]-[17], where the attacker injects crafted texts into the knowledge database to manipulate the system's output for target questions. For instance, PoisonedRAG [12] splits poisoned content to influence both the retriever and the LLM, using different optimization strategies for black-box and white-box settings. LIAR [15] jointly optimizes for both components in white-box environments. Other attacks like BadRAG [16] and Phantom [17] further refine trigger-based poisoning to produce attacker-specified outputs.", "Defenses against poisoning attacks to RAG: Several defenses [12], [14], [16], [20]-[22] have been proposed to detect and mitigate poisoned texts in RAG systems. Normbased methods [20] flag texts with unusually large embedding norms, while perplexity-based approaches [12], [14], [16] identify outliers using a proxy model. RobustRAG [21] reduces poisoning impact by aggregating outputs generated from individual retrieved texts.", "Responsibility attribution: Responsibility attribution, also referred to as forensics in some studies [23]-[26], [29], aims to trace the origin of poisoning attacks after they occur. In centralized learning, the focus is on identifying poisoned training samples that lead to misclassification [23], [29], often by clustering and filtering out benign examples. In federated learning, the goal shifts to identifying malicious clients responsible for the attack [25], [26].", "Responsibility attribution vs. Defenses: Responsibility attribution complements defense by focusing on identifying the root causes of attacks rather than preventing them. Although advanced defenses exist, past work [30]-[34] shows they are often bypassed by adaptive attacks. Responsibility attribution helps trace the source of threats and informs the design of future defenses, making it a crucial component of a robust security framework.", "Limitations of existing works: Existing approaches have notable limitations. Traditional responsibility attribution methods [23]-[26] target training-phase attacks and require access to model internals, which is infeasible in closedsource RAG systems. RAG-specific defenses either struggle to distinguish poisoned texts [12], [14], [16], [20] or incur high computational costs under unrealistic assumptions [21]. The most relevant work, RAGForensics [27], relies on explicit semantic alignment between poisoned texts and outputs, making it ineffective when such alignment is weak or deliberately obfuscated.", "2 Related Works\n\nAddressing the limitations of naive Retrieval-Augmented Generation (RAG) pipelines has led to the integration of hybrid retrieval methods that combine sparse (keyword-based) and dense (semantic) retrievers.", "Spare and Dense Retriever RAG methods: Hybrid approaches, such as the \"Blended RAG\" framework proposed by Sawarkar et al. [12], demonstrate that combining multiple retrievers can improve the retrieval of relevant passages for clearly defined queries. This framework integrates six distinct retrievers-including traditional sparse methods (e.g., BM25) and dense semantic search techniques-selected based on their performance across benchmark datasets like Natural Questions (NQ) [13], TREC-COVID [14], SqUAD [15], and HotPotQA [16].", "A key strength of this hybrid search technique is its ability to return relevant passages, aligning well with Research Goal 3 for well-specified queries. However, these methods do not fully address Research Goal 3 when handling vaguer questions. In such cases, the hybrid approach can retrieve extraneous or unnecessary information, thus failing to eliminate the \"needle in a haystack\" problem. This shortcoming is evident in evaluations on the CoQA dataset [17], where retrieval accuracies ranged only from 45% to 49%. The reliance on document metadata during retrieval-a common feature in these systems-further complicates matters, as such metadata is often minimal in real-world scenarios (e.g., limited to document names and creation dates), thereby increasing both the creation time of the information store (contradicting Research Research Goal 1) and the risk of returning irrelevant passages (contradicting Research Research Goal 3) leading to longer contexts (contradicting Research Goal 2).", "Figure 3: Results of Priyanka Mandikal et al. on 2 example queries [18].\n\nQuery: Do fibroblasts from CF patients grow at a\n\nQuery: What is the prognosis for patients who have\n\nnormal rate?\n\nhad an episode of respiratory failure?\n\nMethod P@1 P@3 P@5 P@10 VSR 33.33 SPECTERZ Hybrid 66.67 Sample Retrieval This paper analyzes the cell cycle of cystic\n\nThe diagnosis, management and prognosis of 17 infants\n\nfibrosis-derived fibroblasts and statistically\n\nwith cystic fibrosis (CF) and severe respiratory disease\n\ndemonstrates their failure support normal\n\nwere reviewed for the\n\nperiod 1968 to 1972.\n\nThe clinical\n\npopulation doubling time in vitro and indicates that this\n\ncourse these infants was characterized by is due largely\n\nto a failure of the\n\ncystic fibrosis-derived\n\nfailure to thrive and\n\ncells to enter the cell cycle in normal numbers.\n\nWe have malnutrition Despite vigorous therapy including no explanation for significant difference antibiotics there was\n\n60% mortality:\n\nA delay in the\n\nbehavior of these cells from patients with cystic fibrosis\n\ndiagnosis of CF from the onset of respiratory symptoms\n\nvis-a-vis either disease control fibroblasts, with a\n\nmean of six weeks was considered an important\n\nsave to conclude that proceeds from an intrinsic\n\nfactor affecting survival. This data supports the need for\n\nproperty of the cells themselves.", "developing a reliable screen test for CF at birth.", "In contrast, recent work by Mandikal et al. [18] has advanced a hybrid retrieval model that integrates a sparse TF-IDF retriever with a dense retriever based on fine-tuned SPECTER2 embeddings [19]. Their approach employs a weighted scoring mechanism, controlled by a hyperparameter \u03bb , to balance the contributions of the two retrieval methods. Empirical evaluations on a medical database concerning Cystic Fibrosis indicate that while dense retrieval alone underperforms, incorporating approximately 20% of the sparse score (i.e., setting \u03bb = 0 . 2 for the sparse component) significantly improves document retrieval accuracies across different retrieval ranks [Figure 3]. Crucially, this method indexes documents solely based on their content, thereby avoiding the pitfalls of metadata dependency. This contributes to reducing information store creation times (Research Goal 1) and partially mitigates the retrieval of unnecessary information (Research Goal 3), although it does not entirely resolve the challenge of handling vaguer queries.", "In summary, while hybrid retrieval approaches represent a significant advancement by returning more relevant passages for well-defined queries complying with Research Goal 3. However they remain imperfect in filtering out unnecessary information for vaguer questions which can lead to longer contexts. This limitation underscores the need for further refinement to minimize the \"needle in a haystack\" problem in large-scale, real-world applications and make a pipeline which is compliant with Research Goal 2 as well.", "Reranker RAG methods: To address the limitation which Hybrid-Search methods have in adhering to Research Goal 2 for vaguer questions, Reranker methods have been developed to reduce the size and improve the quality of the final context.", "Figure 4: A high level view of what a RAG pipeline with a reranker looks like.\n\nIst Stage 2nd Stage Knowledge Embedding-based LLM-based Docs Corpus retrieval reranking\n\nUsually this is a process called 2-step retrieval where the first step involves retrieving documents such as through sparse or dense retriever techniques and then a second stage where an LLM based reranker comes into play to decide which of the documents retrieved are most relevant to the query and sorts them in order [Figure 4] before passing it to an LLM for generation.", "Prior to the formal conceptualization of Retrieval-Augmented Generation (RAG), information retrieval research explored the use of document rerankers. For example, Noguiera et al.'s 2020 study, Document Ranking with a Pretrained Sequence-to-Sequence Model [20], proposed employing a sequence-to-sequence model as a document reranker. This work evaluated multiple fine-tuned T5 models of varying sizes, as well as a fine-tuned BERT model, on documents initially retrieved using the sparse BM25 keyword method. The incorporation of these rerankers led to improvements in retrieval performance across the Robust04, Core17, and Core18 datasets. However, a critical limitation of this framework was its reliance on large models, leaving unresolved whether the observed performance gains were due to the reranking strategy or simply the use of more powerful models.", "Subsequent advancements in RAG have integrated BERT-based rerankers into new pipelines. Michael Glass et al.'s Re 2 G framework [21] embeds a BERT-based reranker-similar to that of Noguiera et al.-within a RAG system. This integration has shown notable improvements in metrics such as Recall (for 5 documents), overall recall, precision, and accuracy on several datasets, as indicated by performance on the KILT leaderboard. Nevertheless, a significant drawback of this approach is the necessity to host an additional model, which directly conflicts with Goal 4: alleviating the monetary and computational costs associated with deploying full-scale LLMs.", "6 Datasets and Evaluation Benchmarks\n\nThe evaluation of IR and RAG systems relies on diverse datasets that test different aspects of retrieval and answer generation capabilities, including the retrieval, ranking, and answer quality.", "6.1 IR Specific\n\nInformation retrieval has a long history of evaluation campaigns, including those from TREC, CLEF, NTCIR, and FIRE. The queries used in these collections are often developed to strike a balance between having too many relevant documents in the target collection (which can be too easy to retrieve and too difficult to properly annotate (Voorhees et al., 2022)) and too few relevant documents. Sometimes challenging topics are also developed deliberately (Voorhees, 2005). The topic development process often involves manual reformulation of queries to ensure good coverage of relevance assessments. Mirroring the annotation process itself, it can be beneficial for automated retrieval systems to also perform various forms of query understanding (expansion or rewriting) to help ensure high recall. Hence, the approaches described in Section 3 show performance gains.", "Comprehensive benchmarks like BEIR (Thakur et al., 2021) offer heterogeneous evaluation across 17 diverse datasets spanning multiple domains and retrieval tasks, enabling zero-shot generalization assessment. In addition, evaluation sets such as TREC DL (Deep Learning) and its variants (Craswell et al., 2020, 2021, 2022, 2023) are used to evaluate IR systems.", "6.2 Question Answering\n\nQuestion Answering datasets for retrieval evaluation fall into two primary categories based on complexity. Single-hop QA benchmarks include Natural Questions (NQ) (Kwiatkowski et al., 2019), TriviaQA (Joshi et al., 2017), SQuAD (Rajpurkar et al., 2016), and PopQA (Mallen et al., 2023) test the RAG system's ability to retrieve and utilize information to generate the final answer. Multi-hop QA benchmarks comprise 2WikiMultiHopQA (Ho et al., 2020) (questions requiring evidence from exactly two Wikipedia articles via bridging/comparison), HotpotQA (Yang et al., 2018), and MuSiQue (Trivedi et al., 2022), which features compositionally complex questions constructed from interconnected sub-questions. Due to the dependency on intermediate answers, such questions cannot be answered through isolated single-step retrieval. Therefore, this design naturally motivates the RAG systems to decompose the original query and new queries based on the intermediate answers help in the retrieval of better context.", "6.3 Fact Checking/Verification\n\nThese datasets assess models' ability to verify claims against retrieved evidence. FEVER (Fact Extraction and VERification) (Thorne et al., 2018) is an open-domain fact-checking benchmark that requires retrieval over a large collection of Wikipedia articles and training NLI models to classify claims as supported, refuted, or not having enough information based on Wikipedia evidence. While FEVER deals with simple claims, HoVeR (Jiang et al., 2020) proposes a fact-checking benchmark with claims that require multi-hop reasoning and multi-turn retrieval. QuanTemp (V et al., 2024) is the first to propose a large open-domain benchmark for fact-checking numerical claims. It comprises claims that require interleaving claim decomposition and retrieval based on the verification output of sub-claims. This relates to query-level feedback and generation-time feedback. AveriTeC (Schlichtkrull et al., 2023) is a real-world claim verification dataset that includes diverse claim types with supporting or refuting evidence gathered through web search, making it particularly valuable for evaluating RAG systems on authentic fact-checking scenarios that require verification against potentially noisy or contradictory web sources.", "6.4 Complex Reasoning\n\nEmerging benchmarks introduce novel relevance criteria requiring awareness of reasoning structure. BRIGHT (SU et al., 2025) defines document relevance not by topical alignment but by whether passages contain logical constructs (deductive steps, analogies, constraints) necessary to derive answers. This challenges lexical retrievers that lack inference-awareness. However, reasoningaugmented retrieval methods, such as CoT (Chainof-Thought), have shown performance gains. Further, there exist reasoning/agentic tasks like Deep Research (Wu et al., 2025), GPQA (Rein et al., 2023), MATH500 (Cobbe et al., 2021), which involve access to the web search engine, have been used in the RAG setting (Li et al., 2025a; Wei et al., 2025). However, due to the absence of the cor- pus, it is hard to evaluate the retrieval performance. Recent benchmarks like BrowseComp-Plus (Chen et al., 2025b) provide a curated corpus for Deep Research tasks and enable the evaluation of retrieval performance.", "6.5 Evaluation Metrics\n\nEvaluating retrieval systems and RetrievalAugmented Generation (RAG) pipelines is critical for ensuring the accuracy, relevance, and reliability of generated responses. Retrieval evaluation typically focuses on metrics such as recall@k, precision, mean reciprocal rank (MRR), and hit rate, which assess how effectively the system retrieves pertinent documents or passages given a query. In contrast, RAG evaluation is more holistic, combining retrieval quality with generation fidelity and coherence. Common approaches include measuring answer correctness using exact match (EM) or F1 score, assessing faithfulness to retrieved evidence to detect hallucinations, and evaluating relevance and fluency through human or automated scoring (e.g., BLEU, ROUGE, or BERTScore). Recent frameworks like RAGAS (ExplodingGradients, 2024), ARES (Saad-Falcon et al., 2024), and CRUX (Ju et al., 2025) also emphasize end-to-end evaluation, where the interplay between retrieval accuracy and generation quality is analyzed to identify bottlenecks-such as irrelevant documents leading to incorrect answers-making comprehensive evaluation essential for diagnosing and improving RAG system performance.", "Challenges . The current RAG evaluation methods mainly focus on the retrieval and final answer performance. However, Reasoning RAG systems are highly dependent on intermediate reasoning steps and retrieval rounds. Therefore, it is also important to consider additional evaluation dimensions such as computational cost, efficiency, or number of retrieval rounds."]}
{"type": "entry", "index": 18, "question": "context too long model forgets info fix?", "answer": "When models see very long contexts, two related problems show up in the papers you provided:\n\n1) They forget earlier parts of the context within a single prompt.\n2) They also \"forget\" their original broad skills after being fine\u2011tuned on long\u2011context tasks (catastrophic forgetting).\n\nFor long-context RAG fine\u2011tuning, the SelfAug method is designed specifically to reduce this kind of forgetting. The authors find that using longer reference documents in RAG creates a big shift between how the model was originally trained and how it is used during fine\u2011tuning, and that this shift strongly correlates with worse forgetting of general abilities like following instructions or writing correct code [1].\n\nSelfAug\u2019s fix is: during fine\u2011tuning, it keeps the model\u2019s internal behavior (its token\u2011by\u2011token \"logits\" over the input) close to how the original model behaves, while still letting it learn the new task. In simple terms, you make the tuned model imitate its old self on the same inputs, so it learns new skills without overwriting old ones [1]. This is especially effective in RAG with long contexts, where forgetting is worst [1].\n\nA second option, if you mainly need better domain knowledge and long\u2011context behavior, is to avoid modifying the base model at all. Memory Decoder adds a small, separate \"memory\" module that\u2019s trained for your domain while leaving the original model\u2019s parameters frozen. This preserves the base model\u2019s general capabilities and avoids catastrophic forgetting seen in traditional fine\u2011tuning [2].\n\nSo, to reduce forgetting with long contexts: (a) use a self\u2011alignment method like SelfAug during fine\u2011tuning, or (b) use a plug\u2011in memory module like Memory Decoder instead of fine\u2011tuning the base model directly.", "tags": ["_untagged"], "full_faithfulness": 0.8125, "full_answer_relevancy": 0.8833098515000001, "seed_faithfulness": 0.8, "seed_answer_relevancy": 0.8843435742, "context_relevance": 0.8454886810232797, "context_relevance_error": null, "context_relevance_reason": "The context passages provide substantial information related to the user's question about how to fix the issue of large language models forgetting information due to fine-tuning. Specifically, the discussion on catastrophic forgetting and the proposed SelfAug method directly addresses the problem and offers potential solutions. However, while the context is relevant, it could be more concise and focused on the specific question posed, which slightly detracts from its overall effectiveness.", "structure_citation": 0.7453779177098743, "structure_error": null, "structure_reason": "The Actual Output effectively addresses the issues of forgetting in long-context models and provides specific solutions like SelfAug and Memory Decoder. It includes claims supported by citations, but it lacks explicit [n] markers to reference the snippets in the Context. While the claims are well-supported by the provided information, the absence of [n] markers slightly detracts from the overall alignment with the evaluation steps.", "hallucination_score": 0.8724469121301478, "hallucination_error": null, "hallucination_reason": "The Actual Output effectively addresses the Input's query about long contexts and forgetting in models, providing detailed explanations of the issues and proposed solutions like SelfAug and Memory Decoder. It aligns well with the context, accurately reflecting the challenges of catastrophic forgetting and the methods to mitigate it. However, while the response is comprehensive, it could be slightly more concise, which prevents a perfect score.", "contexts_used": ["1 Introduction\n\nLarge language models (LLMs) like GPT (Achiam et al., 2023), PaLM (Chowdhery et al., 2023), GLM (GLM et al., 2024), and LLaMA (Touvron et al., 2023) have revolutionized NLP by learning complex linguistic patterns from extensive pre-training\n\n* Corresponding author\n\ndata, demonstrating excellence in contextual understanding and few-shot learning capabilities.", "Supervised fine-tuning (Ouyang et al., 2022; Chung et al., 2024) with general instruction datasets (Taori et al., 2023; Wang et al., 2022) improves models' instruction following abilities but often inadequately addresses specialized domain tasks. Task-specific fine-tuning provides targeted solutions for specialized applications (Roziere et al., 2023; Yang et al., 2024a; Hui et al., 2024; Luo et al., 2023a; Jin et al., 2024; Yin et al., 2024b,a; Huang et al., 2024b; Shen et al., 2024a; Zhang et al., 2025). Particularly, RetrievalAugmented Generation (RAG) (Guu et al., 2020; Lewis et al., 2020; Gao et al., 2023; Cai et al., 2022; Chen et al., 2024b; Shen et al., 2024b, 2025; Wu et al., 2024a; Gu et al., 2025; Yu et al., 2025) enhances LLMs by incorporating external knowledge through retrieval, reducing hallucinations. Recent work (Yang et al., 2024c; Liu et al., 2024b; Zhang et al., 2024b) improves how models utilize relevant information and handle insufficient information.", "However, fine-tuning for downstream tasks introduces catastrophic forgetting (French, 1999; Kemker et al., 2018; Shi et al., 2024; Wu et al., 2024b; Luo et al., 2023b), where models lose previously acquired knowledge and instructionfollowing abilities when adapting to new tasks. This causes performance deterioration across diverse applications. For example, a model finetuned on document extraction may generate structurally incorrect code, despite improved document parsing abilities. Recent research attributes this problem to distribution shift when models adapt to specialized task distributions during fine-tuning (Saha et al., 2021; Yang et al., 2024d).", "To address capability degradation, recent studies (Chen et al., 2024a; Bai et al., 2024; Jin and Ren; Huang et al., 2024a) suggest incorporating general instruction data during downstream fine-tuning to maintain LLM's general capabilities. However,\n\n*", "these strategies are limited by the scarcity of publicly available instruction datasets. Researchers have therefore explored alternative approaches that retain the model's original distribution without accessing general data. Instruction synthesis methods like MAGPIE (Xu et al., 2024b) use the model to generate instruction-response pairs for data replay, though they depend heavily on generation quality. Parameter constraint methods such as Orthogonal Loss (Wang et al., 2023) enforce orthogonality between parameters but compromise downstream task performance. Knowledge reconstruction approaches like SDFT (Yang et al., 2024d) approximate the original distribution by regenerating responses from fine-tuning data but struggle with format-specific tasks, particularly when structured outputs like JSON are required. While each approach offers certain benefits, they all have limitations. These limitations underscore the need for more efficient solutions that better balance capability preservation and task adaptation.", "To address the limitations of previous methods, we propose SelfAug, a novel approach that improves downstream performance while preserving the model's original capabilities. SelfAug is flexible and applicable to various fine-tuning scenarios. The core idea is to align the logits of input sequences during fine-tuning, leveraging the rich information in these logits generated by large language models during sequential processing. These logits reflect both learned knowledge and decision boundaries, helping to prevent catastrophic forgetting and ensuring the model's behavior remains consistent while enabling it to learn new tasks (Hsu et al., 2022; Sun et al., 2024). Our analysis reveals that catastrophic forgetting is most severe in RAG scenarios, especially when longer reference documents are used. We find a direct link between larger distribution shift and greater forgetting, as well as a correlation between longer contexts and more significant shift. SelfAug effectively mitigates these issues, achieving performance similar to LoRA while maintaining the model's original abilities, demonstrating the value of aligning logits distributions (Hsu et al., 2022; Sun et al., 2024). The main contributions of this work are as follows:\n\nWe introduce SelfAug, a novel self-alignment method based on logits. SelfAug aligns input sequence logits to overcome limitations of current methods regarding data access and parameter constraints. It requires no extra data or validation\n\nand avoids downstream performance loss caused by strict parameter updates.", "We provide an empirical analysis of catastrophic forgetting in RAG scenarios, showing that missing RAG capability in general instruction tuning causes significant distribution shifts. We also find a direct link between distribution shift and catastrophic forgetting severity.", "Experiments on various benchmarks demonstrate that SelfAug achieves superior downstream performance compared to existing methods while preserving the model's original distribution and reducing catastrophic forgetting.", "2 Related Works\n2.1 Fine-Tuning\n\nFine-tuning leverages the knowledge of pre-trained large models to improve their performance on specific downstream tasks. This approach has proven effective in areas such as mathematics (Luo et al., 2023a; Yang et al., 2024a; Tang et al., 2024), code (Roziere et al., 2023; Hui et al., 2024), finance (Li et al., 2023; Wu et al., 2023a), and healthcare (Yu et al., 2024). Standard fine-tuning works by aligning the model's output distribution with the downstream data through log-likelihood maximization. Although open-source LLMs are available for fine-tuning, training all parameters remains computationally expensive. Parameter-Efficient FineTuning (PEFT) (Mangrulkar et al., 2022; Han et al., 2024) addresses this by optimizing fewer parameters. Low-Rank Adaptation (LoRA) (Hu et al., 2021) is a popular PEFT method that allows finetuning with significantly fewer trainable parameters. Recent research (Wang et al., 2023; Liu et al., 2024a; Qiao and Mahdavi; Kowsher et al., 2024) has focused on improving LoRA to increase performance with minimal training costs and to support multiple downstream tasks.", "2.2 Catastrophic Forgetting\n\nFine-tuning models causes catastrophic forgetting as the model shift toward downstream task distributions and away from pre-training distributions. Traditional methods seek to balance performance across different tasks through various approaches. Parameter-constraining methods use regularization (Ni et al., 2024; Xinrui et al.) or selective parameter updates (Lin et al., 2024; Alexandrov et al., 2024;", "Figure 1: An illustration of full fine-tuning, LoRA, and methods for catastrophic forgetting mitigation. (a) SFT: Vanilla supervised fine-tuning with full parameter optimization. (b) LoRA: Parameter-efficient adaptation through low-rank decomposition. (c) MAGPIE: Self-synthesizing instruction-response pairs with pre-query templates for data replay. (d) SDFT: Fine-tuning with model-rewritten responses as optimized training dataset. (e) Orthogonal Loss: Imposing orthogonal constraints between LoRA modules and pre-trained parameters. (f) SelfAug: Selfdistillation through input logits distribution alignment to preserve model's original capabilities.\n\nInput Self-synthesizing Pre-query Data Merge Data Step-< Pre-trained Weights LoRA Output Instruction Response Data Synthesis (a) SFT (b) LoRA (c) MAGPIE Rewritten Ground Replace Train Dataset Truth Dataset Orthogonal Constraint Rewrite Logitsinput Logitsoutput Distribution Self-Alignment (d) SDFT (e) Orthogonal Loss SelfAug (Ours)", "Marczak et al., 2025; Jin and Ren, 2024a; Aggarwal et al., 2024; Franke et al., 2024; Panda et al., 2024; Zhang et al., 2024a; Yang et al., 2024b), but these limit downstream task performance. Mixture of Experts inspired approaches (Li et al., 2024a; Zhao et al., 2024; Le et al., 2024; Li et al., 2024b) maintain general capabilities by using different parameters for different tasks but alter model structure and prevent parameter merging. Data replay techniques (Bai et al., 2024; Jin and Ren, 2024b; Aggarwal et al., 2024; Huang et al., 2024a) preserve foundational knowledge but are constrained by the unavailability of pre-training data.", "Among these, some methods focus on continual learning scenarios, emphasizing the balance of performance across multiple downstream tasks. These approaches typically employ mechanisms for knowledge retention or parameter constraints between tasks to minimize the interference of new task training on previously learned tasks. However, their primary goal is to optimize overall task performance, with limited attention to preserving the general capabilities of pre-trained models. In contrast, our approach places greater emphasis on alleviating the forgetting of general capabilities in pre-trained models. We focus on maintaining the model's inherent language understanding, reasoning, and knowledge abilities during fine-tuning, while simultaneously adapting to new task requirements. To address the limitations of the aforementioned methods, we propose a universal strategy aimed at systematically mitigating catastrophic forgetting in large language models during fine-tuning, allowing the model to retain its original capabilities while efficiently adapting to new tasks.", "2.3 Knowledge Distillation\n\nKnowledge distillation is widely used for model compression and performance improvement by transferring knowledge from a teacher model to a smaller student model. Early work (Hinton, 2015; Xie et al., 2018; Liu et al., 2019; Wang et al., 2020) focused on distilling knowledge from large models into smaller ones. Later studies applied knowledge distillation to various tasks (Shu et al., 2021; Zhang and Ma, 2020; Wang et al., 2019). For LLMs, the most common method (Mai et al., 2024; Xu et al., 2024a) uses KL divergence to reduce the difference between the teacher and student output distributions. Other methods (Hou et al., 2020; Liang et al., 2023) align their intermediate hidden states. Some approaches (Wang et al., 2022; Ding et al., 2023) transfer knowledge from closed-source API models by augmenting the training data.", "Most existing knowledge distillation methods focus on transferring output sequences distributions to improve the downstream task performance of smaller models. In contrast, our method aims to reduce catastrophic forgetting during model finetuning by using the distribution of input sequences.", "5 Conclusion\n\nOur research explores the problem of catastrophic forgetting when fine-tuning language models for retrieval-augmented generation tasks. We find that distribution shift during fine-tuning weakens the model's general performance, especially its ability to follow instructions. To address this, we propose SelfAug, a method that does not use data replay or change the model architecture, and can be applied to any fine-tuning setting. SelfAug uses only the original training data and aligns the model's input distributions by constraining input sequence logits. This simple approach reduces distribution shift and helps prevent catastrophic forgetting. Our experiments show that there is a clear link between distribution shift and catastrophic forgetting. SelfAug reduces this shift and preserves model abilities, while matching or exceeding the downstream task performance of standard fine-tuning methods.", "8. Conclusion\n\nIn this paper, we introduced Memory Decoder, a novel plug-and-play approach for domain adaptation of large language models. By pre-training a small transformer decoder to emulate the behavior of non-parametric retrievers, Memory Decoder effectively adapts any compatible language model to a specific domain without modifying its parameters. Our comprehensive experiments across multiple model families and specialized domains demonstrate that Memory Decoder consistently outperforms both parametric adaptation methods and traditional retrieval-augmented approaches.", "The key innovation of Memory Decoder lies in its versatility and efficiency. A single pretrained Memory Decoder can seamlessly enhance any model that shares the same tokenizer, and with minimal additional training, can be adapted to models with different tokenizers and architectures. This capability enables efficient domain adaptation across model families, dramatically reducing the resources typically required for specialized model development. Our results confirm that Memory", "Decoder preserves the performance benefits of retrieval-augmented methods while maintaining the general capabilities of the base models, avoiding the catastrophic forgetting often observed with fine-tuning approaches.", "Memory Decoder introduces a new paradigm for domain adaptation that fundamentally reimagines how we specialize language models for particular domains. By decoupling domain expertise from model architecture through a pretrained memory component, our approach creates a more modular, efficient, and accessible framework for enhancing language model performance in specialized fields."]}
{"type": "entry", "index": 19, "question": "how does agentic rag differ from normal rag?", "answer": "Normal (traditional) RAG is usually a fixed pipeline: the system takes your question, runs a one-shot retrieval over a database (often text only), and then the model answers based on whatever was retrieved. The steps are static and linear\u2014\u201cretrieve, then read\u201d\u2014and do not change much based on query complexity or feedback. This works for simple questions but struggles with multi-step reasoning, complex tasks, and nuanced tool use [1][3].\n\nAgentic RAG adds an explicit decision\u2011making agent (or multiple agents) that actively controls how retrieval and reasoning happen. Instead of one fixed pass, the agent can:\n- Plan a retrieval strategy and decompose the task into sub\u2011queries or subtasks [1][3].\n- Run multi\u2011turn retrieval, refining searches and summarizing evidence over several steps [1][2][5].\n- Route the query through specialized tools or modules (e.g., domain routers, search routers, tissue classifiers, VQA agents) and choose between different retrieval modes (global vs partitioned, text vs image, hybrid) [1][5].\n- Decide whether RAG is needed at all, how many rewrites to perform, and which knowledge partition to search [1].\n- Learn better retrieval and reasoning policies via reinforcement learning, treating the corpus as an environment and optimizing end\u2011to\u2011end for answer quality and reasoning traceability [1][2][4].\n\nIn short, normal RAG is a static helper that fetches context once; agentic RAG is a learned controller that can adaptively plan, call tools, iterate retrieval, and provide more traceable, multi-step reasoning.\n", "tags": ["_untagged"], "full_faithfulness": 0.9354838710000001, "full_answer_relevancy": 0.9647742447000001, "seed_faithfulness": 0.64, "seed_answer_relevancy": 0.9622178753, "context_relevance": 0.939901613035366, "context_relevance_error": null, "context_relevance_reason": "The context passages provide a comprehensive explanation of how Agentic RAG differs from traditional RAG, specifically highlighting the incorporation of autonomous agents and their capabilities in planning retrieval strategies and multi-step reasoning. This directly addresses the user's question about the differences between the two systems, demonstrating strong relevance and alignment with the evaluation steps.", "structure_citation": 0.6267600946717622, "structure_error": null, "structure_reason": "The Actual Output includes several claims supported by citations [1], [2], [3], and [5], which correspond to snippets in the Context. However, there are unsupported claims made in the output, such as the detailed functionalities of Agentic RAG that are not fully substantiated by the provided Context. Additionally, while some citations match, others do not have clear corresponding snippets, leading to a deduction in points for unsupported claims and potential mismatches.", "hallucination_score": 0.9116312055142106, "hallucination_error": null, "hallucination_reason": "The response effectively distinguishes between normal RAG and agentic RAG, accurately reflecting the claims made in the input. It highlights the static nature of traditional RAG and contrasts it with the dynamic, decision-making capabilities of agentic RAG, which aligns well with the context provided. The explanation of multi-turn retrieval and the role of agents in planning and adapting retrieval strategies is well-supported by the context, demonstrating a strong understanding of the concepts. However, a minor shortcoming is the lack of specific examples or references to the context that could further enhance the explanation.", "contexts_used": ["2 Related Work\n\n2.1 Multimodal Agentic RAG\n\nRetrieval-Augmented Generation mitigates the limitations of large language models in knowledge completeness and factual consistency by incorporating external knowledge sources [10, 11]. Traditional RAG systems, such as Na\u00a8 \u0131ve RAG [12, 13] and Advanced RAG [14, 15], typically follow a static, linear 'retrieve-then-read' workflow. While effective for simple queries, they struggle with complex tasks requiring multi-step reasoning, context-aware adaptation, or tool use [16]. Agentic RAG extends the RAG paradigm by embedding autonomous agents into the retrieval and reasoning process [17]. These agents can plan retrieval strategies [18], invoke external tools [19], reflect and revise outputs [20], and collaborate across multiple roles or modalities [21, 22, 23]. Depending on design, they may operate as single decision-makers (e.g., router agents) or as specialized multi-agent systems for handling heterogeneous data sources. Despite these advances, existing Agentic RAG research in the medical domain has focused predominantly on text or structured data [24, 25], overlooking the diagnostic richness encoded in medical images. However, fields like pathology rely heavily on visual features, patterns in tissue morphology, staining, and spatial arrangements that cannot be captured through text alone.", "2.2 Reinforcement Learning for Medical VLMs\n\nReinforcement learning (RL) provides a promising paradigm for aligning the outputs of VLMs with clinical accuracy requirements, especially in high-risk medical domains where hallucinated descriptions can lead to severe consequences [26]. A core challenge in medical VLMs is ensuring factual alignment between visual evidence (e.g., pathology slides, radiology images) and textual outputs [27, 28, 29].", "However, direct end-to-end RL of large VLMs remains highly impractical. The main limitations include the scarcity of high-quality reward data from physicians [30], the instability of RL training on large models, and the lack of interpretability in the learned behaviors [31]. These issues make it difficult to deploy RL-fine-tuned models safely in clinical settings. To address this, recent approaches have turned to agent-centric strategies, where RL is applied not to the internal parameters of VLMs, but to the decision-making policies of external agents that interact with them. These agents may learn how to craft better queries, validate initial answers, or select relevant external knowledge iteratively [4, 22, 23]. This decoupled optimization process not only reduces risk but also enhances transparency and controllability.", "3 Methodology\n3.3 Multimodal Fusion\n\nExplanation To intuitively understand how Equation (1) influences the re-ranking process, we present the similarity matrices of the first- and second-ranked documents with respect to the query text embedding and image embedding in Figure 3. As shown in Figure 3, the second-ranked document exhibits a more uniform similarity, or attention, across the tokens of the document embedding. In contrast, the first-ranked document demonstrates more concentrated and higher similarity on fewer tokens of the document embedding. In this case, the high similarity of the first-ranked document to the query is meaningful, as it is caused by focused attention on the most informative tokens. On the other hand, the high similarity of the second-ranked document to the query results from more diffuse attention, likely caused by noise. Thus, although the second-ranked feature initially has the highest text similarity in the retrieval process, after re-ranking by the fusion formula, it drops to second place, while the original second-ranked document is promoted to", "Figure 2: An illustration of the multi-turn retrieval and summarization process.\n\nFinal Answer:\n\n<think>The Indian-file pattern is a hallmark of invasive lobular carcinoma, where\n\ntumor cells infiltrate the stroma in single-file lines. Lobular carcinoma cells are\n\ntypically small and uniform with scant cytoplasm and round nuclei. In contrast,\n\nductal carcinoma (B) forms solid nests and trabeculae, papillary carcinoma (C) has\n\nfibrovascular cores in papillary structures, and mucinous carcinoma (D) shows tumor\n\ncells in mucin pools. The observation of small cells, uniform nuclei, and single-file\n\ninfiltration aligns specifically with lobular carcinoma morphology.</think>\n\n<answer>A</answer>\n\nText Query:\n\nSmall, uniform tumor cells\n\ninfiltrate in an Indian-file\n\npattern. Which breast cancer\n\ntype is this?\n\nA. Lobular breast\ncancer\n\nB. Ductal breast cancer\n\nC. Papillary breast cancer\n\nD. Mucinous breast cancer\n\nExternal\nKnowledge\n\nTool Call1: {\"name\": \"rag\", \"parameters\": {\"query\": \"Histological\n\nfeatures of invasive lobular carcinoma of the breast,\n\nincluding\nIndian-file pattern\n\"}}\n\n{\"name\": 'classifier\", \"parameters\": {'partition\": 'Breast\"}}\n\nTool Call2: {\"name\": \"rag\", \"parameters\": {\"query\": \"How to\n\ndifferentiate ductal , papillary , and mucinous breast carcinoma histologically\"}} Text Retrieval\n\nMax Sim.\n\n\u2026\n\nTop20 Pages for each query\n\nPatho-R1 Pathology Reasoning Model Rerank by the similiary of image query Crop\n\n& summarize info.\n\nDatabase\n\n:\n\nover 200k pages of textbook over 150m vectors Vrag Agent Multi-Turn Intercation Think\n\nImage Query:", "Ground Truth: A\n\nQ1: Histological features of invasive lobular\n\ncarcinoma of the breast, including Indian-file\n\npattern\n\nA: Histological features of invasive lobular\n\ncarcinoma of the breast include uniform small\n\nround tumor cells infiltrating the stroma in a\n\nsingle-file (Indian-file) arrangement and\n\ncircumferentially around ducts in a target-like\n\npattern.\n\nQ2: How to differentiate ductal, papillary, and\n\nmucinous breast carcinoma histologically\n\nA: Histologically, ductal carcinoma shows glandular,\n\npapillary, cribriform, or diffuse growth patterns,\n\noften forming nests, trabeculae, or cords. Papillary\n\ncarcinoma is characterized by prominent papillary\n\nstructures with central fibrovascular cores.\n\nMucinous carcinoma displays abundant extracellular\n\nmucin, with tumor cells floating loosely in mucin\n\npools.\n\nSummarized Information\nInvasive lobular\ncarcinoma\n\n.\n\nCharacteristic\nhistologic features\n\nare: one cell wide\n\nfiles of round regular tumour cells ( Indian file )\n\nTop1 for tool call 1\n\nand lobular carcinomas represent75% and 25% of metastases\u2026, , cribriform or diffuse patterns\u2026 mucinous carcinoma B Hybrid endometrioid- like/\n\nTop1 for tool call 2\n\nAgentic Router(ours)\n\nthe top rank due to its combination of high similarity and concentrated attention.", "3.4 Agentic Diagnostic Workflow with Multimodal Evidence Tracing\n\nOur system adopts an intelligent multi-agent workflow that transforms a diagnostic query into an evidence-grounded conclusion. Given a user input (e.g., a pathology image and candidate diagnoses), the Agentic Router module first parses the query, decomposes it into sub-tasks aligned with each diagnostic candidate, and formulates a high-level retrieval plan. It delegates retrieval and evidence aggregation to the VRAG Agent , which operates under its guidance.", "The VRAG Agent executes a multi-turn retrieval process against the multimodal knowledge base. It first conducts text-based retrieval using candidate-specific keywords, and then re-ranks the retrieved entries by evaluating imagetext similarity with the query. Through iterative evidence refinement and summarization, as illustrated in Figure 2, the agent constructs a structured prompt containing the top 1ranked visual evidence for each candidate. This prompt is then passed to a specialized vision-language model, which performs contrastive reasoning to produce a diagnosis and evidence-grounded report. Details of each module and step are provided in Appendix B.", "3.5 Tool Integrated RL for Agentic Router\n\nTraditional RAG systems are often treated as static pipelines, without adapting their behavior to each query's complexity. To overcome this limitation, we introduce a reinforcement learning (RL) framework that enables an agent to learn dynamic invocation and routing strategies [35]. The agent's task is to generate a decision path specifying whether\n\nFigure 3: Illustration of the reranking process utilizing modality fusion.\n\nOriginal Text Query:\n\nThis image shows\nmonophasic synovial sarcoma\n\n. What are its key histologic features?\n\nA.", "Dense, uniform spindle cells with scant cytoplasm in short, intersecting fascicles.\n\nB.\n\nBiphasic: glandular epithelium with spindle cell stroma.\n\nC.\n\nScattered adipocytes in fibrous stroma with atypia.\n\nD.\n\nLarge polygonal cells with coarse eosinophilic granules.\n\nRewrited Search Query:\n\nWhat are the histological features of\n\n?\n\nEach query token\n\nembedding is matched to the\n\nmost similar image-patch embedding\n\non\n\nindexed pages, and their similarity scores are summed to rank the pages.\n\nPatho-Fusion\n\nScore:\n\nSimilarity Matrix Image 0.6638 0.6671 Final Top1 Cytologic appearance of Ewing sarcoma\n\n/PNET ...\n\n0.5878 Top20 from \u2026\u2026 Monophasic synovial\n\nsarcoma.\n\nThe tumor\n\n...\n\n0.6558\n\u2026 monophasic\nsynovial sarcoma\n\nwith ...\n\n0.6190 0.6602 Lose with an adenocarcinoma-like appearance of the\n\nepithelial component.", "and how to call the RAG system. Formally, given an input query Q orig, the agent's policy \u03c0 outputs a path P , optimized to maximize the expected similarity to a ground-truth decision path P gt\n\nThe hierarchical reward R final compares the generated path to the target path step by step (see Algorithm 1). The agent performs a sequence of decisions to construct the path:\n\nDecision 1: Whether to invoke RAG?", "-Path A (No RAG Invocation): If the agent decides False , the decision process terminates. This is for simple queries that can be answered without external knowledge. Final path: { rag : False }\n\n-Path B (Invoke RAG): If True , proceed to the next decision.", "Decision 2: How to decompose the task? (Only applies if RAG is invoked)\n\nThe agent may choose to rewrite the query one or more times to better surface its core semantics and improve retrieval quality. This is not a mechanical rewriting process, but a targeted transformation to better align with the retrieval engine.\n\n\u2022\n\nDecision 3: Whether to use a tissue-specific classifier? (Only applies if RAG is invoked) The agent decides whether to enable a classifier to restrict retrieval to a relevant knowledge partition.", "-Path B.1 (Global Retrieval): If False , RAG retrieves documents from the full corpus. Final path: { rag : True , rewrite count : n, classifier : False }\n\n-Path B.2 (Classifier-Based Retrieval): If True , proceed to the next decision.", "ABSTRACT\n\nRetrieval-Augmented Generation (RAG) mitigates hallucination in LLMs by incorporating external knowledge, but relies on chunk-based retrieval that lacks structural semantics. GraphRAG methods improve RAG by modeling knowledge as entity-relation graphs, but still face challenges in high construction cost, fixed one-time retrieval, and reliance on long-context reasoning and prompt design. To address these challenges, we propose Graph-R1 , an agentic GraphRAG framework via end-to-end reinforcement learning (RL). It introduces lightweight knowledge hypergraph construction, models retrieval as a multi-turn agent-environment interaction, and optimizes the agent process via an end-to-end reward mechanism. Experiments on standard RAG datasets show that Graph-R1 outperforms traditional GraphRAG and RL-enhanced RAG methods in reasoning accuracy, retrieval efficiency, and generation quality. Our code is publicly available 1 .", "8 Discussion and Future Direction\n\n8.2 Implications for Proprietary Data\n\nA key development in RAG is its application to enterprise AI, enabling access to proprietary data without embedding sensitive information into model parameters [35]. This architecture supports data privacy and ensures outputs remain grounded in up-to-date knowledge [74]. Unlike traditional LLMs, which require retraining to update internal knowledge, RAG allows retrieval components to be independently refreshed [74].", "However, enterprise RAG deployments introduce new security concerns. The retriever could inadvertently expose confidential information if strict access controls are not enforced. Organizations must implement robust authentication mechanisms, ensuring that retrieved documents align with user permissions. Privacy-preserving techniques such as encrypted search indices and federated retrieval [58] are promising solutions to mitigate risks.", "The ability of RAG to function over proprietary knowledge bases has made it a preferred choice for industries handling sensitive information, including finance, healthcare, and legal sectors. As enterprises scale RAG systems, optimizing retrieval latency and ensuring regulatory compliance will be paramount.", "8.3 Future Research Directions\n\nKey avenues for advancing RAG include:\n\nMulti-hop retrieval : RAG pipelines typically retrieve single passages, but many queries require reasoning over multiple documents. Standard RAG models struggle with multi-hop questions [88], motivating research on multi-step retrieval and reasoning (e.g., chained or iterative RAG methods).", "Secure and privacy-aware retrieval : As RAG is applied to sensitive data, privacy becomes critical. Recent work proposes encrypting knowledge bases and embeddings to prevent unauthorized access while preserving performance [107], and integrating differential privacy and secure computation to guard against information leakage [42].", "Multimodal and agentic RAG : Extending RAG beyond text can leverage rich context. Multimodal RAG (MRAG) systems incorporate images, video, or other modalities into retrieval and generation, reducing hallucinations and improving performance on visual queries [59]. Separately, Agentic RAG embeds autonomous agents into the retrieval pipeline, enabling dynamic planning, tool use, and multi-step reasoning in the loop [83].", "Structured knowledge integration : Traditional RAG relies on vector search over unstructured text, which can miss complex relationships. Incorporating structured knowledge (e.g., knowledge graphs) can improve semantic understanding. Surveys and systems demonstrate that linking RAG to knowledge graphs yields more accurate and coherent answers [14, 24].", "Real-time or streaming retrieval : Many applications require RAG to operate on live data streams. For example, systems like StreamingRAG build evolving knowledge graphs from streaming inputs to provide timely context updates, achieving much faster throughput and higher temporal relevance than static RAG [79].", "8.4 Emerging Practical Applications\n\nRAG is finding use in a range of new applications:\n\nEnterprise search : Companies employ RAG to query internal document repositories. By building RAG pipelines around enterprise vector databases, organizations can retrieve and", "synthesize answers from private knowledge with strong performance, scalability, and security guarantees [89].\n\nReal-time assistants : Personal or domain-specific AI assistants can use RAG to retrieve current information (e.g., news, stock prices, weather) on the fly. The RAG framework naturally supports grounding LLM responses in real-time data, improving accuracy and user trust [33].", "Fact-checking : Integrating RAG into fact-checking pipelines can automate evidence retrieval. Recent systems use RAG-based reasoning (e.g., chain-of-RAG) to fetch and synthesize evidence for claims, improving veracity predictions in political and news domains [1].", "End-to-End Agentic RAG System Training for Traceable Diagnostic Reasoning\n\nQiaoyu Zheng 1,2 , Yuze Sun 1 , Chaoyi Wu 1 , Weike Zhao 1,2 , Pengcheng Qiu 1,2 , Yongguo Yu 3 , Kun Sun 3 , Yanfeng Wang 1,2 , Ya Zhang 1,2, \u2020 and Weidi Xie 1,2, \u2020\n\n3 Xinhua Hospital affiliated to Shanghai Jiao Tong University School of Medicine, Shanghai, China\n\nShanghai Jiao Tong University, Shanghai, China Shanghai AI Laboratory, Shanghai, China", "End-to-End Agentic RAG System Training for Traceable Diagnostic Reasoning\n\nAccurate diagnosis remains a central challenge for medical large language models due to inherent knowledge limitations and hallucinations. While retrieval-augmented generation (RAG) and tool-augmented agentic methods show potential in mitigating these issues, their suboptimal utilization of external knowledge and the decoupling of the feedback-reasoning traceability, stemming from insufficient supervision, remain key limitations. To address these challenges, We introduce Deep-DxSearch, an agentic RAG system trained end-to-end with reinforcement learning (RL) that enables steer tracebale retrieval-augmented reasoning for medical diagnosis . In Deep-DxSearch, we first construct a large-scale medical retrieval corpus comprising patient records and reliable medical knowledge sources to support retrieval-aware reasoning across diagnostic scenarios. More crucially , we frame the LLM as the core agent and the retrieval corpus as its environment, using tailored rewards on format, retrieval, reasoning structure, and diagnostic accuracy, thereby evolving the agentic RAG policy from large-scale data through RL.", "End-to-End Agentic RAG System Training for Traceable Diagnostic Reasoning\n\nExperiments demonstrate that our end-to-end agentic RL training framework consistently outperforms prompt-engineering and training-free RAG approaches across multiple data centers. After training, DeepDxSearch achieves substantial gains in diagnostic accuracy, surpassing strong diagnostic baselines such as GPT-4o, DeepSeek-R1, and other medical-specific frameworks for both common and rare disease diagnosis under in-distribution (ID) and out-of-distribution (OOD) settings. Moreover, ablation studies on reward design and retrieval corpus components confirm their critical roles, underscoring the uniqueness and effectiveness of our approach compared with traditional implementations. Finally, case studies and interpretability analyses highlight improvements in Deep-DxSearch's diagnostic policy, providing deeper insight into its performance gains and supporting clinicians in delivering more reliable and precise preliminary diagnoses. Data, code, and checkpoints are available at https://github.com/MAGIC-AI4Med/Deep-DxSearch .", "6 Conclusion\n\nIn this work, we proposed the QA-Dragon , a query-aware dynamic RAG system tailored for complex and knowledge-intensive VQA tasks. Unlike the traditional RAG system, which retrieves the text and image separately, the QA-Dragon integrates a domain router, a search router, and orchestrates hybrid retrieval strategies via dedicated text and image search agents. Our system supports multimodal, multi-turn, and multi-hop reasoning through orchestrating both text and image search agents in a hybrid setup. Extensive evaluations on the Meta CRAG-MM Challenge (KDD Cup 2025) demonstrate that our system significantly improves answer accuracy, surpassing other strong baselines."]}
{"type": "entry", "index": 20, "question": "when should you not use rag?", "answer": "You should avoid or limit RAG in several situations:\n\n1. When the external documents don\u2019t actually contain the answer. Methods that rely heavily on retrieved text work best when the answer is explicitly in the documents; performance drops when the documents only provide loose background information rather than the exact answer [1]. In such cases, RAG can distract the model instead of helping.\n\n2. When using a large model that already \u201cknows\u201d the domain well. Retrieval gives clear gains mainly for smaller models; as models get larger, the benefit from RAG shrinks because they internalize more knowledge, while retrieval still brings complexity and possible errors [2].\n\n3. When your retrieval quality is poor or the corpus is noisy. If the system can\u2019t reliably estimate what\u2019s relevant, RAG may inject wrong or off-topic passages, pushing the model toward incorrect answers [2][5]. Several works exist mainly to repair these retrieval problems, which shows how harmful bad context can be [5].\n\n4. When latency or computation must be minimal. Token\u2011 or passage\u2011level retrieval adds significant overhead at inference time [4], and fixed or frequent retrieval can introduce unnecessary steps and slow responses, especially when the query didn\u2019t need extra context in the first place [5].\n\n5. For simple, self\u2011contained questions. Many queries don\u2019t benefit from external context at all; blindly doing retrieval for every question can add noise and delay without improving accuracy [5].\n\nIn these settings, relying on the base model (possibly with light fine\u2011tuning) can be better than adding RAG.", "tags": ["_untagged"], "full_faithfulness": 1.0, "full_answer_relevancy": 0.9099008745, "seed_faithfulness": 1.0, "seed_answer_relevancy": 0.9099008745, "context_relevance": 0.284403328938995, "context_relevance_error": null, "context_relevance_reason": "The context passages provide some information about RAG and its challenges, but they do not directly address the user's question about when not to use RAG. While there are mentions of limitations and issues with RAG, such as heavy dependence on documents and the need for task-specific reasoning, these do not clearly outline specific scenarios or guidelines for avoiding RAG usage. The overall relevance to the user's question is minimal.", "structure_citation": 0.7782593699598771, "structure_error": null, "structure_reason": "The Actual Output effectively addresses the question of when to avoid using RAG by providing multiple well-structured claims, each supported by citations. However, while most claims are backed by relevant snippets from the Context, there are instances where the citations do not directly correspond to the claims made, particularly with markers [2] and [5]. This slight mismatch affects the overall strength of the argumentation, leading to a deduction in points.", "hallucination_score": 0.8593375600567466, "hallucination_error": null, "hallucination_reason": "The response accurately identifies several situations where RAG should be avoided, aligning well with the context provided. Each claim made in the output is supported by relevant details from the context, particularly regarding the limitations of RAG in terms of document quality, model size, and computational overhead. However, while the claims are mostly supported, the response could benefit from more explicit references to the context, particularly in discussing the implications of poor retrieval quality and the need for adaptive strategies, which are mentioned in the context but not directly tied to the claims.", "contexts_used": ["1 Introduction\n\nRetrieval Augmentation Generation (hereafter referred to as RAG) helps large language models (LLMs) (OpenAI et al., 2024) reduce hallucinations (Zhang et al., 2023) and access real-time data\n\n1 *Equal contribution.", "by incorporating an information retrieval component. While LLMs often use in-context learning (Gao et al., 2024) for generation, practical issues such as low-quality or poorly ranked retrieved documents can hinder RAG's effectiveness. These challenges emphasize the need for instruction-tuning tailored to RAG tasks. Fine-tuning generative models specifically for RAG improves their ability to integrate retrieved information (Zhang et al., 2024) (Yu et al., 2024), resulting in more accurate and contextually relevant responses compared to generalpurpose models.", "RAFT (Zhang et al., 2024) enhances model performance in domain-specific RAG tasks by introducing distractor documents during training. EvidenceRAG (Schimanski et al., 2024) improves large language models in evidence-based question answering by incorporating an indexing task, enhancing their ability to accurately cite and reflect source information. RankRAG (Yu et al., 2024) employs a two-stage training process to simultaneously optimize the context ranking and answer generation capabilities of large language models (LLMs) in RAG tasks.", "Despite significant research efforts on RAGspecific generative models, several issues remain.\n\nLack of Granular RAG Task Focus : Researchers have primarily concentrated on finetuning RAG models without enhancing their capabilities through more granular RAG tasks, limiting the potential to strengthen RAG abilities effectively.", "Lack of Task-Specific CoT Paradigm Design in RAG : Although there have been proposals to integrate chain-of-thought (CoT) reasoning into the training process to enhance model accuracy (Wei et al., 2023), these methods are not specifically designed for RAG scenarios. Even in the rare cases where RAG models do incorporate CoT (Zhang et al.,", "Figure 1: Traditional RAG methods have primarily focused on retrieving relevant information, with less emphasis on its effective utilization. We propose a method that enhances model performance in complex RAG scenarios by developing three progressive capabilities.\n\nTraditional RAG\n\nOurs: Hierarchical-Thought Instruction-Tuning Retrieval-Augmented Generation(HIRAG)\n\nLevel\n\nLct mc find thc contcnt rclatcd\n\nLet me check! Which part of the\n\nto the question from all\n\nRetrieved Documents\n\ncontent can answer this\n\nquestion?\n\nreferences!\n\nAlot of information is rclcvant.\n\nAbility: Combination\nHow can\n\ndeal with them?\n\nfound somc information!\n\nnccd to\n\nunderstand their relationship and\n\ncombine them Samc INFO INFO from Different perspectives Complementary INFO Levcl\n\nAbility: RAG-Specific Reasoning\n\nThis is hard! The content doesn't\n\nNo matter what,\njust directly\n\ndircctly answcr the qucstion, but\n\n(illogically) piece together\nluckily\n\nhave some reasoning skills\n\ninformation to form\n\nresponse.\n\nCompurative Rcasoning\nDcductivc Rcasoning\nCausal Rcasoning\n\n2024), there remains a lack of differentiated CoT paradigms designed to address the unique challenges posed by different tasks in RAG. Consequently, the full potential of CoT in enhancing RAG performance has yet to be realized.", "Thus, We introduce a new RAG Instruction Tuning method: Hierarchical-Thought InstructionTuning Retrieval-Augmented Generation ( HIRAG ) adapting to complex RAG scenarios and propose that when fine-tuning RAG generation models, we focus on three progressively hierarchical abilities shown in Figure 1: Filtering: The ability that LLM filters out noise and selects the direct information. Combination: The ability of LLMs to merge, integrate, and summarize multiple pieces of useful information. RAG-Specific Reasoning: The capability refers to the ability to answer a question by making implicit or explicit inferences based on the information in the documents when the relevant information is not directly provided.", "To better achieve these three capabilities, a \"think before answering\" approach based on progressively hierarchical thought has been introduced.\n\nThe contributions of this work are summarized as follows:\n\nWe propose three progressive hierarchical capabilities that a RAG model requires: filtering, combination, and RAG-specific reasoning\n\nto enhance the granularity and specificity of RAG tasks when dealing with complex scenarios.", "We introduce HIRAG , a fine-tuning strategy that employs task-specific reasoning patterns to construct a progressive chain of thought. This approach constructs a progressive chain of thought, enabling the model to learn from easier to more complex tasks, thereby significantly enhancing its performance in RAG scenarios.", "Extensive experiments were conducted on six datasets, including the RAG-specific benchmark, single-hop open-domain data, multihop open-domain data, and domain-specific data. Our model significantly outperforms the current state-of-the-art models. We also conducted experiments on Chinese datasets, confirming the robustness of our approach. Furthermore, ablation studies demonstrate that the training tasks for the three capabilities contribute to the performance of HIRAG, and we explored the optimal data ratio.", "Limitations\n\nHeavy Dependence on Documents: Our method performs exceptionally well when the answers are present within the documents. However, its performance declines when the documents only provide supplementary information without containing the exact answers. Further experimentation and adjustments are required to optimize the model's ability to generate direct answers in such scenarios.", "Domain Knowledge Enhancement: Training RAG models solely on general knowledge is insufficient for performance in specialized domains. Future work could consider a two-stage training approach. In the first stage, we conduct general domain RAG fine-tuning. Building on the general reasoning abilities established in the first phase, this phase involves setting different paths of thought and stacked thoughts based on various intents within the vertical domains.", "This study evaluates the effectiveness of Retrieval-Augmented Generation (RAG) systems across diverse knowledge sources and tasks. Utilizing MASSIVEDS, a large-scale datastore, the authors found that retrieval mainly benefits smaller models while larger models exhibit diminishing returns. The analysis shows no single retrieval source consistently outperforms others, highlighting the necessity for adaptive retrieval strategies. Furthermore, efforts such as reranking and query routing have limited improvement on overall performance. Key challenges include inaccurate relevance estimation and training-inference mismatch, suggesting a need for improved integration of retrieval mechanisms with language models. The findings indicate a critical gap in deploying RAG systems effectively in real-world scenarios, particularly for larger models that increasingly internalize domain knowledge without retrieval support.", "2 Related Work\n\n2.1 The Evolution of Automated Question Generation (AQG) in Educational NLP\n\nAutomated Question Generation (AQG) has been a long-standing goal in Natural Language Processing (NLP) for education. Early approaches often relied on rule-based systems and templates, which were rigid and difficult to scale. The advent of neural networks and, more recently, large language models (LLMs) has revolutionized the field, enabling the generation of fluent and complex questions from source text.[4] Recent studies highlight that while LLMs are", "2.1 The Evolution of Automated Question Generation (AQG) in Educational NLP\n\npowerful generators, the quality of their output for specialized educational purposes is highly dependent on the context provided and the prompting strategy employed.[5] The challenge has shifted from merely generating a syntactically correct question to generating one that accurately tests a specific concept or cognitive skill, such as conceptual reasoning rather than simple factual recall.[5]", "2.2 Grounding Generative Models: The Rise of Retrieval-Augmented Generation (RAG)\n\nRetrieval-Augmented Generation (RAG) has emerged as the foremost technique for addressing the 'grounding problem' of LLMs.[9] First proposed by Lewis et al. (2020), the RAG architecture combines the strengths of a parametric memory (the pre-trained LLM) with a non-parametric memory (an external, retrievable knowledge source).[10] In this hybrid model, the LLM does not rely solely on its internal, static knowledge. Instead, given a prompt, it first retrieves relevant documents from the external source and then uses this retrieved context to inform and constrain its generated output. This approach has been shown to significantly reduce hallucinations and improve the factual accuracy of LLM responses. In educational technology, RAG is particularly valuable. It allows for the creation of AI systems that can provide factually consistent, curriculum-aligned answers and content.[9] Furthermore, because the knowledge base is external, it can be easily updated with new or corrected information (e.g., a revised syllabus or new textbook) without the need for costly and time-consuming LLM retraining.[9] This makes RAG a practical and adaptable solution for developing dynamic educational tools.[14]", "2.3 Generative AI for Low-Resource Languages: Gaps and Opportunities\n\nThe development of GenAI has been heavily skewed towards high-resource languages like English, creating a 'digital language divide'.[11] Languages with smaller digital footprints, such as Bahasa Melayu, face significant challenges, including data scarcity, lack of culturally-aware benchmarks, and poor representation in the massive datasets used to train foundational models.[6] Consequently, even state-of-the-art LLMs often exhibit weaker performance in these languages.[6] While NLP toolkits and resources for Bahasa Melayu exist, they are often general-purpose and may not be sufficient for highly specialized, domain-specific tasks like generating questions that adhere to the nuances of the Malaysian national curriculum.[12] This study positions RAG as a pragmatic strategy to bridge this gap. By forcing the model to ground its output in a curated, local-language corpus (the official curriculum documents), RAG can effectively leverage the power of a global LLM for a local, specialized task, circumventing the need for a massive, domain-specific training dataset.", "2.4 Automated Evaluation of Generated Content: Beyond Lexical Overlap\n\nEvaluating the quality of generated text is a complex challenge. Traditional metrics like ROUGE and BLEU, which rely on lexical overlap, are inadequate for assessing semantic meaning and are thus poorly suited for evaluating educational questions.[18] Semantic Textual Similarity (STS) offers a more robust alternative. STS measures the degree of semantic relatedness between two texts, typically by calculating the cosine similarity between their vector embeddings.[19] This allows for an assessment of whether a generated question aligns thematically with a specific learning objective, even if they do not share the exact same words.[21] However, STS is not without its limitations. Research has shown that standard STS can be ambiguous and may not always correlate well with performance on a specific downstream task.[19] The similarity score depends on the aspect of interest, which can be subjective.[22] This critique motivates the need for a second, complementary evaluation metric that is more functional and task-oriented. This study introduces a novel RAG-based Question-Answering (RAG-QA) validation method to address this gap. This method provides a direct, functional test of a question's contextual validity by assessing whether it can be answered using the core curriculum document as a knowledge source, thereby offering a stricter and more objective measure of curriculum relevance.", "7. Related Work\n\nRetrieval-Augmented Generation Retrieval-Augmented Generation (RAG) enhances language models by incorporating knowledge from external sources, with retrieval granularity ranging from documents [Chen et al., 2017] to passages [Guu et al., 2020, Izacard et al., 2023, Lewis et al., 2020] to tokens [He et al., 2021, Khandelwal et al., 2019, Min et al., 2022, Yogatama et al., 2021]. Tokenlevel retrieval achieves superior performance for rare patterns and out-of-domain scenarios but introduces substantial computation overhead during inference. While non-differentiable retrieval mechanisms prevent end-to-end optimization and memory token approaches [Chevalier et al., 2023] enable differentiable access but are limited to local contexts, Memory Decoder provides both differentiable optimization and full-dataset knowledge access without expensive retrieval operations or model-specific datastores.", "Domain Adaptation Domain adaptation techniques have evolved from domain-specific pre-training (SciBERT [Beltagy et al., 2019], BioBERT [Lee et al., 2020], ClinicalBERT [Huang et al., 2019]) to parameter-efficient methods like LoRA [Hu et al., 2022] and adapters [Diao et al., 2021, 2023, Wang et al., 2020]. However, these approaches require model-specific modifications, preventing generalization across architectures. Memory Decoder addresses this limitation by providing a domain-specific memory module that enhances multiple frozen language models without parameter modifications, enabling cross-model adaptation within tokenizer families and efficient cross-tokenizer transfer with minimal additional training.", "5 Generation-time feedback\n\nRAG systems face two fundamental challenges: determining when to retrieve external knowledge, since not all queries benefit from it, and how to retrieve relevant content effectively (Su et al., 2024b). Classical RAG pipelines rigidly follow a fixed sequence of retrieval, optionally ranking, followed by generation, limiting their ability to adapt to the context or task. To address these limitations, recent work has introduced adaptive RAG , where the retrieval strategy is dynamically adjusted according to the query, the model feedback, or the complexity of the task. We categorize this emerging line of work into three main classes.", "5.1 Rule-Based and Discriminative Approaches\n\nIn-Context RALM (Retrieval-Augmented Language Model) (Ram et al., 2023) proposes retrieving relevant context documents during inference at fixed intervals (every s tokens, known as the retrieval stride), using the last l tokens of the input as the retrieval query. In a similar spirit, IRCoT (Interleaving Retrieval in a CoT) (Trivedi et al., 2023) dynamically retrieves documents if the CoT (Wei et al., 2022) step has not provided the answer. At first, it uses the original question to retrieve the context and then uses the last generated CoT sentence as a query for subsequent retrieval. However, both of these methods retrieve the context regardless of whether the LLM needs external context or not. Hence, the unnecessary retrieval steps add additional latency cost during answer generation. Also, the noisy retrieved context can lead to a wrong answer. CtRLA (Huanshuo et al., 2025) devises a latent space probing-based approach for making decisions regarding retrieval timings for adaptive retrieval augmented generation. The authors extract latent vectors that represent abstract concepts like honesty and confidence and use these dimensions to steer retrieval and control LLM behavior, leading to better performance and robust answers. To overcome the over-retrieval limitation of rule-based dynamic RAG methods, retrieval-ondemand approaches have been proposed. These methods trigger retrieval only when the LLM needs it, based on either external feedback (Section 5.2)or the LLM's own assessment (Section 5.3).", "5.2 Retrieval-on-Demand via Feedback Signals\n\nThe feedback signals can come from different sources, including the answer uncertainty, the model's internal states, or context faithfulness and sufficiency. SKR (Wang et al., 2023b) asks LLM itself if additional information is needed to answer the query. If yes, then the retrieval round is triggered; otherwise, the answer is generated from the", "LLM's internal knowledge. However, the judgment is solely based on LLM, and without context, they try to be overconfident (Xiong et al., 2024). FLARE (Jiang et al., 2023) retrieves the documents only if the token probability is below a predefined threshold and uses the last generated sentence as a query for retrieval (excluding the uncertain tokens) and generates the response until the next uncertain token or completion is done. However, these uncertain tokens are not equally important to trigger a retrieval round. Based on this, DRAD (Su et al., 2024a) uses an external module for hallucination detection on entities in the generated answer; if the answer contains hallucination, the retrieval is triggered. The last generated sentence (without a hallucinated entity) is used as a query for retrieval. However, the choice of the new query for retrieval relies on heuristic strategies. Since the model's information needs may extend beyond the last sentence or CoT, it could require context from a broader span of the generation to effectively build confidence. Based on this motivation, DRAGIN (Su et al., 2024b), similar to FLARE, also considers the token probabilities as a criterion of the retrieval round but does not consider the uncertain tokens as a part of the new query. Further, it also reformulates the query using the keywords based on the model's internal attention weights and reasoning. SeaKR (Yao et al., 2025) computes the self-aware uncertainty using internal states of the LLM. If the uncertainty is above a threshold, then a retrieval round is triggered.", "Other types of works, like Rowen (Ding et al., 2024), consider the LLM's answer consistency as feedback. Rowen considers answer consistency across languages of the same question with semantically similar variations, and the consistency over answers generated by different LLMs. If the total consistency is below a predefined threshold, then the retrieval round is triggered. However, similar to SUNAR (V et al., 2025), the consistency can be toward wrong answers.", "However, these approaches consider all queries equally complex and might end up with noisy context retrieval and hence a wrong answer. Adaptive RAG (Jeong et al., 2024) uses a query routing mechanism that predicts whether the query needs retrieval or not. Further, it also decides on the number of retrieval rounds based on query complexity. However, it assumes that the retrieved context is relevant to the query without assessing its relevancy or sufficiency. Towards the relevancy, CRAG", "(Corrective RAG) (Yan et al., 2024) evaluates the relevance scores using a fine-tuned model, and classifies the retrieved document into correct, incorrect, and ambiguous. If the context is not correct, then a rewritten query is issued to the web search engine. Similar fashion, SIM-RAG (Yang et al., 2025b) focuses on the context sufficiency angle, and trains a lightweight critic model that provides feedback if the retrieved context is sufficient to generate the answer. If the information is not sufficient, then a new query is formulated using the original query and the already retrieved context, and a retrieval round is triggered. Further CoV-RAG (He et al., 2024) identifies errors, including reference and answer correctness, and truthfulness, and then scores them using a trained verifier. Based on the scores, either provide a final or rewrite the query and do a further retrieval round.", "5.3 Self-Triggered Retrieval via Reasoning\n\nIn this section, we discuss works where LLM autonomously makes the decision on when to retrieve and how to retrieve through query decomposition or planning-based approaches without external triggers. These approaches are also termed Reasoning RAG or Agentic RAG . These approaches can be divided into mainly two categories: first, where the instructions for query decomposition, when to retrieve, and what to retrieve are provided in the prompt along with few-shot examples; second, where the language models are trained to decide by themselves whether to decompose the query, when to retrieve, and what to retrieve.", "Prompt-Based Methods. DeComP (Khot et al., 2023) divides a task into granular sub-tasks and delegates them to different components through actions. However, DeComP only acts as a trigger for when to retrieve and employs a BM25 retriever for getting relevant documents in a single shot. It does not subsequently generate reasoning steps to improve retrieval, thus not providing much indication as to how to retrieve. ReAct (Yao et al., 2022) interleaves the generation of verbal reasoning traces with actions that interact with the external environment. The verbal reasoning traces act as indicators of how to retrieve, and the actions themselves serve as triggers (when to retrieve). Similarly, SelfAsk (Press et al., 2023) proposes to decompose the original complex query into simpler sub-questions iteratively interleaved by a retrieval step and intermediate answer generation. At each step, the LLM makes a decision to generate a follow-up question if more information is needed, or it may generate the final answer. Authors observed that this approach helped cover diverse aspects of complex queries and improved search and downstream answering performance.", "However, these approaches do not have provision for correction of the entire reasoning trajectory, and an intermediate error may cause cascading failures. Searchain (Xu et al., 2024) proposes to mitigate this by constructing a global reasoning chain first, where each node comprises a retrievaloriented query, an answer from LLM to the query, and a flag indicating if additional knowledge is needed to arrive at a better answer. SMR (State Machine Reasoning) (Lee et al., 2025) identifies the issues of the CoT-based query decomposition and retrieval methods like ReAct (Yao et al., 2022), where the CoT might result in redundant reasoning (new queries that result in the retrieval of the same documents) and misguided reasoning (new query diverges from the user's intent). To address these limitations, SMR proposes three actions: Refine, Rerank, and Stop. Action Refine updates the query using the feedback from the already retrieved documents, and a retrieval round is triggered. Then the retrieved documents are ranked according to the old query to make sure only the relevant information is used to answer. Finally, the Stop action is called to stop the reasoning if a sufficient retrieval quality is achieved, which helps in token efficiency and prevents overthinking.", "MCTS-RAG (Hu et al., 2025) combines Monte Carlo Tree Search (MCTS) with RetrievalAugmented Generation (RAG) to improve reasoning and retrieval in language models. It guides the search for relevant information using MCTS to explore promising retrieval paths, enhancing answer accuracy. However, it is computationally expensive due to the iterative tree search process and may struggle with highly noisy or irrelevant documents. Search-O1 (Li et al., 2025a) proposes an agentic search workflow for reasoning augmented retrieval by letting the Large Reasoning Models (LRMs) like O1 decide autonomously when to retrieve by detecting the presence of salient uncertain words in their output. Additionally, they augment the workflow with a reason-in-documents step, where LRMs analyze the documents in depth to remove noise and reduce redundancy before employing them to generate the final answer."]}
{"type": "entry", "index": 21, "question": "how are people combining rag with graph databases?", "answer": "People are combining RAG with graph databases by turning raw documents into structured graphs and then using those graphs as the main retrieval layer before generation.\n\n1) Build a knowledge graph from documents\n- Systems extract subject\u2013predicate\u2013object triples (like \u201cBank X \u2013 must report \u2013 transaction Y\u201d) from regulatory or domain documents and store them as nodes and edges in a graph [3].\n- In some domains, they also include images and visual relations, forming a multimodal knowledge graph, not just text [1].\n- For medical use cases, tools (e.g., Gemini) are used to generate knowledge graphs that capture relationships between diseases, treatments, and outcomes from guidelines and papers [5].\n\n2) Index the graph for RAG\n- The graph elements (triples, entities, relations) are embedded and stored alongside the original text sections and metadata in a unified vector database. This lets the system do both graph-style reasoning and vector search with one store [3].\n- RAG research explicitly highlights \u201cintegration of structured knowledge bases like databases or graphs\u201d as a key evolution beyond pure text retrieval [4].\n\n3) Graph-aware retrieval before generation\n- Some frameworks use a dual-stage process: first recall relevant documents, then retrieve fine-grained entities and relationships from the graph, often using a specialized retriever tuned to the question [1].\n- Other systems perform triplet-level retrieval so the LLM sees the precise \u201cwho-did-what-to-whom\u201d facts, improving alignment and traceability for complex regulatory questions [3].\n\n4) Generation with structured context\n- The LLM (or multimodal LLM) is then prompted with both the raw text snippets and the graph-derived facts/subgraphs, improving factual accuracy, reasoning over relationships, and explainability [1][3][5].", "tags": ["_untagged"], "full_faithfulness": 1.0, "full_answer_relevancy": 0.9823590249, "seed_faithfulness": 0.8571428571, "seed_answer_relevancy": 0.9823590249, "context_relevance": 0.8595550480858849, "context_relevance_error": null, "context_relevance_reason": "The context passages provide relevant information about the integration of Retrieval-Augmented Generation (RAG) with multimodal knowledge graphs (KGs) specifically for knowledge-based Visual Question Answering (VQA) tasks. This directly addresses the user's question about how RAG is combined with graph databases. However, while the passages are informative, they could be more explicitly focused on practical examples or applications of this integration, which would enhance the relevance to the user's inquiry.", "structure_citation": 0.8386400428116669, "structure_error": null, "structure_reason": "The Actual Output effectively incorporates claims supported by the Context, with multiple [n] markers corresponding to relevant snippets. However, while most claims are well-supported, there are minor instances where the connections could be clearer, particularly in the explanation of the dual-stage retrieval process. Overall, the response demonstrates a strong understanding of how RAG integrates with graph databases, but slight improvements in citation clarity could enhance the overall alignment.", "hallucination_score": 0.8970637544747755, "hallucination_error": null, "hallucination_reason": "The response effectively outlines how RAG is combined with graph databases by detailing the process of building knowledge graphs, indexing them for retrieval, and using graph-aware retrieval methods. It aligns well with the claims made in the input, demonstrating a clear understanding of the integration of structured knowledge into the RAG framework. The output supports the context provided, particularly in addressing the challenges of unstructured data in knowledge-intensive tasks. However, a minor shortcoming is the lack of specific examples or case studies that could further illustrate the practical applications of these methods.", "contexts_used": ["Abstract\n\nRecently, Retrieval-Augmented Generation (RAG) has been proposed to expand internal knowledge of Multimodal Large Language Models (MLLMs) by incorporating external knowledge databases into the generation process, which is widely used for knowledge-based Visual Question Answering (VQA) tasks. Despite impressive advancements, vanilla RAG-based VQA methods that rely on unstructured documents and overlook the structural relationships among knowledge elements frequently introduce irrelevant or misleading content, reducing answer accuracy and reliability. To overcome these challenges, a promising solution is to integrate multimodal knowledge graphs (KGs) into RAG-based VQA frameworks to enhance the generation by introducing structured multimodal knowledge. Therefore, in this paper, we propose a novel multimodal knowledge-augmented generation framework ( mKG-RAG ) based on multimodal KGs for knowledge-intensive VQA tasks. Specifically, our approach leverages MLLM-powered keyword extraction and vision-text matching to distill semantically consistent and modality-aligned entities/relationships from multimodal documents, constructing high-quality multimodal KGs as structured knowledge representations. In addition, a dual-stage retrieval strategy equipped with a question-aware multimodal retriever is introduced to improve retrieval efficiency while refining precision. Comprehensive experiments demonstrate that our approach significantly outperforms existing methods, setting a new state-of-the-art for knowledge-based VQA.", "1 Introduction\n\nVisual Question Answering (VQA) [2, 19] is a challenging task at the intersection of vision and language understanding, requiring models to interpret images and answer related questions. This capability has enabled remarkable advances in various domains, including medical image diagnosis [33] and customer service support [12]. Recently, due to the powerful visual-linguistic understanding and reasoning capabilities, Multimodal Large Language Models ( MLLMs ) [35, 30, 50, 7] have provided a promising solution to conventional VQA tasks. For instance, LLaVA [35] demonstrates strong\n\n\u2217 Corresponding author", "zero-shot performance on commonsense VQA by leveraging pre-trained visual encoders for image representation alongside the reasoning capabilities of large language models (LLMs). Despite notable advancements, MLLMs face critical limitations in knowledge-intensive VQA scenarios [40, 6] (termed knowledge-based VQA ), particularly those requiring encyclopedic knowledge, long-tail factual recall, or contextual reasoning beyond immediate visual inputs. As illustrated in Figure 1 (a), when queried about the latest renovation date of a stadium, typical MLLMs exhibit two characteristic failure modes: generating plausible but factually incorrect responses or refusing to answer altogether. These issues stem from the scarcity of relevant knowledge in MLLMs' training corpus and the inherent difficulty of memorizing low-frequency facts [6].", "Recently, Retrieval-Augmented Generation ( RAG ) [16] has shown great potential in addressing these challenges by leveraging external knowledge databases to supplement the internal knowledge of MLLMs, thereby enabling more accurate answer generation [32, 4, 11]. Specifically, multiple query-relevant documents are retrieved from external knowledge databases and serve as in-context information to augment the generation process of MLLMs. Despite their success, vanilla RAGbased VQA methods that rely on unstructured documents or paragraphs often introduce irrelevant or even misleading information [38, 51], thereby compromising the accuracy and reliability of generated answers. Moreover, these approaches typically overlook the structural relationships among knowledge elements, limiting the reasoning capabilities of MLLMs. As illustrated in Figure 1 (b), the presence of noisy and unstructured context makes it difficult for MLLMs to identify and leverage relevant supporting evidence. To overcome these limitations, a promising direction is to retrieve structured knowledge, such as Knowledge Graphs (KGs) [23] for augmentation generation [22, 15, 59]. However, in the VQA setting, which inherently involves multimodal reasoning, relying solely on textual KGs is suboptimal, as both modalities are crucial for identifying relevant knowledge. Therefore, integrating Multimodal Knowledge Graphs into the retrieval-augmented VQA framework presents a more robust solution to generate reliable and precise responses in knowledge-intensive scenarios, as illustrated in Figure 1 (c).", "However, retrieving relevant knowledge from multimodal knowledge graphs to enhance the generation of knowledge-based VQA tasks is exceptionally challenging. First, offthe-shelf multimodal KGs [36] are generally built around common entities, and often lack the encyclopedic or long-tail knowledge required by knowledge-intensive questions, rendering them ineffective for direct use in knowledge-based VQA. Moreover, current knowledge sources used in knowledge-based VQA [40, 6] are typically organized in unstructured documents containing substantial contextual noise, making it challenging to extract well-structured entities and relationships essential for constructing high-quality multimodal KGs. Furthermore, a large-scale knowledge graph constructed from millions of documents, each potentially containing hundreds of entities and relationships, significantly expands the search space. Consequently, performing direct retrieval over such a graph is computationally inefficient and adversely affects retrieval precision.", "To address the challenges above, this paper proposes mKG-RAG , a novel\n\nFigure 1: Illustration of issues in knowledge-based VQA. (b) Vanilla RA methods suffer from retrieving unstructured knowledge from external documents via unimodal retrievers. (c) Our mKG-RAG augments MLLMs with structural information from multimodal knowledge graphs.\n\nVisual-only\nSearch\nI can't give\n\nthe information.\n\nWhen were the latest renovations of this\n\nstadium? [Answer: 2010] In 2012 Factual Error Lack of Knowledge (a) Knowledge-based VQA\n\n(b) Vanilla RAG-based VQA Framework\n\n(c) mKG-RAG: RAG with Multimodal Knowledge Graphs\n\nIn 2018 In 2010 Textual-only Multimodal Graph Traversal Ski Stadium Marcialonga\n\nRelation:\n\nthe stadium was\n\nrenovated in 2010 for the\n\nWorld Ski Championship.\n\nSkier Word Ski Championship Cross-country Ski Stadium\n\nThe Cross country and\n\nbiathlon center Fabio Can\u00e0l,\n\nuntil December 2018 named\n\nLago di T\u00e9sero Cross Coun-\n\ntry Stadium is a sport venue\n\nlocated in the village of ...\n\nUnstructured Documents\nStructured Knowledge Graphs\nInaccurate Retrieval", "retrieval-augmented generation framework integrated with multimodal knowledge graphs designed to enhance the reasoning capabilities of MLLMs in knowledge-based VQA tasks. More specifically, a multimodal knowledge graph construction module is introduced to transform unstructured multimodal documents, such as Wikipedia articles, into structured knowledge representations. This module leverages MLLM-powered keyword extraction and vision-text alignment to extract semantically consistent and modality-aligned entities and relationships from external multimodal documents. To enable efficient retrieval, mKG-RAG develops a dual-stage search paradigm that combines a coarse-grained document recall and a fine-grained entity/relationship retrieval. The coarse stage efficiently narrows the search space by identifying candidate documents likely to contain relevant evidence, while the fine stage refines the results by retrieving query-relevant entities and relationships from multimodal KGs that are dynamically constructed from these potentially noisy documents. During retrieval, unlike previous methods that rely on isolated unimodal retrievers, we introduce a question-aware multimodal retriever trained on a high-quality question-evidence dataset to further enhance retrieval precision within the proposed search paradigm. Comprehensive evaluations on two frequently used benchmarks demonstrate the superior performance of mKG-RAG, achieving an accuracy of 36.3% on E-VQA and 40.5% on InfoSeek.\n\nThe contributions of this work are summarized as follows:", "We propose mKG-RAG, a novel multimodal knowledge-augmented generation framework that integrates RAG with multimodal KGs to enhance the knowledge reasoning of MLLMs. To the best of our knowledge, this is the first work to investigate the potential of multimodal knowledge graphs in knowledge-intensive VQA tasks.", "Our framework develops a multimodal KG construction pipeline, allowing the extraction of image-text aligned entities and relations from multimodal documents. Additionally, a dual-stage retrieval schema with a question-aware multimodal retriever enables us to unleash the potential of RAG incorporated with multimodal KGs.", "Extensive experiments demonstrate that mKG-RAG significantly outperforms strong baselines, setting new state-of-the-art results on E-VQA and InfoSeek.", "2 RELATED WORK\n2.1 Vision-Language Models\n\nVision-language models (VLMs) have emerged as powerful tools for multimodal tasks, including deepfake detection. The rapid advancement of large language models (LLMs) such as GPT-4 [1] and LLaMA [66] has driven the evolution of VLMs, enabling seamless integration of vision and language modalities. Early approaches to VLMs, such as CLIP [52], utilized contrastive learning to align vision and language representations in a shared embedding space. Subsequent models like BLIP [37] introduced more advanced pretraining strategies, incorporating retrieval-augmented learning and cross-attention mechanisms to enhance contextual understanding.", "Recent advancements, such as LLaVA [40] and Qwen2-VL [68], have integrated ViT with LLMs, enabling vision-conditioned text generation and multimodal reasoning. Building on these foundations, models like Qwen2.5-Omni [77] introduce novel architectures and techniques to handle complex multimodal tasks, such as video dialogue and reasoning, while maintaining strong performance in visual and linguistic understanding.", "2.2 Deepfake Detection Methods\n\nImage deepfake detection has evolved significantly with advancements in AI technologies. Traditional methods [6, 9, 67], primarily based on Convolutional Neural Networks (CNNs), focus on identifying pixel-level anomalies such as unnatural textures and lighting inconsistencies. These Non-VLM-based approaches, exemplified by models like XceptionNet [10] and MesoNet [2], have shown high precision in specific scenarios but often struggle with generalization due to their reliance on dataset-specific features.", "More recently, the integration of VLMs has introduced a new paradigm in deepfake detection. VLM-based methods leverage the reasoning and cross-modal capabilities of these models to provide more flexible and interpretable detection. Models such as AntifakePrompt [7], ForgeryGPT [42], ForgeryTalker [38], ForgerySleuth [63], SIDA [27] and So-Fake-R1 [28] utilize the capabilities of VLM to enhance detection accuracy and provide robust support for localization and explanation tasks. In the video domain, BusterX [74] and BusterX++ [75] are outstanding representatives. Most of these methods rely heavily on extensive manual annotation of masks or textual explanatory descriptions. Additionally, the interpretable descriptions learned by current models tend to be coarse. In contrast,", "Figure 2: Framework of RAIDX: It integrates retrieval-augmented prompting with GRPO-enhanced reasoning for deepfake detection. A ViT and RAG module construct informative prompts using similar images, which are processed by a partially trainable LLM to output saliency-guided localization and fine-grained explanations without requiring any supervision.\n\nViT\n\nIs this image real?\n\nPrompt Engineering\n\nPlease carefully analyze this image\n\nand provide a detailed explanation.\n\nLLM\nLoRA(GRPO Training)\n<think>\n\nThe image depicts a cowboy on horseback chasing a steer through a dusty environment, likely during a rodeo or cattle drive scene. The lighting\n\nsuggests a sunset or sunrise, casting a warm glow over the scene. The dust kicked up by the animals adds to the realism of th e setting. \\n\\n###\n\nAnalysis:\\n1. **Lighting and Shadows**: While the warm directional lighting appears consistent, subtle inconsistencies are visible around the\n\nhorse's hooves and the steer's legs, where shadows and contact points seem unnaturally soft or floating,\n\nsuggesting an imprecise simulation of\n\nground interaction. \\n2. **Anatomical Irregularities**: The hat shows unnatural curvature and blending with the background light, especially\n\naround the edges. The face lacks realistic texture, and fine facial details.", "The horse's face appears slightly stylized, with a glossy finish and\n\ndisproportionately smooth edges around the nostrils and eyes. The hooves, do not deform the dust or ground as expected and cast shadows that\n\nappear artificially rendered\n\u2026\u2026\n\n\\n\\n### Conclusion:\n\nDespite its strong visual coherence, this image reveals several key synthetic artifacts\n\n-\n\nincluding smooth facial features, anatomically\n\nimplausible hooves and tails, and overly uniform dust patterns. These features strongly indicate that the image is AI-generated rather than a real\n\nphotographic capture </think> <answer>FAKE</answer> RAG Tokenizer RAIDX Trainable Frozen\n\nRAIDX relies on the GRPO reinforcement learning strategy, which can produce relatively accurate saliency maps and fine-grained textual explanation outputs without requiring any manual text or mask annotations.", "2.3 Retrieval-Augmented Generation\n\nRetrieval-Augmented Generation (RAG) [21] is a general paradigm which enhances LLMs by including relevant information retrieved from external databases into the input. RAG typically consists of three phases: indexing , retrieval , and generation . Indexing constructs external databases and their retrieval index from external data sources. Retrieval utilizes these indexes to fetch the relevant document chunks as context, given a user query. Generation integrates the retrieved context into the input prompt for LLMs, and LLMs then generate the final output based on the augmented inputs. RAG has been widely used in various domains [34, 61, 81]. Although RAG has achieved many successful cases in LLM tasks, there are few examples [45, 51] of its integration with VLMs. To the best of our knowledge, RAIDX is the first to apply RAG in the deepfake detection task.", "2.4 Reinforcement Learning in VLMs\n\nThe integration of reinforcement learning (RL) into multimodal systems has emerged as a promising direction, particularly for tasks requiring alignment between vision and language. Recent advances in techniques such as Proximal Policy Optimization (PPO) [57] and Group Relative Policy Optimization (GRPO) [15] have demonstrated their potential to enhance reasoning quality and factual correctness in language models, often by optimizing model outputs through structured feedback. GRPO, in particular, has gained significant attention due to its effectiveness in improving the reasoning capabilities of LLMs by leveraging rule-based rewards. In the visual domain, the application of RL to VLMs has shown substantial promise [8, 26, 60]. Experimental results from the VLM-R1 [60] demonstrate that RL-based models not only deliver competitive performance on visual understanding tasks but also surpass supervised fine-tuning (SFT) in generalization ability. To the best of our knowledge, RAIDX is the first to apply the GRPO reinforcement learning training strategy to the field of deepfake detection, which significantly reduces reliance on manual mask annotations and text annotations-forming the foundation of our annotation-efficient approach.", "Abstract\n\nRegulatory compliance question answering (QA) requires precise, verifiable information, and domainspecific expertise, posing challenges for Large Language Models (LLMs). In this work, we present a novel multi-agent framework that integrates a Knowledge Graph (KG) of Regulatory triplets with RetrievalAugmented Generation (RAG) to address these demands. First, agents build and maintain ontology-free KG by extracting subject-predicate-object (SPO) triplets from regulatory documents and systematically cleaning, normalizing, deduplicating, and updating them. Second, these triplets are embedded and stored along with their corresponding textual sections and metadata in a single enriched vector database, allowing for both graph-based reasoning and efficient information retrieval. Third, an orchestrated agent pipeline leverages triplet-level retrieval for question answering, ensuring high semantic alignment between user queries and the factual 'who-did-what-to-whom' core captured by the graph. Our hybrid system outperforms conventional methods in complex regulatory queries, ensuring factual correctness with embedded triplets, enabling traceability through a unified vector database, and enhancing understanding through subgraph visualization, providing a robust foundation for compliance-driven and broader auditfocused applications.", "Retrieval-Augmented Generation (RAG)\n\nRAGis a hybrid approach that integrates retrieval-based and generation-based techniques to improve the performance of language models on knowledge-intensive tasks. By leveraging external knowledge sources, RAG enables models to produce more accurate and contextually relevant responses. The paradigm has gained significant attention due to its ability to combine the strengths of retrieving pertinent documents and generating coherent text. In the real-world, RAG systems are widely adopted with online web search ( e.g., OpenAI (Hurst et al. 2024), and Gemini (Comanici et al. 2025)) and industrial domains with credential documents.", "Early research on RAG established its effectiveness across various natural language processing tasks (Lewis et al. 2020). Subsequent studies have proposed advancements, such as improved retrieval mechanisms using dense retrieval methods (Karpukhin et al. 2020) and the integration of structured knowledge bases like databases or graphs (Edge et al. 2024). RAG has also shown promise in multi-task and few-shot learning scenarios (Izacard et al. 2023), where the retrieval component compensates for limited training data by accessing external information. However, challenges remain, particularly in optimizing the retrieval process, which depends heavily on the quality of the input query-an issue that motivates the exploration of query rewriting (Ma et al. 2023).", "4 Method\n4.1 Datasets\n\nIn this study, we collected 3,683 AP spinal X-rays with corresponding radiological reports from 3,022 patients at X Hospital between December 2019 and July 2023, with all data collection protocols approved by the Institutional Review Board (IRB). The dataset was randomly partitioned into training, validation, and testing sets following an 8:1:1 ratio, resulting in 2,946 samples for training, 368 samples for validation, and 369 samples for testing. To address the inherent class imbalance common in medical datasets, we implemented a stratified sampling approach that maintained consistent distribution of scoliosis severity across all partitions, ensuring that the proportion of each scoliosis category (normal, mild, moderate, and severe) remained constant across all sets. To minimise input image variance, all X-rays were standardized by cropping to a uniform size of 896 \u00d7 448 pixels, ensuring that the entire spinal column was captured within each image.", "4.2 Visual Prompting Strategies\n\nWe established MLLMs zero-shot predictions as baselines and investigated how performance is affected by visual prompts provided by spine keypoint detection", "models. As shown in Figure 1, we designed three visual prompting strategies: Curved Spine Midline (CSM), Vertebral Connection Line (VCL), and Segmented Vertebrae Marks (SVM). These visual prompts were designed to provide models with critical information about spinal curvature and structure. For SDLD and SDDDtasks, we differentiated thoracic, thoracolumbar, and lumbar regions using distinct colors to provide anatomical localization information.", "4.3 Retrieval-Augmented Generation (RAG) Approach\n\nTo enhance model performance on knowledge-intensive tasks, we implemented a specialized RAG framework utilizing an AIS-specific knowledge database. This database was constructed by integrating authoritative sources including clinical practice guidelines, research publications from PubMed, and patient education resources from organizations such as the Scoliosis Research Society (SRS) [15, 24, 23, 2, 6]. To optimize information retrieval, we employed Gemini to generate structured knowledge graphs that capture key relationships between AIS concepts, treatments, and outcomes [25].", "4.4 DKA and PECA Data Population\n\nOur evaluation datasets were designed to assess different aspects of AIS understanding. The DKA dataset consists of multiple-choice questions targeting professional medical knowledge, while the PECA dataset simulates patient-centered scenarios requiring both clinical accuracy and appropriate communication. To ensure comprehensive coverage, we developed questions spanning diagnosis, treatment options, management strategies, and daily living accommodations for patients with varying degrees of spinal deformity. A rigorous quality control process involving two junior doctors and verification by a senior physician was implemented to ensure clinical relevance, accuracy, and appropriate difficulty levels across all questions.", "4.5 Evaluation\n\nFor the VSA task, we addressed dataset class imbalance by employing a comprehensive evaluation approach combining F1 score, AUC, and accuracy metrics. This multi-metric strategy ensures balanced assessment of model performance across both majority and minority classes. For the DKA task, we used accuracy as our primary performance metric, as it directly measures the models' proficiency in selecting correct answers within the multiple-choice format. In evaluating the PECA task, we implemented a structured human evaluation protocol wherein three junior physicians independently assessed model responses using a five-dimensional framework on a 5-point Likert scale. The detailed assessment criteria, outlined in Table 1, were specifically designed to evaluate both clinical accuracy and communication effectiveness, providing comprehensive insight into models' capabilities in addressing patient concerns across varying degrees of spinal deformity."]}
{"type": "entry", "index": 22, "question": "any papers that say rag doesn\u2019t help factuality?", "answer": "Within the supplied documents, none claim that retrieval-augmented generation (RAG) fails to help factuality overall or that it makes models less factual.\n\nInstead, they consistently report that RAG *improves* factual accuracy, while still having important limitations:\n\n- MusT-RAG explicitly states that adding music-specific retrieval \"effectively mitigates the factuality limitations\" of general LLMs and reports a 15% gain in factual performance over GPT\u20114o on music QA tasks [1].\n- MetaRAG describes RAG as improving factuality and domain relevance by grounding answers in external documents, and notes it is particularly effective against intrinsic hallucinations, though it remains vulnerable to extrinsic hallucinations when retrieved evidence is ignored or insufficient [2].\n- The RAG survey notes that tools like RAGAS are designed to measure and improve factual consistency with retrieved documents, and that better retrieval fidelity correlates strongly with factual answer quality in high-performing RAG systems [3].\n- DeepSieve, HeteroRAG, and the multi-stage verification framework all treat RAG as a core mechanism for reducing hallucinations and increasing factual accuracy in specialized domains (music, medicine, multi-modal tasks), showing empirical improvements over baselines without advanced retrieval or verification [4][5][6].\n\nSo, based on these documents, the literature here does not support the claim that RAG \"doesn\u2019t help\" factuality; rather, it portrays RAG as helpful but incomplete, still requiring better retrieval, routing, and verification to fully control hallucinations [2][3][4][5][6].", "tags": ["_untagged"], "full_faithfulness": 1.0, "full_answer_relevancy": 0.8784980169000001, "seed_faithfulness": 0.9411764706, "seed_answer_relevancy": 0.8784980169000001, "context_relevance": 0.2702515745640589, "context_relevance_error": null, "context_relevance_reason": "The context passages primarily discuss the benefits and mechanisms of Retrieval-Augmented Generation (RAG) in improving factual accuracy, but they do not directly address the user's request for papers that argue RAG does not help factuality. While some passages mention limitations of RAG, they do not provide specific papers or evidence that support the user's inquiry, leading to a low relevance score.", "structure_citation": 0.8927779378595277, "structure_error": null, "structure_reason": "The Actual Output effectively addresses the input question by stating that none of the provided documents claim RAG fails to help factuality. It cites multiple relevant snippets from the Context that support the claims made, with appropriate [n] markers corresponding to the citations. The response is consistent with the claims, highlighting improvements in factual accuracy and addressing limitations. However, it could have explicitly mentioned the absence of claims against RAG's effectiveness in factuality to strengthen the argument further.", "hallucination_score": 0.9755247140011246, "hallucination_error": null, "hallucination_reason": "The Actual Output effectively supports the claim made in the Input by providing a comprehensive analysis of multiple documents that consistently report RAG as improving factual accuracy. It directly addresses the question of whether any papers claim that RAG does not help factuality, confirming that none do. The response is well-grounded in the Context, referencing specific studies and their findings, which align with the evaluation steps of identifying claims, analyzing outputs, and verifying accuracy.", "contexts_used": ["7. CONCLUSION\n\nIn this paper, we presented MusT-RAG , a retrievalaugmented framework that enhances text-only Music Question Answering (MQA) by adapting general-purpose LLMs to the music domain. By retrieving relevant passages from a music-specific database and incorporating them into the generation context, MusT-RAG effectively mitigates the factuality limitations commonly observed in LLMs. As a result, our method achieves substantial improvements over GPT-4o [35], particularly in factual accuracy. Beyond simple retrieval, we further demonstrated that RAG-style fine-tuning outperforms traditional QA fine-tuning by improving both factual and contextual performance. Our final model achieves a 15.0% gain in factual performance over GPT-4o while maintaining comparable performance in contextual tasks. Importantly, MusTRAG shows strong generalization capabilities. On the outof-domain benchmark TrustMus [13], it delivers a 5.7% performance improvement over the zero-shot baseline, underscoring its robustness across diverse music-related QA scenarios. To facilitate future work in this underexplored domain, we release two key resources: MusWikiDB , a music-specific retrieval corpus, and ArtistMus , a benchmark focused on artist-level musical knowledge. We hope these contributions will drive further progress in developing accurate and domain-aware LLMs for music understanding and beyond.", "1 Introduction\n\nLarge Language Models (LLMs) such as GPT-4 and Llama-3 are transforming enterprise applications in healthcare, law, and customer service [8, 7, 2]. They power chatbots and virtual assistants that interact in natural language, offering unprecedented convenience and efficiency [30]. However, as these systems move into production, a persistent challenge emerges: hallucinations , i.e., responses that are fluent and convincing but factually incorrect or unsupported by evidence [23, 14].", "In high-stakes domains such as healthcare, law, and finance, hallucinations are not merely a nuisance but a critical barrier to reliable\n\n\u2217 Corresponding Author. Email: channdeth.sok@forvia.com", "adoption, raising concerns about user trust, regulatory compliance, and business risk [20]. Moreover, hallucinations are not uniformly risky: the same unsupported claim can differentially affect specific populations. In healthcare (e.g., pregnancy/trimester-specific contraindications), migration and asylum (e.g., protections for LGBTQ+ refugees), or labor rights (e.g., eligibility by status), ungrounded spans can cause disproportionate harm. Rather than treating users as homogeneous, hallucination detection methods should make such spans reviewable at the factoid level so downstream systems can apply identity-aware policies (e.g., stricter thresholds, forced citations, or escalation to a human) when the topic indicates elevated risk. This perspective connects hallucination detection to identity-aware deployment, where span-level evidence enables topic-conditioned safeguards that reduce disproportionate risk.\n\nJi et al. [14] categorize hallucinations into two types:\n\nIntrinsic hallucination : fabricated or contradictory information relative to the model's internal knowledge.", "Extrinsic hallucination : generated information that conflicts with, misrepresents, or disregards externally provided context or retrieved documents.", "Figure 1. Standard Retrieval-Augmented Generation (RAG) workflow. A user query is encoded into a vector representation using an embedding model and queried against a vector database constructed from a document corpus. The most relevant document chunks are retrieved and appended to the original query, which is then provided as input to a large language model (LLM) to generate the final response.\n\nCorpus Retrieved_Chunks Vectpr DB Embedding model Query Response LLM\n\nRetrieval-Augmented Generation (RAG) [17] aims to mitigate hallucinations by grounding model outputs in retrieved, up-to-date documents, as illustrated in Figure 1. By injecting retrieved text from re-\n\na", "liable external sources and proprietary documents, into the prompt, RAG improves factuality and domain relevance. While effective against intrinsic hallucinations, RAG remains susceptible to extrinsic hallucinations, especially when retrieved evidence is ignored, misinterpreted, or insufficient [10].", "Detecting hallucinations is particularly challenging in real-world settings, where RAG-based chatbots must respond to queries about unseen, proprietary, or confidential content where gold-standard references are typically unavailable [19]. Many existing hallucination detection methods rely on gold-standard reference answers [23, 16], annotated datasets [33], or access to model internals such as hidden states or token log-probabilities [25, 6]. However, in enterprise settings, such internals are often inaccessible: many state-of-the-art LLMs (e.g., GPT-4, Claude) are proprietary and only accessible via APIs that expose the final output text but not intermediate computations, limiting the feasibility of these methods in practice [19].", "To address these challenges, we introduce MetaRAG : a metamorphic testing framework for detecting hallucinations in RAG-based conversational agents. MetaRAG is a zero-resource, black-box setting that decomposes answers into atomic factoids, applies controlled mutations (e.g., synonym and antonym substitutions), and verifies each mutated factoid against the retrieved context. Synonyms are expected to be entailed , while antonyms are expected to be contradicted . Hallucinations are flagged when outputs violate these well-defined metamorphic relations (MRs). Unlike prior approaches, MetaRAG does not require ground-truth labels, annotated corpora, or access to model internals, making it suitable for deployment in proprietary settings.", "We evaluate MetaRAG on a proprietary corpus, thus unseen during model training. Our results show that MetaRAG reliably detects hallucinations, providing actionable insights for enhancing chatbot reliability and trustworthiness. These results establish MetaRAG as a practical tool for reliable deployment, and its span-level detection opens the door to identity-aware safeguards.", "6 Evaluation of RAG Systems\n\n6.1 Evaluation Dimensions and Metrics\n\nMean Average Precision (MAP) -Evaluates the quality of ranked retrieval across relevant items.", "6.2 Benchmarks and Datasets\n\nSeveral benchmarks are widely used to evaluate RAG systems:\n\nNatural Questions (NQ) , TriviaQA , and WebQuestions for open-domain QA [52].", "FEVER and AveriTeC for fact-checking, emphasizing verifiability.\n\nKILT benchmark suite integrates QA, dialog, slot filling, and entity linking over Wikipedia.", "BEIR evaluates retrieval across 31 zero-shot tasks in domains like biomedicine and finance.\n\nMTRAG targets multi-turn conversations requiring sequential retrieval and reasoning.", "TREC RAG Track (2024-) defines unified evaluation of retrieval, generation, and support quality over MS MARCO with metrics like nugget recall and citation coverage [71].", "6.3 Retrieval-Augmented Generation Assessment System\n\nRAGAS (Retrieval-Augmented Generation Assessment System) is an evaluation framework specifically designed for assessing and improving the factuality and grounding of RAG systems. Unlike conventional metrics that measure superficial linguistic overlap, RAGAS emphasizes the alignment between generated content and retrieved documents, providing explicit signals regarding factual correctness and attribution quality. By systematically measuring how well the generated outputs are supported by the retrieved evidence, RAGAS helps identify and penalize hallucinations-instances where the model generates plausible but unsupported statements. Consequently, employing RAGAS during model training or iterative fine-tuning guides RAG systems toward producing outputs firmly grounded in verifiable sources, substantially improving factual accuracy and reducing the incidence of hallucinated information.", "| | Category | Metric/Tool | Description |\n|---:|:-------------------|:---------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 0 | Retrieval Accuracy | Recall@ k MRR MAP | Proportion of queries with a relevant doc in top- k results Average inverse rank of the first relevant document Mean precision across all relevant retrieved documents |\n| 1 | Generation Quality | Exact Match (EM), F1 BLEU, ROUGE Faithfulness Rate | Measures overlap with ground-truth answers N-gram based overlap metrics for generation Consistency with retrieved context (manual/automated) |\n| 2 | Efficiency | Latency (retrieval/generation) Memory/Compute Usage | Time required for each processing step Resource demands for model operation |\n| 3 | Scalability | Index Size vs. Accuracy Adaptability | Impact of increasing corpus size on performance Ability to incorporate new data without retraining |\n| 4 | Benchmarks | Natural Questions, TriviaQA FEVER, AveriTeC KILT BEIR MTRAG TREC RAG Track | Open-domain QA evaluation Fact-checking datasets Multi-task benchmark over Wikipedia 31-task zero-shot retrieval benchmark Multi-turn reasoning benchmark Unified eval. of retrieval, generation, citation |\n| 5 | Tooling | RAGAS | Evaluates factual consistency and grounding |", "Table 1: Evaluation Dimensions, Metrics, Benchmarks, and Tools for RAG Systems", "6.4 Impact of Architecture on Evaluation\n\nEmpirical studies demonstrate that architectural components in RAG-chunking, embedding, and re-ranking-directly affect performance across tasks and benchmarks.", "Fixed-size chunks, as used in early RAG [52], are prone to semantic fragmentation. Semantic chunking (e.g., ChunkRAG ) improves retrieval precision and answer accuracy by aligning chunk boundaries with discourse structure [84]. The choice of embedding model (e.g., DPR [45], HyDE [19]) influences Recall@ k and downstream generation quality. Similarly, re-ranking strategies (e.g., MonoT5) can boost EM and F1 scores by prioritizing more relevant passages.", "In high-performing RAG systems, retrieval fidelity correlates strongly with factual answer quality. Thus, optimizing these architectural stages is essential for achieving competitive results on QA and generation benchmarks.", "1 Introduction\n\nLarge language models (LLMs) have achieved remarkable progress across a wide range of natural language tasks and have demonstrated strong reasoning abilities, including in domains such as mathematics (Guo et al., 2025), fake news detection (Yi et al., 2025; Jin et al., 2024b), and commonsense reasoning (Yao et al., 2023). Yet these LLMs often falter when facing knowledge-intensive questions that require up-to-date or domain-specific information (Guu et al., 2020; Asai et al., 2024;\n\n\u2217 Corresponding Author Email: weicheng@nec-labs.com\n\nFigure 1: Motivation and Overview: Left: Compositional queries are hard to answer under source heterogeneity (e.g., structured, private, and unmergeable databases). Right: DeepSieve performs decomposition, source-aware routing, and iterative fusion to enable structured reasoning.\n\nNEC Confidential\n\u00a9 NEC Laboratories\nAmerica 2025\n\nWhat country is the birthplace of\n\nErik Hort a part of?\n\nPersonnel Database General His birthplace is Montebello. But where is\n\nMontebello?\n\nI don't know\ninformation of\n\nErik Hort.\n\nMerged Expensive Privacy Issue Format confliction\n\nWhat is the [birthplace]\n\nof Eric Hort ?\n\nWhat country is the\n\n[birthplace] of?\n\nMontebello USA [birthplace] United States 1. Decomposition 2. Routing 3. Fusion & Observation 4. Reflexion No answer New York\n\n1. Knowledge from different sources\n\n2. Hard to merge sources\n\nMotivation:\n\nSource Heterogeneity\n\nMethodology:\n\nInformation Sieving", "Fan et al., 2024). This limitation stems from the fixed nature of LLM parameters, which prevents dynamic access to external knowledge and frequently results in hallucinations or factually inaccurate outputs (Huang et al., 2023). RetrievalAugmented Generation (RAG) has emerged as a powerful paradigm for equipping LLMs with access to external knowledge (Lewis et al., 2020; He et al., 2023; Su et al., 2025), enabling them to tackle complex information-seeking tasks more effectively. Recent advances such as GraphRAG, HippoRAG, and so on (Gutierrez et al., 2024; Edge et al., 2024; Zhang et al., 2025) demonstrate the promise of structured and memory-augmented retrieval in improving factual accuracy and multi-hop reasoning.", "However, existing RAG systems still struggle with a fundamental limitation: the lack of an effective information sieving mechanism across the reasoning pipeline. In practice, this deficiency manifests in two critical forms: query-side sieving and source-side sieving . On the query side, most systems treat user queries as atomic units and directly retrieve information without decomposing or analyzing their underlying semantic structure (Trivedi", "et al., 2023; Yao et al., 2023; Lin et al., 2025). This prevents the model from isolating key subgoals or reasoning steps, which is essential for multi-hop or compositional question answering.", "On the source side, knowledge can be heterogeneous both across different sources (e.g., unstructured corpora, structured APIs, private databases) and within a single source that contains multiple domains or topics. However, most existing RAG systems retrieve from a flat, unified index without considering differences in domain, format, access modality, or granularity (Chen et al., 2017; Izacard and Grave, 2021; Gao et al., 2023). This often results in mismatched content, irrelevant retrievals, and unnecessary computational cost. Moreover, many knowledge sources cannot be merged into a single retrieval index because of privacy constraints, structural incompatibility, or deployment considerations (Mavi et al., 2023; Xu et al., 2025), further highlighting the need for selective and modular retrieval strategies.", "To bridge this knowledge gap, we propose DeepSieve , a framework that performs retrievalaugmented reasoning through multi-stage information sieving. As shown in the Figure 1, DeepSieve consists of four key components: question decomposition , thought generation , data-source routing , and recursive reflexion . The pipeline begins by decomposing a complex input query into a set of structured sub-questions using an LLM-based planner (Lin et al., 2025; Trivedi et al., 2022). Each sub-question is then independently processed to generate a 'thought', a latent reasoning step that guides retrieval planning (Yao et al., 2023; Xu et al., 2023). Based on this thought, DeepSieve identifies the most appropriate source (e.g., API, SQL, RAG corpus) and generates a corresponding action plan. This source-aware routing builds on the idea of using LLMs as controllers for modular tool access (Schick et al., 2023; Xu et al., 2025).", "Once evidence is retrieved, the system evaluates whether the information sufficiently addresses the sub-question. If the answer is invalid, DeepSieve enters a reflection loop to reassess the thought, revise the action plan, or reselect the source, iterating until resolution or timeout (Madaan et al., 2023; Baek et al., 2025). Finally, after all sub-questions are addressed, DeepSieve aggregates the answers into a coherent output using LLM module, which also generates intermediate rationales (Wei et al., 2022; Creswell et al., 2023).", "This iterative, modular design allows DeepSieve to sieve and integrate knowledge progressively, making it particularly suited for multi-hop reasoning and heterogeneous information access. Empirically, DeepSieve consistently outperforms the baselines across all three benchmarks, achieving average F1/EM scores of 58.9/49.3 with DeepSeekV3 and 51.2/41.4 with GPT-4o, surpassing both RAG(e.g., HippoRAG (Gutierrez et al., 2024)) and agentic (e.g., ReAct (Yao et al., 2023)) baselines, while using significantly fewer tokens. To sum up, our paper makes the following key contributions:\n\nWe identify the structural and semantic heterogeneity of real-world knowledge sources as a core challenge in RAG. To address this, we propose the DeepSieve framework, which introduces information sieving and, for the first time, uses an LLM-as-a-knowledge-router to dynamically decompose queries and dispatch sub-questions to heterogeneous sources.", "Our experiments demonstrate that DeepSieve not only satisfies multi-source settings but also improves performance in standard singlesource scenarios. Even when operating over a unified corpus, our system yields better retrieval precision and answer accuracy.", "We design DeepSieve as a modular and extensible method. It supports plug-and-play integration with diverse tools, retrieval backends, and RAG models. It provides a flexible backbone for future RAG architectures.", "Abstract\n\nMedical large vision-language Models (Med-LVLMs) have shown promise in clinical applications but suffer from factual inaccuracies and unreliable outputs, posing risks in realworld diagnostics. While retrieval-augmented generation has emerged as a potential solution, current medical multimodal RAG systems are unable to perform effective retrieval across heterogeneous sources. The irrelevance of retrieved reports affects the factuality of analysis, while insufficient knowledge affects the credibility of clinical decision-making. To bridge the gap, we construct MedAtlas, which includes extensive multimodal report repositories and diverse text corpora. Based on it, we present HeteroRAG, a novel framework that enhances Med-LVLMs through heterogeneous knowledge sources. The framework introduces Modality-specific CLIPs for effective report retrieval and a Multi-corpora Query Generator for dynamically constructing queries for diverse corpora. Incorporating knowledge from such multifaceted sources, Med-LVLM is then trained with Heterogeneous Knowledge Preference Tuning to achieve cross-modality and multi-source knowledge alignment. Extensive experiments across 12 datasets and 3 modalities demonstrate that the proposed HeteroRAG achieves state-of-the-art performance in most medical vision language benchmarks, significantly improving factual accuracy and reliability of Med-LVLMs.", "Abstract\n\nThis paper presents the technical solution developed by team CRUISE for the KDD Cup 2025 Meta Comprehensive RAG Benchmark for Multi-modal, Multi-turn (CRAG-MM) challenge. The challenge aims to address a critical limitation of modern Vision Language Models (VLMs): their propensity to hallucinate, especially when faced with egocentric imagery, long-tail entities, and complex, multihop questions. This issue is particularly problematic in real-world applications where users pose fact-seeking queries that demand high factual accuracy across diverse modalities. To tackle this, we propose a robust, multi-stage framework that prioritizes factual accuracy and truthfulness over completeness. Our solution integrates a lightweight query router for efficiency, a query-aware retrieval and summarization pipeline, a dual-pathways generation and a posthoc verification. This conservative strategy is designed to minimize hallucinations, which incur a severe penalty in the competition's scoring metric. Our approach achieved 3rd place in Task 1, demonstrating the effectiveness of prioritizing answer reliability in complex multi-modal RAG systems. Our implementation is available at https://github.com/Breezelled/KDD-Cup-2025-Meta-CRAG-MM."]}
{"type": "entry", "index": 23, "question": "what are examples of rag working really well in production?", "answer": "Several real-world deployments show RAG working well in production:\n\n\u2022 PGA Tour: Uses RAG over its proprietary golf event and player statistics so users can ask detailed questions and get accurate, domain-specific answers. This fixes the problem of generic models not knowing niche sports data and has improved user engagement and satisfaction.[1]\n\n\u2022 Bayer (agriculture): Applies RAG to its internal agricultural datasets so farmers can retrieve precise, timely guidance. This has made critical information easier to access and supports better decisions in crop management and productivity.[1]\n\n\u2022 Rocket Companies (mortgages): Integrates RAG with proprietary financial data to quickly pull the right documents and facts for mortgage processing. This speeds up processing times and improves customer experience by making responses both faster and more accurate.[1]\n\n\u2022 Shorenstein Properties (real estate): Uses RAG to automatically tag and organize large volumes of internal files. This has improved how staff find and manage documents, leading to higher operational efficiency.[1]\n\n\u2022 Cohere: Builds RAG systems that always cite their sources (e.g., company documents, news sites). This reduces hallucinations and makes it easier for humans to verify answers, improving trust and reliability.[1]\n\n\u2022 NVIDIA and IBM: Both connect RAG to proprietary customer data and authoritative research so their enterprise AI can answer highly specific, up-to-date questions while reducing hallucinations and boosting productivity across customer service and data-heavy workflows.[1]\n\nTogether, these show RAG working well when organizations have rich internal data and need accurate, current, explainable answers embedded in real products and workflows.[1]", "tags": ["_untagged"], "full_faithfulness": 1.0, "full_answer_relevancy": 0.9142246752000001, "seed_faithfulness": 0.16, "seed_answer_relevancy": 0.9142246752000001, "context_relevance": 0.9989013055153133, "context_relevance_error": null, "context_relevance_reason": "The context passages provide multiple relevant examples of RAG implementations in production across various industries, directly addressing the user's question about successful applications. Each example, such as the PGA Tour, Bayer, and Rocket Companies, illustrates how RAG enhances information retrieval and operational efficiency, aligning perfectly with the inquiry.", "structure_citation": 0.9715947641059746, "structure_error": null, "structure_reason": "The Actual Output effectively includes claims with [1] markers that correspond to specific snippets in the Context. Each claim made about the organizations using RAG is supported by relevant details from the provided Context, demonstrating consistency and accuracy. There are no missing citations or unmatched markers, and all claims are well-supported by the examples given, showcasing the successful application of RAG in various industries.", "hallucination_score": 0.991490095035233, "hallucination_error": null, "hallucination_reason": "The Actual Output provides accurate and relevant examples of RAG implementations in production, directly aligning with the Context provided. Each example, such as the PGA Tour and Bayer, is supported by specific details that reflect the claims made in the Context, demonstrating how RAG enhances information retrieval and decision-making. No unsupported claims were identified, and the response effectively illustrates the practical applications of RAG across various industries.", "contexts_used": ["5 RAG for Proprietary Data -Industry Implementation\n\n5.1 Current Approaches\n\nOrganizations increasingly apply RAG to private internal knowledge , using a secure pipeline to retrieve from proprietary documents and feed them to a generative model. This often involves on-premise or VPC-hosted vector databases, ensuring that queries and document embeddings never leave the corporate firewall [60, 33]. Enterprises store their text in an index, and at inference time, a local embedding model transforms a user query into a dense vector to find topk relevant chunks. These chunks are appended to the user query as context for an LLM (e.g., GPT-4, or a self-hosted model). The separation of knowledge in a database reduces the risk that proprietary data leaks into the model's parameters, but it does not fully guarantee privacy-the generator might still reveal sensitive details in the output. To mitigate these risks, some enterprise solutions incorporate access control layers that filter out documents the user lacks permission to see. Others experiment with secure enclaves or homomorphic encryption to blind the retrieval operation [105]. However, performance overhead is non-trivial. Another strategy is to fine-tune an LLM on domain data. Although fine-tuning helps the model internalize domain nuances, it can potentially memorize private text. RAG better suits data freshness : the knowledge store can be updated without retraining.", "5.2 Industry Implementation Examples\n\nIn recent years, several organizations have explored the deployment of Retrieval-Augmented Generation (RAG) systems to leverage proprietary data effectively. Notable case studies include:\n\nPGA Tour's Use of RAG for Enhanced Information Retrieval The PGA Tour implemented a RAG system to improve information retrieval related to golf events and player statistics. By integrating their extensive proprietary data into the RAG framework, they enabled more accurate and contextually relevant responses to user queries. This approach addressed previous limitations where general AI models lacked specific domain knowledge, leading to inaccuracies in responses. The RAG system allowed the PGA Tour to provide precise information, enhancing user engagement and satisfaction [11].", "Bayer's Application of RAG in Agricultural Data Management Bayer utilized RAG to manage and retrieve proprietary agricultural data, aiming to provide farmers with accurate and timely information. By incorporating their extensive datasets into the RAG system, Bayer enhanced the accessibility and usability of critical agricultural information. This integration facilitated better decision-making processes for farmers, leading to improved crop management and productivity [11].", "Rocket Companies' Integration of RAG for Mortgage Processing Rocket Companies explored the use of RAG to streamline mortgage processing by integrating proprietary financial data. The RAG system enabled more efficient retrieval of relevant information, reducing processing times and improving customer experiences. By leveraging their internal data within the RAG framework, Rocket Companies enhanced the accuracy and speed of their services, demonstrating the potential of RAG in financial applications [11].", "Shorenstein Properties' Implementation of RAG for Data Organization Shorenstein Properties adopted RAG to automate file tagging and organize proprietary data more efficiently. By integrating their internal documents into the RAG system, they improved data accessibility and management. This implementation showcased RAG's capability to handle complex data organization tasks, leading to increased operational efficiency within the company [11].", "Cohere's Advancement of RAG for Source Citation Cohere advanced RAG technology to ensure AI systems cite their sources, facilitating human verification of the information produced. By integrating external texts such as company documents or news websites, Cohere's RAG system reduced errors known as hallucinations and provided access to current information. This development highlighted RAG's potential in enhancing the reliability and transparency of AI-generated content [50].", "NVIDIA's Deployment of RAG in Enterprise AI Solutions NVIDIA incorporated RAG into their enterprise AI solutions to connect large language models with specific information, such as proprietary customer data and authoritative research. This integration enabled more accurate and relevant responses to user queries, enhancing productivity and protecting against AI hallucinations. NVIDIA's deployment demonstrated RAG's applicability in various industries, including customer service and data management [65].", "IBM's Utilization of RAG for Domain-Specific AI Applications IBM employed RAG to equip models with specific information, such as proprietary customer data and authoritative research documents. This approach allowed their AI systems to incorporate up-to-date information into generated responses, improving accuracy and relevance in domain-specific applications. IBM's utilization of RAG showcased its effectiveness in enhancing AI capabilities across different sectors [32].", "These case studies illustrate the diverse applications of RAG in leveraging proprietary data across various industries. By integrating internal datasets into RAG systems, organizations have enhanced information retrieval, improved decision-making processes, and increased operational efficiency. However, challenges such as data privacy, intellectual property concerns, and computational overhead remain areas for further research and development.", "5.3 Emergent Trends and Patterns in Industry Implementation\n\nAccuracy vs. Latency Trade-offs: Models optimized for retrieval accuracy (e.g., FiD, WebGPT) typically have higher latency due to multiple document processing. Industry applications prioritize real-time performance (e.g., NVIDIA RAG aims for sub-second latency).", "Scaling Beyond Wikipedia: Early RAG models focused on Wikipedia-scale retrieval, while industry applications integrate proprietary databases and real-time web data. WebGPT and IBM Watsonx RAG retrieve from dynamic knowledge bases, offering more up-to-date responses.", "Reducing Hallucination: There is a shift toward factual consistency. WebGPT and IBM Watsonx RAG enforce citation mechanisms to enhance factual accuracy.", "Model Size vs. Retrieval Efficiency: Smaller models augmented with retrieval (e.g., Atlas, NVIDIA RAG) can outperform much larger models without retrieval (e.g., GPT-3 175B), demonstrating that knowledge retrieval reduces the need for extreme model scaling.", "Enterprise Adaptation: Academic models optimize for benchmark performance, whereas enterprise RAG systems prioritize real-world reliability, integration with databases, and data privacy compliance."]}
{"type": "entry", "index": 24, "question": "rag latency problems and how to optimize it", "answer": "Retrieval-augmented generation (RAG) is slower than a plain language model because it has extra steps: looking up documents in a large index, running similarity search over vectors, and then making the model read and reason over a much longer input (your query plus many retrieved chunks). All of these add computation and therefore delay. [1]\n\nMain latency sources in RAG:\n1) Vector search over big indexes (millions or more chunks).\n2) Network and system overhead when talking to separate retrieval services.\n3) Longer prompts: more tokens for the model to process, which grows cost roughly with the square of the input length in Transformers. [1][3]\n\nWays to reduce latency:\n- Faster search structures: use efficient approximate nearest-neighbor indices such as HNSW or FAISS-like structures so you don\u2019t have to scan everything. [1]\n- Caching: reuse results for repeated or similar queries, including caching embeddings and retrieved documents (multi-level or approximate embedding caches such as RAGCache or proximity-based caches). This avoids recomputing retrieval on every request. [1]\n- Lightweight or distilled retrievers: smaller, simpler retrieval models can answer faster, though possibly with slightly lower accuracy. [1]\n- Adaptive retrieval: make easy queries use cheaper, smaller retrieval (fewer documents, cheaper index), and reserve heavier retrieval for hard queries. This balances total throughput. [1]\n- Context management: techniques like BEE-RAG are designed to keep long contexts efficient by controlling attention behavior so that adding more documents doesn\u2019t blow up computation as much, while still focusing on the important parts. This maintains performance with long inputs without excessive extra cost. [3]\n\nIn practice, an optimized RAG system mixes good indexing, aggressive caching, adaptive retrieval depth, and careful control of context length to keep latency acceptable. [1][3]", "tags": ["scaling_cost"], "full_faithfulness": 1.0, "full_answer_relevancy": 0.7728520811, "seed_faithfulness": 0.9333333333, "seed_answer_relevancy": 0.7728520811, "context_relevance": 0.8660954171218966, "context_relevance_error": null, "context_relevance_reason": "The context passages provide substantial information related to the user's question about RAG latency problems and optimization strategies. Specifically, sections discussing latency and efficiency directly address the user's concerns, detailing how RAG increases computational complexity and suggesting techniques to reduce latency. However, while the passages are relevant, they could be more focused on specific optimization strategies rather than general challenges, which slightly limits the overall alignment with the user's query.", "structure_citation": 0.7840634669859524, "structure_error": null, "structure_reason": "The Actual Output effectively discusses RAG latency issues and optimization strategies, with multiple claims supported by citations [1] and [3]. However, while the citations correspond to relevant snippets in the Context, there are instances where the claims could be more explicitly tied to specific details from the Context, particularly regarding the integration of retrieval with LLMs and the challenges of maintaining retrieval quality. This slight lack of clarity in linking all claims to the Context leads to a minor deduction in the score.", "hallucination_score": 0.9141020004281716, "hallucination_error": null, "hallucination_reason": "The response effectively identifies and explains the latency issues associated with RAG systems, aligning well with the input's focus on latency problems and optimization strategies. It provides a comprehensive overview of the main sources of latency and practical solutions, which are supported by the context provided. The only minor shortcoming is a lack of explicit mention of how these optimizations directly relate to the challenges outlined in the context, but overall, the output is highly relevant and informative.", "contexts_used": ["7 Challenges of RAG\n\nThis section discusses the challenges of RAG, cases of manifestation of such challenges in the selected domain of RAG application, and outlines existing solutions and the way forward.", "7.1.1 Retrieval Quality and Relevance\n\nThe quality of retrieved documents significantly impacts the accuracy of RAG-generated answers. High recall and precision are critical since poor retrieval leads directly to incorrect or irrelevant answers [61]. Traditional methods like BM25 are limited, often missing relevant texts or returning noisy results [12]. Modern neural retrievers with dense embeddings improve performance but still face issues like vocabulary mismatches, ambiguous queries, and domain-specific terminology. Specialized domain tuning, such as using legal embeddings or medical synonym expansion, can help, but maintenance of these tailored retrievers remains challenging. Determining the optimal number of retrieved passages ( k ) is also complex. Too few passages limit evidence; too many overwhelm the model and introduce irrelevant context. Approaches like ranking retrieved passages or iterative query reformulation can improve retrieval precision, but add complexity and latency [34, 21].", "7.1.2 Latency and Efficiency\n\nRAG inherently increases computational complexity and latency compared to standalone LLMs due to retrieval overhead, vector searches, and expanded context processing. Techniques like approximate nearest neighbor indices (e.g., FAISS, HNSW), caching, model distillation, or lightweight retrievers can reduce latency at the expense of accuracy. Integrating retrieval efficiently with large language models (LLMs) and ensuring rapid responses in real-time scenarios (e.g., customer support) remains a significant challenge [3]. Interestingly, using retrieval can allow smaller models to match the performance of larger models without retrieval (e.g., RETRO, Atlas), reducing model size requirements but shifting complexity to maintaining external knowledge bases and infrastructure.", "7.1.3 Integration with Large Language Models\n\nIntegrating retrieved evidence effectively with LLMs is subtle. Models may ignore retrieved evidence, especially when internal model knowledge conflicts with external retrieved information, leading to a \"tug-of-war\" effect [41]. Multiple retrieved documents might create confusion or confirmation bias if they contradict each other. Limited input lengths in transformer-based LLMs exacerbate these integration challenges by forcing truncation or summarization, potentially omitting essential context. Fine-tuning models specifically for retrieval-augmented tasks often yields better integration than simple zero-shot prompting but introduces complexity, especially when using non-differentiable or API-based models that do not support custom training.", "7.2.1 Scalability and Infrastructure\n\nDeploying RAG at scale requires substantial engineering to maintain large knowledge corpora and efficient retrieval indices. Systems must handle millions or billions of documents, demanding significant computational resources, efficient indexing, distributed computing infrastructure, and cost management strategies [21]. Efficient indexing methods, caching, and multi-tier retrieval approaches (such as cascaded retrieval) become essential at scale, especially in large deployments like web search engines.", "7.2.2 Freshness and Knowledge Updates\n\nOne motivation for RAG is providing current information. However, continuously updating external knowledge bases and retrieval indices is challenging. Domains requiring real-time updates (e.g., finance, healthcare) demand sophisticated data pipelines for incremental updates, possibly frequent re-encoding of documents, and synchronization of retrieval indices. Delays in updates or inconsistencies between the LLM's internal knowledge and newly retrieved data can produce outdated or contradictory answers [61].", "7.2.3 Hallucination and Reliability\n\nWhile RAG reduces LLM hallucinations, it does not eliminate them completely. Models may fabricate or misattribute information if retrieval provides incomplete or partially contradictory context.", "Legal domain studies found that RAG significantly reduces hallucinations, but still generates errors at concerning rates [56]. Hallucinations also occur in citation generation, with models occasionally inventing nonexistent references. Strategies such as verifying outputs against retrieved sources or calibrating model confidence are needed, but no approach completely prevents hallucination.", "HIPAA), transparent data usage policies, and security testing are essential to protect privacy and prevent breaches.", "7.3.4 Accountability and Transparency\n\nRAG's use of sourced retrieval provides an", "7.2.4 Complex Pipeline and Maintenance\n\nadvantage for accountability, allowing users to trace AI-generated responses back to evidence. However, inaccurate citations or improper synthesis can mislead users. Ethical deployment involves clearly attributing evidence, providing explanations on request, managing user expectations, and clearly delineating accountability-especially when RAG informs critical decisions. Transparency and accountability require ongoing evaluation, oversight, and mechanisms for user feedback and correction [56].", "7.4.3 Customer Support and Knowledge Bases\n\nUser Experience and Tone : Customer support demands empathetic, conversational responses that match user expectations. Ethical transparency in disclosing the AI nature of interactions is essential.", "Escalation and Handling Failures : Systems must effectively identify limits of their capabilities, promptly escalating unresolved issues to human support to maintain customer satisfaction.", "Metrics for Evaluation : Success in customer support is evaluated through resolution rates and customer satisfaction metrics, requiring responses that not only provide factual accuracy but practical utility.", "Ethical considerations include transparency, data security, job impact, and fairness. RAG\n\nimplementations also provide valuable feedback for continual improvement of internal support documentation based on real-time user interactions, thereby enhancing both system performance and documentation quality.", "7.5.1 Retrieval Quality\n\n: Maintaining high retrieval relevance is critical for effective RAG. Strategies to improve retrieval quality include domain-adaptive training, advanced encoders, and query reformulation methods to address vocabulary mismatches [85]. Employing reranking models further boosts relevance by re-scoring initial retrieval results with deeper contextual analysis, enhancing accuracy at the expense of additional computation [4]. Iterative retrieval and chain-of-thought reasoning represent future directions, breaking down complex queries into simpler sub-queries, thus ensuring relevant information retrieval at each reasoning step [90].", "7.5.2 Latency\n\nRAG systems introduce latency due to retrieval processes. Solutions include using efficient nearest-neighbor search structures, such as HNSW graphs, which significantly speed up similarity searches [57]. Caching mechanisms, including multi-level and approximate embedding caches (e.g., RAGCache and Proximity cache), enable reuse of previously retrieved information, drastically reducing retrieval time [40, 8]. Adaptive retrieval methods dynamically balance retrieval complexity based on query difficulty, optimizing overall throughput and reducing latency.", "7.5.3 Model Integration\n\nEffective integration between retrieval and generation models remains essential. Methods include joint end-to-end training of retrievers and generators, enhancing mutual compatibility and performance [52]. Architectural integration techniques, such as RETRO's cross-attention mechanism, dynamically incorporate retrieved facts during generation [10]. Alternatively, prompt-based integration treats LLMs as black-boxes, conditioning on retrieved documents without architectural modifications. Future hybrid approaches involving reinforcement learning and selective retrieval aim to optimize when and how", "external knowledge is incorporated into generation processes.", "7.5.4 Hallucination\n\nReducing factual hallucinations remains a key focus. RAG inherently mitigates hallucinations by grounding outputs in retrieved evidence [82]. Training models to penalize ungrounded assertions and iterative retrieval within reasoning processes further enhance accuracy [90]. Self-check mechanisms (Self-RAG), where models critique and revise their outputs against retrieval results, significantly reduce hallucinated content [6]. External verification and fact-checking modules complement internal methods, collectively ensuring high factual reliability. For instance, RAG systems to cite sources significantly enhance their reliability by directly linking generated information to supporting evidence. This citation capability plays a crucial role in mitigating the common issue of hallucination, where generative models produce plausible yet inaccurate or fabricated information. By explicitly associating each factual statement with retrieved documents, RAG systems encourage transparency and verifiability, enabling users and downstream processes to quickly assess the accuracy and provenance of claims. Moreover, requiring the model to cite sources during generation inherently promotes grounding outputs in verified data, further reducing the risk of generating unsupported statements [82]. Thus, citation functionality not only enhances user trust but also fosters more disciplined, factually accurate generation, substantially decreasing the likelihood of hallucinated outputs.", "7.5.5 Scalability\n\nScalability challenges arise as knowledge corpora expand. Advanced indexing, distributed retrieval, and approximate nearest neighbor techniques facilitate efficient handling of large-scale knowledge bases [57]. Selective indexing and corpus curation, combined with infrastructure improvements like caching and parallel retrieval, allow RAG systems to scale to massive knowledge repositories. Research indicates that moderate-sized models augmented with large external corpora can outperform significantly larger standalone models, suggesting parameter efficiency advantages [10].", "7.5.6 Knowledge Freshness\n\nRapidly evolving information necessitates regularly updated knowledge bases. RAG systems can efficiently maintain knowledge freshness through incremental updates and selective retrieval methods without requiring frequent retraining [30]. Integrating live search APIs and hybrid retrieval methods ensure real-time information retrieval, addressing dynamic knowledge demands [21]. Continuous updates and user-feedback integration support lifelong learning and timely information access.", "RAG System Implementation\n\nIn this experiment, we implemented two distinct RetrievalAugmented Generation (RAG) systems. The first is a multimodal RAG chatbot that generates responses using multimodal embeddings (derived from image-based documents)\n\ncombined with user queries. The second is a text-based RAG chatbot that utilizes parsed text chunks.", "For the multi-modal RAG system, we employed ColQwen2.5-3B (Faysse et al. 2024) for both document parsing and retrieval. We set the MAX_TOKEN limit for image encoding to 764, aligning it with the token size used for text encoding. To optimize multi-modal RAG performance, we pre-computed embeddings for image-based documents and performed retrieval without late interaction.", "For the text-based RAG system, we utilized an inhouse document parser, AI Parser , to convert documents (e.g., PDFs) into text chunks. We adopted three in-house retrievers: ixi-RAG lexical , ixi-RAG semantic , and ixi-RAG hybrid .", "ixi-RAG lexical is a traditional information retrieval system built upon an OpenSearch index and utilizing the BM25 scoring algorithm. This retriever operates by matching the exact tokens present in the user's query against the text chunks in the knowledge base. It excels at precision when the query contains specific terms, acronyms, or identifiers that are also present in the source documents, as its ranking is based on term frequency (TF) and inverse document frequency (IDF).", "ixi-RAG semantic is a modern retrieval system that operates on the principle of conceptual similarity rather than keyword matching. To this end, we utilized ixi-DocEmbedding , an embedding model specialized for Korean documents. The model was further trained on Korean query-document pairs using a domain-specific finetuning approach based on BGE-M3 (Chen et al. 2024). For training the retriever, we constructed a comprehensive corpus of Korean query-document pairs drawn from three sources: (1) carefully-curated open-source datasets, (2) high-quality synthetic pairs generated with large language models, and (3) production-grade interaction logs collected from deployed RAG systems. To enhance the learning signal, we employed iterative hard-negative mining and trained the encoder according to the BGE-M3 fine-tuning recipe, which couples InfoNCE contrastive objectives with selfdistillation and model-merging strategies. After training, we only use the dense retrieval as the semantic retriever. Empirically, the resulting model demonstrates performance on par with state-of-the-art open-source Korean encoders and achieves substantial improvements on internal RAG evaluation benchmarks. It converts both the user's query and the document chunks into high-dimensional vector embeddings. Retrieval is then performed by conducting a k-nearest neighbor (k-NN) (Fix 1985) search within this vector space, identifying documents whose embeddings are closest to the query's embedding. This approach allows the system to understand the user's intent and retrieve relevant information even if the phrasing is different and there is no direct keyword overlap.", "ixi-RAG hybrid combines both lexical and semantic approaches for improved retrieval. It is designed to leverage the complementary strengths of both retrieval paradigms. It first gathers a broad set of candidate documents by running lexical and semantic searches in parallel. The ranked lists from both retrievers are then fused using the Recip-", "| | Query Rewriter | Document Recognizer | Document Retriever | Train Data |\n|---:|:------------------|:----------------------|:---------------------|:---------------|\n| 0 | RL-QR multi-modal | ColQwen2.5-v0.2 | ColQwen2.5-v0.2 | D mm D tm D tm |\n| 1 | RL-QR lexical | AI Parser | ixi-RAG lexical | |\n| 2 | RL-QR semantic | AI Parser | ixi-RAG semantic | |\n| 3 | RL-QR hybrid | AI Parser | ixi-RAG hybrid | D tm |\n\nTable 2: RL-QR Training Scheme\n\nrocal Rank Fusion (RRF) algorithm (Cormack, Clarke, and Buettcher 2009). RRF calculates a new score for each document based on its position in the individual rankings, producing a single, more robustly ordered list that balances keyword relevance with semantic similarity. This method effectively captures both the precision of lexical search and the contextual understanding of semantic search to deliver a highly refined final ranking.", "Introduction\n\nRetrieval-augmented generation (RAG) has emerged as a transformative paradigm for augmenting large language models (LLMs) with dynamically integrated external knowledge (Gao et al. 2023; Lewis et al. 2020). While conventional RAG systems demonstrate remarkable capabilities in knowledge-intensive tasks, their performance exhibits critical fragility when processing extended contextual inputs (Ren et al. 2023). This vulnerability arises from the inherent requirement of RAG frameworks to incorporate external retrieved documents, which typically results in substantially longer input sequences compared to standard generation tasks (Zhang et al. 2024a). This limitation manifests acutely in scenarios demanding long-context comprehension, such as scientific literature analysis, multi-hop reasoning, and cross-document synthesis, where the growing complexity of modern knowledge systems necessitates robust processing of extended text spans (Liu et al. 2023).\n\n* Equal contributions.\n\n\u2020 The work was done during the internship at Baidu.\n\n\u2021 Corresponding authors.", "Figure 1: In vanilla RAG, increasing input context length raises the information entropy of attention scores (left), potentially harming performance, while the LLM is less focused on important segments (right). This paper proposes a balanced entropy engineering strategy, which maintains entropy stability for longer contexts and guide the LLM to focus on critical segments.\n\n\u0000 \u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 H \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000", "Typically, existing efforts to mitigate this challenge focus predominantly on heuristic document filtering, extrapolative training, or architectural modifications, introducing worth noting trade-offs. Threshold-based retrieval document truncation or selection reduces context length at the cost of discarding potentially relevant information (Jeong et al. 2024; Wang et al. 2024). While extended context training may enhance RAG performance (Lin et al. 2024; Zhang et al. 2024a), it substantially increases computational demands and risks compromising model generalization. The introduction of auxiliary modules, such as specialized encoders (Yen, Gao, and Chen 2024), further escalates the complexity of the system.", "As shown in Figure 1, we identify a fundamental oversight in existing explorations: the failure to address the core tension between context entropy growth and attention allocation dynamics. As retrieved document length increases, unconstrained context entropy expansion progressively dilutes attention distributions, undermining LLMs' capacity to prioritize semantically critical content. Such a phenomenon substantially degrades RAG's ability to extract and utilize salient information.", "In this work, we fundamentally rethink RAG's context length adaptability through the perspective of entropy engineering, and propose Balanced Entropy-Engineered RAG ( BEE-RAG ), a principled framework that establishes entropy-invariant dynamics to decouple attention sharpness from context length. First, we introduce balanced context entropy, a novel attention reformulation incorporating document-specific balancing entropy factor \u03b2 to enforce entropy invariance across variable context lengths. Theoretical analysis demonstrates this mechanism maintains entropy levels within a stable regime for various context scales, contrasting with conventional entropy scaling. Building on this foundation, we develop intrinsic multi-importance inference, a zero-shot strategy that derives balancing entropy factors through context aggregation while preserving inter-document independence via parallel context scoring and generation. To ensure broad applicability, we further propose adaptive balancing factor learning, which is a parameter-efficient tuning method that learns task-specific balancing factor through lightweight linear projections for domain adaptation with minimal parameter updates (0.014% of total parameters).", "Empirical analyses across multiple real-world benchmarks reveal that BEE-RAG fundamentally alters the entropy scaling laws governing conventional RAG systems, which provides a robust and scalable solution for RAG systems dealing with long-context scenarios. Our main contributions are summarized as follows:\n\nWe introduce the concept of balanced context entropy, a novel attention reformulation that ensures entropy invariance across varying context lengths, and allocates attention to important segments. It addresses the critical challenge of context expansion in RAG.", "We propose a zero-shot strategy called intrinsic multiimportance inference and parametric-efficient finetuning strategy adaptive balancing entropy factor learning to obtain the balancing entropy factor in different scenarios while maintaining efficiency.", "Extensive empirical validation across multiple benchmarks demonstrates the effectiveness and efficiency of BEE-RAG.", "Discussion\n\nNovelty and Effectiveness. Our key innovation lies in introducing an additive balancing entropy factor into the attention mechanism, achieving two critical objectives: (1) maintaining stable context entropy across varying input lengths through mean and variance constraints, thereby enhancing performance in long-context scenarios; and (2) adaptively guiding the LLM to focus on crucial segments, improving its ability to effectively utilize knowledge. Unlike existing attention reweighting approaches that either uniformly reduce context entropy (Su 2021) (hindering important information extraction) or ignore attention entropy (Yan et al. 2024) (leading to unstable performance across varying context lengths). We further introduce novel adaptations for zero-shot and parameter-efficient fine-tuning scenarios: a prompt-induced importance scoring mechanism with parallel scoring-generation architecture, and an efficient vector fine-tuning strategy. These components collectively enhance BEE-RAG's accuracy in question-answering tasks\n\nEfficiency. We further analyze the efficiency of our EE-RAG framework. The parallel mechanism handles \u03b2 calculation and answer generation simultaneously. Due to the quadratic time complexity in the transformer mechanism, our design effectively controls computational overhead, achieving performance comparable to vanilla RAG with improved scalability for longer context. These efficiency gains result from our optimized attention complexity handling.", "Experimental Settings\n\nDatasets We conduct experiments on both single-hop and multi-hop question-answering (QA) tasks. Specifically, we evaluate our approach on the Natural Questions (NQ) (Kwiatkowski et al. 2019), TriviaQA (Joshi et al. 2017), HotpotQA (Yang et al. 2018), and 2WikiMultihopQA (2WikiQA) (Ho et al. 2020) datasets.", "Metrics We use Exact Match ( EM ) (Lee, Chang, and Toutanova 2019) and F1 (Karpukhin et al. 2020) to assess the QA accuracy of retrieval-augmented LLMs, which are commonly adopted in QA evaluation. EM checks if predicted answers exactly match the ground truth, while F1 measures their overlap in precision and recall.", "Implementation Details We select LLaMA-3-8B (Dubey et al. 2024) and Qwen-2.5-7B (Yang et al. 2024) as base LLM. For the zero-shot setting (Section ), we derive the theoretical values for \u00b5 and \u03c3 2 based on Equation 9, and perform a search around these values to determine their optimal configuration. For the adaptive balancing entropy factor learning, we set d r = 8 . We conduct the experiments on 8 NVIDIA A100 40GB GPUs, with a learning rate of 5e-4, a cosine learning rate scheduler, and a batch size of 16. To demonstrate the robustness of our training approach, we train the LLMs for no more than 500 steps solely on the NQdataset and subsequently validate its performance across multiple datasets.", "Baselines We introduce two types of baselines for comparison with our approach, including zero-shot methods and lightweight fine-tuning methods.", "Zero-shot methods. We select several prompt-based strategies that enhance generation performance in RAG scenarios for comparison, including prompt-based methods (Chain-of-Thought (Wei et al. 2022), Chain-of-Note (Yu et al. 2023), Self-Critic (Asai et al. 2023)), architecturedriven methods (PCW (Ratner et al. 2023), PositionEngineering (He et al. 2024)), and attention reweighting method (Multiply-Attention (Chiang and Cholak 2022)).", "Parametric-efficient fine-tuning methods: We choose two commonly used PEFT approaches in RAG, including LoRA (Hu et al. 2022) and Prefix-Tuning (Li and Liang 2021) for comparison with our parameter-efficient training strategy.", "Figure 3: The overview of the proposed BEE-RAG. The left side shows the overall architecture design, while the right side illustrates the zero-shot strategy intrinsic multi-importance inference and the parametric-efficient fine-tuning strategy adaptive balancing factor learning .\n\nQuestion Document Induction Prompt ( P S ) Generation Prompt ( P Q \u2026 Q D1 D2\n\nParallel Scoring and Generation\n\n\ud835\udefd\n\nZero-shot / Parametric-efficient\n\nFine-tuning key vector Doc 1 Doc 2 query vector attention scores balanced attention scores Learnt Projection Generation +1.2 -1.3 importance representations early-decoding scores doc1 doc2 balanc ing entropy factor\n\n(a) Overall Framework of BEE-RAG\n\n(b) Intrinsic Multi-Importance Inference\n\n(c) Adaptive Balancing Factor Learning"]}
{"type": "entry", "index": 25, "question": "hallucinations in rag vs normal llms and how to reduce them", "answer": "Hallucinations are moments when a model confidently produces information that is false or not supported by any reference text [1][4]. This happens both in plain \u201cbase\u201d LLMs and in RAG systems, but for different dominant reasons.\n\nIn normal (non\u2011RAG) LLMs, hallucinations come mainly from how they\u2019re built: they are prediction machines optimized for fluent language rather than strict truth, so they may \u201cconfabulate\u201d when they lack clear knowledge or face ambiguity, inventing details to keep the narrative smooth [2][4]. This is worse with very precise facts such as numbers and dates, where parametric memory is especially unreliable [5].\n\nRAG adds a retrieval step: the model is given external documents to ground its answer. This usually reduces hallucinations compared with using a base LLM alone [2][3]. However, RAG brings its own failure modes: if retrieval returns irrelevant, incomplete, or wrong documents, the final answer will also be defective; even with good documents, the generator may ignore them and rely on its (possibly wrong) internal knowledge or combine sources incorrectly [2]. Empirical legal benchmarks confirm that RAG tools still hallucinate in 17\u201333% of cases, although less than general-purpose LLMs without RAG [2].\n\nWays to reduce hallucinations in base LLM use:\n- Use safer decoding methods that explicitly compare layers or penalize overconfident, inconsistent continuations (e.g., DoLa, contrastive search) [1].\n- Apply self-checking or multi-answer consistency methods like SelfCheck-style approaches and multi-path reasoning [1][4].\n\nWays to reduce hallucinations in RAG specifically:\n- Improve retrieval quality (better indexing, ranking, and coverage), since bad retrieval is a primary cause of errors [2][3][5].\n- Enforce stronger adherence to retrieved evidence during decoding, balancing internal knowledge with external documents instead of letting the model override them [1][2][5].\n- Add specialized hallucination-detection layers that test answers against the retrieved context using metamorphic or consistency tests [4].", "tags": ["benchmarks_eval"], "full_faithfulness": 1.0, "full_answer_relevancy": 0.8299735049, "seed_faithfulness": 0.7857142857, "seed_answer_relevancy": 0.8299735049, "context_relevance": 0.8729488163032361, "context_relevance_error": null, "context_relevance_reason": "The context passages provide extensive information directly related to hallucinations in LLMs, particularly in RAG settings, and discuss various strategies to mitigate them. Key details such as the definition of hallucinations, their causes, and specific mitigation strategies like RAG and decoding methods are well-covered, aligning closely with the user's question. The only minor shortcoming is that while the context is rich, it could be slightly more focused on direct comparisons between RAG and normal LLMs regarding hallucination rates.", "structure_citation": 0.7382986442862, "structure_error": null, "structure_reason": "The Actual Output effectively discusses hallucinations in both RAG and normal LLMs, providing a clear explanation of their causes and mitigation strategies. It includes multiple citations, such as [1], [2], [3], [4], and [5], which correspond to relevant snippets in the Context. However, while most claims are supported, there are instances where the citations could be more directly linked to specific claims, particularly regarding the empirical benchmarks mentioned. This slight lack of clarity in citation support prevents a perfect score.", "hallucination_score": 0.8705785021648484, "hallucination_error": null, "hallucination_reason": "The response effectively addresses the key claims about hallucinations in both RAG and normal LLMs, explaining their causes and mitigation strategies. It accurately reflects the context by discussing the differences in hallucination rates and the impact of retrieval quality in RAG systems. The output is well-supported by references and empirical data, demonstrating a strong alignment with the evaluation steps, though it could benefit from slightly more emphasis on the specific mechanisms of RAG compared to base LLMs.", "contexts_used": ["5 Related Work\n\nRetrieval Augmented Generation Retrieval-Augmented Generation (RAG) enhances model reasoning by integrating relevant external knowledge retrieved through user queries. Recent advances focus on three directions: refined retrieval mechanisms [25, 6], structured knowledge organization [43, 2], and optimized context embedding [24, 45]. Self-RAG [1] and FLARE [29] achieve adaptive retrieval through self-evaluation and uncertainty prediction respectively, dynamically optimizing knowledge acquisition. GraphRAG [21, 15, 28] advances reasoning capabilities by constructing document-derived knowledge graphs that capture semantic relationships for multi-hop inference. Embedding optimizations include noise injection [10, 54] to counter overfitting through strategic low-relevance document insertion, and context re-ranking [61, 13] that prioritizes high-utility knowledge via learned document scoring. While these methods enhance knowledge orchestration through pipeline improvements, they systematically neglect the internal mechanisms through which LLMs process external information during generation. Our work bridges this fundamental gap by surfacing stratified knowledge integration patterns across LLM's layers through systematic layer-wise analysis.", "Decoding Strategy in LLMs Decoding strategies are pivotal in transforming raw model probabilities into coherent text outputs, critically influencing the quality and factual integrity of LLM generations [47, 56, 55]. While traditional methods like greedy decoding and beam search [39, 57] remain prevalent, recent work has introduced advanced techniques to address their limitations. Contrastive search (CS) [48] balances diversity and coherence by selecting tokens through a weighted combination of probability and semantic dissimilarity to preceding context, mitigating repetition while preserving fluency. Simple Decoding (FSD) [59] suppresses redundant patterns by dynamically constructing an anti-language model to penalize overused token sequences. DoLa [9] addresses hallucinations by contrasting later-layer logit distributions with earlier ones, prioritizing factually consistent predictions. Building on these advances, we propose a novel decoding strategy for RAG that dynamically balances external retrieved knowledge with the model's internal parametric knowledge, enhancing factual accuracy by mitigating interference from outdated or conflicting internal representations.", "Hallucinations in LLMs Hallucinations in LLMs refer to instances where the model generates false or unsupported information not grounded in its reference data [42]. Existing mitigation strategies include multi-agent debating, where multiple LLM instances collaborate to detect inconsistencies through iterative debates [8, 14]; self-consistency verification, which aggregates and reconciles multiple reasoning paths to reduce individual errors [53]; and model editing, which directly modifies neural network weights to correct systematic factual errors [62, 19]. While RAG systems aim to ground responses in retrieved external knowledge, recent studies show that they still exhibit hallucinations, especially those that contradict the retrieved content [50]. To address this limitation, our work conducts an empirical study analyzing how LLMs internally process external knowledge", "in RAG settings by controlling the noise from different granularity. Based on these findings, we propose a novel decoding method designed to improve answer accuracy and reduce hallucination by enhancing the integration of retrieved evidence.", "2 El fen\u00f3meno de las alucinaciones en LLMs legales: naturaleza, causas e impacto\n\n2.3 Causas ra\u00edz de las alucinaciones en LLMs legales\n\n\u00cdntimamente ligada a los datos est\u00e1 la naturaleza probabil\u00edstica y la arquitectura misma de los LLMs . Estos modelos, a pesar de su impresionante capacidad para generar texto coherente, no operan a trav\u00e9s de una comprensi\u00f3n sem\u00e1ntica profunda o un razonamiento l\u00f3gico an\u00e1logo al humano (Searle 1980; Marcus & Davis, 2022). Son fundamentalmente motores predictivos que calculan la secuencia de palabras m\u00e1s probable bas\u00e1ndose en las correlaciones estad\u00edsticas aprendidas de sus vastos datos de entrenamiento. Esta orientaci\u00f3n hacia la predicci\u00f3n estad\u00edstica, optimizada a menudo para la fluidez ling\u00fc\u00edstica por encima de la factualidad estricta, los hace intr\u00ednsecamente propensos a generar afirmaciones que suenan correctas pero que carecen de base real (Ji et al., 2023; Bowman 2015).", "2 El fen\u00f3meno de las alucinaciones en LLMs legales: naturaleza, causas e impacto\n\nEsta optimizaci\u00f3n intr\u00ednseca para la fluidez puede llevar a lo que a veces se describe como 'confabulaci\u00f3n', un proceso mediante el cual el modelo, enfrentado a una falta de informaci\u00f3n f\u00e1ctica directa o a una ambig\u00fcedad en la consulta, 'inventa' detalles o narrativas coherentes para mantener la continuidad del discurso , aunque estos elementos fabricados carezcan de una base real. La confabulaci\u00f3n, en este sentido, es una manifestaci\u00f3n directa de la arquitectura predictiva del LLM priorizando la apariencia de comprensi\u00f3n sobre la factualidad estricta, llevando a la generaci\u00f3n de alucinaciones que, aunque err\u00f3neas, pueden ser enga\u00f1osamente persuasivas por su coherencia superficial.", "2 El fen\u00f3meno de las alucinaciones en LLMs legales: naturaleza, causas e impacto\n\nLa dificultad de los LLMs para conectar los principios legales abstractos con los hechos concretos de un caso es una causa fundamental de las alucinaciones. Un estudio que condicion\u00f3 a los LLMs con diferentes niveles de conocimiento del sistema legal alem\u00e1n para detectar el discurso de odio lo demostr\u00f3 de manera concluyente. Cuando los modelos eran \"condicionados\" \u00fanicamente con el conocimiento m\u00e1s abstracto (como el t\u00edtulo de una norma constitucional o estatutaria), mostraban una falta de comprensi\u00f3n profunda de la tarea, llegando a contradecirse y a alucinar respuestas cuando se les presentaban normas ficticias o irrelevantes (Ludwig et al., 2025). Esto sugiere que la arquitectura probabil\u00edstica de los LLMs, en ausencia de un anclaje en definiciones y ejemplos concretos, lucha por aplicar correctamente el razonamiento jur\u00eddico, recurriendo a la invenci\u00f3n.", "2 El fen\u00f3meno de las alucinaciones en LLMs legales: naturaleza, causas e impacto\n\nFen\u00f3menos como el sobreajuste ( overfitting ), donde el modelo memoriza patrones espec\u00edficos del entrenamiento en lugar de aprender principios generales, pueden exacerbar este problema, limitando su capacidad para generalizar correctamente a situaciones nuevas o ligeramente diferentes (Khma\u00efess Al Jannadi, 2023). Adem\u00e1s, su capacidad inherente para la extrapolaci\u00f3n (aunque esencial para la generalizaci\u00f3n) puede desviarse f\u00e1cilmente hacia la invenci\u00f3n o la conexi\u00f3n espuria de conceptos cuando se enfrenta a consultas que bordean los l\u00edmites de su conocimiento o requieren inferencias complejas (Shaip, 2022; Huang et al. 2021; Domingos 2015).", "2 El fen\u00f3meno de las alucinaciones en LLMs legales: naturaleza, causas e impacto\n\nEl dominio legal en s\u00ed mismo presenta una complejidad intr\u00ednseca que amplifica estos desaf\u00edos. El lenguaje jur\u00eddico es notoriamente t\u00e9cnico, denso en significado, altamente dependiente del contexto y plagado de ambig\u00fcedades y t\u00e9rminos polis\u00e9micos (Khma\u00efess Al Jannadi, 2023). Interpretar correctamente un estatuto, un contrato o una sentencia requiere no solo comprender el significado literal de las palabras, sino tambi\u00e9n el contexto legislativo, la intenci\u00f3n de las partes, la historia procesal y la red de precedentes relevantes - tareas que exigen un nivel de comprensi\u00f3n contextual y razonamiento que desaf\u00eda a los LLMs actuales. El razonamiento jur\u00eddico per se, con sus m\u00e9todos anal\u00f3gicos, deductivos basados en reglas y principios, y su constante ponderaci\u00f3n de factores, representa una forma de cognici\u00f3n de orden superior que los LLMs, basados en patrones estad\u00edsticos, luchan por emular fielmente (Ashley 2017; Choi and Schwarcz, 2024).", "2 El fen\u00f3meno de las alucinaciones en LLMs legales: naturaleza, causas e impacto\n\nIncluso las estrategias dise\u00f1adas para mitigar las alucinaciones, como RAG, introducen sus propios puntos de vulnerabilidad . Como se discutir\u00e1 en detalle en la Secci\u00f3n 4, la efectividad de RAG depende cr\u00edticamente de la calidad de su m\u00f3dulo de recuperaci\u00f3n de informaci\u00f3n. Si la informaci\u00f3n recuperada es irrelevante, incorrecta o incompleta, el LLM generador, incluso si intenta ser fiel al contexto proporcionado, producir\u00e1 una respuesta defectuosa. Adem\u00e1s, el propio LLM generador puede fallar en integrar correctamente la informaci\u00f3n recuperada, priorizando su conocimiento param\u00e9trico err\u00f3neo o sintetizando incorrectamente las fuentes (Addleshaw Goddard, 2024).", "2 El fen\u00f3meno de las alucinaciones en LLMs legales: naturaleza, causas e impacto\n\nEn \u00faltima instancia, el an\u00e1lisis de las causas ra\u00edz de las alucinaciones ser\u00eda incompleto si se limitara al modelo. La causa fundamental m\u00e1s peligrosa no reside en la m\u00e1quina, sino en el humano que la utiliza sin criterio . La IA tiene el potencial de amplificar las capacidades de los profesionales diligentes, mientras que puede inducir a errores a aquellos que la utilizan sin un criterio cr\u00edtico y una supervisi\u00f3n adecuada. Un profesional con pensamiento cr\u00edtico y experiencia utilizar\u00e1 el LLM como un catalizador para acelerar su investigaci\u00f3n, validando cada resultado. Sin embargo, un usuario sin estas bases, seducido por la aparente facilidad, delegar\u00e1 su razonamiento y caer\u00e1 en la trampa de la", "2 El fen\u00f3meno de las alucinaciones en LLMs legales: naturaleza, causas e impacto\n\ncomplacencia. Por tanto, la mayor causa de riesgo no es la alucinaci\u00f3n del modelo, sino la \" alucinaci\u00f3n del usuario \": la creencia de que una herramienta puede sustituir la responsabilidad, el esfuerzo y el juicio profesional. Este fen\u00f3meno, alimentado por una cultura de la inmediatez y los atajos, es el verdadero desaf\u00edo a mitigar en la integraci\u00f3n de la IA en el sector legal.", "2 El fen\u00f3meno de las alucinaciones en LLMs legales: naturaleza, causas e impacto\n\n2.4 Impacto espec\u00edfico y riesgos asociados en la pr\u00e1ctica legal\n\nLa manifestaci\u00f3n de estas causas en forma de alucinaciones tiene un impacto tangible y multifac\u00e9tico en el ecosistema legal:\n\nSocavamiento de la investigaci\u00f3n y el an\u00e1lisis jur\u00eddico : la base de cualquier trabajo legal riguroso es la investigaci\u00f3n precisa. Las alucinaciones, al introducir informaci\u00f3n falsa o fabricada, contaminan este proceso fundamental, haciendo perder tiempo en la verificaci\u00f3n, llevando a an\u00e1lisis err\u00f3neos y, en \u00faltima instancia, a estrategias legales defectuosas.", "2 El fen\u00f3meno de las alucinaciones en LLMs legales: naturaleza, causas e impacto\n\nRiesgos profesionales y \u00e9ticos : para los abogados, confiar en informaci\u00f3n alucinada puede tener consecuencias devastadoras. Puede llevar a la presentaci\u00f3n de escritos judiciales defectuosos (resultando en sanciones como la popular del caso Mata v. Avianca ), al incumplimiento del deber de competencia y diligencia, a la violaci\u00f3n del deber de franqueza ante el tribunal, y a posibles reclamaciones por negligencia profesional (Yamane, 2020; Schwarcz et al., 2024). La necesidad de verificar exhaustivamente cada resultado de la IA puede, ir\u00f3nicamente, anular los beneficios de eficiencia prometidos (Gottlieb 2024).", "2 El fen\u00f3meno de las alucinaciones en LLMs legales: naturaleza, causas e impacto\n\nErosi\u00f3n de la confianza : la prevalencia de alucinaciones, especialmente si no se aborda con transparencia, puede minar la confianza de los profesionales legales, los clientes y el p\u00fablico en general hacia las herramientas de IA y, por extensi\u00f3n, hacia quienes las utilizan (Khma\u00efess Al Jannadi, 2023). Esta erosi\u00f3n de la confianza puede obstaculizar la adopci\u00f3n de tecnolog\u00edas potencialmente beneficiosas.", "3 Evaluaci\u00f3n de alucinaciones en aplicaciones legales de IA: metodolog\u00edas, desaf\u00edos y estado actual\n\n3.2 M\u00e9tricas y metodolog\u00edas para la detecci\u00f3n y cuantificaci\u00f3n\n\nM\u00e9tricas automatizadas: incluyen m\u00e9tricas est\u00e1ndar de NLP como ROUGE o BLEU (m\u00e1s adecuadas para tareas como la sumarizaci\u00f3n o traducci\u00f3n) y m\u00e9tricas emergentes de factualidad que intentan verificar autom\u00e1ticamente las afirmaciones contra bases de conocimiento (p.ej., FActScore - Min et al. 2023) o usando otros LLMs como jueces (Zheng et al., 2023).", "3 Evaluaci\u00f3n de alucinaciones en aplicaciones legales de IA: metodolog\u00edas, desaf\u00edos y estado actual\n\nFortalezas: escalables y computacionalmente eficientes una vez desarrolladas.\n\nDebilidades: su correlaci\u00f3n con el juicio humano sobre la calidad y factualidad legal es a menudo baja o no probada. Pueden ser f\u00e1cilmente \"enga\u00f1adas\" por respuestas fluidas pero incorrectas. Su desarrollo y validaci\u00f3n para el dominio legal a\u00fan est\u00e1 en etapas tempranas.", "3 Evaluaci\u00f3n de alucinaciones en aplicaciones legales de IA: metodolog\u00edas, desaf\u00edos y estado actual\n\nEn la pr\u00e1ctica, un enfoque robusto para la evaluaci\u00f3n probablemente requiera una combinaci\u00f3n de estas metodolog\u00edas: evaluaci\u00f3n basada en referencias para hechos verificables, detecci\u00f3n de auto-contradicci\u00f3n para obtener l\u00edmites inferiores en tareas abiertas, m\u00e9tricas autom\u00e1ticas para an\u00e1lisis a gran escala (con validaci\u00f3n cuidadosa), y evaluaci\u00f3n humana experta como validaci\u00f3n final y para tareas cualitativamente complejas.", "3 Evaluaci\u00f3n de alucinaciones en aplicaciones legales de IA: metodolog\u00edas, desaf\u00edos y estado actual\n\n3.3 Benchmarking de herramientas comerciales: estado actual y hallazgos emp\u00edricos clave\n\nLa necesidad cr\u00edtica de evaluaci\u00f3n ha impulsado el estudio emp\u00edrico sistem\u00e1tico y pre-registrado de Magesh et al. (2025) sobre las plataformas comerciales l\u00edderes de IA legal. Sus hallazgos, basados en un conjunto diverso de m\u00e1s de 200 consultas legales del mundo real, son reveladores y establecen un punto de referencia crucial:\n\nPersistencia alarmante de alucinaciones: Contrario a las audaces afirmaciones de marketing de ser \"libres de alucinaciones\", la mayor\u00eda de las herramientas evaluadas fallaron en una proporci\u00f3n significativa. Utilizando una definici\u00f3n rigurosa de alucinaci\u00f3n (respuesta incorrecta o mal fundamentada), se encontr\u00f3 que Lexis+ AI alucin\u00f3 en el 17% de los casos, y Westlaw AI-Assisted Research lo hizo en m\u00e1s de un tercio de las ocasiones (>33%).", "3 Evaluaci\u00f3n de alucinaciones en aplicaciones legales de IA: metodolog\u00edas, desaf\u00edos y estado actual\n\n3.3 Benchmarking de herramientas comerciales: estado actual y hallazgos emp\u00edricos clave\n\nVariabilidad extrema entre plataformas: El rendimiento no es uniforme. Lexis+ AI tuvo una precisi\u00f3n general del 65% de respuestas correctas y fundamentadas, estableci\u00e9ndose como la herramienta m\u00e1s fiable del grupo. En el otro extremo, Ask Practical Law AI, debido a una base de conocimiento m\u00e1s limitada, tuvo una tasa extremadamente alta de respuestas incompletas o rechazos (>60%), limitando severamente su utilidad pr\u00e1ctica (Magesh et al., 2025).", "3 Evaluaci\u00f3n de alucinaciones en aplicaciones legales de IA: metodolog\u00edas, desaf\u00edos y estado actual\n\n3.3 Benchmarking de herramientas comerciales: estado actual y hallazgos emp\u00edricos clave\n\nConfirmaci\u00f3n de que RAG es una Mitigaci\u00f3n, no una Soluci\u00f3n: Los resultados confirman que la tecnolog\u00eda RAG s\u00ed reduce la tasa de alucinaci\u00f3n en comparaci\u00f3n con los LLMs de prop\u00f3sito general. Sin embargo, las tasas de error residuales demuestran que, en su implementaci\u00f3n actual, el RAG no elimina totalmente las alucinaciones, siendo los fallos en la recuperaci\u00f3n de informaci\u00f3n y en la adhesi\u00f3n del LLM a las fuentes los problemas persistentes.", "3 Evaluaci\u00f3n de alucinaciones en aplicaciones legales de IA: metodolog\u00edas, desaf\u00edos y estado actual\n\n3.3 Benchmarking de herramientas comerciales: estado actual y hallazgos emp\u00edricos clave\n\nPersistencia alarmante de alucinaciones: el hallazgo m\u00e1s contundente es que, contrariamente a las afirmaciones de marketing de \"eliminaci\u00f3n\" o \"ausencia\" de alucinaciones, la mayor\u00eda de las herramientas comerciales evaluadas alucinan en una proporci\u00f3n significativa . Utilizando una definici\u00f3n rigurosa de alucinaci\u00f3n (respuesta incorrecta o fundamentada err\u00f3neamente), se encontr\u00f3 que Lexis+ AI y Ask Practical Law AI alucinaban entre el 17% y el 33% de las veces, mientras que Westlaw AI-Assisted Research alucinaba m\u00e1s de un tercio del tiempo (>34%). Estas tasas, aunque inferiores a las de GPT-4 o 5 base en tareas legales (58-88%), siguen siendo inaceptablemente altas para la pr\u00e1ctica profesional.", "3 Evaluaci\u00f3n de alucinaciones en aplicaciones legales de IA: metodolog\u00edas, desaf\u00edos y estado actual\n\n3.3 Benchmarking de herramientas comerciales: estado actual y hallazgos emp\u00edricos clave\n\n| | Herramienta de IA Legal | Tasa de Alucinaci\u00f3n (%) | Resp. Incompletas (%) | Resp. Precisas (%) |\n|---:|:--------------------------------|:--------------------------|:------------------------|:---------------------|\n| 0 | Lexis+ AI | 17 | 18 | 65 |\n| 1 | Westlaw AI-Assisted Research | >34 | 25 | 41 |\n| 2 | Ask Practical Law AI | 17 | >60 | 19 |\n| 3 | GPT-4/5 (base, como referencia) | ~58-88 | N/A* | N/A* |\n\nTable 2: Rendimiento comparativo de herramientas de IA Legal comerciales (adaptado de Magesh et al., 2024)\n\nNota: Las tasas de alucinaci\u00f3n para herramientas comerciales se refieren a respuestas incorrectas o fundamentadas err\u00f3neamente. GPT-4/5 base se incluye como referencia general de LLMs sin RAG legal espec\u00edfico, sus tasas de alucinaci\u00f3n en tareas legales pueden ser m\u00e1s altas y la estructura de \"respuestas incompletas\" o \"precisas y fundamentadas\" puede no ser directamente comparable sin el componente RAG. *N/A indica que la m\u00e9trica no se report\u00f3 de la misma manera o no es directamente comparable.", "3 Evaluaci\u00f3n de alucinaciones en aplicaciones legales de IA: metodolog\u00edas, desaf\u00edos y estado actual\n\n3.3 Benchmarking de herramientas comerciales: estado actual y hallazgos emp\u00edricos clave\n\nRAG es una (gran) mitigaci\u00f3n, pero no \"la\" soluci\u00f3n: los resultados confirman que la tecnolog\u00eda RAG empleada por estas herramientas s\u00ed reduce la tasa de alucinaci\u00f3n en comparaci\u00f3n con el uso de LLMs de prop\u00f3sito general sin acceso a bases de datos legales externas. Sin embargo, RAG, tal como se implementa actualmente, no elimina totalmente las alucinaciones . Los fallos en la recuperaci\u00f3n de informaci\u00f3n relevante y la incapacidad del LLM generador para adherirse fielmente a las fuentes recuperadas siguen siendo problemas sustanciales.", "3 Evaluaci\u00f3n de alucinaciones en aplicaciones legales de IA: metodolog\u00edas, desaf\u00edos y estado actual\n\n3.3 Benchmarking de herramientas comerciales: estado actual y hallazgos emp\u00edricos clave\n\nVariabilidad entre plataformas: el estudio revela diferencias notables en el rendimiento y el comportamiento entre las distintas herramientas. Lexis+ AI demostr\u00f3 la mayor precisi\u00f3n general (65% de respuestas correctas", "3 Evaluaci\u00f3n de alucinaciones en aplicaciones legales de IA: metodolog\u00edas, desaf\u00edos y estado actual\n\n3.3 Benchmarking de herramientas comerciales: estado actual y hallazgos emp\u00edricos clave\n\ny fundamentadas) y una tasa de alucinaci\u00f3n m\u00e1s baja, pero a\u00fan significativa ( \u02dc 17%). Westlaw AI-AR, aunque a menudo proporcionaba respuestas m\u00e1s largas y detalladas, exhibi\u00f3 la tasa de alucinaci\u00f3n m\u00e1s alta ( \u02dc 33%). Ask Practical Law AI, limitado a su base de conocimientos curada, tuvo una tasa de alucinaci\u00f3n relativamente baja pero sufri\u00f3 de una tasa extremadamente alta de respuestas incompletas o rechazos (>60%), limitando su utilidad pr\u00e1ctica. Esta variabilidad subraya que la etiqueta \"IA legal basada en RAG\" engloba implementaciones muy diferentes con perfiles de riesgo distintos.", "ABSTRACT\n\nRetrieval-Augmented Generation (RAG) mitigates hallucination in LLMs by incorporating external knowledge, but relies on chunk-based retrieval that lacks structural semantics. GraphRAG methods improve RAG by modeling knowledge as entity-relation graphs, but still face challenges in high construction cost, fixed one-time retrieval, and reliance on long-context reasoning and prompt design. To address these challenges, we propose Graph-R1 , an agentic GraphRAG framework via end-to-end reinforcement learning (RL). It introduces lightweight knowledge hypergraph construction, models retrieval as a multi-turn agent-environment interaction, and optimizes the agent process via an end-to-end reward mechanism. Experiments on standard RAG datasets show that Graph-R1 outperforms traditional GraphRAG and RL-enhanced RAG methods in reasoning accuracy, retrieval efficiency, and generation quality. Our code is publicly available 1 .", "2 Related Works\n\n2.1 Definitions of Hallucination\n\nThe term hallucination has been used with varying scope across natural language generation tasks. Some studies emphasize factuality , describing hallucinations as outputs that contradict established facts, i.e., inconsistencies with world knowledge or external ground truth [31, 13]. Others highlight faithfulness , where hallucinations occur when generated responses deviate from the user instruction or a reference text, often producing plausible but ungrounded statements particularly in source-conditioned tasks such as summarization or question answering [26]. Beyond these two dimensions, researchers also note cases of incoherent or nonsensical text that cannot be clearly attributed to factuality or faithfulness criteria [14, 23].", "Alternative terms have also been introduced. Confabulation draws on psychology to describe fluent but fabricated content arising from model priors [9], while fabrication is preferred by some to avoid anthropomorphic connotations [1, 21]. More recently, Chakraborty et al. [3] propose a flexible definition tailored to deployment settings, defining a hallucination as a generated output that conflicts with constraints or deviates from desired behavior in actual deployment, while remaining syntactically plausible under the circumstance.", "2.2 Hallucination Detection in LLMs\n\nBuilding on these definitions, hallucinations have been recognized as a major challenge in text generation. Early work in machine translation and abstractive summarization described them as outputs that are not grounded in the input source [23, 16, 15], motivating the development of evaluation metrics and detection methods for faithfulness and factual consistency across natural language generation tasks.", "More recent reference-free (unsupervised or zero-reference) methods aim to detect hallucinations without gold-standard labels by analyzing the model's own outputs. A prominent method is SelfCheckGPT [22], a zero-resource, black-box approach that queries the LLM multiple times with the same prompt and measures semantic consistency across responses. The intuition is that hallucinated content often leads to instability under stochastic re-generation; true facts remain stable, while fabricated ones diverge. Manakul et al. show that SelfCheckGPT achieves strong performance in sentence-level hallucination detection compared to gray-box methods, and emphasize that it requires no external database or access to model internals [22]. However, SelfCheckGPT may struggle when deterministic decoding or high model confidence leads to repeating the same incorrect output.", "2.3 Metamorphic Testing\n\nMetamorphic Testing (MT) [4] was originally proposed in software engineering to address the oracle problem in which the correct output is unknown. MT relies on metamorphic relations (MRs): transformations of the input with predictable effects on outputs, enabling error detection without access to ground truth [28]. In machine learning, MThas been applied to validate models in computer vision [5] (e.g., rotating an image should not change its predicted class) and NLP [27]\n\nIn hallucination detection for LLMs, MetaQA [32] leverages MRs by generating paraphrased or antonym-based question variants and verifying whether answers satisfy expected semantic or logical constraints. Relying purely on prompt mutations and consistency checks, MetaQA achieves higher precision and recall than SelfCheckGPT on open-domain QA.", "Researchers have also adapted MT for more complex conversational and reasoning settings. MORTAR [12] applies dialogue-level perturbations and knowledge-graph-based inference to multi-turn systems, detecting up to four times more unique bugs than singleturn MT. Drowzee [18] uses logic programming to construct temporal and logical rules from Wikipedia, generating fact-conflicting test cases and revealing rates of 24.7% to 59.8% across six LLMs in nine domains [32].", "These works highlight the promise of MT for hallucination detection, but they primarily target open-book QA or multi-turn dialogue, often over short, single-sentence outputs. Prior studies have not addressed hallucination detection in retrieval-augmented generation (RAG) scenarios over proprietary corpora, a setting in which ground-truth references are unavailable and model internals are inaccessible. MetaRAG builds on MT by decomposing answers into factoids and designing MRs tailored to factual consistency against retrieved evidence in a zero-resource, black-box setting.", "Parametric knowledge can generate accurate answers.\n\nTo further investigate the LLM's ability to generate accurate answers without relying on external retrieval, we categorize the test queries into three groups: (1) queries that are answered by the LLM directly, referred to as Directly Answerable Queries ( DQ ); (2) the remaining queries that require retrievals, referred to as Non-DQ-retrieval ; and (3) a control setting where the same DQs are answered using retrieval, denoted as DQ-retrieval . Fig. 3 presents the exact match (EM) results across six datasets (with corresponding F1 scores and other quantitative results provided in the appendix). We observe that the EM scores achieved by the LLM on DQ are consistently high, and in some cases, even surpass those of DQ-retrieval. Both DQ and DQ-retrieval substantially outperform Non-DQ-retrieval, indicating that DQs are generally simpler or factually grounded queries that fall within the LLM's pre-trained knowledge base. These results indicate that our dual-path generation mechanism not only allows accurate answer generation directly but also supports selective retrieval when needed-offering a more efficient and adaptive RAG framework.", "Effects of LLM hallucinations. To assess the impact of hallucinations when large language models (LLMs) generate answers without retrieval, we conduct a controlled experiment based on a simple heuristic: if a generated answer contains numeric values, it is more likely to be affected by hallucination. This is because LLMs are generally less reliable when producing precise facts such as numbers, dates, or counts from parametric memory alone (Ji et al. 2023; Singh et al. 2025). We filter out all directly answered queries (DQs) whose generated answers contain numbers, and we then rerun our DPR-AIS for these queries (referred to Exclude num ). The results are reported in Tab. 5. Overall, excluding numeric DQs results in slightly improved performance. The average exact match (EM) increases from 35.03 to 35.12, and the average F1 score improves from 35.68 to 35.80. While these gains are modest, they come with an increase in the retriever activation (RA) ratio-from 75.5% to 78.1%. These findings suggest that hallucinations from LLMs, especially when dealing with factual numeric content, can indeed affect answer quality in retrieval-free settings. However, their overall impact remains limited in our setting, as the performance gain is marginal. This reinforces the strength of PAIRS: it naturally handles simple queries with direct parametric answers with trivial effects of LLM hallucinations.", "Case study. Fig. 4 demonstrates a representative example. The query is a simple factual comparison that lies well within the LLM's parametric knowledge, and as shown in the generated pseudo-context, the model accurately generates birth dates and correctly answers Am Rong . However, the standard RAG generates an incorrect answer because the retrievals present Ava DuVernay 's birth year (1972) only, lacking complete evidence on Am Rong . This example demonstrates that LLMs can generate accurate answers to simple queries using their internal parametric knowledge, while noisy or incomplete retrievals-especially those missing critical comparative context-can mislead the final generation. We present more cases in the appendix."]}
{"type": "entry", "index": 26, "question": "long context rag performance drop and possible fixes", "answer": "When you add a lot of retrieved text into a RAG prompt, performance can drop instead of improve.\n\nTwo main issues show up in the documents:\n\n1) Attention gets diluted as context grows\nIn standard RAG, as you stuff in more documents, the model\u2019s attention is spread more and more thinly across tokens. The \"information entropy\" of attention rises, meaning the model becomes less focused on truly important parts of the context, which harms performance on long-context tasks like multi-document reasoning and scientific analysis [1]. Existing fixes like aggressive filtering, special long-context training, or extra encoders either throw away useful info or are expensive/complex [1].\n\n2) Noisy / misaligned retrieval conflicts with the model\nIn FIRESPARQL, adding RAG to a well fine\u2011tuned model dropped execution accuracy (RelaxedEM(all)) from 0.85 to 0.29 because retrieved context from the knowledge graph was often incorrect, irrelevant, or misaligned with the query, and there was no context checker to filter it. This noisy context conflicted with what the model already knew and reduced performance [2]. Prior work has seen the same effect from low\u2011quality RAG context [2].\n\nPossible fixes from the docs:\n- Control attention entropy rather than just truncating: BEE\u2011RAG adds a balancing factor into attention to keep entropy stable across different context lengths, so the model can still focus on critical segments even with many documents [1].\n- Use zero\u2011shot or light fine\u2011tuning to learn how to weight documents: BEE\u2011RAG\u2019s multi\u2011importance inference and adaptive balancing factor learning let the system down\u2011weight less important or noisy documents with minimal extra parameters [1].\n- Add some form of context checking or filtering to avoid obviously irrelevant or wrong retrievals before they reach the model, which is highlighted as missing in FIRESPARQL [2].", "tags": ["_untagged"], "full_faithfulness": 0.9545454545000001, "full_answer_relevancy": 0.7953156857, "seed_faithfulness": 0.9545454545000001, "seed_answer_relevancy": 0.7979037677, "context_relevance": 0.8698985037539437, "context_relevance_error": null, "context_relevance_reason": "The context passages provide substantial information regarding the performance drop in RAG systems when handling long contexts, directly addressing the user's query. Key details such as the introduction of Balanced Entropy-Engineered RAG (BEE-RAG) and its proposed solutions to mitigate performance degradation are well-articulated. However, while the context is relevant, some passages delve into technical specifics that may not be necessary for a general understanding of the fixes, slightly detracting from overall clarity.", "structure_citation": 0.6976078717262529, "structure_error": null, "structure_reason": "The Actual Output includes claims supported by citations [1] and [2], which correspond to snippets in the Context. The claims about attention dilution and noisy retrieval are well-supported by the provided context, demonstrating a clear understanding of the issues with RAG performance. However, while the citations are present and relevant, the response could benefit from more explicit connections to the specific snippets in the Context, which would strengthen the overall argument and clarity.", "hallucination_score": 0.8469760769496979, "hallucination_error": null, "hallucination_reason": "The response accurately identifies key issues related to performance drops in RAG systems when handling long contexts, specifically mentioning attention dilution and noisy retrievals, which are supported by the context. It also proposes relevant fixes, such as controlling attention entropy and context checking, aligning well with the context provided. However, while the claims are mostly supported, the response could benefit from a clearer connection to the specific context examples, particularly regarding the empirical results mentioned in the context.", "contexts_used": ["Introduction\n\nRetrieval-augmented generation (RAG) has emerged as a transformative paradigm for augmenting large language models (LLMs) with dynamically integrated external knowledge (Gao et al. 2023; Lewis et al. 2020). While conventional RAG systems demonstrate remarkable capabilities in knowledge-intensive tasks, their performance exhibits critical fragility when processing extended contextual inputs (Ren et al. 2023). This vulnerability arises from the inherent requirement of RAG frameworks to incorporate external retrieved documents, which typically results in substantially longer input sequences compared to standard generation tasks (Zhang et al. 2024a). This limitation manifests acutely in scenarios demanding long-context comprehension, such as scientific literature analysis, multi-hop reasoning, and cross-document synthesis, where the growing complexity of modern knowledge systems necessitates robust processing of extended text spans (Liu et al. 2023).\n\n* Equal contributions.\n\n\u2020 The work was done during the internship at Baidu.\n\n\u2021 Corresponding authors.", "Figure 1: In vanilla RAG, increasing input context length raises the information entropy of attention scores (left), potentially harming performance, while the LLM is less focused on important segments (right). This paper proposes a balanced entropy engineering strategy, which maintains entropy stability for longer contexts and guide the LLM to focus on critical segments.\n\n\u0000 \u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 H \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000", "Typically, existing efforts to mitigate this challenge focus predominantly on heuristic document filtering, extrapolative training, or architectural modifications, introducing worth noting trade-offs. Threshold-based retrieval document truncation or selection reduces context length at the cost of discarding potentially relevant information (Jeong et al. 2024; Wang et al. 2024). While extended context training may enhance RAG performance (Lin et al. 2024; Zhang et al. 2024a), it substantially increases computational demands and risks compromising model generalization. The introduction of auxiliary modules, such as specialized encoders (Yen, Gao, and Chen 2024), further escalates the complexity of the system.", "As shown in Figure 1, we identify a fundamental oversight in existing explorations: the failure to address the core tension between context entropy growth and attention allocation dynamics. As retrieved document length increases, unconstrained context entropy expansion progressively dilutes attention distributions, undermining LLMs' capacity to prioritize semantically critical content. Such a phenomenon substantially degrades RAG's ability to extract and utilize salient information.", "In this work, we fundamentally rethink RAG's context length adaptability through the perspective of entropy engineering, and propose Balanced Entropy-Engineered RAG ( BEE-RAG ), a principled framework that establishes entropy-invariant dynamics to decouple attention sharpness from context length. First, we introduce balanced context entropy, a novel attention reformulation incorporating document-specific balancing entropy factor \u03b2 to enforce entropy invariance across variable context lengths. Theoretical analysis demonstrates this mechanism maintains entropy levels within a stable regime for various context scales, contrasting with conventional entropy scaling. Building on this foundation, we develop intrinsic multi-importance inference, a zero-shot strategy that derives balancing entropy factors through context aggregation while preserving inter-document independence via parallel context scoring and generation. To ensure broad applicability, we further propose adaptive balancing factor learning, which is a parameter-efficient tuning method that learns task-specific balancing factor through lightweight linear projections for domain adaptation with minimal parameter updates (0.014% of total parameters).", "Empirical analyses across multiple real-world benchmarks reveal that BEE-RAG fundamentally alters the entropy scaling laws governing conventional RAG systems, which provides a robust and scalable solution for RAG systems dealing with long-context scenarios. Our main contributions are summarized as follows:\n\nWe introduce the concept of balanced context entropy, a novel attention reformulation that ensures entropy invariance across varying context lengths, and allocates attention to important segments. It addresses the critical challenge of context expansion in RAG.", "We propose a zero-shot strategy called intrinsic multiimportance inference and parametric-efficient finetuning strategy adaptive balancing entropy factor learning to obtain the balancing entropy factor in different scenarios while maintaining efficiency.", "Extensive empirical validation across multiple benchmarks demonstrates the effectiveness and efficiency of BEE-RAG.", "Overview\n\nThe performance of traditional RAG systems often degrades as the input context length increases, primarily due to the escalating context entropy that dilutes attention distributions across retrieved documents. To mitigate this issue, we propose B alanced E ntropy E ngineering for RAG ( BEERAG ), stabilizing context entropy while adaptively optimizing attention allocation to enhance RAG performance. Our method is driven by two key insights: (1) controlling context entropy growth is critical for maintaining robustness in RAG systems with extended contexts, and (2) document importance should dynamically guide attention allocation without costly computation or parametric updates.", "The BEE-RAG framework introduces three principal innovations. First, we propose balanced context entropy , which incorporates an additive balancing entropy factor \u03b2 into attention computation to maintain entropy invariance across varying context lengths. Theoretical analysis confirms that applying specific distributional constraints can effectively eliminate the impact of context length on information entropy. Second, we develop a intrinsic multiimportance inference strategy that computes the balancing entropy factor in a zero-shot manner , eliminating the need for auxiliary models or training data. Third, for scenarios requiring domain adaptation, we devise a parameterefficient adaptive balancing entropy factor learning approach that achieves enhanced performance with minimal computational overhead.", "In-depth Analysis\n\nIn this section, we further present an analysis from two aspects: the scaling of LLMs and the quality of the retrieved documents.", "Effect of Model Scaling In this part, we first explore the impact of different model scales on experimental performance. We test the Qwen-2.5 family LLMs, ranging from 3B, 7B, 14B to 32B, on their average generation results across four datasets. The results are shown in Figure 4. It is clear that our proposed BEE-RAG consistently improves RAG accuracy across different model sizes. This further demonstrates that our method enables LLMs to better handle generation tasks involving noisy contexts and exhibits strong generalization capabilities.", "Effect of Retrieved Documents Weconsider both context length and retrieval quality to evaluate the effect of retrieved documents, showing the comprehensive priority of the proposed BEE-RAG method.", "Context Length. We first examine the impact of the number of the input document. Figure 5 shows the average performance of our BEE-RAG and the baselines varying the document number. It is clear that our method overall outperforms the baselines. Notably, as the context length increases, the performance gap between BEE-RAG and the baselines becomes significantly larger. This highlights that our strategy is more effective in adapting to larger amounts of contextual information, allowing LLMs to efficiently extract important information through global context entropy reduction and dynamic document-level attention allocation.", "Figure 5: Effect of context length with various document numbers. Mean metrics are reported over four datasets.\n\nNumber of Document 0.24 0.30 0.36 0.42 EM \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000 \u0000\u0000\u0000\u0000\u0000\u0000\n\n| | Retriever | Prompt | Zero-BEE | LoRA | Light-BEE |\n|---:|:------------|---------:|-----------:|-------:|------------:|\n| 0 | BM25 | 23.82 | 26.98 | 28.22 | 35.03 |\n| 1 | Contriever | 28.65 | 31.42 | 33.7 | 38.09 |\n| 2 | E5-Base | 29.36 | 32.59 | 34.92 | 38.55 |\n| 3 | E5-large | 29.77 | 34.02 | 34.02 | 38.84 |\n| 4 | BGE | 30.51 | 33.91 | 36.72 | 39.05 |\n\nTable 3: Effect of retrieval quality with various retrievers. Mean metrics are reported over four datasets.", "Retrieval Quality. Moreover, we explore the impact of various retrievers with different retrieval document qualities for the input of BEE-RAG. Table 3 presents the QA performance of our approach and the baselines using various retrievers across four datasets, including BM25, Contriever, E5-base, E5-large, and BGE. From the table, it is evident that our method achieves the best performance across all retriever settings. Notably, our approach excels when retrieval quality is lower. This highlights that BEE-RAG is better at focusing on the critical parts of the retrieval results, thereby improving the overall accuracy of the generations.", "Related Work\n\nRecent efforts have aimed to improve RAG performance under long contexts (Laban et al. 2024). PCW (Ratner et al. 2023) restricts attention within fixed-size windows to reduce attention dilution. CEPE (Yen, Gao, and Chen 2024) uses a lightweight encoder to process inputs chunk by chunk, enabling better use of long contexts via cross-attention. RAFT (Zhang et al. 2024b) and Chain-of-Notes (Yu et al. 2023) extend the context capacity of models through instruction tuning on long-input data. However, these methods either yield limited performance gains or incur high training costs. Some works explore the effect of attention distribution (Wang et al. 2023). Su (2021) adjust attention scaling by context length to match short-context behavior, but this fails to highlight key segments and may miss important information.", "Our BEE-RAG introduces an attention reallocation mechanism. It reduces attention entropy gap with short contexts and guides focus to key segments. This improves longcontext RAG performance.", "Conclusion\n\nIn this study, we identify and address the fundamental limitation of unconstrained context entropy in RAG systems through the lens of information entropy. We proposed BEERAG for the adaptability of RAG systems to various context lengths. By introducing balanced context entropy, we demonstrate that enforcing entropy invariance enables superior RAG performance with various context lengths without sacrificing computational efficiency. In order to extend applicability to a broader range of scenarios, we propose multi-importance inference for zero-shot reasoning, and also propose adaptive balancing factor learning for parameterefficient fine-tuning. Empirical examination on four realworld knowledge-intensive tasks shows consistent performance gains of BEE-RAG.", "6 Discussion\n\nWe discuss the key findings from the experiments and discuss limitations and future work.", "6.1 Discussion on the results\n\nOur experimental findings provide several key insights into the effectiveness of the FIRESPARQL framework and the performance of different strategies for SPARQL query generation with different epoch settings and different model sizes.", "Fine-Tuning Performance As shown in Table 1, fine-tuning LLMs on NLQ-SPARQL pairs leads to significant improvements over both the zero-shot baseline and the one-shot state-of-the-art methods. The highest performance is achieved by the fine-tuned LLaMA-3-8B-Instruct model trained for 15 epochs, attaining scores of 0.77 (BLEU-4), 0.91 (ROUGE-1), 0.86 (ROUGE-2), 0.90 (ROUGE-L), and 0.85 (RelaxedEM on all test cases). These results indicate that, across both syntactic-level (BLEU, ROUGE) and execution-level (RelaxedEM) evaluations, SPARQL queries generated by finetuned models are not only more accurate but also structurally well-formed and executable. This highlights the effectiveness of supervised adaptation for learning the ontology and structure of the underlying SKG during training.", "Model Size Impact As shown in Fig 3, LLaMA-3-8B-Instruct consistently outperforms LLaMA-3.2-3B-Instruct after fine-tuning across all evaluation metrics. This demonstrates that larger model capacity enhances the ability to internalize domain-specific patterns from training data, including the structure and semantics of the target SKG. Interestingly, the trend reverses in the one-shot setting: LLaMA-3.2-3B-Instruct performs better than the 8B variant on most metrics, except for RelaxedEM(success), as shown in Table 1. This performance gap might be attributed to the fact that LLaMA-3.2-3B-Instruct was released after LLaMA-3-8B-Instruct and incorporates pruning and distillation techniques, which were specifically applied to the 1B and 3B variants [1]. These techniques help to preserve performance while significantly improving efficiency, making the LLaMA-3.2-3B-Instruct model capable of strong instruction-following performance on resource-constrained devices. As a result, despite its smaller size, LLaMA-3.2-3B-Instruct may benefit from a more refined architecture and training strategies, allowing it to better leverage in-context examples in one-shot settings compared to the larger, but earlier, 8B model.\n\nBLEU-4 Avg ROUGE-L 0.875 0.70 0.825 0,65 0.800 0.60 0,750 0,725 Epochs RelaxedEM (success) RelaxedEM (all) Avg\n\nFig. 3: Average BLEU-4, ROUGE-L and RelaxedEM scores with different epochs on different fine-tuned models", "RAG Performance As shown in Table 1, the score of RelaxedEM(all) drops from 0.85 to 0.29 when incorporating RAG into the fine-tuned LLaMA-3-8BInstruct trained for 15 epochs, which does not lead to additional performance gains, and it even degrades performance. This decline can be attributed to the noisy or misaligned nature of the retrieved context-such as incorrect or irrelevant property suggestions from the ORKG-which may introduce confusion instead of providing useful guidance since we don't have a context checker to check if the context is relevant or not. Prior studies [10, 11] have similarly observed that low-quality RAG context can conflict with the knowledge already encoded in fine-tuned models, ultimately leading to reduced task performance.", "One-Shot Performance As shown in Table 1, the one-shot setting-using the most similar example from the training set-achieved strong performance, second only to the fine-tuned models. Specifically, the one-shot approach reached scores of 0.58 (BLEU-4), 0.81 (ROUGE-1), 0.73 (ROUGE-2), 0.78 (ROUGEL), and 0.40 (RelaxedEM on all test cases) on LLaMA-3.2-3B-Instruct model. These results suggest that in the absence of high-quality datasets for fine-tuning, one-shot learning offers a simple yet effective alternative for SPARQL query generation.", "Training Epoch Sensitivity As shown in Figure 3, the number of fine-tuning epochs has a significant impact on all metrics for both LLaMA-3.2-3B-Instruct and LLaMA-3-8B-Instruct models. First, both models start with high scores on all metrics at epoch 3 and then slightly decline during the early training phases (epochs 3-7), suggesting that the models may require sufficient training time to properly internalize the SKG-specific structures and semantics. Second, the training dynamics reveal an upward trend in performance from 7 epochs onward, with the best performance at 20 epochs for the 3B model and 15 epochs for the 8B model. This indicates that larger models tend to converge faster and exhibit stronger generalization early on, while smaller models require more epochs to achieve competitive performance.", "6.2 Error analysis on failed SPARQL queries\n\nWe further analyzed the generated SPARQL queries that either failed to execute or returned empty results, by comparing them with the corresponding ground truth queries. Under the best-performing configuration-using the fine-tuned LLaMA3-8B-Instruct model trained for 15 epochs-448 out of 513 generated SPARQL queries were executed successfully via QLever without any syntax errors and returned meaningful results. Meanwhile, 14 queries failed due to syntax errors, and 51 queries were executed successfully but returned empty results.", "To better understand the causes of failure, we examined the error messages for the 14 syntactically invalid queries and inspected the queries that returned empty results. Our analysis revealed that 11 out of the 14 syntactically invalid queries shared the same error message:", "Invalid SPARQL query: Variable ?metric is selected but not aggregated. All non-aggregated variables must be part of the GROUP BY clause. Note: The GROUP BY in this query is implicit because an aggregate expression was used in the SELECT clause.", "This indicates that these queries included aggregate functions (e.g., MAX(?value) ) in the SELECT clause but did not include the non-aggregated variables (e.g., ?metric , ?metric_lbl ) in a GROUP BY clause. In SPARQL 1.1, such usage is invalid unless all non-aggregated variables are explicitly grouped. This reflects a lack of adherence to SPARQL's syntax rules around aggregation and grouping.\n\nThe remaining 3 queries failed with the following error:\n\nInvalid SPARQL query: Token 'SELECT': mismatched input 'SELECT' expecting '}'."]}
{"type": "entry", "index": 27, "question": "hybrid retrieval in rag pros and cons", "answer": "Pros of hybrid retrieval in RAG:\n\n1. Better coverage and relevance\n- Hybrid retrievers combine lexical (keyword) and semantic (embedding) search, so they can catch both exact term matches and paraphrased or conceptually related content. ixi-RAG hybrid fuses BM25-style lexical scores with semantic scores using Reciprocal Rank Fusion to balance both views and produce a \u201cmore robustly ordered list\u201d of results [1].\n- In code-completion RAG, combining BM25 with semantic retrievers (like GTE-Qwen) yields substantially higher metrics than either alone, especially with larger models, showing that the two methods retrieve largely different but complementary results [4].\n- For multimodal tasks, hybrid orchestration of text and image search agents (QA\u2011Dragon) improves answer accuracy versus treating modalities separately [3].\n\n2. Reduced \u201cneedle in a haystack\u201d issues\n- HybridRAG (knowledge graph + semantic search) improved faithfulness, answer relevance, and context recall, helping the system surface more of the actually useful passages from large corpora [2].\n\nCons / trade-offs of hybrid retrieval:\n\n1. Lower precision / more extraneous context\n- HybridRAG underperforms pure knowledge-graph or pure vector RAG on context precision, meaning it brings in more extra or loosely useful text along with the relevant pieces [2]. This can bloat prompt context and make generation harder.\n\n2. Complexity and cost\n- Orchestrating multiple retrievers or agents (e.g., text + image search, or KG + vector search) increases system complexity compared with a single retriever [2][3]. Building and maintaining the additional structures (like knowledge graphs) can be computationally intensive [2].\n\n3. Model-dependence\n- In the code-completion setting, hybrid BM25+semantic helps mid- to large-scale LLMs (7B+), but offers limited or even negative gains for smaller models, suggesting that not all models can fully exploit hybrid retrieval [4].", "tags": ["hybrid_vs_kg"], "full_faithfulness": 1.0, "full_answer_relevancy": 0.9591360382, "seed_faithfulness": 1.0, "seed_answer_relevancy": 0.9591378159, "context_relevance": 0.8600293104155258, "context_relevance_error": null, "context_relevance_reason": "The context passages provide a comprehensive overview of Retrieval-Augmented Generation (RAG), including its hybrid nature, advantages, and challenges, which directly relate to the user's inquiry about the pros and cons of hybrid retrieval in RAG. While the passages effectively cover the strengths of RAG, such as improved accuracy and contextual relevance, they also address challenges like the optimization of retrieval processes. However, the response could be improved by explicitly summarizing the pros and cons in a more structured manner.", "structure_citation": 0.7850432005702632, "structure_error": null, "structure_reason": "The Actual Output includes several claims supported by citations, and most of the citations correspond to relevant snippets in the Context. However, there are some instances where the claims made are not fully substantiated by the provided snippets, particularly regarding the specifics of hybrid retrieval's performance metrics. Additionally, while the citations are mostly accurate, there are minor discrepancies in the context of the claims made, which slightly detracts from the overall alignment with the evaluation steps.", "hallucination_score": 0.8842476998401338, "hallucination_error": null, "hallucination_reason": "The response accurately identifies and elaborates on the pros and cons of hybrid retrieval in RAG, aligning well with the factual claims in the context. It effectively discusses improved coverage and relevance, reduced 'needle in a haystack' issues, and the trade-offs of lower precision, complexity, and model-dependence. However, while the claims are well-supported, the response could benefit from more explicit references to the context provided, particularly in terms of specific studies or examples mentioned in the context.", "contexts_used": ["Retrieval-Augmented Generation (RAG)\n\nRAGis a hybrid approach that integrates retrieval-based and generation-based techniques to improve the performance of language models on knowledge-intensive tasks. By leveraging external knowledge sources, RAG enables models to produce more accurate and contextually relevant responses. The paradigm has gained significant attention due to its ability to combine the strengths of retrieving pertinent documents and generating coherent text. In the real-world, RAG systems are widely adopted with online web search ( e.g., OpenAI (Hurst et al. 2024), and Gemini (Comanici et al. 2025)) and industrial domains with credential documents.", "Early research on RAG established its effectiveness across various natural language processing tasks (Lewis et al. 2020). Subsequent studies have proposed advancements, such as improved retrieval mechanisms using dense retrieval methods (Karpukhin et al. 2020) and the integration of structured knowledge bases like databases or graphs (Edge et al. 2024). RAG has also shown promise in multi-task and few-shot learning scenarios (Izacard et al. 2023), where the retrieval component compensates for limited training data by accessing external information. However, challenges remain, particularly in optimizing the retrieval process, which depends heavily on the quality of the input query-an issue that motivates the exploration of query rewriting (Ma et al. 2023).", "RAG System Implementation\n\nIn this experiment, we implemented two distinct RetrievalAugmented Generation (RAG) systems. The first is a multimodal RAG chatbot that generates responses using multimodal embeddings (derived from image-based documents)\n\ncombined with user queries. The second is a text-based RAG chatbot that utilizes parsed text chunks.", "For the multi-modal RAG system, we employed ColQwen2.5-3B (Faysse et al. 2024) for both document parsing and retrieval. We set the MAX_TOKEN limit for image encoding to 764, aligning it with the token size used for text encoding. To optimize multi-modal RAG performance, we pre-computed embeddings for image-based documents and performed retrieval without late interaction.", "For the text-based RAG system, we utilized an inhouse document parser, AI Parser , to convert documents (e.g., PDFs) into text chunks. We adopted three in-house retrievers: ixi-RAG lexical , ixi-RAG semantic , and ixi-RAG hybrid .", "ixi-RAG lexical is a traditional information retrieval system built upon an OpenSearch index and utilizing the BM25 scoring algorithm. This retriever operates by matching the exact tokens present in the user's query against the text chunks in the knowledge base. It excels at precision when the query contains specific terms, acronyms, or identifiers that are also present in the source documents, as its ranking is based on term frequency (TF) and inverse document frequency (IDF).", "ixi-RAG semantic is a modern retrieval system that operates on the principle of conceptual similarity rather than keyword matching. To this end, we utilized ixi-DocEmbedding , an embedding model specialized for Korean documents. The model was further trained on Korean query-document pairs using a domain-specific finetuning approach based on BGE-M3 (Chen et al. 2024). For training the retriever, we constructed a comprehensive corpus of Korean query-document pairs drawn from three sources: (1) carefully-curated open-source datasets, (2) high-quality synthetic pairs generated with large language models, and (3) production-grade interaction logs collected from deployed RAG systems. To enhance the learning signal, we employed iterative hard-negative mining and trained the encoder according to the BGE-M3 fine-tuning recipe, which couples InfoNCE contrastive objectives with selfdistillation and model-merging strategies. After training, we only use the dense retrieval as the semantic retriever. Empirically, the resulting model demonstrates performance on par with state-of-the-art open-source Korean encoders and achieves substantial improvements on internal RAG evaluation benchmarks. It converts both the user's query and the document chunks into high-dimensional vector embeddings. Retrieval is then performed by conducting a k-nearest neighbor (k-NN) (Fix 1985) search within this vector space, identifying documents whose embeddings are closest to the query's embedding. This approach allows the system to understand the user's intent and retrieve relevant information even if the phrasing is different and there is no direct keyword overlap.", "ixi-RAG hybrid combines both lexical and semantic approaches for improved retrieval. It is designed to leverage the complementary strengths of both retrieval paradigms. It first gathers a broad set of candidate documents by running lexical and semantic searches in parallel. The ranked lists from both retrievers are then fused using the Recip-", "| | Query Rewriter | Document Recognizer | Document Retriever | Train Data |\n|---:|:------------------|:----------------------|:---------------------|:---------------|\n| 0 | RL-QR multi-modal | ColQwen2.5-v0.2 | ColQwen2.5-v0.2 | D mm D tm D tm |\n| 1 | RL-QR lexical | AI Parser | ixi-RAG lexical | |\n| 2 | RL-QR semantic | AI Parser | ixi-RAG semantic | |\n| 3 | RL-QR hybrid | AI Parser | ixi-RAG hybrid | D tm |\n\nTable 2: RL-QR Training Scheme\n\nrocal Rank Fusion (RRF) algorithm (Cormack, Clarke, and Buettcher 2009). RRF calculates a new score for each document based on its position in the individual rankings, producing a single, more robustly ordered list that balances keyword relevance with semantic similarity. This method effectively captures both the precision of lexical search and the contextual understanding of semantic search to deliver a highly refined final ranking.", "Results\n\nRetrieval Performance on Multi-modal RAG. As shown in Table 3, RL-QRmulti-modal achieves an NDCG@3 score of 82.10, representing a relative improvement of over 11% compared to the original query. Notably, RL-QR hybrid also demonstrates performance gains despite being trained on a different RAG system and dataset. In contrast, other models exhibit a decline in retrieval effectiveness. The baseline model, Qwen3-4B , fails to contribute positively to retrieval performance.", "Retrieval Performance with Lexical Retriever. As reported in Table 4, RL-QR lexical yields a substantial improvement over the original query, achieving a relative gain of 9%. RL-QRhybrid also outperforms the raw query with a mod-", "2 Related Works\n\nFurther refinement in reranking approaches is demonstrated by Xuegang Ma et al., who fine-tuned a LLaMA-2-7B model for reranking in their RankLLaMA system [22]. Although RankLLaMA achieves superior reranking performance, it again relies on a large model comparable to LLaMA-2-7B. Such reliance on resource-intensive models undermines the overarching objective of Goal 4, as the additional computational and financial burden associated with hosting these rerankers contradicts the need for efficiency and cost reduction.", "In summary, while reranker models can mitigate the 'needle in a haystack' problem (Goal 2) by filtering out least relevant information, they are not a sustainable solution. Their requirement for additional large-scale models significantly increases computational and monetary costs, thereby violating Goal 4. Consequently, alternative strategies that achieve comparable retrieval performance without incurring such overhead are essential for the advancement of practical and efficient RAG systems.", "Knowledge Graph RAG methods: Knowledge graph-based solutions have been proposed as a promising alternative for enhancing the RAG process. The core idea behind employing knowledge graphs is to address semantic search challenges by organizing data according to intrinsic relationships. For example, although not originally applied within a RAG context, Dawei Cheng et al.'s 2020 work on constructing knowledge graphs for a financial price prediction tool [23] illustrates how grouping semantically related data into distinct sub-networks enables a query-such as 'who quit Apple'-to retrieve only the pertinent subset of information, rather than returning all semantically similar sentences (e.g., both 'Steve Jobs quits Apple' and 'David Peter leaves Starbucks'). This targeted retrieval helps mitigate the \"needle in a haystack\" problem (Goal 2) and increases the rate of retrieving relevant passages (Goal 3).", "Building on these insights, recent research has directly integrated knowledge graphs within RAG frameworks. For instance, Zhentao Xu et al. [24] developed a system for LinkedIn's customer question answering service by first", "creating a custom knowledge graph and then retrieving related nodes. The retrieved information is subsequently refined using GPT-4 to produce coherent responses. When compared with a baseline semantic search system, this pipeline significantly improved retrieval metrics-including Mean Reciprocal Rank, Recall, and Normalized Discounted Cumulative Gain-as well as evaluation measures such as BLEU, METEOR, and ROUGE, thereby supporting Goals 2 and 3.", "However, despite these promising advances, knowledge graph-based approaches fall short of meeting Goal 1. The process of constructing and training knowledge graphs is computationally intensive and time-consuming, which directly conflicts with the objective of reducing information store creation times.", "| | Model | EM | Fl Score | Accuracy | Hallucination |\n|---:|:--------------|-----:|:-----------|:-----------|:----------------|\n| 0 | Human | 63 | | | |\n| 1 | MHQA-GRN | 33.2 | | | |\n| 2 | Embedding-RAG | 28 | 37 | 46 | 30 |\n| 3 | KG-RAG | 19 | | 32 | 15 |\n\nFigure 5: Results comparing knowledge graph framework with semantic search methods [25].", "For example, Diego Sanmart\u00edn's 2024 work [25] demonstrated a Knowledge Graph RAG system that leverages LLMs to extract entity-relationship triples from unstructured text and employs a 'Chain of Explorations' retrieval strategy. While this approach shows potential for handling vague queries, the high training times required to build and maintain the knowledge graph significantly increase the overall overhead. Moreover, the compressed storage format-using entity-relationship triples-can lead to a loss of detailed contextual information, thereby limiting the system's ability to fully expand upon relevant topics during retrieval hence resulting in even lower EM, F1, Accuracy scores with only an improvement in Hallucination compared to a standard Semantic Search method [Figure 5].", "In summary, while knowledge graph-based solutions offer clear benefits in terms of improving semantic search and enhancing the retrieval of relevant passages (addressing Goals 2 and 3), they do not currently satisfy Goal 1 due to their high training times and associated computational costs. Addressing these inefficiencies is essential for realizing the full potential of knowledge graphs within efficient and practical RAG frameworks.", "Combining Knowledge Graphs and Vector based Search methods: The limitations inherent in a purely knowledge graph-based RAG framework suggest that combining the strengths of knowledge graphs with traditional semantic search methods may yield superior results. Bhaskarjit Sarmah et al. [26] propose such an integrated approach through their HybridRAG framework. This framework concatenates outputs from a knowledge graph query-implemented using Microsoft's GraphRAG [27] with content indexed by extracting entity relationships in chunk sizes of 1024-with results from a semantic search, which similarly employs chunk sizes of 1024 and the text-embedding-ada-002 model. The aggregated context is then provided to GPT-3.5 to generate the final answer. The dataset for this study was constructed by web scraping earnings call documents of companies listed on the NIFTY 50, and evaluation was performed using the RAGAS evaluation platform [28].", "The HybridRAG framework demonstrated improvements in metrics such as Faithfulness, Answer Relevancy Score, and Context Recall. These enhancements support Goal 2 by reducing the \"needle in a haystack\" problem and Goal 3 by increasing the rate of retrieving relevant passages for answering questions. However, the framework underperformed in Context Precision when compared to standalone implementations of both knowledge graph-based and vector database-based RAG systems. This shortcoming indicates that the current integration does not fully leverage the advantages of traditional vector search techniques, particularly hybrid search techniques, which is critical for fully meeting Goal 3.", "6 Conclusion\n\nIn this work, we proposed the QA-Dragon , a query-aware dynamic RAG system tailored for complex and knowledge-intensive VQA tasks. Unlike the traditional RAG system, which retrieves the text and image separately, the QA-Dragon integrates a domain router, a search router, and orchestrates hybrid retrieval strategies via dedicated text and image search agents. Our system supports multimodal, multi-turn, and multi-hop reasoning through orchestrating both text and image search agents in a hybrid setup. Extensive evaluations on the Meta CRAG-MM Challenge (KDD Cup 2025) demonstrate that our system significantly improves answer accuracy, surpassing other strong baselines.", "C. RQ3: Exploration on Retreived Results\n\nDue to the superior performance of similarity-based RAG, we conduct an exploratory analysis of retrieved results from lexical and semantic retrieval techniques. The results reveal minimal overlap between BM25 and semantic techniques: out of 100 test examples in our benchmark, there are 76, 74, and 64 completely distinct retrieved samples when comparing BM25 with UniXcoder, CoCoSoDa, and GTE-Qwen, respectively. This difference in retrieval distributions motivates us to explore the combination of these techniques.", "As shown in Table III, the effectiveness of combining BM25 with different semantic retrieval techniques becomes more pronounced as model size increases. In the 200B+ scale, the BM25+GTE-Qwen combination achieves CB/ES metrics of 63.62/75.26 for DeepSeek-V3, substantially outperforming both individual techniques. Similarly, other semantic retrieval techniques also benefit from the combination of BM25. DeepSeek-V2.5 improves CB/ES metrics from 48.16/63.85 to 56.68/68.20 when combined with BM25. Notably, the advantage of BM25+GTE-Qwen combination is particularly striking for Qwen2.5-32B, which achieves impressive CB/ES scores of 63.73/72.25, rivaling or even surpassing models with significantly larger parameters such as DeepSeek-V2.5 and DeepSeek-V3. However, for smaller models (below 7B), the combination shows limited or even negative impact, suggesting that the complementary benefits of hybrid retrieval methods are more effectively leveraged by larger models.", "Finding 3: Lexical and semantic retrieval techniques exhibit distinct retrieval results distribution and demonstrate complementary characteristics in larger-scale models (7B+). With their combination, especially BM25+GTEQwen, similarity-based RAG achieves optimal performance in most LLMs."]}
{"type": "entry", "index": 28, "question": "retriever reranker differences and which helps rag more", "answer": "Retriever and reranker play different roles in a RAG pipeline.\n\n1) What a retriever does\nThe retriever is the first stage. It searches the whole collection and pulls back a small set of candidate passages that might answer the question. This can use sparse (keyword), dense (embedding), or hybrid methods that mix both to improve recall and handle different query types [4]. Good retrievers (including hybrid ones) mainly determine **whether the right document is even in the candidate set**.\n\n2) What a reranker does\nThe reranker is a second stage. It takes the already-retrieved candidates and **reorders** them by relevance. Instead of encoding query and document separately, it reads them together and scores how well they match, capturing fine\u2011grained semantic relationships [2]. In RAG pipelines this is often called \u201ctwo\u2011step retrieval\u201d [4].\n\n3) Which helps RAG more?\nIf the base retriever is weak, reranking cannot fix missing or totally irrelevant documents; it can only sort what it is given. In complex, mixed-knowledge settings, improving ranking with reranking gave only marginal gains, because the retriever itself lacked or missed the needed knowledge [1]. This suggests that in such settings, **better retrieval (or routing across sources) matters more than reranking**.\n\nHowever, when the retriever already pulls in somewhat relevant content, adding a reranker can noticeably improve QA accuracy: e.g., combining adaptive retrieval (DPR\u2011AIS) with reranking further boosts exact-match and F1 over strong non\u2011reranking baselines [3]. Domain\u2011tuned rerankers can significantly refine top results for specialized areas like geoscience [2].\n\n4) Trade\u2011offs\nRerankers often require extra, sometimes large, models, increasing cost and latency, so they are not always a sustainable solution despite helping reduce \u201cneedle in a haystack\u201d noise [4].\n\nIn practice: prioritize a strong, possibly hybrid retriever; add reranking when you already have decent recall and can afford the extra compute.", "tags": ["rerankers"], "full_faithfulness": 0.9230769231, "full_answer_relevancy": 0.8908844492, "seed_faithfulness": 0.7692307692, "seed_answer_relevancy": 0.8908844492, "context_relevance": 0.38868841295433315, "context_relevance_error": null, "context_relevance_reason": "The context passages provide some relevant information about retrieval and reranking in RAG systems, but they do not directly address the user's question about the differences between retrievers and rerankers or which is more beneficial for RAG. While there are mentions of reranking and its effects on retrieval performance, the overall relevance to the specific question is limited, leading to a low score.", "structure_citation": 0.733627273433527, "structure_error": null, "structure_reason": "The Actual Output effectively describes the roles of retrievers and rerankers in a RAG pipeline, aligning well with the evaluation steps. It includes claims with [n] markers, specifically referencing snippets from the Context. However, while most claims are supported by the cited snippets, there are minor inconsistencies in the depth of explanation regarding the trade-offs and effectiveness of rerankers, which could have been better substantiated with additional context references. Overall, the response is informative and mostly well-cited, but slight improvements in citation consistency could enhance clarity.", "hallucination_score": 0.831658031377877, "hallucination_error": null, "hallucination_reason": "The response accurately describes the roles of retrievers and rerankers in a RAG pipeline, aligning well with the claims made in the input. It effectively explains how a retriever pulls candidate passages and how a reranker reorders them based on relevance. However, while it mentions that a weak retriever limits the effectiveness of reranking, it does not explicitly reference the context provided, which discusses the marginal gains from reranking in mixture-of-knowledge scenarios. This omission slightly detracts from the overall alignment with the evaluation steps.", "contexts_used": ["2 Experiments\n\n2.2 RQ1: Effectiveness of RAG under Mixture-of-Knowledge Scenarios.\n\n1.64 1.81 0.62 0.57 0.85 0.45 1.13 0.91 (e) Qwen-3-32B Pubmed Wiki Pes2o C4 Github Math SE Book Arxiv CC SimpleQA SciQ ARC-C MMLU MMLU-Pro CSBench 0.72 16.28 3.14 0.90 1.20 0.88 1.46 4.35 1.10 1.40 0.30 2.00 0.50 1.90 1.70 0.60 2.60 0.17 0.09 0.00 0.26 0.34 0.44 0.25 0.37 0.22 0.39 0.20 0.43 0.68 0.96 0.49 0.64 0.48 0.52 0.82 0.79 0.74 0.40 0.51 0.28 (f) GPT-4o\n\nand mid-sized models, while the gains diminish as model size increases. For the strongest models, retrieval offers marginal or even slightly negative gains, suggesting that larger models are increasingly able to internalize domain knowledge without the need for external retrieval. These results highlight the generalizability of our findings and demonstrate that the impact of retrieval augmentation is robust across different knowledge domains.", "2.3 RQ 2: Instance-level Analysis of Multiple Retrieval Sources\n\nTo investigate whether different retrieval sources provide unique advantages, we conduct an instancelevel study (Figure 2) measuring the proportion of queries that can be solved only by retrieving from a specific corpus-compared to using all sources or no retrieval at all. Our results show that a significant fraction of cases (e.g., 8%-39% for Llama-3.18B) depend exclusively on retrieval from particular corpora. Crucially, no single retrieval source consistently outperforms others across all query types , highlighting the need for dynamic, query-specific routing to the most relevant corpus to maximize RAG performance in heterogeneous, real-world knowledge environments.", "2.4 RQ 3: Effectiveness of Reranking\n\nOne possible reason for the limited improvement in RAG performance is the inherent limitations of", "Figure 3: Performance with rerank on retrieval effectiveness across different datasets and models.\n\nPubmedWiki Pes2o GithubMath Book Arxiv All llama-3.2-3b llama-3.1-8b qwen3-4b qwen3-8b qwen3-32b gpt-4o-mini gpt-4o -0.63 7.11 -0.36\n\n-0.57 -0.23 -0.49 -0.07 -0.58\n\n2.04 8.49 3.02 31.90 4.42 9.05 3.93 4.32 3.67 4.94 3.19 12.8934.09 -0.82 3.50 0.21\n\n-0.70 -0.60 -0.70 -0.48 -0.75\n\n0.77 -0.85 2.93 0.14\n\n-0.76 -0.62 -0.69 -0.45 -0.76\n\n0.69\n3.81\n1.99\n\n-0.71 -0.13 -0.78 -0.68 -0.75 -0.58 -0.81\n\n0.27\n2.53\n-0.46\n\n-0.33 -0.01 -0.39 -0.29 -0.36 -0.26 -0.32\n\n1.89\n-0.24\n0.23\n\n-0.27 -0.24 -0.27 -0.24 -0.25 -0.26 -0.23 -0.17\n\n(a) SimpleQA -0.22 -0.10 0.24 -0.26 0.11 -0.20 -0.32 0.41 -0.01 0.06 0.03 -0.14 0.46 -0.15 0.07 -0.09 0.19 -0.11 -0.19 0.18\n\n-0.25 -0.13 -0.21 -0.01 -0.23 -0.06 -0.20 -0.08 -0.29 -0.02\n\n-0.32 -0.15 -0.24 -0.10 -0.28 -0.11 -0.27 -0.11 -0.35 -0.07 -0.01\n\n-0.20 -0.15 -0.25 -0.10 -0.19 -0.14 -0.18 -0.14 -0.24 -0.09 -0.05\n\n-0.14 -0.13 -0.18 -0.07 -0.13 -0.09 -0.13 -0.10 -0.13 -0.08 -0.04\n\n(b) SciQ", "-0.32 -0.15 -0.24 -0.10 -0.28 -0.11 -0.27 -0.11 -0.35 -0.07 -0.01\n\n-0.20 -0.15 -0.25 -0.10 -0.19 -0.14 -0.18 -0.14 -0.24 -0.09 -0.05\n\n-0.14 -0.13 -0.18 -0.07 -0.13 -0.09 -0.13 -0.10 -0.13 -0.08 -0.04\n\n(c) ARC-C -0.00 0.05 -0.03 0.04 -0.05 0.10 0.16 0.01 0.02 0.15 -0.04 -0.00 -0.02 -0.04 -0.02 -0.05 -0.06 -0.04 -0.01 -0.02\n\n-0.03 -0.01 -0.04 -0.00 -0.05\n\n-0.06 -0.02 -0.03 -0.02 -0.04 -0.02 -0.05 -0.02 -0.05 -0.01\n\n-0.03 -0.02 -0.02 -0.02 -0.03 -0.02 -0.03 -0.02 -0.04 -0.02 -0.01\n\n-0.01 -0.02 -0.02 -0.01 -0.01 -0.01 -0.01 -0.01 -0.01 -0.01 -0.01\n\n(d) MMLU -0.06 -0.00 -0.04 -0.07 0.12 0.31 -0.04\n\n-0.08 -0.02 -0.04 -0.02 -0.05 -0.02 -0.04 -0.01 -0.06 -0.01\n\n-0.06 -0.02 -0.05 -0.03 -0.08 -0.02 -0.07 -0.02 -0.09 -0.01", "-0.02 -0.01 -0.02 -0.02 -0.01 -0.00 -0.01 -0.01 -0.02 -0.01\n\n(e) MMLU-Pro\n0.08\n0.13\n\n-0.12 -0.01 -0.04 -0.01 -0.06\n\n-0.05 -0.01 -0.09 -0.03\n\n-0.14 -0.06 -0.09 -0.05 -0.07 -0.05 -0.08 -0.07 -0.11 -0.04 -0.03\n\n-0.10 -0.06 -0.09 -0.06 -0.06 -0.04 -0.04 -0.04 -0.09 -0.04 -0.05\n\n-0.03 -0.01 -0.02 -0.02 -0.02 -0.01 -0.00 -0.00 -0.01 -0.01\n\n-0.02 -0.01 -0.02\n\n-0.02 -0.01 -0.01 -0.00 -0.01 -0.00 -0.00\n\n(f) CSBench\n\nFigure 4: Performance comparison of routing strategies across MMLU and MMLU-Pro datasets.\n\nmmlu_pro 0.0 0.2 0.4 0.6 0.8 Accuracy no retrieval all sources qwen3-4b (plain) qwen3-8b (plain) qwen3-32b (plain) qwen3-4b (cot) qwen3-8b (cot) qwen3-32b (cot) Upper Bound", "retrievers. To investigate this, inspired by prior work demonstrating benefits from adding reranking to the RAG pipeline (Shao et al., 2024; Yu et al., 2024), we apply reranking to the top-30 retrieved results. As shown in Figure 3, reranking yields only marginal gains across datasets. This suggests that improving retrieval quality through reranking is insufficient in mixture-of-knowledge scenarios. The retriever's limited capacity and restricted access to relevant knowledge highlight the need for deeper integration between knowledge sources, retrieval mechanisms, and generative models.", "2.5 RQ 4: Evaluating LLMs as Routers for Mixture-of-Knowledge Retrieval\n\nPrevious analyses highlight the need for adaptive retrieval mechanisms that dynamically route queries to the most relevant corpus based on topic context. Here, we investigate whether current LLMs can effectively perform query routing. Figure 4 evaluates LLMs from the Qwen-3 series (4B, 8B, and 32B parameters) as 'routers' that select among heterogeneous knowledge sources at inference time. We compare plain prompting versus chain-of-thought prompting on the MMLU and MMLU-Pro datasets, which cover diverse general and professional domains.", "2.5 RQ 4: Evaluating LLMs as Routers for Mixture-of-Knowledge Retrieval\n\nSpecifically, we benchmark: (1) no-retrieval baselines; (2) static retrieval from all corpora ('all sources'); (3) LLM-prompted routing variants (plain and chain-of-thought); and (4) an oracle router upper bound. Surprisingly, neither prompting strategy consistently outperforms static retrieval. In fact, both routing approaches often underperform compared to simply retrieving from all sources, and occasionally fall below no-retrieval baselines. Chain-of-thought prompting provides only marginal improvements, while scaling model size yields negligible or even negative returns.", "2.5 RQ 4: Evaluating LLMs as Routers for Mixture-of-Knowledge Retrieval\n\nWe attribute this failure to two main factors: (1) Inaccurate relevance estimation: Without dedicated training, LLMs struggle to reliably identify which corpus holds the needed information, especially amid overlapping or stylistically diverse", "2.5 RQ 4: Evaluating LLMs as Routers for Mixture-of-Knowledge Retrieval\n\ncorpora. Minor routing errors propagate to poor retrieval quality. (2) Training-inference mismatch: LLM training typically lacks explicit multi-source comparison tasks, limiting the effectiveness of prompt-based routing as a meta-reasoning problem. Future directions should focus on learned routing modules trained with supervision or reinforcement learning, or on tightly integrated RAG systems that jointly optimize source selection and generation.", "d. Reranker Model Fine-tuning\n\nThe reranker is a crucial component in Retrieval-Augmented Generation (RAG) systems. It refines the initial set of retrieved candidates by reordering them based on their semantic relevance to the query. Unlike embedding models, which encode queries and documents independently into fixed-length", "vectors, the reranker jointly processes the query and document as input and directly predicts a relevance score. By modeling token-level interactions between the query and document, the reranker captures richer semantic relationships, resulting in improved ranking of the top candidates. Given a query-document pair, the model outputs a raw relevance score, which is typically passed through a sigmoid function to produce a normalized value in the range [ 0 , 1 ] .", "Model and Training methods. We adopt an industry-standard cross-encoding scheme for reranking and use the BGE-M3 model (Chen et al., 2024) as our base. BGE-M3 is known for its strong generalization capabilities and is supported by a robust open-source ecosystem, making it well-suited for real-time applications.", "During training, we feed both positive and negative samples corresponding to each query into the model simultaneously. To optimize ranking performance, we apply the Multiple Negative Ranking (MNR) loss, defined as:\n\nwhere \ud835\udc41 is the total number, \ud835\udc36 is the number of negative samples per query, \ud835\udc5d pos is the predicted relevance score for the positive document, and \ud835\udc5d neg \ud835\udc56 is the score for the \ud835\udc56 -th negative document. This loss encourages the model to rank the positive document higher than its negative counterparts, thereby improving retrieval precision in downstream tasks.", "Dataset and Quality Improvement. To maximize the reranker's performance in the geoscience domain, we curated a large training corpus composed of both open-source and domain-specific ranking pairs. The domain-specific data were synthesized using Qwen, conditioned on abstracts from open-access scholarly articles in geography sourced from arXiv.", "After assembling all positive query-document pairs, we applied SimANS (Zhou et al., 2022) to mine hard negative samples from the candidate corpus, thus forming a balanced and challenging training dataset. Since both synthetic and open-source data were generated in batches, individual data points may vary in quality. Even a small number of noisy or biased examples can significantly degrade reranking performance. To address this, we implemented a data quality filtering process to improve the reliability of training examples. For each positive pair, we used the LLaMA3 model to assess answer quality based on a set of predefined criteria. Each sample was assigned a quality score ranging from 0 to 3, following the standards detailed in Table 8. Only samples with sufficient quality were retained for training.Data with a quality label of 0 is excluded from training.", "| | quality label | quality standards |\n|---:|:----------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 0 | 3 2 1 0 | completely correct, detailed, and covers all key information with clear and easy-to-understand expression. mostly correct, covers most key information, and is clearly expressed but with minor shortcomings. generally correct but has some errors or omissions, with mostly clear expression. significant errors or omissions, lacks sufficient detail, and is not clearly expressed. |\n\nTable 8 | data quality standards\n\nWe ultimately utilized diverse types of paired data to train the model. Open-source data included:\n\nQA pairs: DuReader (He et al., 2017), NQ, LCQMC (Liu et al., 2018), HotpotQA, etc.", "Retrieval-oriented pairs: T2Ranking (Xie et al., 2023), mMARCO, MS MARCO, etc.\n\nSimilarity task pairs: NLI (Koreeda and Manning, 2021), NLI-zh, AFQMC (Lee et al., 2022), PAWSX (Yang et al., 2019), etc.\n\nMultilingual data: Mr. TyDi.", "In total, we incorporated over 360,000 open-source data pairs, along with 30,000 synthetically generated QA pairs specifically tailored for geological science applications.", "For training with contrastive objectives, negative sample quality plays a critical role in reranker performance. Whenever available, we retain the original negative samples provided within each dataset. However, many datasets either lack negatives entirely or contain an insufficient number of challenging examples. To ensure training robustness, we construct a set of at least 16 negative samples per query for all datasets.", "When additional negatives are required, we use a weak retriever to select hard negative candidates from a large-scale public Wiki corpus or domain-specific geoscience libraries curated from scholarly literature. We then apply the SimANS sampling strategy (Zhou et al., 2022) to prioritize harder examples from the candidate pool. Each negative sample \ud835\udc5b\ud835\udc52\ud835\udc54 \ud835\udc65 is assigned a sampling probability based on the squared difference between its retriever score and that of the corresponding positive sample:\n\nwhere \ud835\udc5b\ud835\udc52\ud835\udc54 score and \ud835\udc5d\ud835\udc5c\ud835\udc60 score are the scores returned by the weak retriever, and \ud835\udf0e is a task-specific hyperparameter that adjusts the sharpness of the sampling distribution. F Sample ( \ud835\udc5b\ud835\udc52\ud835\udc54 \ud835\udc65 ) represents the probability of selecting \ud835\udc5b\ud835\udc52\ud835\udc54 \ud835\udc65 as a training negative.", "Information retrieval and selection\n\nHowto retrieve and select relevant information is a key issue in RAG systems. As the remarkable success of advancing LLMsusing reinforcement learning (RL), many studies have adopted RL to train LLMs to enhance information retrieval and reranking (Asai et al. 2024; Jin et al. 2025; Li et al. 2025; Song et al. 2025; Yu et al. 2024). Also, a few studies (Jiang et al. 2025; Yan and Ling 2025) aligned the retriever's preferences with LLMs to retrieve more relevant information and improve the generation performance. These methods could achieve significant improvement but may suffer from high computational costs. In addition, with the accessibility to advanced reranking models, such as bge-reranker (Xiao et al. 2023) and Qwen3-reranker (Zhang et al. 2025b),", "a straightforward method is to first rerank the retrievals using a reranker and then select the topk relevant retrievals (Glass et al. 2022; Chang et al. 2025). This method could further select retrievals with a higher semantic similarity with the query and thus enhance the generation performance. However, it may not perform well or even degrade answer accuracy in QA tasks due to the sparse query or large corpus (Kim et al. 2024; Jiang et al. 2025).", "Main results\n\nTab. 2 summarizes the performance of PAIRS and its variants against a range of baselines on six QA datasets. We report both EM and F1 scores, along with the retriever activation ratio ( RA ratio ) of PAIRS across datasets. Overall, the proposed PAIRS framework consistently outperforms baseline methods. Specifically, (1) PAIRS surpasses all prior baselines-including Standard RAG, HyDE, Q2D, and CoT-with an average of 1.1% EM and 1.0% F1 score improvement while maintaining a retriever activation ratio of only 75.5%. This demonstrates PAIRS's ability to enhance both accuracy and efficiency by bypassing unnecessary retrieval for simple queries. (2) DPR-AIS , which integrates dual-path retrieval and adaptive information selection for all queries, further improves performance, achieving the highest average EM (35.60%) and F1 score (36.55%) among non-reranking models. The consistent gains across various datasets highlight the robustness of DPR-AIS in both opendomain and multi-hop QA scenarios. (3) DPR-AIS-rerank extends these gains even further by incorporating a reranker, respectively improving the EM and F1 score by 2.3% and 2.5% on average. Also, it improves over the Rerank baseline by 1.7% EM and 1.5% F1 score on average, reaching state-of-the-art results in 5 out of 6 datasets. These improvements validate the compatibility and synergy between DPR-AIS and post-retrieval reranking mechanisms. (4) Notably, on datasets like WebQuestions, where queries contain sparse signals and retrieval noise is common, PAIRS achieves higher performance than DPR-AIS, illustrating that adaptive retrieval can improve generation accuracy by skipping unhelpful or distracting documents.", "| | Method.Method | HotpotQA.EM | HotpotQA.F1 | NaturalQA.EM | NaturalQA.F1 | SQuAQ.EM | SQuAQ.F1 | TriviaQA.EM | TriviaQA.F1 | Average.EM | Average.F1 |\n|---:|:------------------------|--------------:|--------------:|---------------:|---------------:|-----------:|-----------:|--------------:|--------------:|-------------:|-------------:|\n| 0 | DPR-AIS | 36.61 | 47.02 | 37.42 | 38.95 | 30 | 35.87 | 59.09 | 33.32 | 40.78 | 38.79 |\n| 1 | w/o DPR (w/ q ) | 36.14 | 46.36 | 36.21 | 37.92 | 30.1 | 35.99 | 58.08 | 32.85 | 40.13 | 38.28 |\n| 2 | w/o DPR (w/ p ) | 35.04 | 44.9 | 36.73 | 38.09 | 28.34 | 34.17 | 57.77 | 32.4 | 39.47 | 37.39 |\n| 3 | w/o AIS (w/ 2 q + 1 p ) | 33.94 | 43.49 | 31.99 | 33.9 | 26.11 | 31.43 | 53.05 | 29.72 | 36.27 | 34.64 |\n| 4 | w/o AIS (w/ 1 q + 2 p ) | 28.55 | 37.38 | 28.73 | 31.53 | 22 | 26.76 | 48.76 | 27.38 | 32.01 | 30.76 |", "Table 3: Ablation experiments on four datasets, where w/o DPR (w/ q ) and w/o DPR (w/ q ) remove dual-path retrieval but only uses q and p to retrieve documents, respectively; and w/o AIS (w/ 2 q + 1 p ) and w/o AIS (w/ 1 q + 2 p ) use fixed ratios of retrieved documents instead of adaptive selection. Bold and underlined values represent the best and second-best scores, respectively.", "2 Related Works\n\nSpare and Dense Retriever RAG methods: Hybrid approaches, such as the \"Blended RAG\" framework proposed by Sawarkar et al. [12], demonstrate that combining multiple retrievers can improve the retrieval of relevant passages for clearly defined queries. This framework integrates six distinct retrievers-including traditional sparse methods (e.g., BM25) and dense semantic search techniques-selected based on their performance across benchmark datasets like Natural Questions (NQ) [13], TREC-COVID [14], SqUAD [15], and HotPotQA [16].", "A key strength of this hybrid search technique is its ability to return relevant passages, aligning well with Research Goal 3 for well-specified queries. However, these methods do not fully address Research Goal 3 when handling vaguer questions. In such cases, the hybrid approach can retrieve extraneous or unnecessary information, thus failing to eliminate the \"needle in a haystack\" problem. This shortcoming is evident in evaluations on the CoQA dataset [17], where retrieval accuracies ranged only from 45% to 49%. The reliance on document metadata during retrieval-a common feature in these systems-further complicates matters, as such metadata is often minimal in real-world scenarios (e.g., limited to document names and creation dates), thereby increasing both the creation time of the information store (contradicting Research Research Goal 1) and the risk of returning irrelevant passages (contradicting Research Research Goal 3) leading to longer contexts (contradicting Research Goal 2).", "Figure 3: Results of Priyanka Mandikal et al. on 2 example queries [18].\n\nQuery: Do fibroblasts from CF patients grow at a\n\nQuery: What is the prognosis for patients who have\n\nnormal rate?\n\nhad an episode of respiratory failure?\n\nMethod P@1 P@3 P@5 P@10 VSR 33.33 SPECTERZ Hybrid 66.67 Sample Retrieval This paper analyzes the cell cycle of cystic\n\nThe diagnosis, management and prognosis of 17 infants\n\nfibrosis-derived fibroblasts and statistically\n\nwith cystic fibrosis (CF) and severe respiratory disease\n\ndemonstrates their failure support normal\n\nwere reviewed for the\n\nperiod 1968 to 1972.\n\nThe clinical\n\npopulation doubling time in vitro and indicates that this\n\ncourse these infants was characterized by is due largely\n\nto a failure of the\n\ncystic fibrosis-derived\n\nfailure to thrive and\n\ncells to enter the cell cycle in normal numbers.\n\nWe have malnutrition Despite vigorous therapy including no explanation for significant difference antibiotics there was\n\n60% mortality:\n\nA delay in the\n\nbehavior of these cells from patients with cystic fibrosis\n\ndiagnosis of CF from the onset of respiratory symptoms\n\nvis-a-vis either disease control fibroblasts, with a\n\nmean of six weeks was considered an important\n\nsave to conclude that proceeds from an intrinsic\n\nfactor affecting survival. This data supports the need for\n\nproperty of the cells themselves.", "developing a reliable screen test for CF at birth.", "In contrast, recent work by Mandikal et al. [18] has advanced a hybrid retrieval model that integrates a sparse TF-IDF retriever with a dense retriever based on fine-tuned SPECTER2 embeddings [19]. Their approach employs a weighted scoring mechanism, controlled by a hyperparameter \u03bb , to balance the contributions of the two retrieval methods. Empirical evaluations on a medical database concerning Cystic Fibrosis indicate that while dense retrieval alone underperforms, incorporating approximately 20% of the sparse score (i.e., setting \u03bb = 0 . 2 for the sparse component) significantly improves document retrieval accuracies across different retrieval ranks [Figure 3]. Crucially, this method indexes documents solely based on their content, thereby avoiding the pitfalls of metadata dependency. This contributes to reducing information store creation times (Research Goal 1) and partially mitigates the retrieval of unnecessary information (Research Goal 3), although it does not entirely resolve the challenge of handling vaguer queries.", "In summary, while hybrid retrieval approaches represent a significant advancement by returning more relevant passages for well-defined queries complying with Research Goal 3. However they remain imperfect in filtering out unnecessary information for vaguer questions which can lead to longer contexts. This limitation underscores the need for further refinement to minimize the \"needle in a haystack\" problem in large-scale, real-world applications and make a pipeline which is compliant with Research Goal 2 as well.", "Reranker RAG methods: To address the limitation which Hybrid-Search methods have in adhering to Research Goal 2 for vaguer questions, Reranker methods have been developed to reduce the size and improve the quality of the final context.", "Figure 4: A high level view of what a RAG pipeline with a reranker looks like.\n\nIst Stage 2nd Stage Knowledge Embedding-based LLM-based Docs Corpus retrieval reranking\n\nUsually this is a process called 2-step retrieval where the first step involves retrieving documents such as through sparse or dense retriever techniques and then a second stage where an LLM based reranker comes into play to decide which of the documents retrieved are most relevant to the query and sorts them in order [Figure 4] before passing it to an LLM for generation.", "Prior to the formal conceptualization of Retrieval-Augmented Generation (RAG), information retrieval research explored the use of document rerankers. For example, Noguiera et al.'s 2020 study, Document Ranking with a Pretrained Sequence-to-Sequence Model [20], proposed employing a sequence-to-sequence model as a document reranker. This work evaluated multiple fine-tuned T5 models of varying sizes, as well as a fine-tuned BERT model, on documents initially retrieved using the sparse BM25 keyword method. The incorporation of these rerankers led to improvements in retrieval performance across the Robust04, Core17, and Core18 datasets. However, a critical limitation of this framework was its reliance on large models, leaving unresolved whether the observed performance gains were due to the reranking strategy or simply the use of more powerful models.", "Subsequent advancements in RAG have integrated BERT-based rerankers into new pipelines. Michael Glass et al.'s Re 2 G framework [21] embeds a BERT-based reranker-similar to that of Noguiera et al.-within a RAG system. This integration has shown notable improvements in metrics such as Recall (for 5 documents), overall recall, precision, and accuracy on several datasets, as indicated by performance on the KILT leaderboard. Nevertheless, a significant drawback of this approach is the necessity to host an additional model, which directly conflicts with Goal 4: alleviating the monetary and computational costs associated with deploying full-scale LLMs.", "Further refinement in reranking approaches is demonstrated by Xuegang Ma et al., who fine-tuned a LLaMA-2-7B model for reranking in their RankLLaMA system [22]. Although RankLLaMA achieves superior reranking performance, it again relies on a large model comparable to LLaMA-2-7B. Such reliance on resource-intensive models undermines the overarching objective of Goal 4, as the additional computational and financial burden associated with hosting these rerankers contradicts the need for efficiency and cost reduction.", "In summary, while reranker models can mitigate the 'needle in a haystack' problem (Goal 2) by filtering out least relevant information, they are not a sustainable solution. Their requirement for additional large-scale models significantly increases computational and monetary costs, thereby violating Goal 4. Consequently, alternative strategies that achieve comparable retrieval performance without incurring such overhead are essential for the advancement of practical and efficient RAG systems.", "Knowledge Graph RAG methods: Knowledge graph-based solutions have been proposed as a promising alternative for enhancing the RAG process. The core idea behind employing knowledge graphs is to address semantic search challenges by organizing data according to intrinsic relationships. For example, although not originally applied within a RAG context, Dawei Cheng et al.'s 2020 work on constructing knowledge graphs for a financial price prediction tool [23] illustrates how grouping semantically related data into distinct sub-networks enables a query-such as 'who quit Apple'-to retrieve only the pertinent subset of information, rather than returning all semantically similar sentences (e.g., both 'Steve Jobs quits Apple' and 'David Peter leaves Starbucks'). This targeted retrieval helps mitigate the \"needle in a haystack\" problem (Goal 2) and increases the rate of retrieving relevant passages (Goal 3).", "Building on these insights, recent research has directly integrated knowledge graphs within RAG frameworks. For instance, Zhentao Xu et al. [24] developed a system for LinkedIn's customer question answering service by first", "creating a custom knowledge graph and then retrieving related nodes. The retrieved information is subsequently refined using GPT-4 to produce coherent responses. When compared with a baseline semantic search system, this pipeline significantly improved retrieval metrics-including Mean Reciprocal Rank, Recall, and Normalized Discounted Cumulative Gain-as well as evaluation measures such as BLEU, METEOR, and ROUGE, thereby supporting Goals 2 and 3.", "However, despite these promising advances, knowledge graph-based approaches fall short of meeting Goal 1. The process of constructing and training knowledge graphs is computationally intensive and time-consuming, which directly conflicts with the objective of reducing information store creation times.", "| | Model | EM | Fl Score | Accuracy | Hallucination |\n|---:|:--------------|-----:|:-----------|:-----------|:----------------|\n| 0 | Human | 63 | | | |\n| 1 | MHQA-GRN | 33.2 | | | |\n| 2 | Embedding-RAG | 28 | 37 | 46 | 30 |\n| 3 | KG-RAG | 19 | | 32 | 15 |\n\nFigure 5: Results comparing knowledge graph framework with semantic search methods [25].", "For example, Diego Sanmart\u00edn's 2024 work [25] demonstrated a Knowledge Graph RAG system that leverages LLMs to extract entity-relationship triples from unstructured text and employs a 'Chain of Explorations' retrieval strategy. While this approach shows potential for handling vague queries, the high training times required to build and maintain the knowledge graph significantly increase the overall overhead. Moreover, the compressed storage format-using entity-relationship triples-can lead to a loss of detailed contextual information, thereby limiting the system's ability to fully expand upon relevant topics during retrieval hence resulting in even lower EM, F1, Accuracy scores with only an improvement in Hallucination compared to a standard Semantic Search method [Figure 5].", "In summary, while knowledge graph-based solutions offer clear benefits in terms of improving semantic search and enhancing the retrieval of relevant passages (addressing Goals 2 and 3), they do not currently satisfy Goal 1 due to their high training times and associated computational costs. Addressing these inefficiencies is essential for realizing the full potential of knowledge graphs within efficient and practical RAG frameworks.", "Combining Knowledge Graphs and Vector based Search methods: The limitations inherent in a purely knowledge graph-based RAG framework suggest that combining the strengths of knowledge graphs with traditional semantic search methods may yield superior results. Bhaskarjit Sarmah et al. [26] propose such an integrated approach through their HybridRAG framework. This framework concatenates outputs from a knowledge graph query-implemented using Microsoft's GraphRAG [27] with content indexed by extracting entity relationships in chunk sizes of 1024-with results from a semantic search, which similarly employs chunk sizes of 1024 and the text-embedding-ada-002 model. The aggregated context is then provided to GPT-3.5 to generate the final answer. The dataset for this study was constructed by web scraping earnings call documents of companies listed on the NIFTY 50, and evaluation was performed using the RAGAS evaluation platform [28].", "The HybridRAG framework demonstrated improvements in metrics such as Faithfulness, Answer Relevancy Score, and Context Recall. These enhancements support Goal 2 by reducing the \"needle in a haystack\" problem and Goal 3 by increasing the rate of retrieving relevant passages for answering questions. However, the framework underperformed in Context Precision when compared to standalone implementations of both knowledge graph-based and vector database-based RAG systems. This shortcoming indicates that the current integration does not fully leverage the advantages of traditional vector search techniques, particularly hybrid search techniques, which is critical for fully meeting Goal 3."]}
{"type": "entry", "index": 29, "question": "best rag eval benchmarks and why?", "answer": "There isn\u2019t a single \u201cbest\u201d RAG benchmark; you normally need a small suite that covers different failure modes.\n\n1) General retrieval quality\n- BEIR: 17 heterogeneous search tasks, great for testing if your retriever generalizes beyond one domain [1].\n- TREC Deep Learning + TREC 2024 RAG Track: strong, human\u2011curated IR and now end\u2011to\u2011end RAG comparisons, with industrial baselines via Ragnar\u00f6k [1][4].\n\n2) Open\u2011domain and multi\u2011hop QA (classic RAG setting)\n- Natural Questions, TriviaQA, SQuAD, PopQA: good for single\u2011hop knowledge lookup and answer accuracy (EM/F1) [1][5].\n- HotpotQA, 2WikiMultiHopQA, MuSiQue: require multi\u2011step reasoning and multi\u2011hop retrieval; include supporting\u2011fact metrics, so you can see if retrieval really brings the needed evidence [1][5].\n- KILT: multi\u2011task (QA, fact\u2011checking, etc.) with a shared corpus, widely used to compare RAG models across tasks [4].\n\n3) Fact\u2011checking / hallucination stress\u2011tests\n- FEVER, HoVeR, QuanTemp, AveriTeC: test if the system can verify claims (including multi\u2011hop and numeric claims) against retrieved evidence, which is central for reliable RAG [1].\n- RAGTruth: corpus for fine\u2011grained hallucination analysis in RAG outputs [4].\n\n4) Complex reasoning and long\u2011context RAG\n- BRIGHT and complex tasks like Deep Research, GPQA, MATH500, BrowseComp\u2011Plus: stress reasoning\u2011aware retrieval and multi\u2011step search planning, closer to real \u201cagentic\u201d RAG [1].\n- LongBench plus HiCBench: designed to probe long\u2011document RAG and chunking; HiCBench is particularly good for diagnosing how retrieval and chunking affect evidence recall and answer quality [2].\n\n5) End\u2011to\u2011end RAG evaluation frameworks\n- RAGAS, ARES, CRUX, and Auepora: not datasets but toolkits that jointly score retrieval, faithfulness, and answer correctness, helping you see where your RAG pipeline breaks [1][3][4].", "tags": ["benchmarks_eval"], "full_faithfulness": 1.0, "full_answer_relevancy": 0.0, "seed_faithfulness": 0.6538461538, "seed_answer_relevancy": 0.0, "context_relevance": 0.8364495596147826, "context_relevance_error": null, "context_relevance_reason": "The context passages provide substantial information about various evaluation benchmarks relevant to Retrieval-Augmented Generation (RAG) systems, addressing the user's question about the best RAG evaluation benchmarks. Specific datasets and their purposes are discussed, such as BEIR and TREC, which directly relate to the evaluation of RAG systems. However, while the context is informative, it could be more explicitly focused on the 'best' benchmarks, as the user requested, rather than just listing various datasets and their characteristics.", "structure_citation": 0.7305989070626036, "structure_error": null, "structure_reason": "The Actual Output includes several claims supported by citations, and most of these citations correspond to snippets in the provided Context. However, there are some instances where the citations [1] and [4] are used multiple times without clear differentiation, which could lead to confusion regarding their specific references. Additionally, while the majority of claims are well-supported, a few claims could benefit from more explicit connections to the cited snippets, particularly in the context of complex reasoning and multi-hop QA. Overall, the response demonstrates a strong understanding of RAG benchmarks and their evaluation, but minor issues with citation clarity prevent a perfect score.", "hallucination_score": 0.8915353393638871, "hallucination_error": null, "hallucination_reason": "The response accurately identifies a range of RAG benchmarks and their purposes, aligning well with the context provided. It covers various aspects such as general retrieval quality, open-domain QA, fact-checking, and complex reasoning, all of which are supported by the context. However, while the output is comprehensive, it could benefit from a more explicit connection to the evaluation metrics mentioned in the context, which would enhance its alignment further.", "contexts_used": ["6 Datasets and Evaluation Benchmarks\n\nThe evaluation of IR and RAG systems relies on diverse datasets that test different aspects of retrieval and answer generation capabilities, including the retrieval, ranking, and answer quality.", "6.1 IR Specific\n\nInformation retrieval has a long history of evaluation campaigns, including those from TREC, CLEF, NTCIR, and FIRE. The queries used in these collections are often developed to strike a balance between having too many relevant documents in the target collection (which can be too easy to retrieve and too difficult to properly annotate (Voorhees et al., 2022)) and too few relevant documents. Sometimes challenging topics are also developed deliberately (Voorhees, 2005). The topic development process often involves manual reformulation of queries to ensure good coverage of relevance assessments. Mirroring the annotation process itself, it can be beneficial for automated retrieval systems to also perform various forms of query understanding (expansion or rewriting) to help ensure high recall. Hence, the approaches described in Section 3 show performance gains.", "Comprehensive benchmarks like BEIR (Thakur et al., 2021) offer heterogeneous evaluation across 17 diverse datasets spanning multiple domains and retrieval tasks, enabling zero-shot generalization assessment. In addition, evaluation sets such as TREC DL (Deep Learning) and its variants (Craswell et al., 2020, 2021, 2022, 2023) are used to evaluate IR systems.", "6.2 Question Answering\n\nQuestion Answering datasets for retrieval evaluation fall into two primary categories based on complexity. Single-hop QA benchmarks include Natural Questions (NQ) (Kwiatkowski et al., 2019), TriviaQA (Joshi et al., 2017), SQuAD (Rajpurkar et al., 2016), and PopQA (Mallen et al., 2023) test the RAG system's ability to retrieve and utilize information to generate the final answer. Multi-hop QA benchmarks comprise 2WikiMultiHopQA (Ho et al., 2020) (questions requiring evidence from exactly two Wikipedia articles via bridging/comparison), HotpotQA (Yang et al., 2018), and MuSiQue (Trivedi et al., 2022), which features compositionally complex questions constructed from interconnected sub-questions. Due to the dependency on intermediate answers, such questions cannot be answered through isolated single-step retrieval. Therefore, this design naturally motivates the RAG systems to decompose the original query and new queries based on the intermediate answers help in the retrieval of better context.", "6.3 Fact Checking/Verification\n\nThese datasets assess models' ability to verify claims against retrieved evidence. FEVER (Fact Extraction and VERification) (Thorne et al., 2018) is an open-domain fact-checking benchmark that requires retrieval over a large collection of Wikipedia articles and training NLI models to classify claims as supported, refuted, or not having enough information based on Wikipedia evidence. While FEVER deals with simple claims, HoVeR (Jiang et al., 2020) proposes a fact-checking benchmark with claims that require multi-hop reasoning and multi-turn retrieval. QuanTemp (V et al., 2024) is the first to propose a large open-domain benchmark for fact-checking numerical claims. It comprises claims that require interleaving claim decomposition and retrieval based on the verification output of sub-claims. This relates to query-level feedback and generation-time feedback. AveriTeC (Schlichtkrull et al., 2023) is a real-world claim verification dataset that includes diverse claim types with supporting or refuting evidence gathered through web search, making it particularly valuable for evaluating RAG systems on authentic fact-checking scenarios that require verification against potentially noisy or contradictory web sources.", "6.4 Complex Reasoning\n\nEmerging benchmarks introduce novel relevance criteria requiring awareness of reasoning structure. BRIGHT (SU et al., 2025) defines document relevance not by topical alignment but by whether passages contain logical constructs (deductive steps, analogies, constraints) necessary to derive answers. This challenges lexical retrievers that lack inference-awareness. However, reasoningaugmented retrieval methods, such as CoT (Chainof-Thought), have shown performance gains. Further, there exist reasoning/agentic tasks like Deep Research (Wu et al., 2025), GPQA (Rein et al., 2023), MATH500 (Cobbe et al., 2021), which involve access to the web search engine, have been used in the RAG setting (Li et al., 2025a; Wei et al., 2025). However, due to the absence of the cor- pus, it is hard to evaluate the retrieval performance. Recent benchmarks like BrowseComp-Plus (Chen et al., 2025b) provide a curated corpus for Deep Research tasks and enable the evaluation of retrieval performance.", "6.5 Evaluation Metrics\n\nEvaluating retrieval systems and RetrievalAugmented Generation (RAG) pipelines is critical for ensuring the accuracy, relevance, and reliability of generated responses. Retrieval evaluation typically focuses on metrics such as recall@k, precision, mean reciprocal rank (MRR), and hit rate, which assess how effectively the system retrieves pertinent documents or passages given a query. In contrast, RAG evaluation is more holistic, combining retrieval quality with generation fidelity and coherence. Common approaches include measuring answer correctness using exact match (EM) or F1 score, assessing faithfulness to retrieved evidence to detect hallucinations, and evaluating relevance and fluency through human or automated scoring (e.g., BLEU, ROUGE, or BERTScore). Recent frameworks like RAGAS (ExplodingGradients, 2024), ARES (Saad-Falcon et al., 2024), and CRUX (Ju et al., 2025) also emphasize end-to-end evaluation, where the interplay between retrieval accuracy and generation quality is analyzed to identify bottlenecks-such as irrelevant documents leading to incorrect answers-making comprehensive evaluation essential for diagnosing and improving RAG system performance.", "Challenges . The current RAG evaluation methods mainly focus on the retrieval and final answer performance. However, Reasoning RAG systems are highly dependent on intermediate reasoning steps and retrieval rounds. Therefore, it is also important to consider additional evaluation dimensions such as computational cost, efficiency, or number of retrieval rounds.", "2 Related Works\n\nTraditional Text Chunking. Text chunking divides continuous text into meaningful units like sentences, phrases, and words, with our focus on sentence-level chunking. Recent works have explored various approaches: [Cho et al., 2022] combines text chunking with extractive summarization using hierarchical representations and determinantal point processes (DPPs) to minimize redundancy, [Liu et al., 2021] presents a pipeline integrating topical chunking with hierarchical summarization, and [Zhang et al., 2021] develops an adaptive sliding-window model for ASR transcripts using phonetic embeddings. However, these LSTM and BERT[Devlin et al., 2019] based methods face limitations from small context windows and single-level chunking capabilities.", "RAG-oriented Document Chunking. Recent research has explored content-aware document chunking strategies for RAG systems. LumberChunker[Duarte et al., 2024] uses LLMs to identify semantic shifts,but may miss hierarchical relationships. PIC[Wang et al., 2025] proposes pseudo-instruction for document chunking, guide chunking via document summaries, though its single-level approach may oversimplify document structure. AutoChunker[Jain et al., 2025] employs tree-based representations but primarily focuses on noise reduction rather than multi-level granularity. Late Chunking[G\u00fcnther et al., 2024] embeds entire documents before chunking to preserve global context, but produces flat chunk lists without modeling hierarchical relationships. In contrast, our HierarchyChunk method creates multi-level document representations, chunking from coarse sections to fine-grained paragraphs. This enables RAG systems to retrieve information at appropriate abstraction levels, effectively bridging fragmented knowledge gaps and providing comprehensive document understanding.", "Limitations of Existing Text Chunking Benchmarks. The evaluation of text chunking and RAG methods heavily relies on benchmark datasets. Wiki-727[Koshorek et al., 2018],VT-SSum[Lv et al., 2021] and NewsNet[Wu et al., 2023] are typically chunked into flat sequences of paragraphs or sentences, without capturing the multi-level organization (e.g., sections, subsections, paragraphs) inherent in many real-world documents. This single-level representation limits the ability to evaluate chunking methods that aim to preserve or leverage document hierarchy, which is crucial for comprehensive knowledge retrieval in complex RAG scenarios. While Qasper[Dasigi et al., 2021], HotpotQA[Yang et al., 2018] and GutenQA[Duarte et al., 2024] are designed for RAG-related tasks, they do not specifically provide mechanisms or metrics for evaluating the efficacy of document chunking strategies themselves. Their focus is primarily on end-to-end RAG performance, where the impact of chunking is implicitly measured through retrieval and generation quality. This makes it challenging to isolate and assess the performance of different chunking methods independently, hindering systematic advancements in hierarchical document chunking. Our work addresses these gaps by proposing a method that explicitly considers multi-level document chunking and constructs a novel benchmark from a chunking perspective.", "5 Experiments\n5.3 Experimental Settings\n\nIn the model training of HiChunk, Gov-report[Huang et al., 2021], Qasper[Dasigi et al., 2021] and Wiki727[Koshorek et al., 2018] are the train datasets, which are publicly available datasets with explicit document structure. We use Qwen3-4B[Team, 2025] as the base model, with a learning rate of 1e-5 and a batch size of 64. The maximum length of training and inference is set to 8192 and 16384 tokens, respectively. Meanwhile, the length of each sentence is limited to within 100 characters. Due to the varying sizes of chunks resulting from semantic-based chunking, we limit the length of the retrieved context based on the number of tokens rather than the number of chunks for a fair comparison. The maximum length of the retrieved context is set to 4096 tokens. We also compare the performance of different chunking methods under different retrieved context length settings in subsection 5.6. In the RAG evaluation process, we consistently use Bge-m3[Chen et al., 2024] as the embedding model for context retrieval. As for the response model, we use three different series of LLMs with varying scales: Llama3.1-8b[Dubey et al., 2024], Qwen3-8b, and Qwen3-32b[Team, 2025].", "5.4 Chunking Accuracy\n\nTo comprehensively evaluate the performance of the semantic-based chunking method, we conducted experiments using two publicly available datasets, along with the proposed benchmark, to assess the cutpoint accuracy of the chunking method. Since the SC and LC chunking methods are limited to performing single-level chunking, we evaluated only the F1 scores for the initial level of chunking points and the F1 scores without regard for the hierarchy of chunking points. The evaluation results are presented in Table 3. In the Qasper and Gov-report datasets, which serve as in-domain test sets, the HC method shows a significant improvement in chunk accuracy compared to the SC and LC methods. Additionally, in HiCBench, an out-of-domain test set, the HC method exhibits even more substantial accuracy improvements. These findings demonstrate that HC enhances the base model's performance in document chunking by focusing exclusively on the chunking task. Moreover, as indicated in the subsequent experimental results presented in subsection 5.5, the accuracy improvement of the HC method in document chunking leads to enhanced performance throughout the RAG pipeline. This includes improvements in the quality of evidence retrieval and model responses.", "| | Chunk.Method | Qasper.F 1 L 1 | Qasper.F 1 L 2 | Qasper.F 1 L all | Gov-Report.F 1 L 1 | Gov-Report.F 1 L 2 | Gov-Report.F 1 L all | HiCBench.F 1 L 1 | HiCBench.F 1 L 2 | HiCBench.F 1 L all |\n|---:|:---------------|-----------------:|:-----------------|-------------------:|---------------------:|:---------------------|-----------------------:|-------------------:|:-------------------|---------------------:|\n| 0 | SC | 0.0759 | - | 0.1007 | 0.0298 | - | 0.0616 | 0.0487 | - | 0.1507 |\n| 1 | LC | 0.5481 | - | 0.6657 | 0.1795 | - | 0.5631 | 0.2849 | - | 0.4858 |\n| 2 | HC | 0.6742 | 0.5169 | 0.9441 | 0.9505 | 0.8895 | 0.9882 | 0.4841 | 0.3140 | 0.545 |\n\nTable 3. Chunking accuracy. HC means the result of HiChunk without fixed size chunking. The best result is in bold .", "5.5 RAG-pipeline Evaluation\n\nWeevaluated the performance of various chunking methods on the LongBench, Qasper, GutenQA, OHRBench and HiCBench datasets, with the results detailed in Table 4. The performance of each subset in LongBench is shown in Table A1. The results demonstrate that the HC200+AM method achieves either optimal or suboptimal performance on most LongBench subsets. When considering average scores, LumberChunk remains a strong baseline. However, as noted in Table 2, both GutenQA and OHRBench datasets exhibit the feature of evidence sparsity, meaning that the evidence related to QA pairs is derived from only a few sentences within the document. Consequently, the different chunking methods show minimal variation in evidence recall and response quality metrics on these datasets. For instance, using Qwen3-32B as the response model on the GutenQA dataset, the evidence recall metrics of FC200 and HC200+AM are 64.5 and 65.53, and the Rouge metrics are 44.86 and 44.94, respectively. Another example is OHRBench dataset, the evidence recall metrics and Rouge metrics of FC200, LC, HC200 and HC200+AM are very close. In contrast, the Qasper and HiCBench datasets contain denser evidence, where a better chunking method results in higher evidence recall and improved response quality. Again using Qwen3-32B as an example, on the T 1 task of HiCBench dataset, the evidence recall metric for FC200 and HC200+AM are 74.06 and 81.03, the Fact-Cov metrics are 63.20 and 68.12, and the Rouge metrics are 35.70 and 37.29, respectively. These findings suggest that the evidence-dense QA in the HiCBench dataset is better suited for evaluating the quality of chunking methods, enabling researchers to more effectively identify bottlenecks within the overall RAG pipeline.", "| | Chunk Method.Chunk Method | LongBench Score.LongBench Score | Qasper.ERec | Qasper.F1 | GutenQA.ERec | GutenQA.Rouge | OHRBench( T 0 ).ERec | OHRBench( T 0 ).Rouge | HiCBench( T 1 ).ERec | HiCBench( T 1 ).FC | HiCBench( T 1 ).Rouge | HiCBench( T 2 ).ERec | HiCBench( T 2 ).FC | HiCBench( T 2 ).Rouge |\n|---:|:----------------------------|:----------------------------------|:--------------|:------------|:---------------|:----------------|:-----------------------|:------------------------|:-----------------------|:---------------------|:------------------------|:-----------------------|:---------------------|:------------------------|\n| 0 | Llama3.1-8B | Llama3.1-8B | Llama3.1-8B | Llama3.1-8B | Llama3.1-8B | Llama3.1-8B | Llama3.1-8B | Llama3.1-8B | Llama3.1-8B | Llama3.1-8B | Llama3.1-8B | Llama3.1-8B | Llama3.1-8B | Llama3.1-8B |\n| 1 | FC200 | 42.49 | 84.08 | 47.26 | 64.43 | 30.03 | 67.03 | 51.01 | 74.84 | 47.82 | 28.43 | 74.61 | 46.79 | 30.97 |\n| 2 | SC | 42.12 | 82.08 | 47.47 | 58.30 | 28.58 | 62.65 | 49.10 | 72.14 | 46.80 | 28.43 | 73.49 | 45.28 | 30.92 |\n| 3 | LC | 42.73 | 87.08 | 48.20 | 63.67 | 30.22 | 68.42 | 51.85 | 76.64 | 50.84 | 29.62 | 76.12 | 49.12 | 32.01 |\n| 4 | HC200 | 43.17 | 86.16 | 48.09 | 65.13 | 29.95 | 68.25 | 51.33 | 78.52 | 49.87 | 29.38 | 78.76 | 49.11 | 31.80 |\n| 5 | +AM | 42.90 | 87.49 | 48.95 | 65.47 | 30.33 | 67.84 | 51.92 | 81.59 | 55.58 | 30.04 | 80.96 | 53.66 | 33.04 |\n| 6 | Qwen3-8B | Qwen3-8B | Qwen3-8B | Qwen3-8B | Qwen3-8B | Qwen3-8B | Qwen3-8B | Qwen3-8B | Qwen3-8B | Qwen3-8B | Qwen3-8B | Qwen3-8B | Qwen3-8B | Qwen3-8B |\n| 7 | FC200 | 43.95 | 84.32 | 45.10 | 64.50 | 33.47 | 67.07 | 48.18 | 74.06 | 47.35 | 33.83 | 72.95 | 43.45 | 35.27 |\n| 8 | SC | 43.54 | 82.22 | 44.55 | 58.37 | 32.71 | 62.18 | 46.79 | 71.42 | 46.07 | 33.30 | 72.36 | 42.97 | 34.76 |\n| 9 | LC | 44.83 | 87.43 | 46.05 | 63.67 | 33.87 | 68.79 | 49.28 | 75.53 | 48.27 | 34.12 | 75.14 | 46.80 | 35.93 |\n| 10 | HC200 | 43.90 | 86.49 | 45.95 | 65.20 | 33.89 | 68.57 | 49.06 | 77.68 | 47.37 | 34.30 | 78.10 | 46.20 | 36.32 |\n| 11 | +AM | 44.41 | 87.85 | 45.82 | 65.53 | 34.15 | 68.31 | 49.61 | 81.03 | 50.75 | 35.26 | 80.65 | 49.02 | 37.28 |\n| 12 | Qwen3-32B | Qwen3-32B | Qwen3-32B | Qwen3-32B | Qwen3-32B | Qwen3-32B | Qwen3-32B | Qwen3-32B | Qwen3-32B | Qwen3-32B | Qwen3-32B | Qwen3-32B | Qwen3-32B | Qwen3-32B |\n| 13 | FC200 | 46.33 | 84.32 | 46.49 | 64.50 | 44.86 | 67.07 | 46.89 | 74.06 | 63.20 | 35.70 | 72.95 | 60.87 | 37.17 |\n| 14 | SC | 46.29 | 82.22 | 46.39 | 58.37 | 43.59 | 62.18 | 45.43 | 71.26 | 61.09 | 35.64 | 72.36 | 59.23 | 37.09 |\n| 15 | LC | 47.43 | 87.43 | 46.82 | 63.67 | 44.45 | 68.79 | 47.92 | 75.53 | 64.76 | 36.15 | 75.14 | 62.75 | 38.02 |\n| 16 | HC200 | 46.71 | 86.49 | 46.99 | 65.20 | 44.83 | 68.57 | 47.71 | 77.68 | 63.93 | 36.55 | 78.10 | 62.51 | 38.26 |\n| 17 | +AM | 46.92 | 87.85 | 47.25 | 65.53 | 44.94 | 68.31 | 47.89 | 81.03 | 68.12 | 37.29 | 80.65 | 66.36 | 37.37 |", "Table 4. RAG-pipeline evaluation on LongBench, Qasper, GutenQA, OHRBench and HiCBench. Evidence Recall and Fact Coverage metric is represented by ERec and FC respectively. The best result is in bold , and the sub-optimal result is in underlined", "5.6 Influence of Retrieval Token Budget\n\nSince HiCBench is more effective in assessing the performance of chunking methods, we evaluated the impact of our proposed method on the T 1 task of HiCBench under different retrieve token budgets: 2k, 2.5k, 3k, 3.5k and 4k tokens. We compared the effects of various chunking methods by calculating the Rouge metrics between responses and answers, as well as the Fact-Cov metrics. The experimental findings are illustrated in Figure 3. The results demonstrate that a larger retrieval token budget usually leads to better response quality, so it is necessary to compare different chunking methods under the same retrieval token budget. HC200+AM consistently achieves superior response quality across various retrieve token budget", "settings. These experimental results underscore the effectiveness of HC200+AM method. We further present the correspond curves of the evidence recall metrics in subsection A.2.", "Figure 3. Performance of HiCBench( T 1 ) under different retrieval token budget from 2k to 4k.\n\n2k 2.5k 3k 3.5k 4k 27.5 28.0 28.5 29.0 29.5 30.0 Rouge Llama3.1-8b FC200 SC LC HC200 HC200+AM 32.0 32.5 33.0 33.5 34.0 34.5 35.0 35.5 Qwen3-8b 36.0 36.5 37.0 Qwen3-32b Fact Cov(%)", "5.7 Effect of Maximum Hierarchical Level\n\nIn this section, we examine the impact of limiting the maximum hierarchical level of document structure obtained by HiChunk. The maximum level ranges from 1 to 4, denoted as L 1 to L 4, while LA represents no limitation on the maximum level. We measure the evidence recall metric on different settings. As shown in Figure 4. This result reveals that the Auto-Merge retrieval algorithm degrades the performance of RAG system in the L 1 setting due to the overly coarse-grained semantics of L 1 chunks. As the maximum level increases from 1 to 3, the evidence recall metric also gradually improves and remains largely unchanged thereafter. These findings highlight the importance of document hierarchical structure for enchancing RAG systems.", "Figure 4. Evidence recall metric across different maximum level on HiCBench( T 1 and T 2 ).\n\nQwen3-8b/32b Evidence Recall L1 L2 L3 L4 LA Fact-Cov", "2 Background\n2.1 LLMs\n\nLLMs are transformer-based neural networks [25, 29] trained on massive text corpora to acquire broad language understanding and reasoning capabilities. Their performance can be further enhanced through several complementary strategies. Supervised fine-tuning (SFT) adapts pretrained models to specific domains or tasks by updating their parameters on curated labeled data [19, 8, 30], while reinforcement learning from human feedback (RLHF) aligns their behavior with human preferences through reward-based optimization [16]. Both methods directly modify the model's parametric memory, embedding new knowledge into its weights, yet this knowledge remains static and can quickly become obsolete, especially in rapidly evolving or highly regulated domains [17].", "In contrast, prompt learning [33, 12] and prompt engineering [22] guide model behavior without altering its parameters, using carefully crafted instructions or few-shot exemplars to induce desired task behavior at inference time. Relatedly, chain-of-thought (CoT) [27] prompting encourages models to generate intermediate reasoning steps before producing an answer, which improves multi-step reasoning but does not expand the model's knowledge base. While these approaches enhance task adaptability and reasoning ability, they cannot update or extend the underlying knowledge stored in the model's parameters, leaving LLMs vulnerable to hallucinations, especially in specialized or fast-changing domains.", "These limitations motivate the use of RAG, which complements LLMs and the above approaches with non-parametric memory by retrieving relevant external documents at inference time, thereby grounding their outputs in accurate, verifiable, and up-to-date information.", "2.2 RAG in General\n\nRetrieval-augmented generation (RAG) has emerged as a central paradigm for enhancing LLMs by conditioning them on external knowledge sources rather than relying solely on parametric memory. By retrieving relevant information from corpora, databases, or the web, RAG mitigates key limitations of static LLMs, including hallucination, factual drift, and domain obsolescence.", "Recent surveys have mapped the evolution of RAG systems. Gao et al. [5] and Wu et al. [28] categorize approaches, analyzing design dimensions such as retrieval granularity (passages vs. documents), retriever-generator integration (late fusion vs. joint training), and memory augmentation. They also highlight persistent challenges, including reducing hallucinations, handling outdated or", "Figure 2: Overview of our Retrieval-Augmented Generation (RAG) pipeline for radio regulations QA, combining FAISS-based retrieval with LLM-based answer generation\n\nRadio Regulations Embdedding Database Large Language Model Embedder Question Similarity Search (FAISS) Context Correct Answer\n\nincomplete knowledge, and supporting efficient updates in dynamic domains. In parallel, Yu et al. [31] emphasize the need for principled evaluation that captures the hybrid nature of retrieval and generation. Together, these works establish RAG as a promising direction for improving factuality, adaptability, and transparency in LLMs while underscoring unresolved research questions.", "2.3 Evaluating RAG\n\nEvaluating RAG remains a challenging open problem due to its hybrid nature. Yu et al. [31] provide a comprehensive survey and propose Auepora , a unified evaluation framework that organizes assessment along three axes: Target , Dataset , and Metric .", "On the Target axis, they distinguish between component-level and end-to-end goals. For retrieval, the focus is on relevance (alignment between query and retrieved documents) and accuracy (alignment between retrieved content and ground-truth candidates). For generation, they emphasize relevance (response-query alignment), faithfulness (consistency with supporting documents), and correctness (agreement with reference answers). Beyond these, they highlight broader system requirements such as low latency, diversity, robustness to noisy or counterfactual evidence, and calibrated refusal, which are critical for making RAG systems usable and trustworthy in practice.", "On the Dataset axis, they note that widely used Q&A-style benchmarks often capture only narrow aspects of RAG behavior, motivating domain-specific testbeds that reflect dynamic knowledge and application-specific constraints (e.g., legal, medical, or financial domains).", "On the Metric axis, they collate traditional rank-based retrieval measures (e.g., MAP, MRR [24]), text generation metrics (e.g., ROUGE [4], BLEU [18], BERTScore [32]), and the growing use of LLM-as-a-judge for assessing faithfulness and overall quality. While LLM-based evaluation shows promise, they caution that alignment with human preferences and the need for transparent grading rubrics remain unresolved challenges.", "Our evaluation choice. In contrast to prior work that primarily reuses general-purpose benchmarks, we construct our own domain-specific testbed: a multiple-choice dataset derived directly from the ITU Radio Regulations [11] using an automated pipeline with LLM generation, automated judging, and human verification (Section 4.5). This design ensures that the benchmark reflects realistic regulatory queries while retaining ground-truth answers. Accordingly, our end-to-end metric is simply answer accuracy . To disentangle retrieval from generation, we further introduce a domain-tailored retrieval metric aligned with component-level relevance and accuracy . Technical details of this metric are developed in Section 4.1.", "Benchmarks and evaluation:\n\nKnowledge-intensive benchmarks track RAG progress. OpenQA tasks (Natural Questions, TriviaQA, HotpotQA) and the KILT benchmark suite (including QA, fact-checking, slot filling, etc.) are standard evaluation sets. In 2023, RAG models dominated many KILT tasks and few-shot QA challenges. New evaluation tools also emerged: for example, the RAGAS framework provides reference-free metrics for RAG pipelines, and the RAGTruth corpus (Niu et al. 2024) enables fine-grained analysis of hallucinations in RAG outputs. 2023 also witnessed a major spike in publications and adoption of RAG. Some other works are on Active RAG [38], Improving domain adaptation of RAG models for open question answering [85], content filtering for RAG [94], and RAG with self-memory [16].", "2024 - Recent Advances. In 2024, research on RAG continues to push on secure retrieval frameworks, multi-hop reasoning, and domain specialization. Several groups explore differentiable retrievers that can be tuned in an end-to-end pipeline, while others investigate merging large-context attention with retrieval indexing. Meanwhile, new techniques aim to reduce the chance the generator ignores retrieved data or merges contradictory documents incorrectly. RAG-based chat systems in healthcare, finance, and law now incorporate advanced fact-checking modules to ensure that only vetted external sources influence the final output. Some notable works in 2024 are Evaluating Retrieval Quality in RAG [78], Benchmarking RAG for Medicine [97], Benchmarking LLM in RAG [13], Review of RAG for AI Generated Content [106], Unifying Context Ranking with RAG [100], Searching for the Best Practice in RAG [93], Finetuning Vs RAG for Less Popular Knowledge [86], Integrating RAG wit LLM in Nephrology [61], RAG for Copyright Protection [23], RAG for Textual Graph Understanding and Question Answering [31], Interactive AI with RAG for NExt\n\nGeneration Networking [103], Web Application for RAG: Implementation and Testing [72], Overcoming Challenges for Long Input in RAG [39], and Adapting Language Model for Domain Specific RAG [104]. Some specifics in RAG development as at 2024 are discussed below:", "Infrastructure and standardized evaluation: The community recognised a need for common tooling and shared tasks. Ragnar\u00f6k introduced a reusable end-to-end framework and provided industrial baselines for the inaugural TREC 2024 RAG Track [70]. Beyond code, evaluation methodology itself became a research focus: ARAGOG proposed automatic grading of RAG outputs that correlates with human judgements, analysing retrieval precision and answer similarity across advanced pipelines [20]. These efforts mark a shift from anecdotal demos to systematic, reproducible assessment.", "Adaptive retrieval and self-reflection: Building on ideas such as Self-RAG, several 2024 works taught models to decide when-and how much-to retrieve . SAM-RAG dynamically filters documents and verifies both evidence and final answers in multimodal contexts, improving accuracy without unnecessary retrieval calls [102]. For complex visual-question-answering, OmniSearch plans multi-hop retrieval chains on the fly, demonstrating large gains on the new Dyn-VQA benchmark [53]. These results confirm that retrieval policies, not just retriever quality, matter for difficult queries.", "Multimodal RAG breaks out: Where earlier RAG research was text-only, 2024 saw a surge in multimodal extensions. SAM-RAG and OmniSearch both combine text and image evidence, while concurrent frameworks (e.g. mR 2 AG and M3DocRAG) introduce retrieval-reflection loops or structured vision-language indexes. Surveys published this year chart the design space and highlight open issues such as cross-modal alignment and vision-aware reranking [102, 53].", "Progress in dense retrieval: High-recall retrievers remain the backbone of every RAG system. 2024 research emphasised unsupervised or instruction-tuned retrievers that avoid costly labelled data, building on contrastive pre-training techniques and LLM-augmented embedding models. These retrievers power the top submissions in the TREC track and underpin production deployments discussed in industrial white papers.", "Rapid growth and forward outlook: A bibliometric snapshot counted more than 1,200 RAG-related papers on arXiv in 2024 alone [106],", "compared with fewer than 100 the previous year, underscoring the field's rapid maturation. Looking ahead, challenges include ultra-fast retrieval over trillion-token corpora, faithfulness verification for multi-hop reasoning, and energy-efficient multimodal indexing. Nevertheless, 2024 firmly established RAG as the default strategy for grounding large language (and vision-language) models in up-to-date, attributable knowledge.", "RAG + Graphs in QA\n\nTo address these limitations of traditional RAG, particularly in retrieving contextually precise and semantically rich information, research has tried to integrate structured knowledge sources and graphbased reasoning into the retrieval pipeline. One notable direction of work is LLM-based retrieval, which incorporates knowledge graphs information into the generation process of LLMs. The LLMs are augmented using the retrieved facts from the KG [26], leading to a clear dependence on the quality of extracted graph information, using it to generate responses. Research has also been done towards augmenting knowledge graph information, retrieved through some semantic or some other similarity index, in the prompts given to the LLM [27], to help the model do zero-shot question-answering. Some researchers have tried a different approach to fact retrieval, where the model tries different queries, using structured query languages, until the desired information comes through [28]. All of these approaches have used KGs as an external source to retrieve information from and answer the questions [29]. Then, aligning the retrieval process with LLM even more closely, some researchers have proposed methods which use LLMs in intermediate steps as well to plan the retrieval and judge whether the retrieved information is relevant or not [29,30], continuing the process until the desired output emerges.", "Another interesting direction of work is integrating GNNs with LLMs, which leverages graph neural networks to enhance retrieval and re-ranking using learned graph representations, along with generation capabilities of LLMs. There have been approaches such as GNN-RAG [31], which have tried combining language understanding abilities of LLMs with the reasoning abilities of GNNs in a retrieval-augmented generation (RAG) style. Other methods of GNN-LLM alignment, have been classified into symmetric and asymmetric alignment [32]. Symmetric alignment refers to the equal treatment of the graph and text modalities during the alignment process[33, 34, 35]. Asymmetric alignment focuses on allowing one modality to assist or enhance the other, here leveraging the capabilities of GNNs to improve the LLMs [36, 37, 38].", "The continual evolution of such hybrid architectures reflects the dynamic nature of the QA landscape and its responsiveness to complex information needs. Given the diversity of methods explored to enhance QA systems-from rule-based techniques to advanced neural and hybrid models-it becomes essential to establish robust mechanisms for evaluating their effectiveness. It is thus important to discuss the evaluation paradigms that compare and benchmark these systems. Extractive QA benchmarks almost universally use Exact Match (EM)-the strict proportion of predictions that character-for-character match a ground-truth answer-and F1 score, the harmonic mean of token-level precision and recall, to evaluate answer quality. On SQuAD v2.0 [39], human annotators achieve around 86.831 EM and 89.452 F1 on the test set, whereas state-of-the-art models now exceed 90 EM and 93 F1. Natural Questions [40] extends this paradigm to long-answer (paragraph) and short-answer (span or yes/no) annotations drawn from real Google search queries. Meanwhile, multiple-choice datasets like OpenBookQA [41]-designed to mimic open-book science exams-use simple accuracy. Complex reasoning benchmarks push beyond single-span extraction. HotpotQA [42], a multi-hop dataset built on pairs of Wikipedia articles, evaluates both answer-span EM/F1 and supporting-fact EM/F1, plus a joint metric requiring both to be correct; even top models achieve only ~72 joint F1[43] under the full-wiki setting, far below human performance of ~82 average F1 and ~96 upper bound F1. These core metrics and datasets underpin broader QA evaluation: multi-task suites like GLUE/MMLU [44, 45] include QA subtasks to probe general language understanding, while specialized frameworks such as MT-bench [46] ('LLM as judge') and automated platforms like Scale Evaluation layer on top to assess conversational and retrieval-augmented QA in real-world scenarios.", "This review has highlighted the broad spectrum of methodologies and evaluation strategies used in enhancing and assessing question answering systems. With this foundation in place, we now transition to the specific tools and frameworks employed in our research for graph-based question answering. In the following section, we briefly introduce and contextualize spaCy, Stanford CoreNLP, and GraphRAG-three diverse and widely-used tools we have utilized for knowledge graph construction and integration with LLMs."]}
{"type": "entry", "index": 30, "question": "neighbor expansion for chunks in rag is that a thing?", "answer": "Yes, \u201cneighbor expansion\u201d is a real idea in RAG-style systems, but it\u2019s mainly discussed in the context of graphs rather than plain text chunks.\n\n1. In mKG-RAG, they explicitly do l\u2011hop neighbor expansion on a knowledge graph: once they\u2019ve built a subgraph of entities and relations relevant to the question, they expand it by following neighbors 1 or more hops away to pull in related facts that might be missing from the initial retrieval [1]. When they remove this graph expansion step, accuracy drops on their VQA benchmarks, which shows the expansion is useful [1].\n\n2. A similar idea appears in a large\u2011scale GraphRAG variant (GeAR). They cap the number of expansion steps n=2 and note that graph expansion is mostly helpful for multi\u2011hop questions; for simple questions they can skip using external triples and graph expansion to save compute [3]. This is again expansion over neighbors in a graph, not just adjacent text.\n\n3. For plain text chunk RAG, the standard pipeline is: split docs into chunks, embed them, retrieve top\u2011k most similar chunks, then pass them to the generator [5]. The documents here don\u2019t describe a named \u201cneighbor expansion for chunks\u201d, but they do show related ideas like expanding or refining the retrieved pool via rankers and feedback (\"ranker-based expansion\") [4], and replacing chunk retrieval with graph-based retrieval plus expansion [1].\n\nSo: neighbor expansion is a thing in graph-based RAG (GraphRAG / KG-RAG) and conceptually similar ideas exist for refining or expanding retrieved text sets, but the documents don\u2019t describe a standardized, named technique that does \"neighbor expansion\" purely over adjacent text chunks.", "tags": ["_untagged"], "full_faithfulness": 1.0, "full_answer_relevancy": 0.0, "seed_faithfulness": 0.8260869565000001, "seed_answer_relevancy": 0.0, "context_relevance": 0.021946061336980727, "context_relevance_error": null, "context_relevance_reason": "The context passages provided do not address the user's question about 'neighbor expansion for chunks in rag.' The passages focus on various aspects of VQA accuracy, performance comparisons, and retrieval-augmented generation techniques, but none specifically mention or relate to the concept of neighbor expansion in the context of RAG. Therefore, the relevance to the user's inquiry is nonexistent.", "structure_citation": 0.4858207639318423, "structure_error": null, "structure_reason": "The Actual Output includes claims supported by citations, but there are issues with citation matching. While it references concepts like 'l-hop neighbor expansion' and 'graph expansion', the citations [1], [3], [4], and [5] do not directly correspond to the claims made in the context provided. Additionally, the output suggests that 'neighbor expansion' is not a standardized technique for text chunks, which is a valid point, but it lacks clarity on how the cited snippets support this conclusion. Overall, while there is some alignment, the lack of precise citation support and clarity on claims leads to a moderate score.", "hallucination_score": 0.719524003772658, "hallucination_error": null, "hallucination_reason": "The response accurately identifies 'neighbor expansion' as a concept in RAG-style systems, particularly in graph contexts, which aligns with the input query. It provides specific examples from mKG-RAG and GraphRAG, demonstrating a clear understanding of the topic. However, it notes that there is no standardized technique for 'neighbor expansion' in plain text chunks, which is a valid point but could be seen as a limitation in fully addressing the query. Overall, the claims made are well-supported by the context provided.", "contexts_used": ["4 Experiments\n4.2 Performance Comparison\n\nTable 4: VQA accuracy on E-VQA across different MLLM architectures with varying sizes\n\nQwen2-VL: Jozef KUBA\n\nGPT-4o: Matej Walch mKG-RAG: J. Langer\n\nQ : Who designed this museum?\n\nQwen2-VL \uff1a Indonesian\n\nGPT-4o: Indo-Saracenic mKG-RAG: Javanese\n\nQ : What is the architectural style of this mosque?\n\nQwen2-VL: 400\n\nGPT-4o:\n\nI can't answer without specific details.\n\nmKG-RAG: 250-metre\n\nQ : How long is the track at this velodrome?\n\nQwen2-VL: PARK\n\nGPT-4o: New York City Department of Parks and Recreation mKG-RAG: Abingdon Square Conservancy\n\nQ : Who is in charge of maintaining this park?\n\nQwen2-VL: 1000000\n\nGPT-4o: 26.04 km 2\n\nmKG-RAG: 3.82 km 2\n\nQ\n\n: What is the surface area of this lake?\n\nQwen2-VL: air space\n\nGPT-4o: a ring of dark spots mKG-RAG: brownish spots\n\nQ : What forms a ring on the larger end of the egg of this bird?", "Figure 4: Qualitative results of Qwen2-VL-7B, GPT-4o and mKG-RAG on E-VQA dataset.", "Results on E-VQA and InfoSeek. In this section, we compare mKG-RAG with Zero-shot MLLMs and RAG-based approaches on the benchmarks mentioned above. The results in Table 3 demonstrate that zero-shot MLLMs struggle with knowledge-based VQA tasks, particularly on the InfoSeek dataset. These limitations underscore the critical need for external knowledge integration. By augmenting LLaVA-More with mKG-RAG, we achieve substantial improvements, over 20.3% on E-VQA and 31.9% on InfoSeek, highlighting the value of retrieval augmentation.", "Furthermore, our method achieves state-of-the-art performance on both datasets. Under the finetuning setting, mKG-RAG * surpasses both mR2AG * and ReflectiVA * . Even without fine-tuning, mKG-RAG outperforms EchoSight by 1.2% and 1.7%, respectively. These results highlight the advantages of integrating RAG with multimodal KGs and demonstrate the effectiveness of our QMRetriever. Table 3 also includes two mKG-RAG variants that replace QM-Retriever with text-only and vision-only CLIP for entity/relationship retrieval, while still using documents retrieved by QMRetriever to construct multimodal KGs. In the text-only variant, both questions and image captions are used as queries to provide more context, explaining its better performance over the vision-only version. However, they both remain less effective than our full approach with QM-Retriever.", "Consistency across Architectures. In Table 4, we provide a detailed comparison of VQA scores across MLLMs of varying parameter sizes, including InternVL3 [58], LLaMA-3.2-Vision 2 , LLaVAv1.5 [34], DeepSeek-VL2 [52], and Qwen2.5-VL [3]. When enhanced with our mKG-RAG framework, these models achieve average performance gains of 9.4% on single-hop queries and 8.7% on overall scenarios, demonstrating the method's strong generalization across different architectures and scales.", "Qualitative Results. Figure 4 shows a qualitative comparison of mKG-RAG with zero-shot Qwen2VL and GPT-4o. While the latter two tend to produce plausible but incorrect or evasive responses, mKG-RAG consistently handles knowledge-intensive queries, especially those involving precise numerical and temporal reasoning.\n\n2 https://huggingface.co/meta-llama/Llama-3.2-11B-Vision", "| | Method. | E-VQA.Single-Hop | E-VQA.All | InfoSeek.Un-Q | InfoSeek.Un-E | InfoSeek.All |\n|---:|:--------------------|-------------------:|------------:|----------------:|----------------:|---------------:|\n| 0 | mKG-RAG | 38.4 | 36.3 | 41.4 | 39.6 | 40.5 |\n| 1 | w/o QM-Retriever | 34.2 | 31.6 | 38.9 | 37.9 | 38.4 |\n| 2 | w/o Graph Retrieval | 30.1 | 28.2 | 33.3 | 32.7 | 33 |\n| 3 | w/o Graph Expansion | 37.2 | 35 | 40.8 | 39.4 | 40.1 |\n\nTable 5: The ablation study of the design of mKG-RAG\n\n| | Model | Ret. Mode | K g = 1 | K g = 5 | K g = 10 | K g = 20 |\n|---:|:---------|:------------|----------:|----------:|-----------:|-----------:|\n| 0 | mKG-RAG\u2020 | Textual | 29.1 | 33.9 | 34.9 | 35.9 |\n| 1 | mKG-RAG\u2021 | Visual | 23 | 29.6 | 31 | 32 |\n| 2 | mKG-RAG | Multimodal | 29.2 | 35.1 | 36.3 | 36.9 |\n\nTable 6: The ablation study of how the retrieval number of entities/relationships affects the VQA accuracy on E-VQA.", "4.3 Ablation Study\n\nImpact of Coarse-grained Retrieval. To quantify the impact of coarse-grained document retrieval, we conduct an ablation experiment replacing QM-Retriever with visual-only CLIP (ViT-L/14@336) for topK d document selection. The results in Table 5 reveal significant performance drops: overall VQA accuracy of mKG-RAG decreases by 4.7% on E-VQA and 2.1% on InfoSeek. This ablation conclusively demonstrates the critical role of first-stage retrieval and QM-Retriever's superiority over unimodal alternatives.", "Effectiveness of Graph-based Retrieval. In our method, the entities and relationships extracted from documents form a distilled knowledge graph, reducing noise and enabling more effective retrieval than direct text chunk matching. To validate this insight, we replace graph-based retrieval with a naive chunk-based alternative. Specifically, we segment retrieved documents into fixed-size chunks and select those relevant to the given question and image description. As shown in Table 5, chunk-based retrieval leads to a substantial accuracy drop, 8.1% on E-VQA and 7.5% on InfoSeek.", "Contribution of Graph Expansion. mKG-RAG enhances the constructed subgraph through l -hop neighbor expansion, effectively capturing potentially missing but relevant knowledge connections. Table 5 shows that omitting graph expansion leads to consistent performance drops of 1.3% (E-VQA) and 0.4% (InfoSeek), demonstrating its critical contribution to our mKG-RAG.", "Impact of Varying Retrieval Number. In Table 6, we further analyze the impact of K g , the number of retrieved entities and relationships, on our method. As K g increases from 1 to 20, the overall accuracy of mKG-RAG and its variants gradually improves, as higher recall rates enhance the likelihood of capturing relevant knowledge. However, when K g > 10 , the benefit diminishes due to longer contexts and more noise. Thus, setting K g = 10 offers a practical trade-off. Notably, mKG-RAG still performs competitively even at K g = 1 , thanks to its graph expansion strategy, which enables the model to gather additional relevant information.", "ABSTRACT\n\nFine-tuning is an immensely resource expensive process when trying to retrain Large Language Models (LLMs) to have access to a larger bank of knowledge. To alleviate this issue there have been many different fine-tuning techniques proposed which have shown good progress in trying to reduce time and computational resources to achieve fine-tuning but with LLMs becoming more intelligent and larger, this issue continues to arise. Hence a new method of enabling knowledge expansion on LLMs had to be devised. Retrieval-Augment-Generate (RAG) is a class of techniques where information is stored in a database and appropriate chunks of information are retrieved to help answer the question. However there are many limitations to naive RAG implementations. This paper proposes the KeyKnowledgeRAG ( K 2 RAG ) framework to address the scalability and answer accuracy limitations associated with naive RAG implementations. This framework takes inspiration from divide-and-conquer ideology, and combines dense and sparse vector search, knowledge graphs and text summarization to help address these limitations. The framework also involves a data preprocessing step to reduce training times. The MultiHopRAG dataset was used for evaluation where the our implemented K 2 RAG pipeline was trained with the document corpus and then evaluated with the test data. Results have shown that there is an improvement in answering questions compared to common Naive RAG implementations where K 2 RAG achieved the highest mean answer similarity at 0.57 and also was able to answer more questions with more ground truth similarity with a highest Q 3 quartile at 0.82 . Additionally our proposed framework is trainable much faster due to the inclusion of a train data corpus summarization step which reduces training times of the individual components by 93% on average. Furthermore K 2 RAG has shown to operate even faster than a traditional knowledge graph based naive RAG implementation with a mean execution times reduced by up to 40% . In addition to superior question-answering capabilities, K 2 RAG offers being a more scalable solution with VRAM requirements reduced by 3 times in comparison to implementations of several naive RAG pipelines evaluated in this paper. Hence K 2 RAG can help companies in sophisticated decision-making through implementing a more lightweight and robust question-answering systems built on internal documents.", "5 Experiments\n\nFor the external knowledge graph, we use the full Wikidata 1 dump, filtering out any triples whose object is a string literal. We include one alias 2 for each included entity by creating a separate triple with 'alias' as predicate. We use a separate Pinecone sparse index for storing this data.", "We set the maximum number of steps \ud835\udc5b = 2. Throughout our experiments, we identified that benefits of graph expansion for simpler questions coming from DataMorgana were limited. Consequently, we opted for a more efficient implementation that does not use Wikidata triples and graph expansion during the first iteration. Questions requiring multi-hop reasoning would require additional iterations, and consequently, the full pipeline described in Section 4 is used for \ud835\udc5b > 1.", "In order to monitor improvement in the pipeline, we built our evaluation according to the suggested methodology, focusing on correctness and faithfulness. Experiments were conducted by constructing a sample of questions using DataMorgana [3].", "5.1 Using DataMorgana\n\nFollowing the best practices presented by Filice et al., we divide the set of users into novice and expert with equal probability, and further define a set of question and answer type categories. We expand upon their original set by incorporating the 'path-following' and 'path-finding' multi-hop question categorisation introduced by Gutierrez et al.. Moreover, we refrain from including the 'linguistic variation' question type and redistribute their probability mass among the remaining categories. Table 1 presents our final taxonomy of question and answer types.", "| | Question | Do frilled lizards and geoducks share any reproductive characteristics? | How come I always have to reset the high limit switch on my hot tub heater after draining and refillin the spa? |\n|---:|:-----------------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 0 | Proximal triples T \u2032 q ( \ud835\udc5b ) from read -ing FineWeb chunks | [('Pacific Geoducks', 'larvae swimming duration', 'first 48 hours after hatching'), ('Pacific Geoducks', 'fertilization method', 'external fertilization'), ('Pacific Geoducks', 'release eggs', '7 to 10 million eggs'), ('Pacific Geoducks', 'reproductive method', 'broadcast spawning'), ('Pacific Geoducks', 'development stage', 'develop a tiny foot and drop to the ocean floor in a few weeks')] | [('faulty parts', 'can cause', 'high limit switch to trip'), ('high limit switch', 'trips due to', 'water temperature exceeding safe limits'), ('primary operating thermostat', 'failure can lead to', 'high limit switch tripping'), ('blocked or clogged vents', 'can cause', 'high limit switch to trip'), ('thermistor', 'failure can lead to', 'high limit switch tripping')] |\n| 1 | Wikidata triples T q ( \ud835\udc5b ) linked by proximal triples | [('Larval development in the Pacific oyster and the impacts of ocean acidification: Differential genetic effects in wild and domesticated stocks', 'cites work', 'Gene expression correlated with delay in shell formation in larval Pacific oysters (Crassostrea gigas) exposed to experimental ocean acidification provides insights into shell formation mechanisms.'), ('Egg consumption and risk of cardiovascular disease: three large prospective US cohort studies, systematic review, and updated meta-analysis', 'cites work', 'Land, irrigation water, greenhouse gas, and reactive nitrogen burdens of meat, eggs, and dairy production in the United States'), ('Cryptic diversity, geographical endemism and allopolyploidy in NE Pacific seaweeds', 'cites work', 'Temporal windows of reproductive opportunity reinforce species barriers in a marine broadcast spawning assemblage.'), ('The Probable Method of Fertilization in Terrestrial Hermit Crabs Based on a Comparative Study of Spermatophores', 'published in', 'Pacific Science'), ('Pacific', 'located in the administrative territorial entity', 'Long Beach'), ('Genetic variation of wild and hatchery populations of the Pacific oyster Crassostrea gigas assessed by microsatellite markers', 'cites work', 'Isolation and characterization of di- and tetranucleotide microsatellite loci in geoduck clams, Panopea abrupta.')] | [('STUDY OF EFFECT OF CONSECUTIVE HEATING ON THERMOLUMINESCENCE GLOW CURVES OF MULTI-ELEMENT TL DOSEMETER IN HOT GAS-BASED READER SYSTEM', 'published in', 'Radiation Protection Dosimetry'), ('Heat killing of Bacillus subtilis spores in water is not due to oxidative damage', 'cites work', 'A superoxide dismutase mimic protects sodA sodB Escherichia coli against aerobic heating and stationary-phase death.'), ('Mineralogy of Sn-W-As-Pb-Zn-Cu-bearing alteration zones in intracontinental rare metal granites (Central Mongolia)', 'cites work', 'The 'chessboard' classification scheme of mineral deposits: Mineralogy and geology from aluminum to zirconium'), ('Water as a reservoir of nosocomial pathogens', 'cites work', 'Superficial and systemic illness related to a hot tub'), ('Journal of Research of the U. S. Geological Survey, 1974, volume 2, issue 4', 'has part(s)', 'A mineral separation procedure using hot Clerici solution'), ('Optimal Water-Power Flow-Problem: Formulation and Distributed Optimal Solution', 'published in', 'IEEE transactions on control of network systems'), ('Hot tub-associated dermatitis due to Pseudomonas aeruginosa. Case report and review of the literature', 'published in', 'Archives of Dermatology')] |", "Table 2: Example of misalignment between FineWeb and Wikidata. The green keywords indicate the topic of the proximal triples, while the red keywords indicate the topic of the linked Wikidata triples.", "2 Preliminaries\n2.1 Retrieval System\n\nThe core objective of a retrieval system is to identify and rank a subset of documents ( d 1 , d 2 , ..., d k ) from a large corpus C based on their estimated relevance to a query q . Classical retrieval approaches, such as BM25 (Robertson et al., 1995), rely on exact term matching and produce sparse relevance scores. In contrast, dense retrieval methods employ neural encoders to project queries and documents into a shared embedding space, enabling semantic similarity matching (Karpukhin et al., 2020). Since first-stage retrievers often produce noisy candidates, modern pipelines incorporate a second-stage re-ranking step using more expressive models. This includes LLM-based rankers (Pradeep et al., 2023b;\n\nMa et al., 2024; Sun et al., 2023) and reasoningaugmented models such as ReasonIR (Shao et al., 2025), Rank-1 (Weller et al., 2025), and RankR1 (Zhuang et al., 2025), which refine the initial rankings by modeling deeper interactions between the query and candidate documents.", "2.2 Retrieval-Augmented Generation\n\nRetrieval-Augmented Generation (RAG) (Lewis et al., 2020b) is a hybrid paradigm that enhances the generative capabilities of large language models (LLMs) by incorporating non-parametric external knowledge during inference. This design mitigates well-documented limitations of standalone LLMs, including hallucinations, confident but incorrect outputs, and inability to reflect up-to-date or domain-specific information due to static pretraining (Hurst et al., 2024; Grattafiori et al., 2024; Yang et al., 2025a).", "RAG introduces an explicit retrieval step: for a query q , a retriever selects a set of topk documents { d 1 , . . . , d k } from an external corpus. A generator G then conditions on both q and the retrieved context to produce the output a = G ( q, d 1 , . . . , d k ) where G is typically an encoderdecoder or decoder-only LLM fine-tuned to integrate retrieved evidence into its generation process.", "2.3 Challenges in RAG\n\nA central challenge in RAG is that generation quality is tightly coupled with retrieval quality. If relevant (i.e., answer-containing) documents are not retrieved, or if irrelevant context is included, the generator is prone to producing incorrect or misleading outputs (Cuconasu et al., 2024, 2025; Liu et al., 2024).", "Consequently, improving the topk retrieval results is crucial. This can be viewed both as a selection problem (how to retrieve the most relevant documents) and a filtering problem (how to suppress distracting or noisy context). To this end, several methods have been proposed that incorporate various forms of feedback , ranging from simple lexical overlap to more sophisticated agentic or reasoning-based signals, to guide and refine the retrieval process.", "In this survey, we systematically categorize these feedback mechanisms and analyze their effectiveness across different components of the RAG pipeline. We focus on how feedback is acquired, represented, and integrated into retrieval, with the", "Figure 1: Illustration of feedback signals across the RAG pipeline. Feedback can modify the query (e.g., rewriting), the retrieved pool (e.g., ranker-based expansion), or the generation loop (e.g., retrieval triggers based on uncertainty).\n\nRetrieval Reranker results Generation Query Encoder representation new results Ranker Relevance Dynamic Understanding Generated Output Feedback levels Query Level Retrieval Level Generation Time Query Understanding Ranker Relevance Dynamic Retrieval - Query Expansion - Query Rewriting\n\n- Query Vector Adaption\n\n- Query Decomposition\n- Result refinement\n\n- When to retrieve\n\n- What (new query) to retrieve\n\naim of providing a comprehensive taxonomy and highlighting open research challenges.", "Preliminary\n\nIn a retrieval-augmented generation (RAG) system, a collection of documents is first split into small chunks D , which are then embedded into vectors using an embedding model f . Given a query q , the retriever R first searches top k relevant chunks D k = { d 1 , d 2 , ..., d k } \u2282 D from the collection according to the similarity between the query and chunks, i.e., D k = R ( D,q ) . The similarity s is calculated using the inner product (IP) or L2 norm between the query embedding q = f ( q ) and chunk embeddings d = f ( d ) , \u2200 d \u2208 D . Finally, the retrievals and query are incorporated into a prompt P ( q, D k ) , which is used as the input of the generator model g to generate final answer, i.e., \u02c6 a = G ( P ( q, D k )) . We list all notations and abbreviations in the appendix.", "Related Work Retrieval-Augmented Generation\n\nRetrieval-Augmented Generation was introduced as a powerful paradigm to mitigate the intrinsic knowledge limitations of LLMs by grounding them in external information (Lewis et al. 2020). The standard RAG framework operates by retrieving relevant text chunks from a corpus and providing them as context to an LLM for answer generation (Wang et al. 2024). While effective, this approach is fundamentally constrained by the 'chunking dilemma': small, fine-grained chunks risk losing critical context, whereas larger chunks often introduce significant noise and dilute the LLM's focus (Tonellotto et al. 2024).", "Substantial research has been dedicated to overcoming this limitation. One line of work focuses on improving the retriever itself, evolving from sparse methods like BM25 (Robertson, Zaragoza et al. 2009) to dense retrieval models such as DPR (Karpukhin et al. 2020) and Contriever (Izacard et al. 2021), which learn to better capture semantic relevance. Another direction targets the indexing and organization of the source documents (Jiang et al. 2023). Recent advancements have explored creating hierarchical summaries of text chunks, allowing retrieval to occur at multiple levels of granularity. For instance, RAPTOR builds a tree of recursively summarized text clusters, enabling retrieval of both fine-grained details and high-level summaries (Sarthi et al. 2024). Despite these improvements, these methods still largely treat knowledge as a linear sequence or a simple tree of text. They do not explicitly model the complex, non-hierarchical relations that often exist between different entities and concepts within a document. This limitation hinders their ability to answer complex queries that require reasoning over these intricate connections, motivating the shift towards KG-based RAG methods."]}
{"type": "entry", "index": 31, "question": "my query rewriting sometimes makes results worse, how do i tell when it\u2019s hurting", "answer": "You can tell rewriting is hurting by watching both quantitative metrics and qualitative patterns.\n\n1) Compare retrieval quality with and without rewriting. In the paper, they directly score how well the retriever surfaces the right documents using a metric like NDCG; the reward for the rewriter is based on whether the target document is retrieved [1]. If your NDCG (or any relevance metric) is lower for rewritten queries than for raw queries, rewriting is hurting.\n\n2) Check retriever\u2013rewriter mismatch. The authors show that a rewriter trained for one kind of retriever can badly hurt another, especially when using a rewriter reinforced on non\u2011lexical (semantic) retrievers with a purely lexical system [1]. If you see drops only when you switch retrievers, that\u2019s a sign your rewriter isn\u2019t specialized correctly.\n\n3) Watch performance on semantic vs lexical retrievers. The paper finds that their rewriting methods did not improve, and could degrade, retrieval for semantic and hybrid retrievers, partly because those retrievers were not trained on the long, LLM\u2011style queries produced by the rewriter [1]. If your semantic retriever works worse on longer, \"reasoned\" rewrites than on the original short query, that\u2019s a red flag.\n\n4) Look for bad rewrite patterns. Their failing rewrites tended to (a) more than double query length [1], (b) repeat keywords or even duplicate sentences, and (c) adopt unnatural formulas like constantly saying \"must be expanded\" [1]. If your rewrites show similar patterns and coincide with metric drops, they are likely hurting.", "tags": ["_untagged"], "full_faithfulness": 1.0, "full_answer_relevancy": 0.8618374569, "seed_faithfulness": 0.8333333333, "seed_answer_relevancy": 0.861841959, "context_relevance": 0.503383836557567, "context_relevance_error": null, "context_relevance_reason": "The context passages provide relevant information about query rewriting and its impact on retrieval performance, which relates to the user's concern about query rewriting making results worse. However, while some passages discuss the importance of query formulation and the effects of different techniques, they do not directly address how to identify when query rewriting is detrimental, which is the core of the user's question. This lack of direct guidance on the user's specific query limits the overall relevance.", "structure_citation": 0.8108324178462784, "structure_error": null, "structure_reason": "The Actual Output effectively addresses the query about identifying when query rewriting is detrimental by providing specific metrics and patterns to observe. It includes multiple claims supported by citations marked with [1], which correspond to relevant snippets in the Context. However, while the claims are generally consistent with the Context, there could be a clearer connection to the specific examples provided in the Context, which would strengthen the overall alignment.", "hallucination_score": 0.8530536059521063, "hallucination_error": null, "hallucination_reason": "The response effectively identifies key methods to assess when query rewriting is detrimental, aligning well with the context provided. It accurately references quantitative metrics like NDCG and qualitative patterns, which are supported by the context discussing the importance of query formulation in retrieval systems. The only minor shortcoming is the lack of explicit mention of the need for retriever specialization, which is emphasized in the context but is implied in the output.", "contexts_used": ["Query Rewriting for RAG\n\nQuery rewriting is a pivotal component in RAG systems, as the effectiveness of the retrieval step hinges on how well the query is formulated. A poorly designed query can lead to irrelevant or low-quality retrieved documents, undermining the generation process. Conversely, an optimized query enhances the relevance of retrieved information, directly improving the overall system performance.", "Traditional query rewriting techniques, such as query expansion and reformulation, have roots in information retrieval and rely on heuristics or statistical methods to refine queries (Zhu et al. 2016). In the context of RAG, however, query rewriting must align with the needs of the retriever (Ma et al. 2023). Recent efforts have introduced learning-based approaches, including neural network models and reinforcement learning, to dynamically adapt queries based on system feedback (Wang et al. 2025; Chan et al. 2024; Li et al. 2024; Ma et al. 2023; Jin et al. 2025). Despite these advances, existing query rewriting techniques often require extensive annotated data or are constrained to specific domains (Liu et al. 2021). Our work addresses these gaps by developing a generalized reinforcement learning framework for query rewriting, tailored to enhance retrieval across diverse indices without relying on large-scale human annotations.", "Reinforcement Learning Query Rewriter\n\nIt is important to individualize query rewriter with respect to the indices, because each retriever has distinct characteristics. For example, lexical retrievers such as BM25 (Robertson, Zaragoza et al. 2009) count on the number of the words, in which simply repeating important word can augment the performance. Whereas, (multi-modal) semantic retrievers that embed (text-parsed-)documents into embedding (Faysse et al. 2024; Zhang et al. 2025) work better if the querydocument resembles their trained data, which is hard to manage. The reinforcement learning aims to align user query into the index representation space by the query rewriter QR per specific retriever R . In other words, for N online RAG systems consisting of the data source DB i and the retriever R i for i \u2208 N , we suggest to have N retrievers respectively, rather than a single universal rewriter.", "The precedent RL approaches (Ma et al. 2023; Jin et al. 2025; Nguyen et al. 2025) implicitly train the rewriter by optimizing", "where x refers to the sample from the training data D , y denotes the rewritten query by the rewriter, and z represent the final response. \u03c0 \u03b8 and \u03c0 LLM are the target rewriter and the final-responding language model. r \u03d5 is the reward function.", "In contrast, ours optimizes the rewriter explicitly, which down-scales the objective and boosts the training process. Some (Wang et al. 2025) tried explicit rewarding with massive document-wise positive and negative pair annotation, which limits in scaling covering domain and indices. On the other hand, leveraging the synthesized long user queries, we formulate the RL objective function as follows:\n\nWe adopt two function rewards, one for the query rewriting reward and the other for the formatting and redundant penalty. The retrieval reward r retrieval uses NDCG (J\u00a8 arvelin and Kek\u00a8 al\u00a8 ainen 2002) score directly that measures if the target document is retrieved.", "The penalty r penalty targets to match the format, placing the rewritten query inside <answer>...</answer> , and reduce redundant generations outside the format.\n\nThe reward function becomes\n\nwhere the lambdas are the hyper-parameters. More specifically, for each sample x , we optimize the rewriter by maximizing the following object:\n\nwhere \u03f5 and \u03b2 are hyper-parameters, and \u02c6 A i,t is the advantage based on the relative rewards of the outputs inside each group. Further, we normalized the penalty r penalty groupwise by ranging [0.5, 1] for the non-zero values.", "translation\n\nThe camera cover function needs to be expanded in order to cover the camera using the U+ Smart Home app. It is necessary to expand how the camera cover can be used to monitor the pet's condition and detect any abnormal signs using the Smart Home app, and how to protect privacy by covering the camera while connecting the Smart Home app with the smartphone. In particular, the method for covering the camera using the U+ app needs to be expanded. The method for monitoring the pet's condition and detecting abnormalities through the Smart Home app while covering the camera also needs to be expanded. In addition, the connection between the Smart Home app and the camera cover needs to be expanded. Also, using the camera cover to monitor the pet's condition through the Smart Home app-\n\nLBS URL\nLBSMHI\n\nLG U+ \uc758 \uc804 \uad6d \ub300 \ud45c \ubc88 \ud638 \uc11c \ube44 \uc2a4 \ub97c \uc774 \uc6a9 \ud558 \uc5ec \ud2b9 \uc815 \uc9c0 \uc5ed \uc758 LBS( \uc704 \uce58 \uae30 \ubc18 \uc11c \ube44 \uc2a4 ) \ub97c \uc124 \uc815 \ud558 \uace0 \uc790 \ud558 \ub294 \uae30 \uc5c5 \uc758 IT \ub2f4\ub2f9 \uc790 \uac00 \uc788 \uc2b5 \ub2c8\ub2e4 . \uc774 \ub2f4\ub2f9 \uc790 \ub294 \uc11c \uc6b8 \uac15 \ub0a8 \uad6c \uc640 \uc911 \ub791 \uad6c \uc5d0 \uc11c\uc11c \ube44 \uc2a4 \ub97c \uc81c \uacf5 \ud558 \ub824 \uace0 \ud569 \ub2c8\ub2e4 . \uadf8 \ub7ec \ub098 \uad6c \uccb4 \uc801 \uc778 \uc124 \uc815 \ubc29\ubc95 \uacfc URL \uc5d0 \ub300 \ud55c \uc815 \ubcf4 \uac00 \ud544 \uc694 \ud569 \ub2c8\ub2e4 . \uc11c \uc6b8 \uac15 \ub0a8 \uad6c \uc640 \uc911 \ub791 \uad6c \uc5d0 \uc11c LG U+ \uc758 LBS \uc11c \ube44 \uc2a4 \ub97c \uc124 \uc815 \ud558 \uae30 \uc704 \ud574 \uc5b4 \ub5a4 \uc808 \ucc28 \ub97c \uac70 \uccd0 \uc57c \ud558 \uba70 , \ud544 \uc694 \ud55c URL \uc740 \ubb34 \uc5c7 \uc778 \uac00 \uc694 ?", "There is an IT manager of a company who wants to configure an LBS (LocationBased Service) for a specific area using LG U+'s nationwide representative number service. This manager intends to provide the service in Gangnam-gu and Jungnang-gu of Seoul. However, detailed information on the configuration method and the URL is required. What procedures must be followed to configure LG U+'s LBS service in Gangnam-gu and Jungnang-gu of Seoul, and what is the required URL?", "\uc11c \uc6b8 \uac15 \ub0a8 \uad6c \uc640 \uc911 \ub791 \uad6c \ub97c \ud3ec \ud568\ud558 \ub294 LG U+ \uc758 LBS \uc11c \ube44 \uc2a4 \ub97c \uc11c \uc6b8 \uac15 \ub0a8 \uad6c \uc640 \uc911 \ub791 \uad6c \ub97c \uc11c \ube44 \uc2a4 \ub97c \uc81c \uacf5 \ud558 \ub824 \uace0 \ud558 \ub294 IT \ub2d8 \uacfc \ud568 \uaed8 \uc0ac \uc6a9 \ud558 \ub294 \uacbd \uc6b0 , \uc11c \uc6b8 \uac15 \ub0a8 \uad6c \uc640 \uc911 \ub791 \uad6c \ub97c \uc704 \ud55c \uad6c \uccb4 \uc801 \uc778 \uc124 \uc815 \ubc29\ubc95 \uacfc URL \uc744 \ud655 \uc7a5 \ud574 \uc57c \ud558 \uba70 . \ud2b9 \ud788 \uc11c \uc6b8 \uac15 \ub0a8 \uad6c \uc640 \uc911 \ub791 \uad6c \ub97c \uc0ac \uc6a9 \ud560 \ub54c LG U+ \uc758 \uc804 \uad6d \ub300 \ud45c \ubc88 \ud638 \uc11c \ube44 \uc2a4 \ub97c \uc774 \uc6a9 \ud574 LBS \ub97c \uc124 \uc815 \ud558 \uae30 \uc704 \ud55c URL \uacfc \uad6c \uccb4 \uc801 \uc778 \uc808 \ucc28 \ub97c \ud655 \uc7a5 \ud574 \uc57c \ud569 \ub2c8\ub2e4 . \uc11c \uc6b8 \uac15 \ub0a8 \uad6c \uc640 \uc911 \ub791 \uad6c \uc5d0 \uc11c \uc0ac \uc6a9 \ud558 \ub294 \uacbd \uc6b0 \ud544 \uc694 \ud55c URL \uc740 \ubb34 \uc5c7 \uc778 \uc9c0 \uc5d0 \ub300 \ud55c \ub0b4 \uc6a9 \uc744 \ud655 \uc7a5 \ud574 \uc57c \ud569 \ub2c8\ub2e4 . \ub610 \ud55c \uc911 \ub791 \uad6c \uc640 \uac15 \ub0a8 \uad6c \ub97c \uc0ac \uc6a9 \ud560 \ub54c LG U+ \uc758 LBS \uc11c \ube44 \uc2a4 \ub97c \uc704 \ud55c \uad6c \uccb4 \uc801 \uc778 URL \uacfc \uc124 \uc815 \ubc29\ubc95 \uc744 \ud655 \uc7a5 \ud574 \uc57c \ud569 \ub2c8\ub2e4 . \uadf8 \ub9ac \uace0 \uc11c \uc6b8 \uac15 \ub0a8 \uad6c \uc640 \uc911 \ub791 \uad6c \ub97c \uc0ac \uc6a9 \ud560 \uacbd \uc6b0 \ud544 \uc694 \ud55c URL \uc744 \ud655 \uc7a5 \ud574 \uc57c \ud569 \ub2c8\ub2e4 . \ub610 \ud55c \uad6c \uccb4 \uc801 \uc778 \uc124 \uc815 \uc744 \uc704 \ud55c URL \uc744 \ud655 \uc7a5 \ud574 \uc57c \ud569 \ub2c8\ub2e4 . \ub610 \ud55c URL \uc744 \ud655 \uc7a5 \ud574 \uc57c \ud569 \ub2c8\ub2e4 . \ub610 \ud55c", "When using LG U+'s LBS service that includes Seoul Gangnam-gu and Jungnang-gu together with the IT person who intends to provide services in Seoul Gangnam-gu and Jungnang-gu, the specific configuration method and URL for Seoul Gangnam-gu and Jungnang-gu must be expanded. In particular, when using Seoul Gangnam-gu and Jungnang-gu, the URL and detailed procedure to set up LBS using LG U+'s nationwide representative number service must be expanded. The content about what URL is required when used in Seoul Gangnam-gu and Jungnang-gu must be expanded. Also, when using Jungnang-gu and Gangnamgu, the specific URL and configuration method for LG U+'s LBS service must be expanded. And when using Seoul Gangnam-gu and Jungnang-gu, the required URL must be expanded. Additionally, the URL for detailed configuration must be expanded. Also, the URL must be expanded. Also\n\nFigure 2: Real-world examples of long user query synthesis and query rewriting.", "| | 0 | 1 |\n|---:|:--------------------------------|:--------------------------------|\n| 0 | ixi-RAG semantic | ixi-RAG semantic |\n| 1 | Raw query | 74.09 |\n| 2 | Qwen3-4B | 25.56 |\n| 3 | Different retriever specialized | Different retriever specialized |\n| 4 | RL-QR multi-modal | 27.92 |\n| 5 | RL-QR lexical | 56.43 |\n| 6 | RL-QR hybrid | 72.76 |\n| 7 | Retriever specialized | Retriever specialized |\n| 8 | RL-QR semantic | 71.02 |\n\nTable 5: Retrieval performance comparison on text-modal RAG with semantic retriever", "| | ixi-RAG hybrid | ixi-RAG hybrid |\n|---:|:--------------------------------|:--------------------------------|\n| 0 | Raw query | 81.93 |\n| 1 | Qwen3-4B | 26.99 |\n| 2 | Different retriever specialized | Different retriever specialized |\n| 3 | RL-QR multi-modal | 27.07 |\n| 4 | RL-QR semantic | 65.45 |\n| 5 | RL-QR lexical | 72.39 |\n| 6 | Retriever specialized | Retriever specialized |\n| 7 | RL-QR hybrid | 81.20 |\n\nTable 6: Retrieval performance comparison on text-modal RAG with semantic retriever\n\nest 2% gain, likely due to its partial exposure to lexical retriever signals during training. Conversely, all other approaches demonstrate significantly degraded performance, indicating that models reinforced using non-lexical retrievers may be ineffective when applied to a purely lexical retrieval system.", "Retrieval Performance with Semantic and Hybrid Retrievers. As presented in Tables 5, 6, the proposed rewriting methods did not enhance retrieval performance. Recent work (Su et al. 2024) reports pool performances of semantic retrievers than which of lexical retrievers (e.g., BM25) with rewrote queries. The train data for semantic retrievers have little relation with the LLM-rewrote queries which are relatively longer, resulting poor representation learning for the LLM-rewrote queries. If we adopt reasoning semantic retriever ( e.g., ReasonIR (Shao et al. 2025)), the proposed RLQR might improved with the semantic retrievers. We leave it for the future work.", "Qualitative results. Figure 2 illustrates real-world examples with long query synthesis and re-wrote queries by RL-QRmulti-modal . The synthetic long user queries are adequate and dedicated to the documents. The rewrote queries shows patterns (1) emphasizing keywords by repetition in manner of duplicating sentences, (2) unique accents ( must be expanded ) which are far the from natural languages.", "Correlation between the query length and the retrieval performance. As indicated in Table 7 and observed in and Figure 2, excluding the RL-QR multi-modal case on D tm where", "| | Method.Method | Data.Data | Average Length.Origin | Average Length.Rewrote |\n|---:|:------------------|:------------|:------------------------|-------------------------:|\n| 0 | RL-QR multi-modal | | | 504 |\n| 1 | RL-QR lexical | D mm | 191 | 357 |\n| 2 | RL-QR semantic | | | 436 |\n| 3 | RL-QR hybrid | | | 415 |\n| 4 | RL-QR multi-modal | | | 60 |\n| 5 | RL-QR lexical | D | | 353 |\n| 6 | RL-QR semantic | tm | 159 | 435 |\n| 7 | RL-QR hybrid | | | 405 |\n\nTable 7: Average length of the rewritten query.", "no improvements were observed, the proposed methods generally increase the query length by more than twofold. This trend may stem from the limited training scheme, which involves only thousands of training samples and a single epoch. Alternatively, the tendency to expand queries could be attributed to the simplistic reward training approach."]}
{"type": "entry", "index": 32, "question": "how do i check if a question is even answerable from my corpus without having gold answers", "answer": "You can treat your corpus itself as the \u201cjudge\u201d of answerability, without needing human\u2011written gold answers, by doing a functional QA test over the corpus.\n\n1. Build a QA system over your corpus (RAG-style)\n- Index your corpus into chunks and build an embedding/keyword-based retriever.\n- At query time, retrieve the most relevant chunks and let a model answer only using those chunks as context [1].\n\n2. Use the corpus to test each question\n- Feed the question to this QA pipeline.\n- Mark the question as **answerable** if the system can (a) retrieve relevant context from the corpus and (b) produce a plausible answer grounded in that context [1].\n- Mark it as **unanswerable** if the system fails to retrieve relevant context or can only say \u201cI don\u2019t know\u201d given the corpus [1][2].\n\n3. Explicitly model \u201cI don\u2019t know\u201d behavior\nDocument [2] describes evaluating four scenarios: (S1) context sufficient and an answer is given, (S2) context insufficient and the system correctly responds with \u201cIDK\u201d, (S3) context insufficient but the system still tries to answer, and (S4) context sufficient but system wrongly says \u201cIDK\u201d [2].\nFor your purpose:\n- Treat S1 as \u201cquestion is answerable from the corpus.\u201d\n- Treat S2 as \u201cquestion is not answerable from the corpus.\u201d\n- Use S3 and S4 rates to monitor hallucinations and overly cautious behavior [2].\n\n4. Tighten the test by restricting the knowledge base\nIn [1], they deliberately restrict the QA system\u2019s knowledge base to a very small, high-level curriculum document so that only questions truly grounded in that document are counted as valid/answerable [1]. You can do the same with your own corpus slices when you want a stricter notion of answerability.\n", "tags": ["_untagged"], "full_faithfulness": 0.9615384615, "full_answer_relevancy": 0.8916388759, "seed_faithfulness": 0.6923076923, "seed_answer_relevancy": 0.9008076089, "context_relevance": 0.4912420523725494, "context_relevance_error": null, "context_relevance_reason": "The context passages provide some relevant information about automated question generation and evaluation methods, but they do not directly address how to check if a question is answerable from a corpus without gold answers. The passages discuss techniques like RAG and STS, which are related to question generation and evaluation, but they lack specific guidance on assessing the answerability of questions in the absence of gold standards.", "structure_citation": 0.562087382191051, "structure_error": null, "structure_reason": "The Actual Output provides a detailed approach to assessing question answerability using a corpus, referencing citations [1] and [2] appropriately. However, while it supports its claims with relevant methodologies, it lacks direct citations from the provided Context for some of its assertions, particularly in the explanation of the QA system's functionality. This results in a minor deduction for unsupported claims, but overall, the response aligns well with the evaluation steps.", "hallucination_score": 0.859683511087896, "hallucination_error": null, "hallucination_reason": "The response effectively addresses the input question about checking answerability from a corpus without gold answers. It outlines a clear methodology for using the corpus as a judge, detailing steps for building a QA system, testing questions, and modeling 'I don\u2019t know' behavior. The output aligns well with the context provided, particularly in its emphasis on retrieval-augmented generation (RAG) and the evaluation framework. The only minor shortcoming is a lack of explicit mention of potential limitations or challenges in implementing the suggested methods.", "contexts_used": ["2 Related Work\n\n2.1 The Evolution of Automated Question Generation (AQG) in Educational NLP\n\nAutomated Question Generation (AQG) has been a long-standing goal in Natural Language Processing (NLP) for education. Early approaches often relied on rule-based systems and templates, which were rigid and difficult to scale. The advent of neural networks and, more recently, large language models (LLMs) has revolutionized the field, enabling the generation of fluent and complex questions from source text.[4] Recent studies highlight that while LLMs are", "2.1 The Evolution of Automated Question Generation (AQG) in Educational NLP\n\npowerful generators, the quality of their output for specialized educational purposes is highly dependent on the context provided and the prompting strategy employed.[5] The challenge has shifted from merely generating a syntactically correct question to generating one that accurately tests a specific concept or cognitive skill, such as conceptual reasoning rather than simple factual recall.[5]", "2.2 Grounding Generative Models: The Rise of Retrieval-Augmented Generation (RAG)\n\nRetrieval-Augmented Generation (RAG) has emerged as the foremost technique for addressing the 'grounding problem' of LLMs.[9] First proposed by Lewis et al. (2020), the RAG architecture combines the strengths of a parametric memory (the pre-trained LLM) with a non-parametric memory (an external, retrievable knowledge source).[10] In this hybrid model, the LLM does not rely solely on its internal, static knowledge. Instead, given a prompt, it first retrieves relevant documents from the external source and then uses this retrieved context to inform and constrain its generated output. This approach has been shown to significantly reduce hallucinations and improve the factual accuracy of LLM responses. In educational technology, RAG is particularly valuable. It allows for the creation of AI systems that can provide factually consistent, curriculum-aligned answers and content.[9] Furthermore, because the knowledge base is external, it can be easily updated with new or corrected information (e.g., a revised syllabus or new textbook) without the need for costly and time-consuming LLM retraining.[9] This makes RAG a practical and adaptable solution for developing dynamic educational tools.[14]", "2.3 Generative AI for Low-Resource Languages: Gaps and Opportunities\n\nThe development of GenAI has been heavily skewed towards high-resource languages like English, creating a 'digital language divide'.[11] Languages with smaller digital footprints, such as Bahasa Melayu, face significant challenges, including data scarcity, lack of culturally-aware benchmarks, and poor representation in the massive datasets used to train foundational models.[6] Consequently, even state-of-the-art LLMs often exhibit weaker performance in these languages.[6] While NLP toolkits and resources for Bahasa Melayu exist, they are often general-purpose and may not be sufficient for highly specialized, domain-specific tasks like generating questions that adhere to the nuances of the Malaysian national curriculum.[12] This study positions RAG as a pragmatic strategy to bridge this gap. By forcing the model to ground its output in a curated, local-language corpus (the official curriculum documents), RAG can effectively leverage the power of a global LLM for a local, specialized task, circumventing the need for a massive, domain-specific training dataset.", "2.4 Automated Evaluation of Generated Content: Beyond Lexical Overlap\n\nEvaluating the quality of generated text is a complex challenge. Traditional metrics like ROUGE and BLEU, which rely on lexical overlap, are inadequate for assessing semantic meaning and are thus poorly suited for evaluating educational questions.[18] Semantic Textual Similarity (STS) offers a more robust alternative. STS measures the degree of semantic relatedness between two texts, typically by calculating the cosine similarity between their vector embeddings.[19] This allows for an assessment of whether a generated question aligns thematically with a specific learning objective, even if they do not share the exact same words.[21] However, STS is not without its limitations. Research has shown that standard STS can be ambiguous and may not always correlate well with performance on a specific downstream task.[19] The similarity score depends on the aspect of interest, which can be subjective.[22] This critique motivates the need for a second, complementary evaluation metric that is more functional and task-oriented. This study introduces a novel RAG-based Question-Answering (RAG-QA) validation method to address this gap. This method provides a direct, functional test of a question's contextual validity by assessing whether it can be answered using the core curriculum document as a knowledge source, thereby offering a stricter and more objective measure of curriculum relevance.", "3 System Design and Methodology\n\n3.1 Corpus and Grounding Documents\n\nThe experiment utilizes two official Malaysian educational documents in PDF format, both focusing on Chapter 1 ('Nombor Nisbah' or Rational Numbers) of the Form 1 Mathematics syllabus.", "Knowledge Source (Nota Bab 1.pdf): This is a comprehensive 84-page document of teacher-prepared notes, examples, and exercises.[1] It provides the rich, detailed content knowledge for the topic, including definitions, worked examples ( Contoh ), and practice problems ( Latih Diri ).[1] This document serves as the non-parametric memory for the RAG pipelines.", "Pedagogical Blueprint (RPT Bab 1.pdf): This is a concise, 1-page official document from the Malaysian Ministry of Education outlining the Yearly Teaching Plan (RPT).[1] It lists the specific learning standards ( Standard Pembelajaran ) that students are required to master, such as 'Mewakilkan integer pada garis nombor' (Represent integers on a number line) and 'Menyelesaikan masalah yang melibatkan integer' (Solve problems involving integers).[1] This document serves as the ground truth for evaluating curriculum alignment.", "3.2 System Architecture Overview\n\nThe system is designed as a comparative study of four incremental pipelines for generating multiple-choice questions. All pipelines leverage the OpenAI GPT-4o model as the core generative engine.[2] The generated questions are then passed through a dual-pronged evaluation framework. The overall architecture is depicted in Figure 1.", "Figure 1: Architecture of the Incremental GenAI Question Generation Pipeline INPUTS: [Nota Bab 1.pdf] -> [RPT Bab 1.pdf] [Nota Bab 1.pdf] -> (feeds knowledge to) -> [RAG Pipelines (Methods 3 & 4)] GENERATION METHODS (in parallel) \u00b7 Method 1: Structured Prompt \u00b7 Method 2: Basic Prompt \u00b7 Method 3: LangChain RAG \u00b7 Method 4: Manual RAG All methods send prompt -> [GPT-4o Generation Core] -> Output: [Structured JSON MCQ] EVALUATION MODULE [Structured JSON MCQ] splits into two paths: \u00b7 Path 1: STS Evaluation vs [RPT Bab 1.pdf] -> Output: [Alignment Score] \u00b7 Path 2: RAG-QA Validation vs [RPT Bab 1.pdf] -> Output: [Valid/Invalid]\n\nFigure 1: A textual representation of the generation pipeline's architecture as described in the source.", "3.3 The Generation Pipelines: An Incremental Approach\n\nFour distinct methods were implemented to test the efficacy of different prompting and grounding strategies.", "3.3.1 Method 1: Structured Prompting with Function Calling\n\nThis method leverages the advanced 'tool mode' or function calling capability of the GPT-4o API to ensure syntactically valid output. A Pydantic schema defining the desired MCQ structure (question, options, correct answer, explanation) is passed to the model. The prompt is generic, simply requesting a question on the topic of 'Form 1 Mathematics, Rational Numbers.' The model is instructed to use the provided schema, which guarantees the output is a well-formed JSON object, eliminating the need for post-processing or error handling of the output structure.", "3.3.2 Method 2: Basic Prompting with JSON Parsing\n\nThis method serves as a simpler, non-grounded baseline. The prompt explicitly instructs the model to generate an MCQ on the same topic and to format the response as a JSON object. Unlike Method 1, this approach does not guarantee a valid JSON structure. Therefore, the pipeline includes a parsing step with error handling to manage potentially malformed or incomplete outputs. This method represents a common, straightforward approach to structured data generation with LLMs.", "3.3.3 Method 3: Framework-based RAG with LangChain and FAISS\n\nThis method represents a standard, off-the-shelf implementation of the RAG pattern, designed to ground the model's generation in the provided knowledge source. The pipeline is orchestrated using the LangChain framework.[23] The process is as follows:\n\nIngestion: The Nota Bab 1.pdf is loaded using LangChain's PyPDFLoader.", "Chunking: The document text is split into smaller, overlapping chunks using the RecursiveCharacterTextSplitter.\n\nEmbedding: Each text chunk is converted into a high-dimensional vector using OpenAI's embedding models.", "Storage & Retrieval: The vector embeddings are stored in a FAISS (Facebook AI Similarity Search) vector store, which allows for efficient similarity search.[24]\n\nGeneration: When a query is made, the system retrieves the most relevant text chunks from the FAISS store and passes them, along with the prompt, to GPT-4o via LangChain's RetrievalQA chain to generate the final MCQ.", "3.3.4 Method 4: Manual RAG with Custom Chunking and Similarity Ranking\n\nThis method was developed to test whether a more controlled, domain-aware RAG pipeline could outperform a generic framework-based approach. While frameworks like LangChain offer convenience, their generic components, such as text splitters, are agnostic to the logical structure of the source document. An educational text like Nota Bab 1 has a clear visual and semantic structure (e.g., headings, example boxes, exercise sections) that a simple character-based splitter might disrupt, potentially severing a mathematical example from its explanation and degrading the quality of the retrieved context. A manual approach allows for more intelligent chunking that respects this inherent structure. The implementation is as follows:\n\nIngestion & Chunking: The PyMuPDF library is used to parse the Nota Bab 1.pdf.[25] Custom logic is applied to group text blocks based on their position and font characteristics, aiming to keep semantically coherent units (like an entire 'Contoh' block) together in a single chunk.", "3.3.4 Method 4: Manual RAG with Custom Chunking and Similarity Ranking\n\nEmbedding: As in Method 3, OpenAI's embedding models are used to vectorize the custom chunks.\n\nRetrieval: A custom retrieval function is implemented using scikit-learn's cosine_similarity function to find the most relevant chunks for a given query by comparing query and chunk embeddings.[20]\n\nGeneration: The top-ranked chunks are formatted and passed as context to the GPT-4o API to generate the MCQ.", "3.4 The Evaluation Framework\n\nA dual evaluation framework was designed to automatically assess each generated question on two critical dimensions: curriculum alignment and contextual validity. This framework is depicted in Figure 2.", "Figure 2: Semantic Similarity Scoring and RAG-QA Validation Process INPUT: [Generated MCQ] [Generated MCQ] -> Path 1: \"Evaluation 1: STS for Curriculum Alignment\" \u00b7 MCQ is embedded and compared (cosine similarity) with pre-embedded chunks from RPT Bab 1.pdf. \u00b7 OUTPUT: Numerical \"Alignment Score\" [Generated MCQ] -> Path 2: \"Evaluation 2: RAG-QA for Contextual Validity\" \u00b7 MCQ is fed as a query to a RetrievalQA pipeline. \u00b7 Knowledge Base for QA is exclusively RPT Bab 1.pdf. \u00b7 OUTPUT: Binary \"Valid/Invalid\" label\n\nFigure 2: A textual representation of the dual evaluation framework as described in the source.", "3.4.1 Evaluation 1: Curriculum Alignment via Semantic Textual Similarity (STS)\n\nThis metric quantifies how closely a generated question aligns with the official learning standards. The RPT Bab 1.pdf is chunked, with each chunk corresponding to a specific learning standard (e.g., '1.2.1 Menambah dan menolak integer...'). Each RPT chunk is converted into a vector embedding. Each generated question is also converted into a vector embedding. The cosine similarity is calculated between the question's embedding and every RPT chunk's embedding. The maximum similarity score across all RPT chunks is recorded as the question's final alignment score. A higher score indicates a stronger semantic relationship to an official learning objective.", "3.4.2 Evaluation 2: Contextual Validity via RAG-based Question-Answering (QA)\n\nThis novel evaluation method provides a functional test of a question's relevance. The underlying principle is that a valid, curriculum-aligned question should be answerable using the curriculum's own definition of its scope. By using the sparse, high-level RPT document as the knowledge source, this becomes a stringent test. A question that can be answered using only the learning objectives must be highly relevant to one of those objectives. This creates a symbiotic validation loop where a RAG-like process is used to evaluate the output of another generative process. The implementation is as follows:\n\nA separate RetrievalQA pipeline is configured using LangChain.", "3.4.2 Evaluation 2: Contextual Validity via RAG-based Question-Answering (QA)\n\nThe knowledge base for this QA pipeline is populated exclusively with the content of the RPT Bab 1.pdf.\n\nEach generated MCQ's question stem is fed as a query to this QA pipeline.", "3.4.2 Evaluation 2: Contextual Validity via RAG-based Question-Answering (QA)\n\nThe outcome is recorded as 'Valid' if the QA system can retrieve relevant context from the RPT and generate a plausible answer.", "3.4.2 Evaluation 2: Contextual Validity via RAG-based Question-Answering (QA)\n\nIt is recorded as 'Invalid' if the system fails to find relevant context, indicating the question is likely off-topic or misaligned with the core learning standards.", "II. Answer Evaluation Metrics\n\nWe evaluate generated answers based on four distinct scenarios: (S1) context is sufficient and an answer is provided; (S2) context is insufficient and the system responds with 'IDK'; (S3) context is insufficient but the system attempts a factual answer (not IDK); and (S4) context is sufficient but the answer is 'IDK'. Key metrics include Factuality/Faithfulness , which measures whether answers are factually correct and grounded in the retrieved context, computed as Grounded Accuracy = \ud835\udc41 \ud835\udc46 1 ,\ud835\udc39\ud835\udc36 \ud835\udc41 \ud835\udc46 1 . Answer Completeness assesses if all sub-questions are answered, given by Completeness = \ud835\udc41 \ud835\udc46 1 ,\ud835\udc36\ud835\udc5c\ud835\udc5a\ud835\udc5d \ud835\udc41 \ud835\udc46 1 . Precision quantifies the fraction of valid factual answers among all attempted (S1 and S3): Precision = \ud835\udc41 \ud835\udc46 1 ,\ud835\udc3a\ud835\udc5c\ud835\udc5c\ud835\udc51 \ud835\udc41 \ud835\udc46 1 + \ud835\udc41 \ud835\udc46 3 . Recall measures the fraction of correctly answered, truly answerable queries (S1 and S4): Recall = \ud835\udc41 \ud835\udc46 1 ,\ud835\udc3a\ud835\udc5c\ud835\udc5c\ud835\udc51 \ud835\udc41 \ud835\udc46 1 + \ud835\udc41 \ud835\udc46 4 . Accuracy reflects overall correctness, rewarding both valid S1 answers and correct S2 IDK responses: Accuracy = \ud835\udc41 \ud835\udc46 1 ,\ud835\udc3a\ud835\udc5c\ud835\udc5c\ud835\udc51 + \ud835\udc41 \ud835\udc46 2 \ud835\udc40 , where \ud835\udc40 is the total number of queries.\"Good\" requires both factual correctness AND completeness. Finally, the Hallucination Rate captures the proportion of unsupported or incorrect answers (S3 and incorrect S1), calculated as Hallucination Rate = \ud835\udc41 \ud835\udc46 3 + \ud835\udc41 \ud835\udc46 1 ,\ud835\udc35\ud835\udc4e\ud835\udc51 \ud835\udc40 .", "Table 5 summarizes generation performance across these metrics and user intent types, highlighting the system's strengths in managing contextual ambiguity and mitigating hallucinations.", "| | Intent | Coverage (%) | Precision (%) | Recall (%) | Hallucination (%) |\n|---:|:---------------------|:---------------|:----------------|:-------------|:--------------------|\n| 0 | authenticity | 94.0% | 98.2% | 98.4% | 1.69% |\n| 1 | checkout | 85.9% | 98.0% | 99.5% | 1.32% |\n| 2 | delivery_sla | 92.4% | 96.6% | 98.4% | 3.11% |\n| 3 | offers_and_discounts | 90.85% | 96.42% | 98.62% | 2.90% |\n| 4 | payment_options | 95.9% | 96.5% | 97.6% | 3.36% |\n| 5 | product_exchange | 79.4% | 91.3% | 92.9% | 6.89% |\n| 6 | product_spec | 86.4% | 97.2% | 98.7% | 1.93% |\n| 7 | return_policy | 84.0% | 96.7% | 96.9% | 2.72% |\n| 8 | size_and_fit | 35.2% | 90.3% | 94.8% | 3.42% |\n| 9 | stock_availability | 51.4% | 95.1% | 97.1% | 2.50% |\n| 10 | variant | 77.6% | 94.4% | 95.8% | 4.38% |\n| 11 | warranty | 90.0% | 97.7% | 98.7% | 2.00% |\n\nTable 5: Answer Generation Performance Across Intents: Coverage, Precision, Recall, and Hallucination Rate", "5 Experiments\n5.1.1 Benchmarks\n\nIn our experiments, we employed a diverse set of 12 public RAG QA benchmarks, covering various question domains, answer formats, and reference document types. Specifically, we used the web portion of the CRAG dataset (Yang et al., 2024), after filtering out the false-premise type of questions, we utilized BGE embedding (Xiao et al., 2024) to rank references, resulting in a total of 643 samples.", "For the remaining 11, we leveraged pre-processed data from RAGBench (Friel et al., 2024), down-sampling each to 100 samples. These benchmarks cover relevant real-world application areas, including health (Jin et al., 2019; M\u00f6ller et al., 2020), finance (Chen et al., 2021; Zhu et al., 2021), customer support (Sadat et al., 2023; Nandy et al., 2021; Malaviya et al., 2023), legal (Hendrycks et al., 2021), and general knowledge (Yang et al., 2018; Kamalloo et al., 2023; Malaviya et al., 2023).", "5.1.2 Metrics\n\nTo evaluate the factual quality of generated answers, we follow metrics suggested by Yang et al. (2024), classifying each response as either accurate, hallucinated, or missing. Accurate responses fully and accurately answer the question. Hallucinated answers contain inaccurate or misleading information. Missing responses either refuse to answer or fail to completely address the question. To summarize the overall factuality, we use a factuality score , defined as the accuracy rate minus the hallucination rate. Factuality score provides a scalar measure of overall answer quality, where hallucinated answers are penalized twice as much as missing answers.", "To evaluate factuality for CRAG, HotpotQA, MS Marco, FinQA, TAT-QA, HAGRID, and ExpertQA , we employed an LLM-as-judge to compare the ground-truth with the candidate answers. For other benchmarks,", "| | Benchmark | Baseline | NaiveSFT | STaR (Zelikman et al., 2022) | LLMQuoter (Bezerra et al., 2025) | RAFT (Zhang et al., 2024a) | PrismRAG (This Work) |\n|---:|:---------------------------------|:-----------|:-----------|:-------------------------------|:-----------------------------------|:-----------------------------|:-----------------------|\n| 0 | CRAG (Yang et al., 2024) | 34.2% | 27.8% | 37.2% | 34.4% | 34.3% | 39.2% |\n| 1 | CovidQA (M\u00f6ller et al., 2020) | 80.0% | 83.0% | 76.0% | 89.0% | 90.0% | 95.0% |\n| 2 | DelucionQA (Sadat et al., 2023) | 89.0% | 90.0% | 92.0% | 89.0% | 92.0 | 97.0% |\n| 3 | Emanual (Nandy et al., 2021) | 92.0% | 91.0% | 91.0% | 91.0% | 92.0% | 98.0% |\n| 4 | ExpertQA (Malaviya et al., 2023) | 83.0% | 83.0% | 84.0% | 82.0% | 83.0% | 83.0% |\n| 5 | FinQA (Chen et al., 2021) | 83.0% | 68.0% | 72.0% | 83.0% | 75.0% | 71.0% |\n| 6 | HAGRID (Kamalloo et al., 2023) | 89.0% | 89.0% | 83.0% | 89.0% | 87.0% | 90.0% |\n| 7 | HotpotQA (Yang et al., 2018) | 93.0% | 63.0% | 58.0% | 92.0% | 90.0% | 89.0% |\n| 8 | MS Macro (Nguyen et al., 2016) | 82.0% | 76.0% | 81.0% | 81.0% | 81.0% | 82.0% |\n| 9 | PubMedQA (Jin et al., 2019) | 80.0% | 78.0% | 76.0% | 77.0% | 78.0% | 90.0% |\n| 10 | TAT-QA (Zhu et al., 2021) | 77.0% | 66.0% | 66.0% | 79.0% | 90.0% | 90.0% |\n| 11 | TechQA (Castelli et al., 2019) | 58.0% | 62.0% | 58.0% | 75.0% | 79.0% | 82.0% |\n| 12 | Avg. | 78.4% | 73.1% | 72.9% | 80.1% | 80.9% | 83.8% |", "Table 2 Comparison of RAG QA factuality scores for different approaches. PrismRAG improves average factuality by 5.4% over the baseline, and it outperforms SOTA such as STaR, LLMQuoter, and RAFT by \u223c 3% -11% .", "we utilized a fact-checking tool similar to VeriScore (Song et al., 2024), which is more suitable for long-form answers or scenarios with multiple potential answers.", "5.1.3 Implementation\n\nWe used Llama-3.1-70b-instruct (Grattafiori et al., 2024) as the base model across all our experiments. For fine-tuning, we experimented with learning rate of 10 -5 , computing loss over the entire assistant response (strategy, thought, and answer when present) but not using loss over the instruction prompt. For inference, we use a typical RAG prompt as in Appendix G.2 with generation temperature of 1 . 0 and top-p of 0 . 9 .", "For comparisons with other work, to ensure fairness, we leveraged the same seed data and dataset size, while re-implementing their train data generation logic. Specifically, for: ( a ) NaiveSFT : Directly trained on final answers without strategization and CoT. ( b ) STaR (Zelikman et al., 2022): Implemented a rationalization chain for RAG QA using static CoT instructions. ( c ) LLMQuoter (Bezerra and Weigang, 2025): Implemented the same quote extraction instructions to produce training data. ( d ) RAFT (Zhang et al., 2024a): Reused their data generation code-base replacing closed-sourced model endpoints with LLaMA.", "5.2 Results\n\nOverall results: Table 2 presents a comparison of the proposed method ( PrismRAG ) with the baseline model and other related work in the literature. As shown in this Table, the proposed method demonstrates substantial improvements in factuality across benchmarks. Notably, PrismRAG achieves a factuality score improvement of 5% for the CRAG dataset, delivering best results in 9 out of 12 benchmarks, and an overall macro-average gain of 5 . 4% over the baseline. From the average results, NaiveSFT and STaR regress over the baseline, while LLMQuoter and RAFT show more promise. This finding confirms the drawbacks of directly training on QA labels, while highlighting the benefits of training on intermediate tasks (e.g., quote extraction in LLMQuoter) and training for resilience (e.g., irrelevant references in RAFT). The breakdown of results is provided in Appendix D.", "Sensitivity to references: We conducted an experiment using the CRAG benchmark by limiting the number of references and measuring the impact on performance. From Figure 1 results, the proposed method consistently outperforms the baseline, with its margin of improvement increasing as more references are used. This demonstrates the effectiveness of PrismRAG in utilizing retrieved documents (see breakdown charts in Appendix E).", "7 Evaluation\n\nIn this section, we outline our methodology for evaluating the system's ability to (1) retrieve the correct sections of a regulatory corpus, (2) generate factually accurate answers and (3) demonstrate flexibility of navigation through the interconnection of triplets in related sections. We detail our sampling procedure, the construction of queries, the measurement of section-level overlap, the assessment of factual correctness, and the analysis of triplet-based navigation.\n\nFigure 4: Evaluation Methodology\n\nSystem Evaluation\n\nSampling and Ground Truth\n\nLLM QA Generation Factual Accuracy Sample Sections Find Related Sections Concat Text Section Overlap Triplet Navigation LLMgen on\n\nLM eval or Expert\n\nand Gj\nTriplet vs Non-Triplet", "7.1 Sampling and Ground Truth Construction\n\nRandom Sampling of Sections. Let S = { s 1 , s 2 , . . . , s N } be the full set of sections of the regulatory corpus. We draw a random subset\n\nwhere each s i j is considered a target section for evaluation, and k \u226a N .", "Identifying All Ground Truth Mentions. For each sampled section s i j , we locate all other sections in the corpus that reference or expand upon the same regulatory ideas or entities. Formally, let\n\ndenote the set of sections that contain overlaps or references relevant to s i j . We then create a re-told story by concatenating s i j with all sections in M ( s i j ):\n\nThis concatenated text \u02dc s i j is treated as the ground truth context for the focal section s i j .", "7.2 LLM-Generated Questions and Answers\n\nWe employ a Large Language Model, denoted LLM gen , to produce a set of questions and corresponding reference answers based on each concatenated text \u02dc s i j . Formally,\n\nwhere Q i j = { q 1 , q 2 , . . . , q m } and A i j = { a 1 , a 2 , . . . , a m } . Each pair ( q r , a r ) is presumed to be responsible via the original information in \u02dc s i j .", "7.3.1 Section-Level Overlap\n\nTo answer each question q r \u2208 Q i j , our system retrieves a set of sections R i j ,r deemed relevant (based on embedding retrieval, triplet matching, or both). We measure the level of overlap between the recovered sections R i j ,r and the ground truth target section s i j (along with its reference set M ( s i j )).", "Definition: Overlap score. Let G i j = { s i j } \u222a M ( s i j ) be the set of ground truth sections. Suppose that the system returns R i j ,r = { r 1 , r 2 , . . . , r \u2113 } . We define the overlap score O for question q r as\n\nThus,\n\nif R i j ,r \u2229 G i j = \u2205 , then O = 0;\n\nif R i j ,r returns exactly one section, r 1 , and r 1 = s i j , then O = 1;\n\nif, for instance, the system returns three sections, only one of which matches any in G i j , then O = 1 / 3.", "We can further refine this measure by applying a similarity threshold \u03b8 for the equivalence between the retrieved sections and the ground truth sections (e.g., if the sections partially overlap or are highly similar). In that case,", "7.3.2 Factual Correctness of Answers\n\nOnce the system retrieves relevant sections and processes them through the QA pipeline (with or without associated triplets), it produces an answer a \u22c6 r . We compare a \u22c6 r with the reference answer a r from LLM gen .", "LLM-Based Fact Checking. We use a secondary evaluation model LLM eval or a domain expert to assess whether a \u22c6 r is factually correct with respect to the original text \u02dc s i j . We denote:\n\nWe measure correctness with two conditions:\n\nWith Triplets : The system's answer is grounded in the set of triplets that directly link to the retrieved sections.", "Without Triplets : The system response is derived purely from the retrieval of raw text, without referencing the triplet data structure.", "By comparing the correctness scores for these two conditions, we quantify the impact of structured triplets in factual precision."]}
{"type": "entry", "index": 33, "question": "how do i verify citations are actually backing the claim, not just random \u201clooks related\u201d refs", "answer": "You can think of citation checking as doing your own small \u201cmethodological audit\u201d of how a claim uses prior work, similar to VERIRAG\u2019s Source Consistency check (C9) [1].\n\nHere\u2019s a practical way to do it:\n\n1. Isolate the core claim.\n   Treat the claim as a clear statement you want to test (the core assertion in VERIRAG\u2019s terms) [1].\n\n2. Look at the exact sentences around each citation.\n   Ask: what, precisely, is the author using this citation to support (size of effect, direction of effect, safety, etc.)? This mirrors how VERIRAG ties an assertion to its evidence set and methodological context [1].\n\n3. Open the cited paper and read its abstract, results, and conclusions.\n   Check whether the cited study actually says what the citing sentence claims. VERIRAG\u2019s C9 is explicitly about checking that claims and citations are consistent with the broader evidence base, not just loosely related [1].\n\n4. Look for ignored or contradictory evidence.\n   C9 is designed to detect cherry\u2011picking by asking if the introduction and discussion fairly represent prior work, \"including any contradictory findings\" [1]. If the citing paper only highlights supportive results and skips well\u2011known opposing studies, that\u2019s a warning sign.\n\n5. Judge the overall pattern, not a single citation.\n   VERIRAG aggregates multiple checks into a profile of credibility rather than relying on one metric [1]. Similarly, consider whether citations, taken together, give a balanced picture or seem one\u2011sided.\n\nIf you do this systematically\u2014checking whether each cited study\u2019s actual findings match how it is described, and whether important opposing work is missing\u2014you are, in essence, performing the kind of source-consistency audit VERIRAG automates [1].", "tags": ["_untagged"], "full_faithfulness": 1.0, "full_answer_relevancy": 0.8472903685000001, "seed_faithfulness": 0.3529411765, "seed_answer_relevancy": 0.8721894068, "context_relevance": 0.8352804259540563, "context_relevance_error": null, "context_relevance_reason": "The context passages provide substantial information relevant to verifying citations and assessing their validity in supporting claims. Specifically, the discussion on methodological audits and the structured approach of VERIRAG directly addresses the user's concern about ensuring citations are not merely related but substantively support the claims made. However, while the passages are informative, they could be more explicitly focused on practical steps for users to verify citations, which slightly limits the overall relevance.", "structure_citation": 0.7567321512111536, "structure_error": null, "structure_reason": "The Actual Output effectively outlines a systematic approach to verifying citations, aligning well with the evaluation steps. It includes multiple claims supported by the citation [1], which corresponds to the Context provided. However, while the claims are generally supported, the response could benefit from explicitly referencing more specific snippets from the Context to strengthen the connection between the claims and the evidence. Overall, it demonstrates a strong understanding of the citation verification process but lacks some depth in citation matching.", "hallucination_score": 0.85, "hallucination_error": null, "hallucination_reason": "The response effectively outlines a systematic approach to verifying citations, closely aligning with the methodological audit framework described in the context. It accurately reflects the claims made in the input by detailing steps such as isolating the core claim and checking the consistency of citations with the original studies. However, while it provides a comprehensive method, it could benefit from explicitly referencing the importance of considering contradictory evidence as highlighted in the context, which would strengthen its alignment with the evaluation steps.", "contexts_used": ["Critical Analysis\n\"Overgeneralizes the claim\"\n\nFigure 1: Illustration of VERIRAG's Effectiveness based on a sample record in test set: a standard RAG system treats a potentially flawed paper as an equally valid source, while VERIRAG performs a methodological audit.", "2 Methodological Audit Framework\n\n2.1 Preliminaries\n\nA healthcare research claim is not merely an assertion; it is the culmination of a complex process of data collection, analysis, and interpretation. Our framework deconstructs this process as it is represented in the text.", "DEFINITION2.1 (EVIDENCE-BASEDCLAIM).Aresearchclaim c is a tuple ( A, E, M ), where A is the core Assertion being made, E is the Evidence Set (e.g., reported statistics, figures, tables) presented in its support, and M is the Methodological Context (e.g., study design, statistical methods, inclusion criteria) that qualifies the evidence.", "DEFINITION 2.2 (METHODOLOGICAL AUDIT VIA NL). A validation check V is an operator that assesses the integrity of a claim by analyzing the natural language (NL) representation of its evidence and methods. It evaluates the consistency, sufficiency, and rigor of the reported information to identify potential threats to the validity of the findings.", "2.2 Document Representation\n\nTo operationalize the methodological audit, VERIRAG transforms each source paper into a standardized, two-part representation. This ensures that all downstream processes, from evidence retrieval to the Veritable checks, can operate on consistent and predictable data objects.", "First, the raw text of each paper is processed into a set of contentaware chunks. This ingestion pipeline, leveraging Google's Document AI, parses the source PDF to preserve its semantic structure, identifying headers, paragraphs, lists, and tables. These text chunks form a common evidence base that is used by both VERIRAG and the baseline models, ensuring a fair comparison across all systems.", "In parallel, a second LLM-driven process generates a focused, structured JSON object containing high-level methodological signals extracted from the paper. This analysis schema represents a metadata layer designed to support the audit. Its high-level structure enables the key objectives of VERIRAG:\n\nglobal_integrity_signals: This object captures top-level indicators of research quality, such as funding source transparency, the presence of a conflict of interest statement, and the data availability statement.", "veritable_check_signals: This is the core of the analysis, containing a nested object for each of the 11 Veritable Checks. For each check, it provides a boolean flag for its methodological applicability and a concise, objective analysis grounded in the paper's text.", "This dual-component design-separating the raw evidence chunks from the structured methodological analysis-is a key architectural feature of our framework. We foresee VERIRAG's audit functionality to act as a modular analysis layer that can augment any standard RAG pipeline, while operating on the shared base of text chunks. With a standard schema for the metadata, we build an object that can be queried for quantitative and qualitative assessments relevant to the factuality check for the claims.", "2.3 The Veritable Taxonomy\n\nOur taxonomy, shown in Figure 2, organizes 11 distinct checks that were designed by mapping core statistical analysis methods with guidelines from CONSORT [8], STROBE [35], and PRISMA [23] into two primary categories based on the aspect of the research process they scrutinize. These checks are designed to be operationalized via targeted information extraction and semantic analysis. A lightweight JSONPath query on the paper's structured JSON analysis object serves as an applicability filter -it decides whether a check is methodologically relevant for a given paper. The substantive pass/fail judgment is delivered by the LLM prompt that follows, which inspects the relevant text span and returns a structured JSON verdict.", "Figure 2: A hierarchical taxonomy of the Veritable checks, organized by the type of textual evidence they evaluate.\n\nVeritable Taxonomy Data Quality Checks Inferential Validity Evidence Assessment Data Completeness Statistical Rigor Consistency Analysis C1\n\n:\n\nIntegrity C3 Sample Representativeness C2 Missing Data Patterns C4 Outcome Variability C5 Estimation Validity C6 Power C7 Outlier Influence C8 Confounding Control C9 Source C10 Effect Homogeneity C11 Subgroup\n\n2.3.1 Data Quality Checks (Evidence-Based Assessment). These checks evaluate the quality of the underlying data as described in the text, identifying anomalies that are visible without access to the raw dataset.", "2.4 Operationalizing the Audit\n\nThe taxonomy serves as a foundation for VERIRAG's auditing pipeline design. Each check functions as a structured prompt snippet within our framework. These prompts identify textual evidence corresponding to specific methodological criteria. All prompt snippets are accessible through our shared repository. The aggregated results of this audit provide a multidimensional profile of a claim's credibility, enabling a more nuanced and reliable assessment than any single metric could provide. Although not a substitute for validation based on raw data [6], this process allows for scalable and context-aware validation using NL inference capabilities of modern LLM systems.", "C9: Source Consistency\n\nDefinition: Detects cherry-picking by assessing whether the paper's claims and citations are consistent with the broader evidence base [11].", "Specific Checks: CONSORT 6 : Scientific background and rationale CONSORT 29 : Interpretation consistent with results, balancing benefits and harms, and considering other relevant evidence\n\nScoring Query (LLM Prompt Snippet): C9 (Source Consistency): Check if the paper's introduction and discussion accurately represent prior work, including any contradictory findings.", "6 Related Work\n\nVERIRAG builds upon and extends three primary research areas: (1) automated systems for maintaining scientific integrity, (2) specialized frameworks for claim validation in high-stakes domains, and (3) the architectural evolution of RAG.", "6.1 Automated Scientific Integrity\n\nThe movement to automate the detection of flawed research includes statistical heuristic tools like Statcheck [22] and GRIM [4], and community-driven post-publication review platforms like PubPeer [1]. These tools are often narrow in scope or cannot scale effectively. VERIRAG distinguishes itself by integrating a comprehensive, automated audit directly into the generative workflow, providing a quantitative, evidence-based decision in real time. It assesses broader methodological issues like sample representativeness ( \ud835\udc36 3) or confounding control ( \ud835\udc36 8).", "6.2 Claim Validation in Healthcare and Clinical\n\nSpecialized claim validation systems in healthcare often focus on fact-checking against established knowledge bases or use methods like Evidence-Based Medicine (EBM) hierarchies [29] and bibliometrics. These approaches face challenges with novel or contested research and can conflate visibility with validity. VERIRAG introduces a complementary approach by performing a deep, semantic audit of the internal methodological characteristics of the evidence, allowing for a granular assessment of novel claims where no established reference exists.", "6.3 Retrieval-Augmented Generation (RAG)\n\nRecent advances in Retrieval-Augmented Generation (RAG) have increasingly focused on improving factual accuracy and self-correction mechanisms. Notable frameworks such as CRAG [39] introduce reflective and evaluative modules that guide the model to critique its own generation using special reflective tokens, while CRAG employs a retrieval evaluator to remove irrelevant evidence. Building on the success of these mechanisms, Li et al. [19] demonstrated strong performance by applying Self-RAG and CRAG to biomedical fact-checking tasks, particularly in the context of COVID-19. More structured approaches, like CoV-RAG [16], explicitly incorporate a verification module that assesses the accuracy of the retrieved", "evidence and generated answers through a chain-of-verification process. RAFTS [41] enhances claim verification by generating both supporting and refuting arguments using retrieved evidence, allowing fine-grained credibility assessments. Despite these advances, existing RAG frameworks largely rely on internal or heuristic-based critique and often lack an understanding of formal, domain-specific standards. As a result, they cannot detect more subtle yet critical issues like p-hacking ( \ud835\udc36 11) or selective reporting ( \ud835\udc36 9). The core contribution of VERIRAG is the introduction of a formal, statistically informed verification layer into the RAG pipeline. Our Veritable transforms the vague instruction to \"critique the source\" into a systematic, 11-point audit, combined with a quantitative synthesis model (the HV score) and an adaptive decision threshold ( \ud835\udf0f auto )."]}
{"type": "entry", "index": 34, "question": "what metric catches when it uses the right info but credits the wrong paper", "answer": "The kind of problem you\u2019re describing\u2014where the model gives mostly correct scientific information but attributes it to the wrong paper\u2014is mainly caught by *citation-focused* metrics rather than content-focused ones.\n\nIn SurveyGen, this is handled by their **citation quality** metrics. They compare each generated reference against the human-chosen, ground\u2011truth references using text similarity; if a generated citation doesn\u2019t sufficiently match any correct paper, it counts against citation precision/recall/F1, even if the surrounding explanation is otherwise accurate [2]. They also compute **citation accuracy** to check whether the cited papers are real or hallucinated; a mis-attributed paper (wrong paper for the right fact) will similarly lower these scores because it doesn\u2019t match the intended reference list [2].\n\nSo, in this setup, \u201cright info, wrong paper\u201d shows up as **poor citation quality (low precision/recall/F1 and citation accuracy)**, even if content quality metrics like semantic similarity or ROUGE remain high [2].\n\nSeparately, VERIRAG\u2019s **Source Consistency (C9)** checks whether a paper\u2019s claims and its cited sources are consistent with the broader evidence base, which can also flag misuse or cherry\u2011picking of citations, though it\u2019s framed at the paper level rather than per\u2011citation matching [1].", "tags": ["_untagged"], "full_faithfulness": 0.8571428571, "full_answer_relevancy": 0.8046398829, "seed_faithfulness": 0.33333333330000003, "seed_answer_relevancy": 0.8129172692000001, "context_relevance": 0.3473557055725437, "context_relevance_error": null, "context_relevance_reason": "The context passages provide some relevant information about citation quality and source consistency, which indirectly relates to the user's question about metrics that credit the wrong paper. However, the passages do not directly address the specific issue of how metrics catch the use of the wrong paper, leading to a lack of comprehensive alignment with the user's inquiry.", "structure_citation": 0.8273457597695304, "structure_error": null, "structure_reason": "The response effectively addresses the question about metrics that catch misattributed citations by discussing citation-focused metrics and their evaluation methods. It references specific metrics like citation quality and citation accuracy, which align with the context provided. However, while it mentions [2] markers, it does not explicitly confirm that these markers correspond to snippets in the context, which slightly detracts from the overall alignment.", "hallucination_score": 0.8614548460401084, "hallucination_error": null, "hallucination_reason": "The response accurately identifies that citation-focused metrics are essential for catching instances where correct information is attributed to the wrong paper. It effectively explains how citation quality metrics in SurveyGen assess the accuracy of citations, aligning well with the context provided. The mention of VERIRAG's Source Consistency adds depth, although it is framed at the paper level rather than per-citation matching, which slightly detracts from the overall focus on citation metrics. Overall, the response is well-supported and relevant to the claims made in the input.", "contexts_used": ["C9: Source Consistency\n\nDefinition: Detects cherry-picking by assessing whether the paper's claims and citations are consistent with the broader evidence base [11].", "Specific Checks: CONSORT 6 : Scientific background and rationale CONSORT 29 : Interpretation consistent with results, balancing benefits and harms, and considering other relevant evidence\n\nScoring Query (LLM Prompt Snippet): C9 (Source Consistency): Check if the paper's introduction and discussion accurately represent prior work, including any contradictory findings.", "3 Experiments\n3.1 Baselines\n\nWe selected three baselines for comparison.\n\nFully-LLMGen (Tang et al., 2025): Surveys are generated by LLMs based only on the given topic, without external inputs.", "Naive-RAG (Wu et al., 2025): Candidate papers are retrieved from an external literature database based on semantic similarity between the abstract and the survey topic. We use the same input fields as QUAL-SG to prompt LLMs for survey generation.", "Human-written : The human-written surveys are from our SurveyGen dataset.", "For generation stages, we employed six LLMs as agents, including three Open-source LLMs: GLM-4-Flash (GLM et al., 2024), LLaMA-3.170B (Meta, 2024), and DeepSeek-V3 (DeepSeekAI et al., 2025), and three Closed-source LLMs: GPT-4.1-2025-04-14 (OpenAI, 2025), Gemini-2.0Flash (Team et al., 2024), and Claude-3.7-Sonnet20250219 (Anthropic, 2025). Implementation details are provided in Appendix C.", "To be cost-effective, our experiments are conducted on 120 highly cited surveys from SurveyGen , with 30 selected from each of four domains: Biology, Medicine, Psychology, and Computer Science. For Task 1 and Task 3, we directly report the performance of different LLMs. For Task 2, we provide a comparative analysis between our QUAL-SG and the baseline methods. A subset of survey examples is provided in Appendix G.", "3.2 Evaluation Metrics\n\nWe consider human-written surveys as the ground truth for both automatic and human evaluations.\n\nAutomatic evaluation: The automatic evaluation includes three parts: citation quality , content quality , and structural consistency . The formulas for the metrics and calculation details in this section are provided in the Appendix D.", "(1) Citation quality evaluation. First, we assess how closely the references retrieved by RAG or generated by LLMs match those selected by humans. To address variations in title phrasing and formatting of the same article, we compute the textual similarity between each generated or retrieved reference and the human-selected ones. A reference is considered matched if the similarity exceeds a predefined threshold. We use precision,", "| | Model. | Citation Quality.Acc. \u2191 | Citation Quality.P \u2191 | Citation Quality.R \u2191 | Citation Quality.F1 \u2191 | Content Quality.Sim. \u2191 | Content Quality.R-L \u2191 | Content Quality.KPR \u2191 | Structural Consistency.Rel LLM | Structural Consistency.Overlap (%) |\n|---:|:----------------------------|:----------------------------|:----------------------------|:----------------------------|:----------------------------|:----------------------------|:----------------------------|:----------------------------|:---------------------------------|:-------------------------------------|\n| 0 | /lock-open Open-source LLMs | /lock-open Open-source LLMs | /lock-open Open-source LLMs | /lock-open Open-source LLMs | /lock-open Open-source LLMs | /lock-open Open-source LLMs | /lock-open Open-source LLMs | /lock-open Open-source LLMs | /lock-open Open-source LLMs | /lock-open Open-source LLMs |\n| 1 | GLM-4-Flash | 9.27 | 9.03 | 3.26 | 4.79 | 81.27 | 15.04 | 41.71 | 2.44 | 10.62 |\n| 2 | LLaMA-3.1-70B | 15.43 | 11.48 | 2.74 | 4.42 | 82.43 | 15.36 | 44.36 | 2.62 | 13.48 |\n| 3 | DeepSeek-V3 | 33.63 | 10.85 | 4.09 | 5.94 | 82.05 | 14.18 | 43.53 | 2.57 | 11.03 |\n| 4 | /lock Closed-source LLMs | /lock Closed-source LLMs | /lock Closed-source LLMs | /lock Closed-source LLMs | /lock Closed-source LLMs | /lock Closed-source LLMs | /lock Closed-source LLMs | /lock Closed-source LLMs | /lock Closed-source LLMs | /lock Closed-source LLMs |\n| 5 | GPT-4.1 | 21.07 | 12.31 | 3.72 | 5.71 | 79.51 | 13.48 | 39.21 | 2.39 | 10.95 |\n| 6 | Gemini-2.0-Flash | 22.20 | 8.97 | 3.59 | 5.13 | 80.20 | 14.65 | 42.67 | 2.50 | 12.39 |\n| 7 | Claude-3.7-Sonnet | 35.84 | 11.79 | 5.78 | 7.76 | 81.32 | 13.77 | 46.59 | 2.65 | 14.89 |", "Table 2: Performance comparison of different LLMs on Task 1. 'Acc' indicates whether the generated references are factually accurate and correspond to real papers. 'Sim', 'R-L', and 'KPR' represent 'Semantic similarity', 'ROUGE-L', and 'Key Point Recall', respectively. 'RelLLM' represents structural consistency in LLM evaluations. The best results are marked bold and the second-best are underlined.", "recall, and F1 score to evaluate citation overlap. Additionally, for Task 1, we compute citation accuracy to check whether the generated references are fabricated or hallucinated.", "(2) Content quality evaluation. We first compute the semantic similarity between the LLMgenerated and human-written surveys, and then report ROUGE 6 score to quantify their textual overlap. Apart from semantic similarity evaluation, we employ Key Point Recall (KPR) (Qi et al., 2024; Tang et al., 2025) to evaluate how effectively LLMgenerated surveys capture the key points conveyed in human-written ones.", "(3) Structural consistency evaluation. In scientific writing, a well-structured survey typically features clear section divisions and coherent thematic development (Wee and Banister, 2016; Paul and Criado, 2020). To evaluate structural consistency, we adopt two metrics: Overlap score and RelevanceLLM. Specifically, the Overlap score is defined as the number of sections between the generated and human-written surveys with semantic similarity exceeding a predefined threshold. Then, we prompt the LLM-as-judge to evaluate the structural alignment between LLM-generated and human-written surveys using a 5-point scale.", "Human evaluation: Inspired by previous works (Kasanishi et al., 2023; Liang et al., 2025), we also conduct human evaluation to compare the LLMgenerated and human-written surveys from the following four aspects: topic relevance , information coverage , critical analysis , and overall rating . The evaluation criteria and the detailed annotation process are provided in the Appendix E.", "6 https://pypi.org/project/pyrouge/ . All reported Rouge scores have a 95% confidence interval in this paper.", "8. Conclusion and Future Work\n\nIn this paper, we present RAGOrigin, a responsibility attribution system that traces poisoned texts causing misgeneration in RAG systems. RAGOrigin combines adaptive scope determination with multi-dimensional analysis to identify poisoned texts accurately. Extensive evaluations show that RAGOrigin maintains high accuracy with low false positives and negatives, and remains robust across retrievers, similarity metrics, and LLMs. Future work includes extending attribution to multi-turn [69] and multimodal [70] scenarios.", "Evaluation Metric: Answer Recall\n\nWe adopt Answer Recall as the primary metric for end-to-end evaluation. This metric quantifies the completeness of generated answers by measuring the proportion of reference (ground-truth) statements that appear in the model-generated response (Es et al., 2023). It reflects the model's ability to accurately retrieve and synthesize relevant information from the underlying knowledge base.", "| | Model | Answer Recall |\n|---:|:------------------|----------------:|\n| 0 | GeoGPT-0630 | 0.529 |\n| 1 | GeoGPT-0630 + RAG | 0.666 |\n\nTable 3 | Evaluation Results\n\nThe integration of RAG significantly boosts answer completeness. Compared to the base model, GeoGPT enhanced with RAG achieves a 13.7% absolute gain in Answer Recall, highlighting the effectiveness of retrieval-based augmentation in improving factual grounding."]}
{"type": "entry", "index": 35, "question": "how often does entity linking mess up acronyms and lookalikes, and how do i test that", "answer": "The documents do not report a direct number for \u201chow often\u201d entity linking fails specifically on acronyms or look\u2011alike names.\n\nWhat we do see is that:\n- Even when structure is correct, LLMs often link to the wrong property or entity (semantic inaccuracies) in SPARQL generation over a scholarly KG [2]. This is the same failure mode you\u2019d see when an acronym (e.g., NMF, ACE) is mapped to the wrong node.\n- In medical KGs, entity linking relies on exact keywords and then semantic similarity of embeddings when no exact match exists [3]. This last step explicitly allows linking to a merely \u201csemantically related\u201d concept rather than the exact one, which is where lookalikes and acronym collisions can occur [3].\n- In conversational entity\u2011centric systems, the largest single error category is coreference/entity confusion (27% of failures) [4], showing that confusing similar mentions is a major practical problem even outside strict EL.\n\nHow to test this in your setting, using patterns from the papers:\n1) Build a targeted test set\n   - Create a small benchmark that stresses acronyms and visually/lexically similar entities, analogous to how MATE is used to stress cross\u2011modal entity linking [1].\n   - Include positive pairs (correct acronym\u2192entity) and hard negatives (different entities sharing acronym or near\u2011identical names).\n\n2) Run your full EL pipeline\n   - Use the same identifier / keyword / embedding\u2011based linking strategy described in MedKGent [3].\n\n3) Manually and/or LLM\u2011assisted evaluation\n   - Have humans or an LLM score whether each link is correct vs. incorrect, similar to the triple\u2011validity rubric used in MedKGent [3] and the SPARQL error audit in FIRESPARQL [2].\n\n4) Analyze error types\n   - Break down errors like the semantic vs. structural analysis in [2] and the error table in [4], but with categories \u201cacronym collision,\u201d \u201clookalike surface form,\u201d etc.\n\nThis will give you an empirical error rate for your own stack, focused specifically on acronyms and lookalikes.", "tags": ["long_tail_entities"], "full_faithfulness": 0.24, "full_answer_relevancy": 0.0, "seed_faithfulness": 0.8260869565000001, "seed_answer_relevancy": 0.0, "context_relevance": 0.208549739206933, "context_relevance_error": null, "context_relevance_reason": "The context provided does not directly address the user's question about how often entity linking messes up acronyms and lookalikes, nor does it offer methods for testing that. While it discusses entity linking and its evolution, it lacks specific information on the issues related to acronyms and lookalikes, which is the core of the user's inquiry.", "structure_citation": 0.5733559905428909, "structure_error": null, "structure_reason": "The Actual Output does not include any [n] markers, which is a significant shortcoming as per the evaluation steps. While it provides relevant information and claims about entity linking issues, it fails to reference specific snippets from the Context using the required markers. Additionally, some claims made are supported by the context, but without proper citations, the overall alignment with the evaluation criteria is weakened.", "hallucination_score": 0.6572611409640697, "hallucination_error": null, "hallucination_reason": "The response accurately identifies that the documents do not provide a specific frequency for how often entity linking fails with acronyms or lookalikes, which aligns with the input query. It also discusses relevant issues such as semantic inaccuracies and coreference confusion, which are pertinent to the topic. However, it lacks direct references to specific claims made in the context, such as the limitations of traditional EL methods or the role of exact keywords in medical KGs, which could have strengthened the response further.", "contexts_used": ["Entity Linking\n\nEntity Linking (EL) has evolved from text-only methods to Multimodal Entity Linking (MEL), and more recently to Cross-Modal Entity Linking (CMEL), which supports crossmodal reasoning. Traditional EL methods associate textual entities with their corresponding entries in a knowledge base, but overlook non-textual information (Shen, Wang, and Han 2015; Shen et al. 2023). MEL extends EL by incorporating visual information as auxiliary attributes to enhance alignment between entities and knowledge base entries (Gan et al. 2021; Liu et al. 2024b; Song et al. 2024).", "However, MEL does not establish cross-modal relations beyond these auxiliary associations, thereby limiting genuine cross-modal interaction.", "CMEL goes further by treating visual content as entities-aligning visual entities with their textual counterparts-to construct MMKGs and facilitate explicit crossmodal inference (Yao et al. 2023). Research on CMEL remains in its early stages, lacking a unified theoretical framework and robust evaluation protocols. The MATE benchmark is introduced to assess CMEL performance, but its synthetic 3D scenes fall short in capturing the complexity and diversity of real-world images (Alonso et al. 2025). To bridge this gap, we construct a CMEL dataset featuring greater real-world complexity and propose a spectral clustering-based method for candidate entity generation to drive further advances in CMEL research.", "3 Error type analysis on generated SPARQL queries\n\nDespite the improvements of LLMs on QA over SKGs, LLMs face limitations when handling KG-specific parsing. The experimental results conducted by S\u00f6ren Auer et al.[2] showed that only 63 out of 100 handcrafted questions could be answered by ChatGPT, of which only 14 answers were correct. To better understand why LLMs fail to generate the correct SPARQL query to a NLQ, we conduct a pilot experiment on using ChatGPT(GPT-4) with a random one-shot example to generate SPARQL queries for 30 handcrafted questions in the SciQA benchmark datasets.", "Insights from this pilot experiment revealed two major categories of errors LLMs tend to make in this task: semantic inaccuracies and structural inconsistencies. Semantic inaccuracies occur when LLMs fail to link the correct properties and entities in ORKG, despite generating SPARQL queries with correct structure. Our observations reveal that LLMs tend to rely on the example provided in the one-shot learning process to generate the correct structure for a certain type", "of questions, but often struggle with linking the correct properties and entities because LLMs do not learn the content of the underlying KG. Structural inconsistencies arise due to LLMs' lack of ontological schema of the underlying KG, leading to errors in query structure, such as missing or abundant links (triples), despite correctly linking to the mentioned entities or properties.", "Figure 1 shows the example of semantic inaccuracies and structural inconsistencies problem with the generated SPARQL queries in our pilot study. In the example of the semantic inaccuracies problem, ChatGPT failed to link the correct property orkgp:P15687; instead, it linked to a wrong property orkgp:P7101. In the example of the structural inconsistencies problem, the SPARQL query generated by ChatGPT directly links Contribution to Metrics, fails to detect the correct schema of the ORKG where Contribution and Metric are connected via Evaluation.", "Fig. 1: Examples of semantic inaccuracies and structural inconsistencies problem with the generted SPARQL queries\n\nSemantic inaccuracies Problem\n\nFail to link the correct properties and entities in ORKG\n\nWhat is the maximum sample size?\n\nContribution Evaluation Metric P34 P2006 P7046\n\nStructural inconsistencies Problem\n\nMake errors in query structure, such as missing or abundant links (triples)\n\nWhat are the metrics used by paper \"Using NMF-based text summarization\n\nto improve supervised and unsupervised classification?\n\norkgp:P15687 rdfs:label Sample size (n)\n\norkgp:P7101 rdfs:label has elements", "2 Results\n2.1 Method overview\n\nas its confidence score. For instance, if the triple (NPPA, Negative Correlate, Water) appears in 90% of the outputs, its confidence score is 0.9. Low-confidence triples (score < 0.6) are filtered out, and only high-confidence triples are retained for downstream graph construction. Each triple is also annotated with the PubMed ID of the source abstract and a timestamp, ensuring traceability and source attribution. For example, (NPPA, Negative Correlate, Water) would have a PubMed ID of 10494624 and a timestamp of 2000-01-01.", "As shown in Figure 1 c , for each retained triple, such as (NPPA, Negative Correlate, Water), the Constructor Agent checks its presence in the current KG. If absent ( i.e. , either the head or tail entities are missing), it is inserted; if present, its confidence score is updated according to Equation (1). The associated PubMed ID is appended, and the timestamp is updated to reflect the latest publication. For example, if an existing triple (NPPA, Negative Correlate, Water) has a confidence score of 0.7, PubMed ID 10691132, and timestamp 1999-12-31, and a new occurrence with a confidence score of 0.9, PubMed ID 10494624, and timestamp 2000-01-01 is encountered, the updated triple will have a confidence score of 0.97, PubMed IDs [10691132, 10494624], and a timestamp of 2000-01-01. If the head and tail entities are present but the relation differs, such as existing (NPPA, Associate, Water) vs. incoming (NPPA, Negative Correlate, Water), only the most appropriate relation is maintained. The Constructor Agent invokes the LLM to resolve the conflict by selecting the more suitable relation, considering both the existing and incoming triple's confidence scores and timestamps. If the LLM selects the new triple, the existing one is replaced; otherwise, no changes are made. The prompt design for relation conflict resolution is shown in Extended Data Figure 2 c . Together, the two agents extract structured medical facts and integrate them into a dynamic, time-aware KG. See more details in the Section 4.", "2.2 Structural Characterization of the Knowledge Graph\n\nIn this section, we detail the structural characteristics of the medical KG we constructed, with an emphasis on the distribution of node types, relationship types, and the confidence scores of relationship triples. We also present a visualization of a subgraph centered on COVID-19 to illustrate the graph's structure.", "Using the MedKGent framework, we extracted knowledge triples from the abstracts of 10,014,314 medical papers, with 3,472,524 abstracts (34.68%) yielding extractable triples. The relatively low extraction rate can be attributed to several factors: first, some abstracts lacked sufficient structured information for triple extraction; second, only triples with a confidence score exceeding 0.6 were retained, excluding those with lower confidence; and third, some triples extracted by LLMs contained formatting issues, such as extraneous or irrelevant characters, which were discarded. In total, our Extractor Agent identified 8,922,152 valid triples from the abstracts. However, the extracted triples contained a significant number of duplicates and conflicts. To resolve this, our Constructor Agent integrates the triples in chronological order. During this process, duplicates are merged, with the confidence score for each triple increasing in proportion to its frequency, reflecting greater certainty. For conflicting triples, where the same entity pair is associated with multiple relations, the Constructor Agent retains the most appropriate relationship. Following this consolidation, the final KG comprises 2,971,384 distinct triples.", "We conducted a comprehensive statistical analysis of the final constructed KG, which comprises 156,275 nodes. As shown in Figure 2 a , the node distribution is predominantly dominated by Gene and Chemical nodes, with smaller proportions of other entities such as Disease, Variant, Species, and CellLine. The KG includes 2,971,384 relationship triples (edges), representing a range of interactions between entities, as illustrated in Figure 2 b . The most common relationship type is 'Associate', followed by 'Negative Correlate' and 'Positive Correlate', indicating strong associations between medical entities. Less frequent relationships, such as 'Interact', 'Prevent', and 'Drug Interact', provide additional insights into the complexities of medical interactions. The distribution of confidence scores for these relationship triples, shown in Figure 2 c , with confidence values discretized to the nearest smaller 0.05 increment (rounding down to the closest multiple of 0.05), reveals a clear dominance of high-confidence triples. A significant proportion of triples exhibit confidence scores of 0.95, reflecting the cumulative increase in confidence resulting from the repetition of triples during the graph construction process. This high-confidence distribution reinforces the reliability and robustness of the KG.", "We visualized a local subgraph of the constructed KG with COVID-19 as the central node, highlighting five surrounding relationship triples, as shown in Figure 2 d . Each node is characterized by six key attributes: the Identifier, which uniquely references the node and normalizes multiple synonymous mentions to a standardized terminology entry; the Entity Type, which classifies the entity; the Terminology, which maps the entity type to its corresponding standard terminology; the Page Link, providing a reference to the entity in the Terminology; the Exact Keywords, which lists common names and aliases of the entity in lowercase; and the Semantic Embedding, a vector representation of the entity. In practice, these attributes facilitate entity linking within a query by matching entities to their corresponding nodes in the KG. When the Identifier of an entity in the query is available, entity linking can be efficiently performed using this unique reference. In the absence of an Identifier, precise matching", "Figure 2: A comprehensive statistical analysis and visualization of the constructed KG, consisting of 156,275 nodes and 2,971,384 relationship edges. a . Node distribution within the KG, with Gene and Chemical nodes predominating, and smaller proportions of Disease, Variant, Species, and CellLine. b . Relationship type distribution within the KG, highlighting the prevalence of 'Associate' relationships, followed by 'Negative Correlate' and 'Positive Correlate', with less common interactions such as 'Interact', 'Prevent', and 'Drug Interact'. c . The distribution of confidence scores for relationship triples, discretized to the nearest smaller 0.05 increment, ensures values are rounded down to the closest multiple of 0.05. This distribution reveals a clear dominance of high-confidence triples, particularly those with scores of 0.95, underscoring the robustness of the KG. d . Local subgraph visualization centered on COVID-19, displaying five surrounding relationship triples. Each node is characterized by key attributes, including Identifier, Entity Type, Terminology, Page Link, Exact Keywords, and Semantic Embedding, facilitating efficient entity linking through exact or similarity matching. The relationships in the KG are further enriched by attributes such as Confidence, PubMed IDs, and Timestamp, enhancing traceability, accuracy, and temporal relevance.\n\nCOVID -19 ACE2 Pneu- monia Lung Disea -ses MAD00 04J08 tociliz- umab Treat Identifier : MESH:C000718219 Entity Type : Chemical Terminology : NCBI MeSH Page Link", ": meshb.nlm.nih.gov/record/ui?ui=C000718219\n\nExact Keywords : [mad0004j08] Semantic Embedding : [- 0.12, \u2026, 0.10 ] : MESH:D000086382\n\nEntity Type:\n\nDisease\n\n: meshb.nlm.nih.gov/record/ui?ui=D000086382\n\n: [ncp, covid-19]\n0.25, \u2026, 0.09\n\nIdentifier:\n\nMESH:C502936\nChemical\n\nTerminology:\n\nNCBI MeSH\n\n: meshb.nlm.nih.gov/record/ui?ui=C502936\n\n: [mra, tocilizumab] 0.12, \u2026, 0.13 Associate 59272 Gene\n\n:\n\nNCBI Gene\n\n: www.ncbi.nlm.nih.gov/gene/59272\n\n: [ace2, ace2p]\n0.22, \u2026, 0.09]\nMESH:D011014\n\n: meshb.nlm.nih.gov/record/ui?ui=D011014\n\n: [pneumonia]\n0.18, \u2026, 0.01\nMESH:D008171\n\n: meshb.nlm.nih.gov/record/ui?ui=D008171\n\n: [lung diseases,lung damage]\n\n: [ 0.06, \u2026, 0.11 d a b Drug_Interact (0.1%) 0.70 0.65 'Prevent (0.79 0.75 7.89) (7.5%) 0.60 (8.1%) (5.4% (47.7%) 0.80 CellLine Positive (8.9%) (0.5%) Correlate 0.85 (19.9%) (10.3%) Variant (1.49) (5.9%) Cause (1.4% 0.90 (33.6%) Inhibit (1.2% Negative_Correlate Stimulate (0.5%) (13.7%) Species Compare (26.1%) Cotreat (1.0%)", "Figure 3: Comprehensive evaluation of extraction quality for relationship triples generated by the Extractor Agent. Systematic assessment of extraction accuracy using both automated evaluations by LLMs and independent manual expert review. a . Proportion of valid relationship triples (score \u2265 2.0) across relation types, as assessed by GPT4.1 on a randomly selected subset of 34,725 abstracts (83,438 triples). b . Proportion of valid relationship triples across relation types, as assessed by DeepSeek-v3 on the same subset. c . Validity rates from independent manual evaluation by three domain experts on a subset of 400 abstracts (1,060 triples), demonstrating high inter-expert consistency. d-f . Performance of GPT-4.1 and DeepSeek-v3 compared to three expert evaluations on the shared evaluation subset, reporting precision, recall, and F1 score. g . Pairwise inter-rater agreement between experts and LLMs quantified by Cohen's kappa coefficients, demonstrating substantial consistency across all evaluators.\n\nGPT-4.\n\nAutomated Evaluation\n\nDeepSeek-v3 Automated Evaluation\n\nManual Evaluation 0936| 0.0307 0,8875 0,8880 0 8700 0.7160 0.4\n\nExpert1's Evaluation as Reference\n\nExpert2's Evaluation as Reference\n\nExpert3's Evaluation as Reference\n\nPairvise Cohen's 0 9761 09761 0 0602 00760 0.9502 00537 0,9503 0 9440 0.5663 08143 0,8818 0 5446 0.6762 0,8853 0.5446 0.6906 06818 0.6008 0 6560 GPT-4,1 DeepSeek-v3 GPT-4.1 Correlale Corelate Cause Inhon Irhon Cotcat Inlatact Colrcat Kappa ison", "is achieved by checking whether the entity appears in the Exact Keywords list of a specific node. Alternatively, semantic vectors of the query entities can be compared with those in the KG to identify the most similar entities, enabling semantic similarity matching. This approach is particularly beneficial for entities with multiple names, ensuring accurate linking even when not all aliases are captured in the Exact Keywords list.", "The relationships between entities are characterized by three key attributes. Confidence reflects the reliability of the relationship, with higher values indicating greater certainty based on its frequency across multiple sources. The PubMed IDs attribute lists the PubMed identifiers of the papers from which the relationship is derived, enabling easy access to the original publications via the PubMed website 2 . If the relationship appears in multiple papers, all relevant PubMed IDs are included, further increasing the confidence score. Finally, Timestamp denotes the most recent occurrence of the relationship, specifically the publication date of the latest paper. Notably, while Timestamp captures only the latest appearance, the full temporal span of the relationship-including its earliest mention-can be readily retrieved through the associated PubMed IDs via the PubMed website. These attributes collectively enhance the traceability, accuracy, and temporal relevance of the relationships within the KG.", "4 Methods\n4.2.2 Constructor Agent\n\nA chemical/drug treats a disease. The Treat relationship typically occurs between Chemical and Disease.\n\nMeSH (Medical Subject Headings)\n\ndbSNP, otherwise HGNV format\n\nNCBI Taxonomy\nCell Line\nCellosaurus\n\nYour task is to select the most appropriate relation\n\nbetween two medical entities to form more\n\nreasonable knowledge triple.\n\nThere is an and Now, a new between e1 and\n\ne2 is proposed.\n\nPlease decide which relation should be retained\n\nbetween e1 and e2.\n\nIf r1 should be kept, respond with \"Y\".\n\nIf r2 should replace it, respond with \"N\".\n\nYou may consider the following two factors to assist\n\nyour decision:\n\n(1) The\n, and\nthat of\n\n;\n\n(2) The\nfor\n\nIn general, relations with\n\nhigher confidence scores or more recent timestamps are likely\n\nretained.\n\nYour output should contain only \"Y\" or \"N\". Do not\n\nprovide any explanations.\n\nOutput:\n\nc", "Extended Data Figure 2: a . Prompt template for relation extraction. Given a biomedical abstract and its extracted entities, the Extractor Agent prompts the LLM to infer semantic relations between entity pairs using a predefined relation set and textual descriptions. b . Reference terminologies for entity normalization. Each biomedical entity type is mapped to a standard terminology: Gene (NCBI Gene), Disease and Chemical (MeSH), Variant (dbSNP or HGNV), Species (NCBI Taxonomy), and Cell Line (Cellosaurus). c . Prompt design for relation conflict resolution. When conflicting relations exist between the same entity pair, the Constructor Agent prompts the LLM to select the most appropriate one based on confidence scores and timestamps. d . Schema for predefined relation types. The 12 core relation types-seven bidirectional and five unidirectional-are listed alongside their directionality, descriptions, and allowed entity-type combinations.", "4.3 Quality Assessment\n\nWe assessed the quality of relational triples extracted by the Extractor Agent through both automated and manual evaluations, leveraging two state-of-the-art LLMs-GPT-4.1 [74] and DeepSeek-v3 [75]-as well as three PhD students with interdisciplinary expertise in medicine and computer science. For each medical abstract and its corresponding set of extracted triples, individual triples were evaluated using a standardized four-level scoring rubric: 3.0 (Correct), 2.0 (Likely Correct), 1.0 (Likely Incorrect), and 0.0 (Incorrect). The evaluation prompt provided to both LLMs and human annotators is illustrated in Extended Data Figure 3 a .", "A relational triple was defined as valid if it received a score of \u2265 2 . 0 . The validity rate was calculated as:\n\nTo assess the reliability of automatic evaluation, we compared LLM-based assessments with human annotations on a shared evaluation subset, treating human judgments as ground truth. The precision, recall, and F 1 -score of the automatic evaluations were computed as:\n\nwhere TP, FP, and FN represent true positives, false positives, and false negatives, respectively. To further quantify inter-rater agreement, we calculated Cohen's Kappa coefficient [82] for each pair of evaluators, including both LLMs and human annotators, resulting in 10 pairwise comparisons across the five raters. The Kappa coefficient was computed as:\n\nwhere p 0 represents the observed agreement and p e denotes the expected agreement by chance. This analysis provides a quantitative measure of rating consistency across evaluators.", "4.4 Retrieval-Augmented Generation\n\nThe constructed KG serves as a reliable external source for information retrieval and can be integrated into LLMs via a RAG framework. By providing structured biomedical context, the KG enhances LLM performance across a range of medical QA benchmarks.", "Given a user query q , we first extract the set of medical entities present in the question, denoted as E q = { e q 1 , e q 2 , \u00b7 \u00b7 \u00b7 } . When using PubTator3 [80]-the same entity recognition tool employed during KG constructioneach extracted entity is assigned a unique identifier. This allows for efficient entity linking by matching these identifiers to the corresponding nodes N q = { n q 1 , n q 2 , \u00b7 \u00b7 \u00b7 } within the graph. Alternatively, if medical entities are extracted using other methods-such as prompting a LLM-they may lack standardized identifiers. In such cases, the extracted entity mentions are first converted to lowercase and matched against the Exact Keywords attribute of each node in the KG. A successful match enables linkage of the entity to the corresponding graph node. In both approaches, if an entity cannot be linked via its identifier or if its surface form does not appear in any node's Exact Keywords list, we apply a semantic similarity strategy to complete the entity linking process. Specifically, the embedding of the query entity is computed using the same model employed for generating node-level semantic representations ( i.e. , BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext [81]) and is compared against the Semantic Embedding of all nodes in the KG. The entity is then linked to the node with the highest semantic similarity score, which may correspond to either the exact concept or a semantically related medical entity. This entity linking framework-combining identifier-based matching, lexical normalization, and semantic embedding-ensures robust and flexible integration of KG-derived knowledge into downstream QA tasks.", "Following entity linking, we construct evidence subgraphs using a neighbor-based exploration strategy [86] to enhance the reasoning capabilities of LLMs. For each entity-linked node in the query-specific set N q , we retrieve its one-hop neighbors within the KG. Specifically, for each node n q i \u2208 N q , all adjacent nodes n q \u2032 i are identified, and the corresponding triples ( n q i , r, n q \u2032 i ) are appended to form a localized subgraph G q i . This expansion captures the immediate relational context surrounding the query entities, which is essential for enabling fine-grained medical reasoning. The complete evidence set for a given query is then defined as the union of these localized subgraphs: G q = { G q 1 , G q 2 , \u00b7 \u00b7 \u00b7 } . The resulting subgraph G q may contain a large number of relational triples, including redundant or irrelevant information, which can adversely impact LLM reasoning [87]. To address this, we leverage the LLM's inherent ranking capability to selectively filter high-value knowledge [88]. Given the question q and", "You are tasked with evaluating the validity of the knowledge triples extracted from the abstract of a medical paper.\n\nGiven the abstract (\n\n) of a medical paper and the extracted triples\n\n) from this abstract.\n\nEach triple is represented in the format:\n\n\"Head Entity Name (Alias1, Alias2) | Relationship Name | Tail Entity Name (Alias1, Alias2)\"\n\n,\n\nwith triples separated by ' $ '.", "Some entities may have no aliases or multiple aliases, which are separated by ', ' within the '()'.\n\nYour task is to evaluate the validity of each triple, with a particular focus on the\n\nrelationship\n\nit describes, based on the information provided in the abstract. Consider whether the stated relationship accurately\n\nreflects the connection between the head and tail entities as presented or implied in the text.", "For each triple, evaluate its validity using the following scoring scale and assign a confidence score:\n\n\u2022\n\nCorrect (3.0):\n\nThe relationship logically and accurately describes the relation between the head and tail entities as\n\nexplicitly mentioned or directly and strongly supported\n\nby the abstract. The\n\nrelationship type is\nprecise\n\nand the connection is\n\nundeniable\n\nbased on the text, requiring minimal inference.\n\nLikely Correct (2.0):\n\nThe relationship is\n\ngenerally acceptable and directionally correct\n\n. The core connection between the entities is\n\nvalid and supported by the text (explicitly, implicitly, or via\n\nreasonable inference)\n\n, even if the relationship type has\n\nminor inaccuracies or lacks ideal precision\n\n.\n\nLikely Incorrect (1.0):\n\nsubstantially inaccurate or misleading\n\nsignificantly misrepresenting\n\nthe connection described in the abstract, even if the entities are mentioned together.\n\nIncorrect (0.0):\n\nnot supported by the abstract whatsoever\n\n, is\n\nclearly and undeniably contradicted\n\nby the text, or involves a\n\nfundamental misunderstanding\n\nof the entities or their\n\nconnection as presented.\n\nOutput the evaluation in a fixed format:\n\nFirst line: 'Analysis: ' followed by the analysis of all triples, separated by '; '. Each triple's analysis should explain\n\nwhy\n\nthe specific confidence score (3.0, 2.0, 1.0, or 0.0) was assigned based on the criteria\n\nabove and the abstract's content.", "Second line: Only the numerical confidence scores for all triples, separated by ' $ ', in the same order as the input triples (e.g., 3.0 $ 2.0 $ 1.0 $ 0.0). This line must contain only numbers (formatted to one\n\ndecimal places like 3.0, 2.0, 1.0, 0.0), decimal points, and ' $ ' as separator, with no additional text or English letters.", "5 Results\n5.1 Main Results\n\n| | Model | FR (%) | DC (%) | UCS (/5) |\n|---:|:-------------------|:-----------|:-----------|:-----------|\n| 0 | Stateless LLM | 54.1 (0.4) | 48.3 (0.5) | 2.1 (0.1) |\n| 1 | Vector RAG | 71.6 (0.6) | 66.4 (0.7) | 3.4 (0.1) |\n| 2 | Entity-RAG | 75.9 (0.5) | 72.2 (0.6) | 3.7 (0.1) |\n| 3 | Semantic Anchoring | 83.5 (0.3) | 80.8 (0.4) | 4.3 (0.1) |\n\nTable 1: Overall performance on MultiWOZ-Long. Semantic Anchoring outperforms all baselines across metrics. Improvements in FR and DC are statistically significant at p < 0 . 01 ; UCS gains are significant at p < 0 . 05 . Values are mean \u00b1 stdev over three runs.", "Figure 2 analyzes how performance varies with session depth. While all models degrade as dialogue span increases, Semantic Anchoring sustains over 75% recall at 10 sessions, indicating stronger long-range tracking.", "5.2 Per-Dataset Breakdown\n\nTo test generality, we evaluate on DialogRE-L , which emphasizes relation extraction across sessions. Results in Table 2 show consistent improvements, though broader domains are needed to claim robustness.", "Figure 2: Factual Recall by session depth on MultiWOZ-Long. Semantic Anchoring exhibits the slowest degradation, maintaining > 75% recall at 10-session distance. Error bars denote standard deviation across three runs.\n\nFactual Recall vs. Session Depth (MultiWOZ-Long)\n\nStateless LLM Vector RAG Entity-RAG Semantic Anchoring Session Depth\n\n|---:|:-------------------|---------:|---------:|-----------:|\n| 0 | Stateless LLM | 49.8 | 44.1 | 2 |\n| 1 | Vector RAG | 68.7 | 62.5 | 3.2 |\n| 2 | Entity-RAG | 72.1 | 68.3 | 3.6 |\n| 3 | Semantic Anchoring | 81.4 | 77.9 | 4.2 |\n\nTable 2: Performance on DialogRE-L. Semantic Anchoring achieves consistent gains across metrics, suggesting effectiveness in relation extraction tasks that require long-range entity tracking.", "5.3 Ablation Studies\n\nTable 3 examines the role of linguistic components. Removing discourse tagging reduces FR by 4.7 points, while excluding coreference resolution reduces DC by 6.2 points. Eliminating all symbolic features collapses performance to Vector RAG levels. These results align with observed error patterns (\u00a75.6), underscoring the value of symbolic features.", "5.4 Qualitative Examples\n\nIn MultiWOZ-Long, when the user later asks 'Did he confirm the time for the taxi?' , Semantic Anchoring retrieves:\n\n[Entity: John Smith][CorefID: E17] confirmed the taxi is booked for 9 AM.", "By contrast, Vector RAG surfaces unrelated mentions of 'taxi.' Additional examples, including cases where Semantic Anchoring fails, are shown in Appendix C.", "| | Variant | FR (%) | DC (%) | UCS (/5) |\n|---:|:-------------------------|---------:|---------:|-----------:|\n| 0 | Full Model | 83.5 | 80.8 | 4.3 |\n| 1 | - Discourse Tagging | 78.8 | 75.6 | 4 |\n| 2 | - Coreference Resolution | 80.1 | 74.6 | 4.1 |\n| 3 | - Dependency Parsing | 81.2 | 78.5 | 4.1 |\n| 4 | Dense-only (Vector RAG) | 71.6 | 66.4 | 3.4 |\n\nTable 3: Ablation results on MultiWOZ-Long. Removing discourse or coreference modules significantly reduces FR and DC, respectively. Without all symbolic features, performance falls to the dense-only baseline.", "5.5 Human Evaluation\n\nFive trained annotators rated 50 randomly sampled conversations for User Continuity Satisfaction (UCS). Agreement was high ( \u03b1 = 0 . 81 ). As Table 1 shows, Semantic Anchoring achieves the highest UCS (4.3), with annotators noting better consistency in entity references. Full protocol details are in Appendix B.", "5.6 Error Analysis\n\nTable 4 categorizes common failures. Coreference mistakes (27%) and parsing errors (19%) are the most frequent, consistent with ablation findings. Discourse mislabeling (15%) often arises in sarcasm or overlapping speech. While overall error frequency is lower than dense retrieval, these remain open challenges.", "| | Error Type | Proportion of Failures |\n|---:|:----------------------|:-------------------------|\n| 0 | Parsing errors | 19% |\n| 1 | Coreference mistakes | 27% |\n| 2 | Discourse mislabeling | 15% |\n| 3 | Other / miscellaneous | 39% |\n\nTable 4: Error analysis on MultiWOZ-Long. Coreference mistakes are the most frequent error type, followed by parsing and discourse issues. These patterns align with ablation results."]}
{"type": "entry", "index": 36, "question": "does retrieval make the model more \u201chonest\u201d or just more confident sounding", "answer": "Retrieval by itself does not automatically make a model more \u201chonest.\u201d It changes *what* the model talks about, but without safeguards it can actually make confident-sounding mistakes worse.\n\nEvidence from the multimodal RAG paper shows that adding a retrieval agent on top of a vision-language model slightly increased accuracy, but also *increased* the hallucination rate (from 59.62% to 62.50%) and left the truthfulness score equally bad [2]. The authors note that naive RAG \u201ccan introduce more misleading content,\u201d meaning the model may sound more informed while still being wrong [2].\n\nSimilarly, the AC-RAG work describes \u201cretrieval hallucination,\u201d where an expert model is overconfident in its own knowledge or in low-quality retrieved documents and proceeds to answer incorrectly instead of admitting uncertainty or searching better [1]. That is the opposite of honesty: retrieval supports the hallucination instead of correcting it.\n\nTo actually make answers more truthful, extra mechanisms are needed on top of retrieval:\n- Adversarial collaboration between a cautious detector and an expert resolver to challenge overconfident answers and push for better evidence [1].\n- Verification stages and self-checks that reduce hallucinations and improve truthfulness, even if they make the system more conservative (more \u201cI don\u2019t know\u201d responses) [2].\n- Confidence-based filtering (Confident RAG) that picks the answer with the most reliable internal confidence signal, which improves accuracy over both vanilla LLM and vanilla RAG [3].\n\nSeparately, larger models sometimes behave more conservatively on ambiguous or unanswerable questions, explicitly acknowledging limitations instead of guessing [4]. That\u2019s the kind of behavior closer to \u201chonesty,\u201d but it comes from scale and training choices, not retrieval alone.\n\nSo: retrieval can help *if* paired with careful verification and calibration; by itself, it tends to make the model sound more confident, not necessarily more honest.", "tags": ["_untagged"], "full_faithfulness": 0.9230769231, "full_answer_relevancy": 0.0, "seed_faithfulness": 0.7307692308, "seed_answer_relevancy": 0.0, "context_relevance": 0.4611598129584781, "context_relevance_error": null, "context_relevance_reason": "The context passages provide some relevant information about Retrieval-Augmented Generation (RAG) and its implications for model confidence and honesty. However, they do not directly address the user's question about whether retrieval makes the model more 'honest' or just more confident sounding. The passages discuss issues like semantic discrepancy and retrieval hallucination, which are related but do not explicitly answer the user's inquiry. Overall, while there is some alignment with the topic, the lack of direct relevance to the specific question limits the score.", "structure_citation": 0.8164362404274769, "structure_error": null, "structure_reason": "The Actual Output effectively addresses the question about retrieval's impact on model honesty and confidence, providing a nuanced explanation supported by relevant citations from the Context. It includes claims about retrieval not inherently increasing honesty and discusses the concept of 'retrieval hallucination,' which aligns with the Context's mention of similar issues. However, while it references citations [1] and [2], it does not explicitly match all claims to the corresponding snippets, which slightly detracts from the overall score.", "hallucination_score": 0.8844120709209022, "hallucination_error": null, "hallucination_reason": "The response effectively addresses the input question by clarifying that retrieval does not inherently make models more honest, but rather can lead to overconfidence and increased hallucination rates. It supports its claims with specific evidence from the multimodal RAG paper and AC-RAG work, aligning well with the context provided. The explanation of the need for additional mechanisms to ensure truthfulness further strengthens the response, demonstrating a thorough understanding of the complexities involved in retrieval-augmented generation.", "contexts_used": ["1. INTRODUCTION\n\nRetrieval-Augmented Generation (RAG) seeks to enhance Large Language Models (LLMs) by integrating external knowledge bases, thereby mitigating factual inaccuracies and hallucinations [1, 2]. This approach has garnered significant interest for its potential in domain-specific applications. However, the performance of RAG systems is heavily dependent on the quality of retrieved information, and existing methods often face critical challenges that degrade their effectiveness, as illustrated in Figure 1.", "One primary issue is Semantic Discrepancy, where retrievers, optimized for semantic similarity, fail to capture the nuanced 'relevance' required by complex queries. For instance, a query about 'callus formation' might retrieve documents discussing 'rigid immobilization' due to superficial semantic overlap, even though 'movement at the fracture site' is the more relevant concept for answering the question. This discrepancy between similarity and relevance often leads to the retrieval of suboptimal or misleading information.", "Amore subtle and pernicious problem, which we identify as Retrieval Hallucination, occurs particularly with LLMs that have been fine-tuned on specialized domain data. These\n\n1 Code is available: https://anonymous.4open.science/r/AC-RAG/\n\nFig. 1 : AC-RAG overcomes key failures of vanilla RAG: (1) Semantic Discrepancy (retrieving irrelevant content) and (2) Retrieval Hallucination (overconfidence in accepting poor results). Our framework addresses both through adversarial collaboration.\n\nWhich of the following increases callus formation?", "A. Rigid immobilization B. Movement at fracture site\n\nC. Compression plating D. Intraosseous nailing\n\nWhat is\nRigid immobilization\n\n?\n\nrefers to \u2026 KG Immobilization techniques such as \u2026\n\nNot sufficient for problem-solving.\n\ncallus formation\n\nCallus formation results from\n\nSufficient for problem-solving.\n\nB. Movement at fracture site\n\nWhich of the following increases\n\nA.\n\nB.\n\nMovement at fracture site\n\nC.\n\nCompression plating\n\nD.\n\nIntraosseous nailing\n\nrespiratory distress syndrome\n\nin adults may be produced by any\n\nof a number of causes resulting in\n\nsimilar pathophysiologic changes\n\nand having identical therapeutic\n\nimplications. the most important\n\nfactors in treatment are early\n\nrecognition and early institution\n\nof therapy\n\nAlthough some concepts still require\n\nexplanation, my existing knowledge is\n\nalready sufficient for problem-solving.\n\nA. Rigid immobilization \u2460 Semantic Discrepancy \u2461 Retrieval Hallucination Self-Reflect \u2713 \u2717", "expert models can exhibit overconfidence in their own parametric knowledge, causing them to either bypass the retrieval process entirely or uncritically accept irrelevant search results during self-reflection phases. As shown in Figure 1, even when retrieval provides a low-quality document about 'respiratory distress syndrome' for a question on 'callus formation', a fine-tuned model might erroneously conclude its existing knowledge is sufficient and proceed to generate an incorrect answer. This overconfidence prevents the model from initiating further, more targeted searches, effectively trapping it in a state of hallucination supported by poor retrieval. This challenge is particularly acute in systems that utilize self-correction or reflection mechanisms but are hindered by an overconfident generator [3, 4].", "To address these interconnected challenges, we propose the Adversarial Collaborative RAG (AC-RAG), a novel framework designed to foster more rigorous and accurate knowledge acquisition. AC-RAG orchestrates a dynamic interaction between two heterogeneous agents: a generalist, non-fine-tuned Detector and a domain-expert, fine-tuned Re-", "Fig. 2 : Overview of our framework. We employ two different LLMs in our Adversarial Collaboration process (Detector + Retriever). A Neutral Moderator supervises the entire operation. For each task Q , our workflow is divided into four phases, involving between 0 and N retrievals. The process concludes at an optimal point through Adaptive Retrieval. The memory M from each interaction is used as a reference for subsequent interactions and for generating the final response.\n\nPart2: Challenge Dissection\n\nPart3: Retrieval & Integration\n\nList the professional terms needed for further explanation.\n\nSaltatory conduction\n\nExplain the definition of the professional terms.\n\nWalk me through the context below and write a summary.\n\nSaltatory conduction:\n\nSaltatory conduction is a process\n\nby which nerve impulses travel along myelinated axons,\n\njumping from one node of Ranvier to the next.", "In neuroscience, saltatory conduction (from Latin saltus 'leap, jump') is the\n\npropagation of action potentials along myelinated axons from one node of\n\nRanvier to the next node, increasing the conduction velocity of action potentials.", "The uninsulated nodes of Ranvier are the only places along the axon where ions\n\nare exchanged across the axon membrane, regenerating the action potential\n\nbetween regions of the axon that are insulated by myelin, unlike electrical\n\nconduction in a simple circuit.\n\n: Neutral Moderator\n: Resolver\n: Detector\n\nWhich of the following is\n\nnot true for myelinated\n\nnerve fibers?\n\nA. Impulse through\n\nmyelinated fibers is slower\n\nthan non-myelinated fibers\n\nB. Membrane currents are\n\ngenerated at nodes of\n\nRanvier\n\nC. Saltatory conduction of\n\nimpulses is seen\n\nD. Local anesthesia can be\n\neffective only when the\n\nnerve is not covered by\n\nmyelin sheath Instruction Tuning on Domain-specific Data Part1: Pre-Check\n\nAre there any medical terms in this\n\nquestion that you do not understand?\n\nAnalysis: Myelinated nerve fibers as local anesthesia\n\ncan be less effective on them compared to non-\n\nmyelinated fibers,\n\nAnswer: A. Impulse through myelinated fibers is\n\nslower than non-myelinated fibers.\n\nAnswer the question directly.\n\nNo\nYes\n\nMyelinated Fibers:\n\nMyelinated fibers are nerve fibers\n\ncovered with myelin sheath, enhancing the speed and\n\nefficiency of nerve impulse transmission.\n\nMembrane currents:\n\nMembrane currents refer to the\n\nflow of ions across the cell membrane, driven by\n\ndifferences in ion concentration and electrical potential.\n\nMemory\nPart4: Post-Check\n\nDo you think the context is sufficient\n\nfor answering?\n\nAnalysis: Myelinated nerve fibers conduct impulses\n\nfaster than non-myelinated fibers due to the", "presence of the myelin sheath,\n\nAnswer the question given the\n\nreference context.\n\nTask\n\n:\n\nMyelinated nerve fibers have a myelin sheath,\n\nenabling faster impulse conduction via saltatory\n\nconduction at nodes of Ranvier, unlike non-\n\nmyelinated fibers. Local anesthesia blocks nerve\n\nconduction.", "solver. The Detector's lack of specialized knowledge makes it less prone to overconfidence, positioning it as an ideal agent for identifying potential knowledge gaps and dissecting complex problems into simpler, more retrievable sub-questions. In contrast, the expert Resolver is tasked with answering these sub-questions, leveraging its deep domain knowledge to generate precise queries and synthesize retrieved information. This interaction follows a 'Dissect-Retrieve-Reflect' workflow, where the Detector's persistent questioning and the Resolver's expert synthesis create a productive adversarial tension. This process, guided by a neutral moderator, systematically filters out noise, probes for deeper understanding, and converges on a well-supported, collaborative solution, effectively mitigating both semantic discrepancies and retrieval hallucinations.", "4 Experiments\n\nIn this section, we present our experimental setup and evaluation results. To assess the effectiveness of our multimodal RAG pipeline under settings aligned with the CRAG-MM 2025 challenge. All local experiments were conducted on the same hardware as the challenge environment: a single NVIDIA L40S GPU with 48GB of memory. For the generated answer, we employed the GPT-4o-mini [7] model as the evaluation LLM.", "4.1 Metrics\n\nWeevaluate our system using the official metric suite defined by the CRAG-MM challenge. Specifically, we report the total number of queries, correct answers, missed answers, and hallucinations. From these, we derive key indicators such as accuracy, hallucination rate, and missing rate. Additionally, we compute the truthfulness score to assess factual grounding.", "4.2 Single-source Augmentation\n\nTo evaluate the effectiveness of our method, we conduct an ablation study on Task 1 under local evaluation, as shown in Table 1. The goal is to assess the incremental impact of the key component in our verification-centric pipeline.", "The LLaMA Vision Only baseline refers to the vision-language model (VLM) directly answering questions from image and text input without any external retrieval or augmentation. This setting yields a relatively high accuracy of 25.00%, but suffers from severe hallucination issues (59.62%) and a negative truthfulness score of -34.62%, indicating that the answers often conflict with the ground truth. This is expected, as the VLM attempts to answer all questions regardless of its knowledge limitations, resulting in a low", "| | | Accuracy (%) | Missing Rate (%) | Hallucination Rate (%) | Truthfulness Score (%) |\n|---:|:---------------------------|---------------:|-------------------:|-------------------------:|-------------------------:|\n| 0 | LLaMA Vision Only | 25 | 15.38 | 59.62 | -34.62 |\n| 1 | RAG Agent | 27.88 | 9.62 | 62.5 | -34.62 |\n| 2 | w/o CoV & Self-Consistency | 3.85 | 95.19 | 0.96 | 2.88 |\n| 3 | w/o CoV | 4.81 | 95.19 | 0 | 4.81 |\n| 4 | Ours | 14.42 | 82.69 | 2.88 | 11.54 |\n\nTable 1: Ablation study on Task 1 single-source augmentation under local evaluation.", "missing rate but poor factual reliability. The RAG Agent baseline incorporates external retrieval via the official search API, enhancing access to relevant knowledge. This improves the accuracy to 27.88% and slightly reduces the missing rate to 9.62%. However, hallucination rate increases further to 62.50%, and the truthfulness score remains at -34.62%, showing that naive RAG without verification can introduce more misleading content. To further assess the role of verification, we remove key components from our pipeline to observe their individual effects. The w/o CoV & SelfConsistency variant disables both the self-consistency check and the Chain-of-Verification (CoV). As a result, the model becomes overly conservative, with a missing rate of 95.19% and very low accuracy (3.85%). While hallucination rate drops sharply to 0.96%, the overall coverage and utility of the model are severely degraded. A similar trend is observed in the w/o CoV variant, which enables self-consistency but disables CoV. Although slightly more accurate (4.81%), it still fails to provide meaningful coverage. Our final agent, denoted as Ours , integrates all proposed components. This configuration strikes the best balance between caution and informativeness. It achieves a truthfulness score of 11.54% , the highest among all systems, with an acceptable hallucination rate of 2.88% and an improved accuracy of 14.42%. The results demonstrate that each verification stage contributes to reducing hallucinations while selectively increasing answer confidence, enabling the agent to provide more trustworthy responses.", "Our method is developed specifically for the Single-source Augmentation (Task 1) setting. The same pipeline and implementation are directly applied to Task 2 and Task 3 during the CRAG-MM challenge phase. Thus, we focus our discussion on Task 1.", "2 Preliminary and Related Work\n\n2.1 Retrieval Augmented Generation (RAG)\n\nWhen constructing domain-specific foundation models or private agents, modifying a trained LLM is often necessary. However, the computational and resource costs associated with full finetuning render it impractical for a large number of users. Even though techniques such as LowRank Adaptation (LoRA) and prefix-tuning provide lighter training paradigms, RAG offers its own advantages in terms of plug-and-play capabilities. Furthermore, RAG allows external knowledge to remain independent of LLMs, facilitating the replacement of databases with other domains or more up-to-date versions.", "Currently, a common paradigm of vanilla RAG is illustrated in Fig. 1. Given a question q , an external corpus C divided into chunks as C = [ c 1 , c 2 , . . . , c m ] , and an embedding model g () , we first generate the embeddings for the question and the corpus, respectively, as follows:\n\nwhere e q , e c j \u2208 R d g are the embedding results for question q and chunk c j , and d g is the output dimension of the model g () .", "Based on the understanding that two sentences with higher similarity yield more similar embeddings, we calculate the similarity between the question and each chunk using the cosine similarity metric, defined as:\n\nwhere s q,c j represents the cosine similarity between question q and chunk c j , \u00b7 denotes the dot product, and \u2225 \u00b7 \u2225 denotes the Euclidean norm. Finally, we select k chunks with the highest similarity and incorporate them into the prompt, expressed as:\n\nwhere h () is a montage function that combines the question and the retrieved chunks using a specified template t , and K represents the index set of the selected k chunks. In this way, we construct a prompt that includes external knowledge from the corpus C that is most similar to question q .", "Previous studies have validated a wide range of modifications to the RAG pipeline, demonstrating effectiveness in open-domain question answering by improving various components such as adaptive retrieval strategies (Tang et al., 2024; Guan et al., 2024), dynamic document selection (Kim and Lee, 2024; Hei et al., 2024), joint retrievergenerator optimization (Siriwardhana et al., 2021), reinforcement learning (Huang et al., 2025a), and hybrid retrieval-reranking frameworks (Sawarkar et al., 2024; Zhang et al., 2025).", "Since embedding models can produce different similarity calculations, many studies have innovated in retriever embedding models to improve the performance of the RAG pipeline. For example, Invar-RAG (Liu et al., 2024) constructs an LLM-based retriever with LoRA-based representation learning and an invariance loss, directly addressing retriever embedding variance and locality, and demonstrates improved retrieval and response accuracy. Blended RAG (Sawarkar et al., 2024) combines dense semantic embeddings with sparse retrievers, empirically showing that this hybrid embedding space achieves higher QA accuracy, especially as corpus size scales. W-RAG (Nian et al., 2024) further validates that enhancing retriever embeddings using weak signals from the downstream QA task offers gains comparable to methods requiring full human annotations.", "However, most studies have focused on the innovation of a single embedding model. Due to variations in model architecture and training datasets, different embedding models operate effectively in different domains. Our approach will leverage the strengths of various embedding models to provide the most relevant information for LLMs.", "2.2 Confidence of LLMs\n\nThe confidence level of the LLM output can indicate the response quality to some extent. Several approaches have been employed to measure model confidence. Some of them depend on model prompting, external similarity or model-expressed judgments. These include linguistic confidence measures (Shrivastava et al., 2023; Xiong et al., 2023), where the model is prompted to state its confidence explicitly as part of the output, as well as LLM-as-judge or self-evaluation strategies that involve prompting the model to rate or compare its own answers in scalar or relative terms (Shrivastava et al., 2025; Ren et al., 2023; Hager et al., 2025). However, these methods are often poorly calibrated, not directly related to the true predictive uncertainty of the model, and can be biased or inconsistent due to their reliance on language generation rather than statistical likelihood.", "In contrast, probability-based metrics directly ground into models' token probability output (Chen and Mueller, 2023; Jiang et al., 2021; Lin et al., 2024). These include average logprobability as the mean token log-probabilities, entropy-based measures that quantify prediction uncertainty via sequence or per-token entropy, self-certainty (Kang et al., 2025) that utilizes the intrinsic probability distribution of LLM outputs to assess the model confidence.", "Currently, generative LLMs operate based on an auto-regressive process, formulated as:\n\nwhere f () is an LLM, x is the prompt, y <i represents the first generated i -1 tokens, and y i denotes the i th token. The output of f () is the probability distribution over each word in the vocabulary, from which y i is sampled according to this probability vector. This process can be viewed as a Markov Decision Process (MDP), where the state consists of the prompt x along with the currently generated tokens y <i , and the action space corresponds to the vocabulary, allowing each token to be chosen with a probability given by\n\nwhere v j is the j th element in the vocabulary v , and \u03c0 f represents the strategy. For simplicity, we will neglect the \u03c0 f term in the following discussion.", "Many studies have observed that the probability of each token reflects the LLMs' confidence. Intuitively, if the selected token has a higher chosen probability p ( y i | x, y <i ) , or if the generated probability vector f ( x, y <i ) exhibits a less uniform distribution, the LLMs demonstrate higher confidence in their generated results. Our study will use the confidence measurement metrics summarized in (Kang et al., 2025):\n\nAverage Log-Probability (AvgLogP) : This metric computes the mean logarithmic likelihood of the generated tokens, reflecting the model's confidence in the entire sequence:", "where n represents the answer length. Higher log-probabilities suggest more reliable predictions.\n\nGini Impurity (Gini) : This metric measures the concentration of the predicted distribution:\n\nwhere | v | represents the vocabulary size. Higher values indicate more peaked distributions, reflecting greater certainty in the model's predictions.", "Entropy : This metric computes the uncertainty of the predicted distribution:\n\nLower entropy signifies higher confidence, as it indicates a more deterministic output distribution.", "Distributional Perplexity (DP) : This metric generalizes perplexity to the entire predicted distribution:\n\nLower values of distributional perplexity indicate that the model is more confident about its predictions across the entire distribution, as it reflects a clearer understanding of token relationships.", "Self-Certainty : This metric assesses the KLdivergence from a uniform distribution:\n\nHigher self-certainty values reflect greater confidence in the predictions, as they indicate a more concentrated distribution of outputs.", "The Cumulative Distribution Function (CDF) relationship between these metrics and accuracy is provided in Fig. 2, where we utilize the negative of Entropy and DP.", "4 Experiment\n\n4.2.3 Results of Confident RAG method\n\n| | LLMModel | Mix-Embedding RAG.2 Embs | Mix-Embedding RAG.3 Embs | Mix-Embedding RAG.4 Embs | Mix-Embedding RAG.Avg | Mix-Embedding RAG.v.s. VanillaLLM | Mix-Embedding RAG.v.s. Vanilla RAG |\n|---:|:----------------------------------------|:---------------------------|:---------------------------|:---------------------------|:------------------------|:------------------------------------|:-------------------------------------|\n| 0 | Qwen2.5-Math-7B (Yang et al., 2024) | 74.20% | 76.00% | 74.80% | 75.0% | -0.2% \u2193 | -5.5% \u2193 |\n| 1 | Llama-3.1-8B (Grattafiori et al., 2024) | 20.90% | 19.60% | 22.80% | 21.1% | 4.5% \u2191 | -0.2% \u2193 |\n| 2 | OLMo-2-1124-7B (OLMo et al., 2024) | 26.20% | 26.80% | 26.40% | 26.5% | 5.5% \u2191 | 0.5% \u2191 |\n| 3 | Average | 40.4% | 40.8% | 41.3% | 40.9% | 3.3% \u2191 | -1.7% \u2193 |", "Table 2: Performance of Mixture-Embedding RAG: This table illustrates the performance when using between 2 and 4 different embedding models randomly (denoted as 2 Embs to 4 Embs), comparing the results with those of the vanilla LLM and vanilla RAG.", "researchers should seek a suitable trade-off based on their own condition when choosing the optimal N.", "4.3.1 Mixture-embedding RAG\n\nFor general LLMs (e.g., Llama-3.1-8B and OLMo-2-1124-7B) without math fine-tuning, their internal math knowledge is limited, leading to lower accuracy in direct answer generation. In these cases, even noisy references retrieved by RAG are more reliable than the LLMs' own outputs, as RAG at least provides partially correct information. However, while the mixtureembedding RAG method may optimize the retrieval ranking process and improve the quality of the references, the general LLMs' capabilities prevent them from fully leveraging higher-quality references, resulting in performance similar to vanilla RAG. Additionally, if different embedding models return highly diverse references, directly combining the top-ranked documents may cause information overload or contextual confusion, negating the potential benefits of mixture-embedding method. Therefore, the performance of general LLMsmatches that of vanilla RAG rather than surpassing it.", "On the other hand, for the LLMs that have been fine-tuned based on math corpora, vanilla", "RAG may result in smaller improvements, and lower-quality references can lead to poorer answer performance. For these types of LLMs, the mixture-embedding method may introduce additional noise in mathematical contexts, resulting in lower accuracy compared to vanilla RAG. The decline in accuracy may be caused by several factors: (1) Mathematical symbols and formulas may vary drastically across embedding models, making similarity calculations unstable. (2) Different models may encode math terms differently, causing the top-ranked reference to be suboptimal. (3) An embedding model might incorrectly rank an irrelevant math material highly, while better references from other models are ignored. If the LLM generates hallucinated answers based on incorrect references, its performance can degrade below that of the vanilla LLM. (4) Information overload or contextual confusion may also occur, similar to what happens with general LLMs.", "4.3.2 Confident RAG\n\nAs shown in Figure 2, there exists a positive correlation between confidence and accuracy. Therefore, the Confident RAG method improves overall accuracy by integrating multiple embedding models to generate answers and selecting the highestconfidence results using the most effective metric. This process effectively filters out low-confidence", "| | LLM | Emb. Model | AvgLogP | Self-certainty | Gini | Entropy | DP |\n|---:|:----------------------------------------|:------------------------|:--------------|:-----------------|:-------------|:--------------|:------------------|\n| 0 | | 1,2 | 82.0% | 85.0% | 81.8% | 82.8% | 83.6% |\n| 1 | | 1,3 | 79.4% | 83.4% | 79.2% | 79.6% | 81.0% |\n| 2 | | 1,4 | 81.4% | 84.6% | 81.6% | 81.8% | 82.2% |\n| 3 | | 2,3 | 81.8% | 84.4% | 81.4% | 81.8% | 82.4% |\n| 4 | | 2,4 | 81.4% | 83.6% | 81.0% | 81.4% | 82.2% |\n| 5 | | 3,4 | 79.2% | 82.2% | 79.8% | 79.8% | 80.6% |\n| 6 | | Avg (n=2) | 80.9% | 83.9% | 80.8% | 81.2% | 82.0% |\n| 7 | | 1,2,3 | 79.4% | 85.0% | 79.0% | 79.8% | 81.6% |\n| 8 | Qwen2.5-Math-7B (Yang et al., 2024) | 1,2,4 | 79.6% | 85.0% | 79.6% | 80.6% | 82.0% |\n| 9 | | 1,3,4 | 79.0% | 84.8% | 79.6% | 80.0% | 80.8% |\n| 10 | | 2,3,4 | 79.2% | 84.4% | 79.0% | 79.8% | 81.0% |\n| 11 | | Avg (n=3) | 79.3% | 84.8% | 79.3% | 80.1% | 81.4% |\n| 12 | | 1,2,3,4 | 78.2% | 84.8% | 78.4% | 79.2% | 80.6% |\n| 13 | | Avg (n=2,3,4) | 80.1% | 84.3% | 80.0% | 80.6% | 81.6% |\n| 14 | | v.s. Vanilla RAG | 0.4% \u2193 | 3.8% \u2191 | 0.4% \u2193 | 0.1% \u2191 | 1.2% \u2191 |\n| 15 | | v.s. Vanilla LLM | 4.9% \u2191 | 9.1% \u2191 | 4.8% \u2191 | 5.4% \u2191 | 6.4% \u2191 |\n| 16 | | 1,2 | 27.2% | 27.2% | 27.0% | 27.0% | 27.2% |\n| 17 | | 1,3 | 26.8% | 26.8% | 26.6% | 27.0% | 27.2% |\n| 18 | | 1,4 | 26.8% | 26.4% | 26.4% | 26.4% | 26.4% |\n| 19 | | 2,3 | 23.2% | 24.0% | 24.0% | 24.0% | 23.6% |\n| 20 | | 2,4 | 26.6% | 26.8% | 26.4% | 26.0% | 26.6% |\n| 21 | | 3,4 | 25.8% | 26.4% | 26.6% | 26.6% | 26.4% |\n| 22 | | Avg (n=2) | 26.1% 28.6% | 26.3% 29.2% | 26.2% 28.8% | 26.2% | 26.2% 29.4% |\n| 23 | Llama-3.1-8B (Grattafiori et al., 2024) | 1,2,3 1,2,4 | 28.8% | | | 29.4% | |\n| 24 | | | | 28.6% | 28.0% | 28.0% | 28.2% 27.6% |\n| 25 | | 1,3,4 | 27.4% | 27.4% | 27.2% | 27.6% | |\n| 26 | | 2,3,4 | 25.8% | 26.4% | 26.0% | 26.0% | 26.4% |\n| 27 | | Avg (n=3) | 27.7% 27.8% | 27.9% 27.6% | 27.5% 27.0% | 27.8% 27.4% | 27.9% 27.6% |\n| 28 | | 1,2,3,4 Avg (n=2,3,4) | 26.8% | 27.0% | 26.7% | 26.9% | 27.0% |\n| 29 | | | | | 5.5% \u2191 | | |\n| 30 | | v.s. Vanilla RAG | 5.6% \u2191 | 5.7% \u2191 | 10.1% \u2191 | 5.6% \u2191 | 5.7% \u2191 |\n| 31 | | v.s. Vanilla LLM 1,2 | 10.2% \u2191 31.0% | 10.4% \u2191 31.2% | 30.4% | 10.3% \u2191 31.2% | 10.4% \u2191 32.2% |\n| 32 | | 1,3 | 29.8% | 29.6% | 29.8% | 30.0% | 30.0% |\n| 33 | | 1,4 | 29.4% | 29.2% | 28.6% | 29.6% | 31.0% |\n| 34 | | 2,3 | 31.6% | 30.8% | 30.2% | 31.4% | 32.8% |\n| 35 | | 2,4 | 32.6% | 32.0% | 31.0% | 33.0% | 33.8% |\n| 36 | | 3,4 | 29.6% | 30.2% | 29.6% | 30.6% | 31.4% |\n| 37 | | Avg (n=2) | 30.7% | 30.5% | 29.9% | 31.0% | 31.9% |\n| 38 | OLMo-2-1124-7B (OLMo et al., 2024) | 1,2,3 | 32.6% | 32.0% | 31.4% | 32.2% | 33.2% |\n| 39 | | 1,2,4 | 32.8% | 32.8% | 31.6% | 33.2% | 34.8% |\n| 40 | | 1,3,4 | 31.2% | 31.2% | 30.6% | 32.0% | 34.6% |\n| 41 | | 2,3,4 | 32.6% | 32.2% | 30.8% | 33.6% | 36.6% |\n| 42 | | Avg (n=3) | 32.3% | 32.1% | 31.1% | 32.8% | 34.8% |\n| 43 | | 1,2,3,4 | 32.8% | 32.4% | 31.0% | 33.2% | 35.8% |\n| 44 | | Avg (n=2,3,4) | 31.5% | 31.2% | 30.5% | 31.8% | 33.3% |\n| 45 | | v.s. Vanilla RAG | 5.5% \u2191 | 5.2% \u2191 | 4.5% \u2191 | 5.8% \u2191 | 7.3% \u2191 |\n| 46 | | LLM | 10.5% \u2191 | | \u2191 | \u2191 | 12.3% \u2191 |\n| 47 | | v.s. Vanilla | | 10.2% \u2191 | 9.5% | 10.8% | |\n| 48 | | Avg (n=2) Avg (n=3) | 45.9% 46.4% | 46.9% 48.3% | 45.6% 46.0% | 46.1% 46.9% | 46.7% 48.0% 48.0% |\n| 49 | Average | Avg (n=4) Avg (n=2,3,4) | 46.3% | 48.3% | 45.5% | 46.6% | 47.3% |\n| 50 | | v.s. Vanilla RAG | 46.1% 3.5% \u2191 | 47.5% 4.9% \u2191 | 45.7% 3.2% \u2191 | 46.4% 3.9% \u2191 | 4.7% \u2191 |\n| 51 | | v.s. Vanilla LLM | 8.5% \u2191 | 9.9% \u2191 | 8.1% \u2191 | 8.8% \u2191 | 9.7% \u2191 |", "Table 3: Accuracy Comparison Across Multi-RAG with Different Embedding Models: Avg( n ) denotes the average accuracy across different combinations of n embedding models. Each line uses underline to indicate the best embedding combination within each LLM. Each row uses bold to signify the best metric for confidence evaluation. For performance comparison with Vanilla RAG, we use the average accuracy of all single embedding models as the baseline.", "incorrect answers. Meanwhile, the combined effect of RAG and confidence filtering enhances the robustness, leading to significant improvements compared to vanilla LLMs. When employing the optimal confidence metric, all LLMs achieved an accuracy increase of nearly 10%, demonstrating the method's universality. In the experiments, when the number of embedding models N > 3, the accuracy improvement became limited, likely due to redundant or noisy retrievals introduced by additional models. At N=3, the method achieved an optimal balance between diversity and computational efficiency. Increasing the number of models further yields only marginal benefits.", "Self-Certainty and DP outperform other metrics since they directly measure the concentration and divergence of the probability distribution. Specifically, Self-Certainty measures how far the predicted distribution deviates from uniform. By scaling the probabilities by | v | and taking the negative logarithm, it heavily penalizes uniform-like distributions, favoring sharp peaks. This makes it highly discriminative for high-confidence answers. Additionally, DP is an exponential version of entropy. The exponentiation amplifies differences in entropy, making it more sensitive to the sharpness of the distribution. Low DP values indicate tightly clustered high-probability tokens, which strongly correlate with correct answers. In contrast, other metrics are less sensitive because they either average out uncertainties (AvgLogP) or lack normalization across vocabularies (Gini). While entropy can be useful, it is linear and less discriminative compared to DP's exponential scaling. Therefore, Self-Certainty and DP are more sensitive to subtle variations in model confidence.", "F.4 Implications for Retrieval System Design\n\nThese case studies reveal several insights that should guide the design of future document retrieval systems intended for deployment across diverse computational environments and use cases.", "First, the universal effectiveness of hierarchical retrieval across all model scales represents a finding. Even our smallest 4B parameter model achieves reasonable performance on simple factual queries when equipped with our pseudoTOC based hierarchical retrieval system. This validates a key architectural decision: the structured document representation created by our pseudo-TOC generation helps compensate for limited model capacity by reducing the search space to relevant sections. Rather than requiring the model to search through potentially thousands of chunks, the hierarchical structure allows even resource-constrained models to focus their limited attention on the most promising document regions. This finding suggests that investing in better document structure inference may yield greater returns than simply scaling model size, for deployment scenarios with computational constraints.", "Second, our analysis reveals that evidence aggregation capabilities scale strongly with model parameters. Tasks requiring synthesis across multiple document pages show the clearest performance stratification between model tiers. The ability to maintain coherent tracking of evidence across extended contexts appears to be correlated with model parameters. For instance, when counting human quotes across a 40-page document, the Pro model tracks all five individuals with their precise page locations, while the Lite model overcounts to 17, suggesting it loses track of which quotes it has already counted. This pattern indicates that applications requiring document analysis (such as due diligence reviews, systematic literature surveys, or regulatory compliance checking) should prioritize larger models despite their increased computational cost.", "Third, visual understanding capabilities demonstrate an interesting graceful degradation pattern with model scale. While all models can identify that visual content exists within documents, the ability to accurately count, categorize, and interpret specific visual elements clearly correlates with model size. Our analysis of the hand-drawn cartoon counting task exemplifies this: the Pro model not only counts correctly but also provides semantic understanding of each cartoon's purpose, the Base model acknowledges its limitations, and the Lite model attempts counting but conflates different visual elements. This suggests a tiered deployment strategy where basic visual awareness tasks can be handled by smaller models, but applications requiring detailed visual analysis (such as technical diagram interpretation or chart data extraction) necessitate larger models.", "Fourth, we observe that conservative behavior in handling ambiguous or unanswerable questions emerges as a function of scale. Larger models demonstrate more nuanced responses, providing helpful context about available information rather than attempting to guess. When asked about 2024 voting data in a document containing only information through 2018, all models correctly identify the question as unanswerable, but larger models additionally provide context about what related information is available. This conservative behavior proves valuable for high-stakes applications in legal, medical, or financial domains where acknowledging uncertainty is preferable to confident errors that could lead to costly mistakes.", "Fifth, our edge cases reveal that model scaling does not guarantee monotonic improvement across all task types. Some specific capabilities may actually degrade or show non-linear patterns with scale. The chart counting task where smaller models outperformed the Pro model exemplifies this phenomenon. The larger model's more sophisticated understanding led it to apply stricter criteria for what constitutes a relevant chart, resulting in undercounting. This finding necessitates careful evaluation for particular use cases and suggests that blind trust in larger models may be misplaced for certain task types.", "These findings suggest that future document retrieval systems should adopt a more nuanced approach than maximizing model size. System designers should consider the specific types of questions and evidence patterns common in their target domains. A portfolio approach using different model scales for different query types may provide optimal cost-performance trade-offs. For instance, a production system might route simple factual queries to a Lite model for efficiency, escalate multi-page synthesis tasks to a Base model, and reserve the Pro model for cases requiring precise visual interpretation or complex reasoning. Such an adaptive routing strategy could significantly reduce computational costs while maintaining high accuracy for challenging queries."]}
{"type": "entry", "index": 37, "question": "how do i measure if i\u2019m getting diverse sources instead of 10 chunks from the same pdf", "answer": "You can treat \u201cdiverse sources\u201d as something you measure directly, not just eyeball.\n\n1. Simple source-diversity score (like in the paper)\nIn Document 1, they define a retrieval diversity metric as the fraction of unique review IDs among the top\u201110 chunks: Diversity = (number of distinct IDs) / 10 [1].\nYou can copy this idea for documents:\n- Give every PDF (or document) a unique ID.\n- When you retrieve K chunks (say K=10), look at which document each chunk came from.\n- Compute: Diversity = (number of distinct document IDs) / K.\n  \u2022 If all 10 chunks come from the same PDF, Diversity = 1 / 10 = 0.1 (very low).\n  \u2022 If all 10 chunks come from different PDFs, Diversity = 10 / 10 = 1.0 (perfect diversity).\n\nYou can log this number for each query to monitor whether you\u2019re repeatedly pulling from the same file or actually spreading across your corpus.\n\n2. Combine with relevance\nIn Document 1, they also track how close the chunks are to the query using an average cosine similarity between the query embedding and each chunk [1]. In practice you want both:\n- High similarity (on\u2011topic).\n- High diversity (many different source IDs).\n\n3. Optional: richer diversity later\nDocument 2 shows that diversity can also be measured at different levels (lexical, semantic, ingredient) when you have multiple outputs for one input [2]. By analogy, once you\u2019re happy with source diversity, you could later add:\n- Semantic diversity between chunks (are they saying different things?).\nBut the easiest starting point is the per-query \u2018distinct document IDs / K\u2019 metric.", "tags": ["_untagged"], "full_faithfulness": 0.8823529412000001, "full_answer_relevancy": 0.8588673985, "seed_faithfulness": 0.5294117647000001, "seed_answer_relevancy": 0.8521419176, "context_relevance": 0.5303662744007874, "context_relevance_error": null, "context_relevance_reason": "The context passages provide relevant information about measuring diversity in retrieval systems, particularly in the context of recipe adaptation and LLMs. However, while some passages discuss diversity metrics and retrieval methods, they do not directly address the user's specific question about measuring diverse sources versus multiple chunks from the same PDF. The relevance of the context is somewhat indirect, leading to a moderate score.", "structure_citation": 0.7571815858330162, "structure_error": null, "structure_reason": "The Actual Output effectively addresses the question of measuring source diversity by providing a clear method for calculating a diversity score based on unique document IDs. It references Document 1 for the retrieval diversity metric and Document 2 for additional diversity measurement methods, aligning well with the evaluation steps. However, while it includes citations [1] and [2], it does not explicitly verify that all claims are supported by the cited snippets, which slightly detracts from its completeness.", "hallucination_score": 0.7209711030557213, "hallucination_error": null, "hallucination_reason": "The response effectively outlines a method for measuring source diversity by introducing a simple source-diversity score and providing a clear formula. It aligns well with the context, which discusses retrieval diversity metrics. However, while it mentions combining relevance with diversity, it does not explicitly connect this to the context's focus on average cosine similarity, which could enhance the completeness of the answer. Overall, the claims are mostly supported, with minor gaps in direct references to the context.", "contexts_used": ["D. Question and Answering (QA)\n\nFor retrieval of reviews, we sampled five Spotify-centric queries and retrieved the top K = 10 review chunks for each. We measured two unsupervised metrics:\n\nAverage Cosine Similarity : the mean cosine similarity between each query embedding and its top-10 chunk embeddings.", "Retrieval Diversity : the fraction of unique review IDs among all retrieved chunks (distinct IDs / 10).\n\nOur retriever achieved perfect diversity and cosine scores from 0.618 to 0.754, demonstrating reliable, on-topic retrieval. Table IX summarizes these proxy metrics.", "For generation of answers, we randomly sampled 20 generated answers (each paired with its cited snippets) and annotated them ourselves, confirming that each answer (1) reflected the cited excerpts, (2) covered the main points of those excerpts, and (3) was written in clear, reader-friendly prose. We found the responses to be accurate and comprehensive.", "| | Query | Avg. Cosine Sim. | Diversity |\n|---:|:-------------------------------------------------------------------------------|-------------------:|------------:|\n| 0 | What complaints do users have about | 0.713 | 1 |\n| 1 | What do listeners say about Spotify crashing or freezing on startup? | 0.754 | 1 |\n| 2 | How do listeners describe the app's offline playback experience? | 0.696 | 1 |\n| 3 | How do users report errors or failures when downloading songs for offline use? | 0.618 | 1 |\n| 4 | What do users say about Spotify's crossfade and track-transition experience? | 0.65 | 1 |\n\nTABLE IX RETRIEVAL PROXY METRICS (K=10) FOR SELECTED SPOTIFY QUERIES (HIGHER DIVERSITY IS BETTER)", "2 Related Work\n\nRecipe Cross-Cultural Adaptation Recipe cross-cultural adaptation (Cao et al., 2024) involves modifying recipes to suit the dietary preferences and writing styles of the target culture. This includes not just translation, but also adjusting formats, ingredients, and cooking methods to align with cultural norms. Previous studies (Cao et al., 2024; Pandey et al., 2025; Zhang et al., 2024) often treat recipe adaptation as a cross-cultural translation task, exploring how prompt-based LLMs can be used for Chinese-English recipe adaptation.", "However, LLM-based recipe adaptation still faces challenges. Magomere et al.'s (2024) show that such methods can be misleading and may reinforce regional stereotypes. Hu et al.'s (2024) further identify two main challenges: First, LLMs lack culinary cultural knowledge, leading to insufficient cultural appropriateness. Second, the adapted recipes have quality issues, such as changing ingredients without adjusting the cooking steps accordingly. They propose another way to address these issues, namely through cross-cultural recipe retrieval, which sources recipes from real cooking practices within the target culture, generally offering better quality and cultural alignment. However, compared to directly using LLMs, the retrieved recipes often have low similarity to the original.", "All the above-mentioned studies primarily focus on the quality of generated results, including cultural appropriateness and their preservation of the original . However, they overlook the diversity of the results and do not explore the use of RAG for cross-cultural recipe adaptation. Our study emphasizes the trade-off between diversity and quality, with a particular focus on RAG-based approaches.", "Diversity in text generation, IR, and RAG Previous studies (Lanchantin et al., 2025) have shown that post-training LLMs tend to sharpen their output probability distribution, leading to reduced response diversity. This has raised a common concern about the insufficient diversity of LLMs, particularly in creative tasks. Several stochastic sampling-based decoding methods are widely used to control the level of diversity, most notably by adjusting hyperparameters such as temperature (Shi et al., 2024). However, these methods often still fall short in achieving sufficient diversity and may lead to a rapid decline in output quality, which is another important factor to consider when measuring diversity (Lanchantin et al., 2025).", "Figure 2: Overview of CARRIAGE . Diversity components are highlighted. We first enhance the diversity of retrieved results, then we enable more diverse use of contextual information via dynamic context selection, and inject contrastive context to prevent the LLM from generating outputs similar to previously generated recipes.\n\nMulti-Query Retrieval Source Culture Recipe Target Culture Diversity-aware Reranking Query Rewriting Dynamic Context Organization Pool of Previously Generated Recipes LLM Generation Contrastive Context Injection Previously : Diversity component Reference Recipes Selection Relevance Diversity\n\nMay generate multiple times\n\nIn IR, retrieving text with high diversity can cover a wider range of subtopics, thereby accommodating the potentially diverse preferences of different users. Methods such as diverse query rewriting (Mohankumar et al., 2021) and diversity-aware re-ranking (Carbonell and Goldstein, 1998; Krestel and Fankhauser, 2012) can effectively enhance the diversity of retrieval results. Some recent works (Carraro and Bridge, 2024) have explored using LLMs to enhance diversity in re-ranking.", "In RAG, prior works have mainly focused on retrieving diverse results to obtain more comprehensive information, such as mitigating context window limitations (Wang et al., 2025) and addressing multi-hop question answering tasks (Rezaei and Dieng, 2025). These works are primarily framed as question answering, aiming to acquire comprehensive knowledge to produce a single correct answer. Consequently, the evaluation metrics emphasize answer accuracy rather than diversity. In contrast, our task naturally permits multiple valid answers. Therefore, we adopt different strategies to encourage answer diversity and use metrics that explicitly evaluate the diversity of final outputs. While prior works have largely focused on retrieving diverse contexts, our approach goes a step further by investigating how to utilize such diverse contexts to produce diverse outputs.", "5 Metrics\n\nOur evaluation metrics focus on two key aspects: diversity and quality . To assess diversity, we consider factors such as lexical , semantic , and ingredient diversity from a per-input perspective. As a trade-off, we evaluate quality from two dimensions: the preservation of the source recipe, and cultural appropriateness for users in the target culture.", "5.1 Diversity\n\nKirk et al.'s (2023) have proposed two paradigms for measuring diversity: across-input (over pairs of one input and one output) and per-input diversity (one input, several outputs). Per-input diversity helps us investigate whether a single recipe can be adapted into multiple variants to meet different dietary preferences, while across-input diversity assesses whether the generated recipes collectively exhibit a diverse range of linguistic patterns. Because our investigation primarily focuses on whether a single recipe can be adapted into diverse variations to meet a broader range of needs, we adopt the per-input diversity setting as our main experimental focus. The across-input diversity setting is discussed further in Section 7.", "For a diversity metric D , under model configuration c , A denotes a set of adapted recipes,", "containing N source recipes, we define A i c = { a i c, 1 , a i c, 2 , . . . , a i c,K } as the set of K adaptations for the i -th source recipe under configuration c . The per-input diversity is defined as follows:\n\nLexical Diversity Lexical diversity is a measure of the variety of vocabulary used within a set of text. High lexical diversity indicates using a broad range of unique words, which may correspond to a wider variety of ingredients, cooking methods, and flavors. We employ Unique-n (Johnson, 1944) to evaluate lexical diversity, calculated as the ratio of unique n -grams to the total number of n -grams, reflecting the proportion of distinct n -grams and indicates vocabulary richness. Following prior work (Guo et al., 2024), we report the average Unique-n across unigrams, bigrams, and trigrams.", "Semantic Diversity Semantic diversity refers to the variety of meanings within a set of texts. High semantic diversity suggests a wide range of culinary ideas. We measure per-input semantic diversity using the average pairwise cosine distance between Sentence-BERT embeddings because embedding-based semantic diversity enables a more fine-grained evaluation of variation beyond surface-level vocabulary (Stasaski and Hearst, 2023). Specifically, for a set of K adapted recipes, we define the sum of their average semantic similarity and semantic diversity to be 1. In this formulation, higher semantic similarity implies lower semantic diversity. We define semantic diversity, scaled to the range [0 , 1] , as follows:\n\nwhere e represents embeddings of the recipe.", "Ingredient Diversity Ingredient diversity measures the variation in sets of ingredients across different recipes. Ingredient choice plays a crucial role in recipe diversity (Borghini, 2015). Compared to general lexical variation, ingredient changes offer a more precise signal for capturing the key factors driving diversity in recipes.", "Recipes often describe the same ingredient in varying ways, such as differences in quantity or units of measurement. To mitigate this, we introduce Standard Ingredients , which retain only the ingredient name by stripping away non-essential details. Since ingredient descriptions typically follow the format < quantity > < unit > < ingredient name >, we extract only the < ingredient name > to compute ingredient diversity. The detailed procedure is provided in Appendix B.", "To avoid the influence of differing ingredient counts across recipes, we define ingredient diversity as the ratio of unique standardized ingredients to the total number of ingredients. For a set of K adapted recipes, let the set of standardized ingredients for each recipe be I 1 , I 2 , . . . , I K . We define ingredient diversity as follows:", "5.2 Quality\n\nWe define automatic quality metrics to serve as a trade-off when evaluating recipe diversity. Further details on the training and evaluation of the CultureScore model are provided in Appendix B.", "Source Recipe Preservation Following prior work (Cao et al., 2024; Hu et al., 2024), we employ BERTScore (Zhang* et al., 2020), a common cosine embedding-based method for measuring the similarity between source and output recipes. Previous studies have shown that BERTScore aligns well with human evaluations in terms of source recipe preservation (Hu et al., 2024).", "Cultural Appropriateness We propose a novel metric, the Recipe Cultural Appropriateness Score (CultureScore), to assess how well the output recipes align with the target culture. Specifically, we employ a BERT-based classifier (Devlin et al., 2019; Ca\u00f1ete et al., 2020) to predict the country of origin of a recipe using its title and list of ingredients as input. The CultureScore is defined as the average predicted probability assigned by the model to the target culture across all adapted recipes, with higher scores indicating better cultural alignment. Since Latin American and Spanish recipes share the same language, the model cannot rely on linguistic cues; instead, it must learn to distinguish them based on culturally relevant features such as ingredients, flavors, and writing styles. Given that the classification model achieves an F1-score of over 90% in distinguishing between Latin American and Spanish recipes, we consider CultureScore a reliable proxy for assessing cultural appropriateness.", "| | | Method. | Diversity ( \u2191 ).Lexical | Diversity ( \u2191 ).Ingredient | Diversity ( \u2191 ).Semantic | Quality ( \u2191 ).CultureScore | Quality ( \u2191 ).BERTScore |\n|---:|:------------------|:----------------------------------------------------------------------------|:--------------------------|:-----------------------------|:---------------------------|:-----------------------------|:--------------------------|\n| 0 | Closed- Book LLMs | Llama3.1-8B Qwen2.5-7B Gemma2-9B | 0.557 0.551 0.538 | 0.667 0.531 0.639 | 0.232 0.247 0.196 | 0.451 0.404 0.468 | 0.404 0.439 0.370 |\n| 1 | IR | JINA-ES CARROT CARROT-MMR | 0.742 0.735 0.741 | 0.937 0.925 0.941 | 0.459 0.462 0.527 | 0.511 0.512 0.503 | 0.295 0.301 0.298 |\n| 2 | RAG | Vanilla-LLaMA RAG CARROT-LLaMA RAG CARROT-MMR-LLaMA RAG CARROT-MMR-Qwen RAG | 0.518 0.525 0.520 0.532 | 0.748 0.765 0.748 0.536 | 0.155 0.152 0.164 0.212 | 0.383 0.385 0.393 0.402 | 0.551 0.545 0.545 0.448 |\n| 3 | Ours | CARRIAGE -LLaMA CARRIAGE -Qwen | 0.577 0.628 | 0.739 0.676 | 0.269 0.303 | 0.463 0.590 | 0.442 0.342 |", "Table 1: Evaluation of diversity and quality on the RecetasDeLaAbuel@ dataset shows that our proposed CARRIAGE -LLaMA outperforms all closed-book LLMs in terms of Pareto efficiency across both diversity and quality metrics. In contrast, IR-based methods struggle with preserving the source recipe, while other RAG-based approaches tend to underperform in terms of diversity and cultural appropriateness."]}
{"type": "entry", "index": 38, "question": "what chunking works best when papers have math and symbols everywhere", "answer": "For math\u2011heavy papers full of formulas and symbols, the best chunking is structure\u2011 and meaning\u2011aware, not simple fixed character windows.\n\n1) Preserve logical units, not arbitrary characters\nNaive fixed\u2011size chunking can break examples away from their explanations and destroy coherence, which hurts retrieval quality.[3] This is especially bad in math, where a formula, diagram, and the sentence interpreting it must stay together.\n\nA better approach is to:\n- Respect natural boundaries like sentences and paragraphs.\n- Keep whole worked examples, definition boxes, and exercises in a single chunk whenever possible.[2]\nA manual/custom chunker that groups text blocks by layout and role (e.g., keeping an entire \u201cexample\u201d block together) is explicitly recommended over generic splitters for math education material.[2]\n\n2) Use semantic or pseudo\u2011TOC segmentation\nSemantic segmentation that splits at coherent sentence boundaries and recursively shortens over\u2011long chunks keeps meaning intact while staying within token limits (e.g., under ~512 tokens).[1] Systems that build a pseudo Table of Contents to organize content into topical sections avoid the fragmentation caused by naive fixed\u2011window chunking.[3]\n\n3) Aim for moderate chunk sizes (\u2248256\u2013512 tokens) with small overlap\nEmpirical results show that chunk sizes of 256\u2013512 tokens give the best balance of answer faithfulness and relevancy, outperforming both much smaller and much larger chunks.[4] Overlap (e.g., 20 tokens) helps preserve context across boundaries.[4]\n\nSo for math\u2011 and symbol\u2011heavy papers:\n- Chunk by logical/visual units (sections, examples, definitions),\n- Use semantic or TOC\u2011like boundaries instead of raw character counts, and\n- Target 256\u2013512\u2011token chunks with small overlaps.", "tags": ["retrieval_tuning"], "full_faithfulness": 0.92, "full_answer_relevancy": 0.9186124242, "seed_faithfulness": 0.8571428571, "seed_answer_relevancy": 0.9177942154000001, "context_relevance": 0.6582673851209053, "context_relevance_error": null, "context_relevance_reason": "The context passages provide detailed information on chunking strategies, particularly in relation to scientific documents, which aligns well with the user's question about effective chunking for papers with math and symbols. The passages discuss methods for maintaining semantic coherence and optimizing chunk sizes, which are relevant to the user's needs. However, the response could be improved by explicitly addressing the specific challenges posed by math and symbols, which are not directly mentioned in the context.", "structure_citation": 0.6380690359690401, "structure_error": null, "structure_reason": "The Actual Output includes several claims supported by citations, but there are issues with citation matching. For instance, the claim about preserving logical units is supported by citation [3], which is present in the Context, but the citation for the chunking strategy recommendations ([2]) does not have a corresponding snippet in the Context. Additionally, while the output discusses chunk sizes and overlaps, it cites [4] without clear support in the provided Context. These mismatches and unsupported claims lead to a deduction in the score.", "hallucination_score": 0.8756919838334399, "hallucination_error": null, "hallucination_reason": "The response effectively addresses the input question about chunking strategies for math-heavy papers by emphasizing structure- and meaning-aware chunking. It supports its claims with specific recommendations, such as preserving logical units and using semantic segmentation, which align well with the context provided. The only minor shortcoming is the lack of explicit references to the context's mention of token limits, although the response does imply adherence to these limits through its recommendations.", "contexts_used": ["a.Text Segmentation\n\nInformation chunking plays a critical role in enabling effective retrieval within RAG systems. By dividing lengthy documents into semantically coherent segments, we ensure that each chunk can be independently indexed, retrieved, and interpreted by the language model. This step is particularly important for scientific documents, which often contain dense technical language and long paragraphs with multiple concepts.", "To implement chunking, we developed a semantic segmentation pipeline tailored for long-form scientific texts. For input passages exceeding 512 tokens-the typical limit for embedding models-we apply a multi-stage process:\n\nFirst, the input text is tokenized into individual sentences using NLTK's sentence segmentation toolkit. This creates a candidate list of sentence boundaries.", "Next, we apply a BERT-based Next Sentence Prediction (NSP) model to all adjacent sentence pairs. The NSP model outputs a probability score (logit) indicating the likelihood that the second sentence naturally follows the first.", "We then normalize these logits and identify the optimal split points by selecting the boundaries with the highest confidence where the predicted NSP label is 1 (i.e., coherent sentence continuation).", "Once optimal split points are selected, we segment the text accordingly. If any resulting chunk still exceeds the 512-token threshold, we recursively apply the same NSP-based segmentation strategy until all chunks are within the token limit.", "This method preserves semantic continuity while adhering to token constraints, avoiding arbitrary cuts that could distort meaning. Figure 6 illustrates the final output of this segmentation strategy applied to long paragraphs, showing the alignment between sentence boundaries and token count thresholds.", "Predict logits (label=1) after softmax 0.2 0.3 0.1 0.5 Sent1 Sent2 Sent3 Sent4 Sent5 Sent6 Sent7 Sent8 Sent9\n\nSentI ~Sentg > 512 tokens , split @;\n\nSent5~Sent9 > 512 tokens , split @\n\n@; split,\n\nFigure 6 | Segmentation with 512 tokens", "iii) Vectorization\n\nIn the final stage, the cleaned and structured content is prepared for integration into the vector database:\n\nData Formatting: Extracted content and associated metadata are serialized into a standardized format, such as JSON or structured key-value pairs, which aligns with the schema expected by the downstream indexing system.", "Text Chunking: To maintain semantic coherence while accommodating input length limitations of embedding models, long text blocks are segmented into overlapping or context-aware chunks. Chunking strategies are optimized to preserve sentence boundaries and thematic continuity.", "Data Ingestion: Each chunk is passed through a pretrained embedding model to generate dense vector representations. These vectors are indexed in the vector database (e.g., Zilliz Cloud or Milvus), along with their corresponding metadata. This enables fast, semantic search during inference and supports scalable document-level and passage-level retrieval.", "3 System Design and Methodology\n\n3.3.1 Method 1: Structured Prompting with Function Calling\n\nThis method leverages the advanced 'tool mode' or function calling capability of the GPT-4o API to ensure syntactically valid output. A Pydantic schema defining the desired MCQ structure (question, options, correct answer, explanation) is passed to the model. The prompt is generic, simply requesting a question on the topic of 'Form 1 Mathematics, Rational Numbers.' The model is instructed to use the provided schema, which guarantees the output is a well-formed JSON object, eliminating the need for post-processing or error handling of the output structure.", "3.3.2 Method 2: Basic Prompting with JSON Parsing\n\nThis method serves as a simpler, non-grounded baseline. The prompt explicitly instructs the model to generate an MCQ on the same topic and to format the response as a JSON object. Unlike Method 1, this approach does not guarantee a valid JSON structure. Therefore, the pipeline includes a parsing step with error handling to manage potentially malformed or incomplete outputs. This method represents a common, straightforward approach to structured data generation with LLMs.", "3.3.3 Method 3: Framework-based RAG with LangChain and FAISS\n\nThis method represents a standard, off-the-shelf implementation of the RAG pattern, designed to ground the model's generation in the provided knowledge source. The pipeline is orchestrated using the LangChain framework.[23] The process is as follows:\n\nIngestion: The Nota Bab 1.pdf is loaded using LangChain's PyPDFLoader.", "Chunking: The document text is split into smaller, overlapping chunks using the RecursiveCharacterTextSplitter.\n\nEmbedding: Each text chunk is converted into a high-dimensional vector using OpenAI's embedding models.", "Storage & Retrieval: The vector embeddings are stored in a FAISS (Facebook AI Similarity Search) vector store, which allows for efficient similarity search.[24]\n\nGeneration: When a query is made, the system retrieves the most relevant text chunks from the FAISS store and passes them, along with the prompt, to GPT-4o via LangChain's RetrievalQA chain to generate the final MCQ.", "3.3.4 Method 4: Manual RAG with Custom Chunking and Similarity Ranking\n\nThis method was developed to test whether a more controlled, domain-aware RAG pipeline could outperform a generic framework-based approach. While frameworks like LangChain offer convenience, their generic components, such as text splitters, are agnostic to the logical structure of the source document. An educational text like Nota Bab 1 has a clear visual and semantic structure (e.g., headings, example boxes, exercise sections) that a simple character-based splitter might disrupt, potentially severing a mathematical example from its explanation and degrading the quality of the retrieved context. A manual approach allows for more intelligent chunking that respects this inherent structure. The implementation is as follows:\n\nIngestion & Chunking: The PyMuPDF library is used to parse the Nota Bab 1.pdf.[25] Custom logic is applied to group text blocks based on their position and font characteristics, aiming to keep semantically coherent units (like an entire 'Contoh' block) together in a single chunk.", "3.3.4 Method 4: Manual RAG with Custom Chunking and Similarity Ranking\n\nEmbedding: As in Method 3, OpenAI's embedding models are used to vectorize the custom chunks.\n\nRetrieval: A custom retrieval function is implemented using scikit-learn's cosine_similarity function to find the most relevant chunks for a given query by comparing query and chunk embeddings.[20]\n\nGeneration: The top-ranked chunks are formatted and passed as context to the GPT-4o API to generate the MCQ.", "3.4 The Evaluation Framework\n\nA dual evaluation framework was designed to automatically assess each generated question on two critical dimensions: curriculum alignment and contextual validity. This framework is depicted in Figure 2.", "Figure 2: Semantic Similarity Scoring and RAG-QA Validation Process INPUT: [Generated MCQ] [Generated MCQ] -> Path 1: \"Evaluation 1: STS for Curriculum Alignment\" \u00b7 MCQ is embedded and compared (cosine similarity) with pre-embedded chunks from RPT Bab 1.pdf. \u00b7 OUTPUT: Numerical \"Alignment Score\" [Generated MCQ] -> Path 2: \"Evaluation 2: RAG-QA for Contextual Validity\" \u00b7 MCQ is fed as a query to a RetrievalQA pipeline. \u00b7 Knowledge Base for QA is exclusively RPT Bab 1.pdf. \u00b7 OUTPUT: Binary \"Valid/Invalid\" label\n\nFigure 2: A textual representation of the dual evaluation framework as described in the source.", "3.4.1 Evaluation 1: Curriculum Alignment via Semantic Textual Similarity (STS)\n\nThis metric quantifies how closely a generated question aligns with the official learning standards. The RPT Bab 1.pdf is chunked, with each chunk corresponding to a specific learning standard (e.g., '1.2.1 Menambah dan menolak integer...'). Each RPT chunk is converted into a vector embedding. Each generated question is also converted into a vector embedding. The cosine similarity is calculated between the question's embedding and every RPT chunk's embedding. The maximum similarity score across all RPT chunks is recorded as the question's final alignment score. A higher score indicates a stronger semantic relationship to an official learning objective.", "3.4.2 Evaluation 2: Contextual Validity via RAG-based Question-Answering (QA)\n\nThis novel evaluation method provides a functional test of a question's relevance. The underlying principle is that a valid, curriculum-aligned question should be answerable using the curriculum's own definition of its scope. By using the sparse, high-level RPT document as the knowledge source, this becomes a stringent test. A question that can be answered using only the learning objectives must be highly relevant to one of those objectives. This creates a symbiotic validation loop where a RAG-like process is used to evaluate the output of another generative process. The implementation is as follows:\n\nA separate RetrievalQA pipeline is configured using LangChain.", "1 Introduction\n\nDocument understanding remains a critical challenge in natural language processing, particularly for complex documents containing heterogeneous content such as text, images, charts, tables, and technical diagrams. We address this heterogeneity through text-centric representations generated by multimodal LLMs rather than explicit spatial modeling. Consequently, tasks that hinge on absolute layout coordinates or correlations between multiple simultaneous images fall beyond our current investigation. Recent advances in multimodal large language models (LLMs) have shown impressive capabilities in processing diverse content types, yet most document retrieval systems fail to fully leverage these capabilities, instead relying on traditional pipelines with separate tools for OCR, table extraction, and image analysis.", "The fundamental challenge lies in the unstructured nature of most real-world documents. Unlike well-formatted\n\n* Correspondence to: taehoonkim@sogang.ac.kr\n\ntextbooks or research papers with explicit tables of contents, the majority of documents, from business reports to technical manuals, lack clear structural markers. Existing retrieval-augmented generation (RAG) systems typically employ naive chunking strategies that split documents into fixed-size segments, destroying semantic coherence and leading to fragmented retrieval results. Moreover, traditional approaches require extensive training on document-specific datasets, making them impractical for the diverse document types encountered in real applications.", "We present DocsRay, a training-free document understanding system that demonstrates how the synergistic integration of existing techniques can create a novel, practical solution exceeding the performance of its individual components. While pseudo-TOC generation, multimodal processing, and hierarchical retrieval are individually known, our contribution lies in their careful orchestration into an endto-end system that requires zero training data or task-specific fine-tuning. This training-free nature is crucial for practical deployment, as it enables immediate application to diverse document types without the resource-intensive data collection and model training required by existing approaches.", "Our core innovation is a prompt-based pseudo Table of Contents (TOC) generation algorithm that transforms unstructured documents into intelligently organized hierarchies. Unlike traditional segmentation methods that rely on formatting cues, fixed windows, or trained models, as shown in Table 1, our approach leverages an LLM's inherent semantic understanding through carefully designed prompts. This zero-shot structuring requires only two prompts: one for boundary detection and another for title generation. Despite its simplicity, the method achieves document organizations comparable to those produced by human annotators, all without any need for model training.", "DocsRay's strength lies in the synergy of three components: (1) prompt-based pseudo-TOC generation that semantically structures documents without training; (2) zeroshot multimodal understanding that unifies diverse content through LLM-native processing; and (3) hierarchical retrieval leveraging the generated structure for efficient access. Together, these components enable accurate understanding and fast retrieval in a fully training-free system.", "This design improves both accuracy and efficiency. Hierarchical retrieval reduces computational complexity from\n\nByeong Hyun Yoon Taehoon Kim *", "O ( N ) to O ( S + k 1 \u00b7 N s ) , where S \u226a N , while semantic structuring enhances precision by preserving topical coherence. Crucially, the system requires no training, which allows immediate deployment across diverse languages, domains, and document types. This is a significant advantage in scenarios where training data is scarce or unavailable. DocsRay-Pro achieves 64.7% accuracy on the MMLongBench-Doc (Ma et al. 2024) benchmark, marking a 15.0 percentage point improvement over the strongest previously reported baseline, further validating the efficacy of this training-free, structured retrieval approach.", "3 Methodology:\n\n3.1 K 2 RAG Framework\n\n4. Lightweight Models: For VRAM resources efficiency, K 2 RAG leverages a lightweight Longformer-based text summarizer [29], along with quantized LLMs to produce a low VRAM and resource-efficient pipeline without compromising output quality.", "3.1.1 Corpus Summarization Process\n\nTo address research goal 1, we decrease training times of the K 2 RAG framework compared to naive RAG frameworks by summarizing the training corpus to reduce its size while preserving as much information as possible. Hence a Longformer based model [29] fine tuned for long text summarization is used as it's self-attention mechanism is highly optimized for capturing and processing large chunks of texts compared to those seen in traditional transformer models. This helps speed up training times of information store components used in K 2 RAG . To create our summarized corpus we iterate over each article in the corpus and then summarize it using the loaded in Longformer summarizer model. Then these results are stored into a CSV format from which then they can be loaded in for training the RAG pipeline [Algorithm 1] [Figure 6].", "3.1.2 Indexing\n\nThe chunking strategy to generate a set of chunks C d from a single document or text d is defined as:\n\nFigure 6: How to generate summaries of corpus.\n\nSummarise Document Load corpus For each Longformer Encoder Decoder Model Store into list Repeat Export to CSV", "where S is the desired chunk size, O is the desired chunk overlap, Nt d is the total number of tokens in the document d . The inner set denotes a chunk constructed from tokens x (( i -1) \u2217 ( S -O ))+1 to x (( i -1) \u2217 ( S -O ))+ S and L t is the limit of tokens to process to ensure the number of chunks of size S and overlap O are maximized.", "We then apply the chunking strategy defined in (1) across all documents in the summarized corpus cor sum and take the union of chunks to give us the final set of chunks C to pass to the knowledge graph and vector stores for indexing:\n\nThe configuration used to train all information retrievers is S = 256 , O = 20 when chunking for the dense and sparse vector stores indexing, and S = 300 , O = 100 when chunking for knowledge graph indexing.", "This was decided in accordance with Xiaohua Wang et al.'s paper for vector stores indexing which showed 512 and 256 token chunks achieved the best results in Faithfulness and Relevancy metrics [9] [Figure 7]. Using the chunk size less than 512 is used to help promote in smaller contexts generated to pass to the generator LLM to address the \"Needle-in-Haystack\" challenge [8] by ensuring enough information is contained within the chunks retrieved and minimising noisy information.", "| | Chunk Size | lyft_2021.Average Faithfulness | lyft_2021.Average Relevancy |\n|---:|-------------:|---------------------------------:|------------------------------:|\n| 0 | 2048 | 80.37 | 91.11 |\n| 1 | 1024 | 94.26 | 95.56 |\n| 2 | 512 | 97.59 | 97.41 |\n| 3 | 256 | 97.22 | 97.78 |\n| 4 | 128 | 95.74 | 97.22 |\n\nFigure 7: Chunk sizes and their effects on Answer Faithfulness and Answer Relevancy.", "3.1.3 Retrieval and Generation\n\nFor all pipelines the following system prompt was used for generation of the final answer:", "| | 0 |\n|---:|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 0 | Answer Generation System Prompt |\n| 1 | Additional Information: context from retrieval Instruction: You are a smart LLM who gives an answer to the question in as little words as possible using the additional information provided above. Question: question Short Answer: |\n\nWe establish the top k chunks C q retrieved from the dense and sparse vector databases for a question q is a hybrid retriever [Figure 8 (b)] defined as:\n\nwhere S is the set of scores computed for each chunk in C (2) using a scoring function scr of the question q , chunk c and a weighting parameter \u03bb :", "Where e represents the dense embeddings model as a function and z represents the sparse embedding function.", "The first step in the K 2 RAG pipeline [Figure 8] is to use a knowledge graph trained on summarized data to generate an answer containing the identified topics relevant to the query [Figure 8(A)]. Then a Longformer model is used to summarize the results obtained from the knowledge graph [Figure 8(B)]. Over the knowledge graph results summary, we split it into chunks of S = 128 and O = 10 following our text chunking strategy (1) . This is so that each chunk contains information about a clear topic needed to help answer the question from which for each chunk we use a quantized LLM to create a sub-question out of it [Figure 8(C)] with the following specialized prompt:"]}
{"type": "entry", "index": 39, "question": "how should i chunk tables/figures/captions so retrieval doesn\u2019t ignore them", "answer": "Use chunking that preserves the meaning of each table/figure unit, instead of blindly slicing by fixed size.\n\n1) Avoid naive fixed-size chunks\nDocsRay shows that naive, fixed-window chunking \u201cdestro[y] semantic coherence and lead[s] to fragmented retrieval results.\u201d[2] If you slice pages every N tokens, a caption may end up in one chunk, the table in another, and the explanatory text in a third\u2014so retrieval will often miss the full picture.\n\n2) Treat each table/figure + caption (+ nearby text) as one semantic chunk\nSince DocsRay\u2019s gain comes from keeping semantically coherent sections together via pseudo-TOC segmentation[2], you can mirror that idea locally:\n- When you detect a figure or table, group its:\n  - Caption\n  - The visual element (or its text-centric representation)\n  - The immediate explanatory paragraph(s)\ninto a single chunk. This makes it much more likely that a query about that figure or table retrieves everything relevant at once.\n\n3) Convert non-text elements into text-centric representations\nDocsRay handles heterogeneous content (images, charts, tables, diagrams) through \u201ctext-centric representations generated by multimodal LLMs\u201d instead of raw layout features[2]. For retrieval, do the same:\n- Ask a multimodal model to summarize a table/figure in natural language.\n- Store that summary in the same chunk as the caption and nearby text.\n\n4) Use document structure to scope chunk sizes\nDocsRay\u2019s pseudo-TOC creates a hierarchy of semantically coherent sections and then does hierarchical retrieval over that structure[2]. Even if you don\u2019t build a full pseudo-TOC, you can:\n- Use headings and section breaks to define larger chunks.\n- Within each section, keep each table/figure block intact, only splitting if the chunk becomes extremely long.\n\nIn short: keep each table/figure, its caption, and its local explanation together, and represent the visual content in text so the retriever can see it.", "tags": ["_untagged"], "full_faithfulness": 1.0, "full_answer_relevancy": 0.9239836264, "seed_faithfulness": 0.6666666667000001, "seed_answer_relevancy": 0.9239836264, "context_relevance": 0.160463339004293, "context_relevance_error": null, "context_relevance_reason": "The context provided does not directly address the user's question about chunking tables, figures, and captions for retrieval. While it discusses multilingual image captioning and retrieval-augmented generation, it lacks specific information or strategies related to the chunking of tables and figures, which is the main concern of the user. The relevance of the context to the user's question is minimal, leading to a low score.", "structure_citation": 0.6448308909048366, "structure_error": null, "structure_reason": "The Actual Output effectively includes claims with [n] markers, specifically referencing the context with citations. However, while the citations are present, the consistency of the snippets with the claims made could be improved, as some claims about chunking strategies and their implications are not fully supported by the context provided. Overall, the response demonstrates a strong understanding of the topic and aligns well with the evaluation steps, but there are minor shortcomings in the depth of citation support.", "hallucination_score": 0.7298192451309407, "hallucination_error": null, "hallucination_reason": "The response provides a clear and relevant approach to chunking tables and figures for effective retrieval, aligning well with the input question. It emphasizes the importance of semantic coherence and offers practical strategies, such as treating tables and captions as single chunks and converting non-text elements into text-centric representations. However, while the claims are generally supported by the context, there is a lack of direct references to specific examples or studies from the context that could further strengthen the factual basis of the claims made.", "contexts_used": ["1 Introduction\n\nVision Language Models (VLMs) have achieved increasingly strong performance in image captioning, driven by the scaling of model size and the use of extensive training data (Liu et al., 2023; 2024; Tschannen et al., 2025). Although VLMs demonstrate strong performance in image captioning, most remain English-centric, lacking the ability to generate highquality captions for other languages. Growing efforts aim to build high-quality multilingual datasets for training and evaluation and develop multilingual VLMs on these (Geigle et al., 2024; Yue et al., 2025). However, even with massive, costly multilingual training and high parameterization to accommodate multiple languages, recent models continue to exhibit substantial performance gaps between English and other languages (Yue et al., 2025).", "To alleviate the need for expensive multilingual training, recent work has explored retrievalaugmented generation (RAG) as a promising direction (Ramos et al., 2023a; 2024). In this approach, models are conditioned not just on the image, but also on a set of retrieved captions in the target language. By leveraging these retrieved examples, the model requires far less multilingual training data, as it benefits from demonstrations of how to generate captions across different languages. However, a key limitation of multilingual retrievalaugmented captioning models is that retrieved captions often come from translated datasets (Thapliyal et al., 2022) and datasets sourced from Western-centric image repositories (Chen et al., 2015), which may not fully capture the nuances of the target language and culture. Furthermore, the retrieved captions come from images similar to the input image, but almost certainly differ from it in some aspects, which inherently introduces noise through the retrieval augmentation.", "In this work, we introduce CONCAP, a multilingual image captioning model that integrates retrieved captions and image-specific concepts . CONCAP builds upon the mBLIP architecture (Geigle et al., 2024), augmenting generation with retrieved information from captions of sim-", "ilar images and concepts relevant to the input image. This enriches the captioning process with more informative and precise context and improves the quality of generated captions. Finetuning CONCAP on just 0.6M multilingual image-caption pairs yields considerably better results on low- and mid-resource languages in the XM3600 multilingual captioning benchmark (Thapliyal et al., 2022), compared to the original mBLIP model trained on 4M samples. Despite its lightweight and data-efficient training setup, CONCAP achieves stronger multilingual capabilities on average, even compared to Pangea, a state-of-the-art multilingual VLM with 7B parameters trained on 6M data points (Yue et al., 2025). Through ablation studies of the caption and concept retrieval augmentations, we show that each component makes a valuable contribution, and the two complement each other.", "2 Related work\n2.1 Image Captioning\n\nImage captioning has seen significant progress in recent years, driven by large-scale pretraining and powerful vision-and-language models (VLMs) that leverage off-the-shelf unimodal components such as CLIP encoders and language decoders (Li et al., 2023; Alayrac et al., 2022; Mokady et al., 2021; Panagopoulou et al., 2023). Very recently, captioning performance has been further enhanced by the rise of large language models (LLMs) (OpenAI, 2024; Touvron et al., 2023; Taori et al., 2023; Chiang et al., 2023) and visual instruction-tuned data (Liu et al., 2023; Laurenc \u00b8on et al., 2024). These models generally follow similar architectural trends. The unimodal vision and language components, typically kept frozen, are connected either through a cross-attention mechanism (e.g., Flamingo; (Alayrac et al., 2022)), trainable Q-former networks (e.g., BLIP2; (Li et al., 2023)), or a fully autoregressive architecture, where the output of the vision encoder is directly mapped to a sequence of text embeddings (e.g., LLaVA; (Liu et al., 2023)).", "Despite progress in captioning quality, most models remain English-centric, while multilingual image captioning has been largely overlooked. Notable multilingual efforts include Thapliyal et al. (2022) that created XM3600, an established human-annotated captioning benchmark in 36 languages. Thapliyal et al. (2022) also proposed COCO-35 and CC3M-35L, translated versions of the standard COCO (Chen et al., 2015) and Conceptual Captions (Sharma et al., 2018) datasets in 35 languages, aimed at training multilingual captioning models. Using these datasets, they introduced BB+CC, the first large-scale multilingual image captioning model trained from scratch. Following this, mBLIP, based on BLIP2 (Li et al., 2023), introduced a more efficient multilingual vision and language model, employing the CLIP encoder (Radford et al., 2021) and multilingual LLMs. Despite having orders of magnitude fewer parameters, mBLIP is also trained on a mixture of massive machine-translated multilingual data. Very recently, Yue et al. (2025) developed PangeaIns, a culturally-aware instruction dataset with 6M samples across 39 languages, and Pangea, a multilingual multimodal large-scale model designed to bridge language and cultural gaps. However, despite extensive multilingual training, these models still demonstrate a bias toward English captioning, with substantially lower performance for other languages. In contrast, we propose a method that effectively bridges the gap between languages and bypasses the need for massive training data by leveraging retrieval.", "2.2 Retrieval-augmented Image Captioning\n\nRetrieval-augmented generation (RAG) methods provide additional context to the model by incorporating information retrieved from an external datastore (Lewis et al., 2020). RAG models have demonstrated improved performance not only across various NLP tasks (Fan et al., 2024; Huang & Huang, 2024) but also in vision-and-language tasks (Yasunaga et al., 2022), such as image captioning Li et al. (2024b). Closely related to our work, Ramos et al. (2023b) proposes SmallCap, showing that augmenting an image captioning model with retrieved captions not only improves captioning performance but also reduces the number of trainable parameters and facilitates adaptation to out-of-domain settings. In contrast, Li et al. (2024a) introduces the EVCap model, which incorporates retrieved concepts (e.g., object", "Figure 1: (Left) CONCAP architecture combining visual features with retrieved captions and concepts via Q-Former and LLM. (Right) Multilingual image-text retrieval using cosine similarity with prefixed concept prompts and cross-lingual caption mapping.\n\nEin Hund mit Braunem\n\nFell und Locken l\u00e4uft\n\nmit seinem Besitzer auf\n\neiner gr\u00fcnen Wiese\n\nSimilar images show:\n\na beautiful view of a lake at the base of a mountain,\n\na view of a lake and a mountain on a sunny day,\n\na cross country skier high in the hills above a lake,\n\na large body of water surrounded by a mountain range.\n\nThis image might contain:\n\nlakes, lake, carpathian, hikes, crater, zealand, reservoir,\n\nlagoon, trekking, treks.\n\nCaption in English:\n\nRetrieved\nCaptions\nConcepts\n\nmBLIP:\n\na waterfall on top of a\n\nhillside.\n\nConCap:\n\na lake surrounded by\n\nmountains and a rocky hillside.\n\nImage\nEncoder\n\nPatio com\u00fan de edificio\n\nde departamentos\nsencillo, puerta de\nmadera al final\n\nSandwich on a white\n\nplate served with a\n\nsmall bowl of shredded\n\ncabbage.\n\nText Ein Hund mit Braune\n\n...\n\nPatio com\u00fan de edificio Sandwich on Plate served + Prefix\n\nIn this image there is a\n\nNearest Neighbor Captions\n\na plate with a sandwich\n\na meal consisting of a couple of wrap sandwiches and some\n\nvegetables\n\na white plate topped with two sandwiches and a salad\n\na plate with two sandwiches and a side\n\nNearest Neighbor Concepts\n\nSandwiches, hotdogs, toppings, bites, platter, serving, meal,\n\ntasty, lunch, coleslaw.\n\nQ-Former\nProjection", "LLM LORA Cos( \u03b8 )\n\nnames) instead of full captions to avoid redundancy and mitigate misleading information in the retrieved text. Zeng et al. (2024) also explores augmenting image captioning with key concepts related to the image, but in a zero-shot setup.", "While RAG has gained traction in image captioning, its use in multilingual image captioning has been limited, with only two models so far. Ramos et al. (2023a) explored retrievalaugmented generation in a training-free setup using in-context learning. Ramos et al. (2024) introduced PAELLA, a supervised multilingual retrieval-augmented image captioning model that is not only parameter-efficient, like SmallCap, but also data-efficient through the incorporation of retrieval augmentation. It is important to note that both of these models rely on the standard approach of using retrieved captions. However, concept-based augmentation, as seen in (Li et al., 2024a; Zeng et al., 2024), has yet to be explored in a multilingual context. Moreover, unlike previous works, we explore the integration of both retrieved captions and concept retrieval to enhance multilingual caption generation.", "3 Proposed Approach\n\nCONCAP is an efficient multilingual retrieval-augmented image captioning model that combines retrieved captions and retrieved concepts to generate more accurate and contextually grounded captions across languages. An overview of the approach is shown in Figure 1.", "3.1 Architecture\n\nThe method proposed here is applicable to any vision-language modeling architecture, as it only modifies an input-specific prompt passed to the language decoder. In our experiments, we concretely adopt the mBLIP architecture (Geigle et al., 2024), one of the established state-of-the-art models for multilingual image captioning, while being parameter-efficient.", "3.2 Caption Retrieval\n\nCONCAP builds on standard retrieval strategies (Ramos et al., 2023b; 2024), which use vision-language representation models (Radford et al., 2021; Zhai et al., 2023) to align images and text in a shared representation space. Given a corpus of image captions in a specific language, the text encoder of a CLIP-style model is used to pre-compute and store the caption embeddings in a datastore. This indexed datastore is then queried using the", "image encoder of the same model, with cosine similarity computed to find the most relevant captions based on the image embeddings.", "CONCAP enhances caption generation by incorporating the topn most similar captions to the input image, guiding the language decoder towards generating captions in the target language with syntactic and semantic relevance to the intended prediction. However, we note that these retrieved captions come from similar images and may not fully capture the content of the input image, potentially introducing noise or mismatches in terms. For example, while a caption might correctly describe a bus in the street, it could mistakenly specify a 'red bus,' which may not be accurate for the input image.", "3.3 Concept Retrieval\n\nConcept retrieval follows a similar process to caption retrieval, but instead of full sentences, it concerns individual lexical items. To contextualize them and enhance their semantic representation, each concept is wrapped in a short, language-specific template (e.g., 'a photo of a dog '). We used GPT-4o to generate the templates (OpenAI, 2024), then verified them using Google Translate. This latter step was particularly important for morphologically rich and syntactically distinct languages, where direct translations may not always yield fluent or semantically accurate prompts. A complete list of language-specific templates is provided in Figure 9. While concepts are wrapped in a template for retrieval, they are inserted into the final prompt in raw form (e.g., ' dog ').", "Concept retrieval is performed analogously to caption retrieval, taking the topm most relevant concepts per image. The intuition here is that augmenting the captioning process with highly relevant retrieved concepts will counteract noise from the retrieved captions and fill in possible gaps in the concept coverage of retrieved captions.", "3.4 Prompt Format\n\nThe final prompt for the language decoder of CONCAP combines both retrieved captions and concepts to provide informative and semantically relevant context for caption generation. The prompt is organized into three segments:\n\nSimilar images show: caption 1, caption 2, ..., caption n. 2. This image might contain: concept 1, concept 2, ..., concept m.\n\nCaption in { lang } :\n\nThe first segment presents n captions retrieved from visually similar images, providing sentence-level context. The second segment includes m concepts retrieved from languagespecific wordlists. Together, these components guide the model toward producing accurate and semantically grounded captions in the target language. The language of the prompt is fixed to English, and the full name of the target language is used (e.g., 'Caption in Chinese'.)\n\nDuring training, the language decoder receives the full prompt as input and generates a caption conditioned on the image features V and the task prompt X . The model is optimized using teacher forcing, minimizing the cross-entropy loss over the reference caption tokens.", "6 Conclusion\n\nWe introduced CONCAP, a multilingual image captioning model that enhances caption generation by integrating retrieved captions with image-specific concepts. This approach improves caption quality while reducing the need for extensive multilingual training. Experiments on XM3600 show that CONCAP outperforms mBLIP and Pangea despite using fewer training resources. Ablation studies confirm the additive benefits of caption and concept retrieval. Further analysis informs the choice of retriever and highlights the brittleness of standard caption retrieval strategies. Concepts retrieved from enriched lexicons prove to be noisier and to hinder the accurate generation of image captions. Oracle experiments with cultural concepts show that this form of retrieval augmentation improves captioning initially, but the benefits fade as training progresses: the model increasingly depends on its learned vocabulary, limiting integration of new cultural concepts. Future work should investigate more effective ways for concept enrichment and consider targeting datasets with more pronounced cultural representation, where well-informed concept enrichment may prove more effective.", "1 Introduction\n\nDocument understanding remains a critical challenge in natural language processing, particularly for complex documents containing heterogeneous content such as text, images, charts, tables, and technical diagrams. We address this heterogeneity through text-centric representations generated by multimodal LLMs rather than explicit spatial modeling. Consequently, tasks that hinge on absolute layout coordinates or correlations between multiple simultaneous images fall beyond our current investigation. Recent advances in multimodal large language models (LLMs) have shown impressive capabilities in processing diverse content types, yet most document retrieval systems fail to fully leverage these capabilities, instead relying on traditional pipelines with separate tools for OCR, table extraction, and image analysis.", "The fundamental challenge lies in the unstructured nature of most real-world documents. Unlike well-formatted\n\n* Correspondence to: taehoonkim@sogang.ac.kr\n\ntextbooks or research papers with explicit tables of contents, the majority of documents, from business reports to technical manuals, lack clear structural markers. Existing retrieval-augmented generation (RAG) systems typically employ naive chunking strategies that split documents into fixed-size segments, destroying semantic coherence and leading to fragmented retrieval results. Moreover, traditional approaches require extensive training on document-specific datasets, making them impractical for the diverse document types encountered in real applications.", "We present DocsRay, a training-free document understanding system that demonstrates how the synergistic integration of existing techniques can create a novel, practical solution exceeding the performance of its individual components. While pseudo-TOC generation, multimodal processing, and hierarchical retrieval are individually known, our contribution lies in their careful orchestration into an endto-end system that requires zero training data or task-specific fine-tuning. This training-free nature is crucial for practical deployment, as it enables immediate application to diverse document types without the resource-intensive data collection and model training required by existing approaches.", "Our core innovation is a prompt-based pseudo Table of Contents (TOC) generation algorithm that transforms unstructured documents into intelligently organized hierarchies. Unlike traditional segmentation methods that rely on formatting cues, fixed windows, or trained models, as shown in Table 1, our approach leverages an LLM's inherent semantic understanding through carefully designed prompts. This zero-shot structuring requires only two prompts: one for boundary detection and another for title generation. Despite its simplicity, the method achieves document organizations comparable to those produced by human annotators, all without any need for model training.", "DocsRay's strength lies in the synergy of three components: (1) prompt-based pseudo-TOC generation that semantically structures documents without training; (2) zeroshot multimodal understanding that unifies diverse content through LLM-native processing; and (3) hierarchical retrieval leveraging the generated structure for efficient access. Together, these components enable accurate understanding and fast retrieval in a fully training-free system.", "This design improves both accuracy and efficiency. Hierarchical retrieval reduces computational complexity from\n\nByeong Hyun Yoon Taehoon Kim *", "O ( N ) to O ( S + k 1 \u00b7 N s ) , where S \u226a N , while semantic structuring enhances precision by preserving topical coherence. Crucially, the system requires no training, which allows immediate deployment across diverse languages, domains, and document types. This is a significant advantage in scenarios where training data is scarce or unavailable. DocsRay-Pro achieves 64.7% accuracy on the MMLongBench-Doc (Ma et al. 2024) benchmark, marking a 15.0 percentage point improvement over the strongest previously reported baseline, further validating the efficacy of this training-free, structured retrieval approach."]}
{"type": "entry", "index": 40, "question": "should i retrieve whole papers first or just chunks, what actually works better", "answer": "Based on these studies, retrieving chunks with a smart structure usually works better than pulling in whole papers at once.\n\nWhen one team tried giving GPT-4o entire regulatory documents directly in the prompt (\"full documents without retrieval\"), accuracy barely improved. In contrast, using retrieval to select relevant pieces of text (RAG) significantly boosted accuracy for all models tested [1]. This suggests that simply dumping whole papers into the model is inefficient and not very helpful.\n\nFor retrieval quality itself, they found that medium\u2013large chunks (around 500\u2013700 words) combined with retrieving several of them (e.g., top\u20117) gave the best results, reaching about 97% retrieval accuracy. Much smaller chunks or fewer retrieved pieces performed notably worse, because they missed necessary context [1]. So you generally want reasonably big chunks, not tiny snippets, and more than just 1\u20132 of them.\n\nAnother line of work shows that a targeted retrieval strategy\u2014selecting relevant chunks\u2014can perform nearly as well as feeding up to 128k tokens of raw clinical notes, while using far fewer tokens [5]. This again favors selective chunk retrieval over giving the model everything.\n\nFinally, adding a hierarchical structure (like a pseudo table of contents) lets the system narrow down the search to the most relevant sections instead of sifting through thousands of chunks, helping even smaller models answer factual questions well [3].\n\nSo in practice, it\u2019s usually best to:\n- Index documents as structured, medium\u2013large chunks.\n- Use targeted or hierarchical retrieval to pick relevant chunks.\n- Avoid retrieving entire papers unless you truly need full-document reasoning and can afford the cost.\n", "tags": ["_untagged"], "full_faithfulness": 1.0, "full_answer_relevancy": 0.7974018961, "seed_faithfulness": 0.8666666667, "seed_answer_relevancy": 0.7940334234, "context_relevance": 0.47163665826906237, "context_relevance_error": null, "context_relevance_reason": "The context passages provide some relevant information about retrieval methods and their effectiveness, but they do not directly address the user's question about whether to retrieve whole papers or just chunks. While there are mentions of chunk sizes and retrieval performance, the overall relevance to the user's specific query is limited, leading to a low score.", "structure_citation": 0.8035709873304695, "structure_error": null, "structure_reason": "The Actual Output effectively discusses the advantages of retrieving chunks over whole papers, aligning well with the evaluation steps. It includes claims supported by citations, specifically referencing studies that demonstrate improved accuracy with chunk retrieval [1] and [5]. However, while the output mentions multiple studies, it could benefit from clearer connections to specific snippets in the Context, particularly regarding the details of the experiments and results. Overall, the response is strong but lacks some explicit matching of claims to the provided Context.", "hallucination_score": 0.8926535476971086, "hallucination_error": null, "hallucination_reason": "The Actual Output effectively summarizes the key claims from the Input regarding the advantages of retrieving chunks over whole papers, supported by specific studies and data from the Context. It accurately reflects the findings that structured retrieval improves accuracy and provides practical recommendations. However, it could have explicitly mentioned the limitations of using full documents, which would strengthen the alignment with the evaluation steps.", "contexts_used": ["5 Results and Impact\n\n5.1 Setup\n\nAll experiments were run on a Slurm-managed HPC cluster using a single NVIDIA A100 GPU and 200GB of host memory per job. Retrieval used FAISS-GPU with sentence embeddings from the all-MiniLM-L6-v2 model, indexed over our corpus. Generation used open-source models locally and GPT-4o for comparison with one of the most popular models; the reranker was disabled by default and enabled only in the ablation. We report mean accuracy with standard error across runs, fixing random seeds for repeatability.", "5.2 The RAG Results\n\nOur experiments presented in Fig. 4 demonstrate that integrating RAG significantly improves model accuracy across all tested LLMs. Notably, GPT-4o [10] exhibited a +11.9% improvement, suggesting that even sophisticated commercial models significantly benefit from structured retrieval of regulatory contexts. DeepSeek-R1-Distill-Qwen-14B [6] exhibited the largest absolute improvement, +23%. Smaller models like DeepSeek-R1-Distill-Qwen-1.5B [6] showed modest but notable gains of +3%, indicating that retrieval contexts help models of all scales, albeit differently. The figure 5 shows an example of how RAG efficiently helped GPT-4o to get the correct answer. More examples are provided in the Appendix, Section B.", "Interestingly, as shown in Table 1, directly uploading regulatory documents into the prompt, namely GPT-4o + full documents without retrieval, barely improved accuracy, underscoring that structured retrieval is crucial.", "5.3 Retrieval-Only Results\n\nTable 4 summarizes results evaluating exclusively the retrieval component, isolating its performance from downstream generation tasks. The table reports retrieval accuracy across various configurations from a comprehensive hyperparameter sweep. Accuracy was measured using a ROUGE-L F 1 similarity threshold set at 0.7. We observe that configurations with chunk sizes of 500 and 700 words and higher values of topk consistently achieve superior retrieval performance, notably 97% accuracy for the configuration with 700-word chunks and top-7 retrieval. Configurations with smaller chunks, 150 and 300 words, or fewer retrieved contexts, topk = 3, yielded significantly lower accuracy, demonstrating the importance of sufficient context for capturing regulatory information.", "Figure 5: Qualitative comparison of vanilla GPT-4o versus our RAG-augmented approach on a regulatory question, where RAG retrieves the rule and yields the correct answer.\n\nPrompt\n\nQuestion:\n\nWhat is the maximum power flux-density produced at the geostationary-satellite\n\norbit by any Earth exploration- satellite service space station?\n\nGPT-4o (vanilla)\n\nAnswer:\n\n-168 dB(W/m\n\n) in a 4 kHz\n\nband\n\u2717\n\nExplanation:\n\nAccording to the ITU Radio\n\nRegulations (specifically Article 21.16.6),\n\nthe maximum power flux-density (PFD)\n\nproduced at the geostationary-satellite orbit\n\n(GSO) by any Earth exploration-satellite\n\nservice (EESS) space station is - dB(W/m\n\n) in any 4 kHz band.\n\nThis is to\n\nprotect geostationary satellites from harm-\n\nful interference from non-geostationary\n\nsatellites transmitting in the direction of\n\nthe GSO.\n\nGPT-4o with RAG context\n\n) in any 4kHz band \u2713\n\nFrom the provided context:\n\n'. . . the\n\nmaximum power flux-density produced at the geostationary-satellite orbit by Earth exploration-satellite shall not exceed\n\n) in any 4 kHz band.'\n\nThis directly matches the value. The nega-\n\ntive sign is correct-PFD values are often\n\nexpressed in negative dB(W/m\n\n) because\n\nthey are very small.", "Limitations\n\nWe acknowledge several limitations in our work.", "Input Limitation. For copyright reasons, our approach is restricted to using only abstracts and bibliographic metadata of the retrieved papers, without access to full-text content. This limitation may hinder the LLM's ability to capture finer-grained details and structural elements that are often present in full-length papers. Hence, the generated surveys may lack depth and completeness compared to human-written surveys that draw on the entire papers.", "Post-generation Refinement. To reduce API call costs, we did not perform post-generation refinement to the LLM outputs, such as language polishing, citation formatting, or structural adjustments. These post-processing steps could further improve the personalization and overall quality of the generated surveys. Also, our work focuses on generating textual survey content and does not include visual elements such as figures, tables, or diagrams, which are often present in published scientific surveys. Lastly, for longer surveys, models like Claude-3Haiku 10 may offer superior performance due to their extended context handling capabilities.", "Data Contamination. We acknowledge the possibility of data contamination, as some surveys or key references (\u00a73.1) used are open access and may have been included in the training data of the LLMs, potentially leading to slightly different performance estimates. Although we do not explicitly control for this factor in our evaluation process, such contamination is a general challenge in benchmarking LLMs on open-domain generation tasks (Xu et al., 2024). Moreover, since all baselines in our study are based on mainstream LLMs, any potential contamination would be shared and thus unlikely to impact the relative comparison.", "Evaluation Sample Scope. While our empirical evaluation focuses on a subset of 120 relatively short surveys spanning multiple disciplines-selected to balance cost and feasibility in academic settings -weexpect similar performance trends to hold across the full dataset. We encourage the broader research community to further benchmark models using our dataset and framework to extend our findings across broader contexts.\n\nhttps://www.anthropic.com/news/ claude-3-haiku", "F.4 Implications for Retrieval System Design\n\nThese case studies reveal several insights that should guide the design of future document retrieval systems intended for deployment across diverse computational environments and use cases.", "First, the universal effectiveness of hierarchical retrieval across all model scales represents a finding. Even our smallest 4B parameter model achieves reasonable performance on simple factual queries when equipped with our pseudoTOC based hierarchical retrieval system. This validates a key architectural decision: the structured document representation created by our pseudo-TOC generation helps compensate for limited model capacity by reducing the search space to relevant sections. Rather than requiring the model to search through potentially thousands of chunks, the hierarchical structure allows even resource-constrained models to focus their limited attention on the most promising document regions. This finding suggests that investing in better document structure inference may yield greater returns than simply scaling model size, for deployment scenarios with computational constraints.", "Second, our analysis reveals that evidence aggregation capabilities scale strongly with model parameters. Tasks requiring synthesis across multiple document pages show the clearest performance stratification between model tiers. The ability to maintain coherent tracking of evidence across extended contexts appears to be correlated with model parameters. For instance, when counting human quotes across a 40-page document, the Pro model tracks all five individuals with their precise page locations, while the Lite model overcounts to 17, suggesting it loses track of which quotes it has already counted. This pattern indicates that applications requiring document analysis (such as due diligence reviews, systematic literature surveys, or regulatory compliance checking) should prioritize larger models despite their increased computational cost.", "Third, visual understanding capabilities demonstrate an interesting graceful degradation pattern with model scale. While all models can identify that visual content exists within documents, the ability to accurately count, categorize, and interpret specific visual elements clearly correlates with model size. Our analysis of the hand-drawn cartoon counting task exemplifies this: the Pro model not only counts correctly but also provides semantic understanding of each cartoon's purpose, the Base model acknowledges its limitations, and the Lite model attempts counting but conflates different visual elements. This suggests a tiered deployment strategy where basic visual awareness tasks can be handled by smaller models, but applications requiring detailed visual analysis (such as technical diagram interpretation or chart data extraction) necessitate larger models.", "Fourth, we observe that conservative behavior in handling ambiguous or unanswerable questions emerges as a function of scale. Larger models demonstrate more nuanced responses, providing helpful context about available information rather than attempting to guess. When asked about 2024 voting data in a document containing only information through 2018, all models correctly identify the question as unanswerable, but larger models additionally provide context about what related information is available. This conservative behavior proves valuable for high-stakes applications in legal, medical, or financial domains where acknowledging uncertainty is preferable to confident errors that could lead to costly mistakes.", "Fifth, our edge cases reveal that model scaling does not guarantee monotonic improvement across all task types. Some specific capabilities may actually degrade or show non-linear patterns with scale. The chart counting task where smaller models outperformed the Pro model exemplifies this phenomenon. The larger model's more sophisticated understanding led it to apply stricter criteria for what constitutes a relevant chart, resulting in undercounting. This finding necessitates careful evaluation for particular use cases and suggests that blind trust in larger models may be misplaced for certain task types.", "These findings suggest that future document retrieval systems should adopt a more nuanced approach than maximizing model size. System designers should consider the specific types of questions and evidence patterns common in their target domains. A portfolio approach using different model scales for different query types may provide optimal cost-performance trade-offs. For instance, a production system might route simple factual queries to a Lite model for efficiency, escalate multi-page synthesis tasks to a Base model, and reserve the Pro model for cases requiring precise visual interpretation or complex reasoning. Such an adaptive routing strategy could significantly reduce computational costs while maintaining high accuracy for challenging queries.", "2 Related Works\n\nAddressing the limitations of naive Retrieval-Augmented Generation (RAG) pipelines has led to the integration of hybrid retrieval methods that combine sparse (keyword-based) and dense (semantic) retrievers.", "Spare and Dense Retriever RAG methods: Hybrid approaches, such as the \"Blended RAG\" framework proposed by Sawarkar et al. [12], demonstrate that combining multiple retrievers can improve the retrieval of relevant passages for clearly defined queries. This framework integrates six distinct retrievers-including traditional sparse methods (e.g., BM25) and dense semantic search techniques-selected based on their performance across benchmark datasets like Natural Questions (NQ) [13], TREC-COVID [14], SqUAD [15], and HotPotQA [16].", "A key strength of this hybrid search technique is its ability to return relevant passages, aligning well with Research Goal 3 for well-specified queries. However, these methods do not fully address Research Goal 3 when handling vaguer questions. In such cases, the hybrid approach can retrieve extraneous or unnecessary information, thus failing to eliminate the \"needle in a haystack\" problem. This shortcoming is evident in evaluations on the CoQA dataset [17], where retrieval accuracies ranged only from 45% to 49%. The reliance on document metadata during retrieval-a common feature in these systems-further complicates matters, as such metadata is often minimal in real-world scenarios (e.g., limited to document names and creation dates), thereby increasing both the creation time of the information store (contradicting Research Research Goal 1) and the risk of returning irrelevant passages (contradicting Research Research Goal 3) leading to longer contexts (contradicting Research Goal 2).", "Figure 3: Results of Priyanka Mandikal et al. on 2 example queries [18].\n\nQuery: Do fibroblasts from CF patients grow at a\n\nQuery: What is the prognosis for patients who have\n\nnormal rate?\n\nhad an episode of respiratory failure?\n\nMethod P@1 P@3 P@5 P@10 VSR 33.33 SPECTERZ Hybrid 66.67 Sample Retrieval This paper analyzes the cell cycle of cystic\n\nThe diagnosis, management and prognosis of 17 infants\n\nfibrosis-derived fibroblasts and statistically\n\nwith cystic fibrosis (CF) and severe respiratory disease\n\ndemonstrates their failure support normal\n\nwere reviewed for the\n\nperiod 1968 to 1972.\n\nThe clinical\n\npopulation doubling time in vitro and indicates that this\n\ncourse these infants was characterized by is due largely\n\nto a failure of the\n\ncystic fibrosis-derived\n\nfailure to thrive and\n\ncells to enter the cell cycle in normal numbers.\n\nWe have malnutrition Despite vigorous therapy including no explanation for significant difference antibiotics there was\n\n60% mortality:\n\nA delay in the\n\nbehavior of these cells from patients with cystic fibrosis\n\ndiagnosis of CF from the onset of respiratory symptoms\n\nvis-a-vis either disease control fibroblasts, with a\n\nmean of six weeks was considered an important\n\nsave to conclude that proceeds from an intrinsic\n\nfactor affecting survival. This data supports the need for\n\nproperty of the cells themselves.", "developing a reliable screen test for CF at birth.", "In contrast, recent work by Mandikal et al. [18] has advanced a hybrid retrieval model that integrates a sparse TF-IDF retriever with a dense retriever based on fine-tuned SPECTER2 embeddings [19]. Their approach employs a weighted scoring mechanism, controlled by a hyperparameter \u03bb , to balance the contributions of the two retrieval methods. Empirical evaluations on a medical database concerning Cystic Fibrosis indicate that while dense retrieval alone underperforms, incorporating approximately 20% of the sparse score (i.e., setting \u03bb = 0 . 2 for the sparse component) significantly improves document retrieval accuracies across different retrieval ranks [Figure 3]. Crucially, this method indexes documents solely based on their content, thereby avoiding the pitfalls of metadata dependency. This contributes to reducing information store creation times (Research Goal 1) and partially mitigates the retrieval of unnecessary information (Research Goal 3), although it does not entirely resolve the challenge of handling vaguer queries.", "In summary, while hybrid retrieval approaches represent a significant advancement by returning more relevant passages for well-defined queries complying with Research Goal 3. However they remain imperfect in filtering out unnecessary information for vaguer questions which can lead to longer contexts. This limitation underscores the need for further refinement to minimize the \"needle in a haystack\" problem in large-scale, real-world applications and make a pipeline which is compliant with Research Goal 2 as well.", "Reranker RAG methods: To address the limitation which Hybrid-Search methods have in adhering to Research Goal 2 for vaguer questions, Reranker methods have been developed to reduce the size and improve the quality of the final context.", "Figure 4: A high level view of what a RAG pipeline with a reranker looks like.\n\nIst Stage 2nd Stage Knowledge Embedding-based LLM-based Docs Corpus retrieval reranking\n\nUsually this is a process called 2-step retrieval where the first step involves retrieving documents such as through sparse or dense retriever techniques and then a second stage where an LLM based reranker comes into play to decide which of the documents retrieved are most relevant to the query and sorts them in order [Figure 4] before passing it to an LLM for generation.", "Prior to the formal conceptualization of Retrieval-Augmented Generation (RAG), information retrieval research explored the use of document rerankers. For example, Noguiera et al.'s 2020 study, Document Ranking with a Pretrained Sequence-to-Sequence Model [20], proposed employing a sequence-to-sequence model as a document reranker. This work evaluated multiple fine-tuned T5 models of varying sizes, as well as a fine-tuned BERT model, on documents initially retrieved using the sparse BM25 keyword method. The incorporation of these rerankers led to improvements in retrieval performance across the Robust04, Core17, and Core18 datasets. However, a critical limitation of this framework was its reliance on large models, leaving unresolved whether the observed performance gains were due to the reranking strategy or simply the use of more powerful models.", "Subsequent advancements in RAG have integrated BERT-based rerankers into new pipelines. Michael Glass et al.'s Re 2 G framework [21] embeds a BERT-based reranker-similar to that of Noguiera et al.-within a RAG system. This integration has shown notable improvements in metrics such as Recall (for 5 documents), overall recall, precision, and accuracy on several datasets, as indicated by performance on the KILT leaderboard. Nevertheless, a significant drawback of this approach is the necessity to host an additional model, which directly conflicts with Goal 4: alleviating the monetary and computational costs associated with deploying full-scale LLMs.", "9 Conclusion\n\nIn this work, we introduced three clinically relevant tasks designed to evaluate the effectiveness of retrieval-augmented generation across varying information demands in electronic health records. Each task was selected for its clinical relevance, reproducibility across health systems, and varying degrees of reasoning complexity.", "Our results demonstrate that a targeted retrieval approach can reach near parity with using up to 128K tokens of recent clinical notes on these three tasks, while requiring significantly fewer input tokens. These findings show RAG's continued value even as LLMs grow more capable of processing long sequences. Further tuning the retrieval approach (queries, embedding model, retrieving more than 60 chunks, etc.), may close the remaining gap.", "Future work should explore additional tasks that can be devised without extensive manual effort and informed by clinical workflows and documentation practices in order to provide a more robust assessment of models and retrieval methods over longitudinal EHR tasks."]}
{"type": "entry", "index": 41, "question": "when it fails, is it usually because it missed the right paper or the right paragraph", "answer": "Based on the studies you provided, failures are often triggered at the retrieval stage (missing or mis-ranking the right paper), but a large share also happens after the right document is in hand (focusing on or interpreting the wrong paragraph).\n\nOne RAG study explicitly splits failures into three types: retrieval failure, reasoning failure, and answer failure.[2] In their fine-grained analysis of wrong answers, about 30% were cases where the model pulled the wrong knowledge (wrong document/entity) and stopped too early, i.e., it effectively \u201cmissed the right paper.\u201d[2] Another 22% involved only partially relevant content plus premature stopping, which is a mix of weak retrieval and weak reasoning.[2]\n\nHowever, even when the retrieved content was sufficient, the model often still failed: it either didn\u2019t follow the evidence correctly, mis-reasoned from correct evidence, or produced a wrong final answer despite a sound chain of thought.[2] These are \u201cright paper, wrong paragraph/use of it\u201d kinds of errors.\n\nOther work in health RAG shows that once bad or misleading documents rise to the top ranks, models \u201crarely recover,\u201d underscoring retrieval as the primary point of vulnerability.[4] Document QA analyses similarly show errors when systems miss one relevant page or section among many, even if the rest of the document is well covered.[3][5]\n\nSo: many failures do start because the system never retrieves or surfaces the crucial document/section, but even with good retrieval, a substantial fraction of errors come from how the model selects, interprets, and reasons over specific passages within those documents.[2][3][4][5]", "tags": ["_untagged"], "full_faithfulness": 1.0, "full_answer_relevancy": 0.8072747338, "seed_faithfulness": 0.7222222222, "seed_answer_relevancy": 0.8072747338, "context_relevance": 0.2798763768067044, "context_relevance_error": null, "context_relevance_reason": "The context passages provide some insights into issues related to reasoning and retrieval failures in models, which could indirectly relate to the user's question about missing the right paper or paragraph. However, none of the passages directly address the specific question posed, leading to a low relevance score. The information is too general and does not specifically analyze the causes of failure in relation to the user's query.", "structure_citation": 0.3999121444311454, "structure_error": null, "structure_reason": "The Actual Output includes claims supported by citations, but there are issues with citation matching. The citations [2], [3], [4], and [5] are present, but the context provided does not contain corresponding snippets that directly support the claims made in the output. This leads to unsupported claims, particularly regarding the specific percentages and types of failures mentioned, which are not substantiated by the context. Therefore, while there is some alignment with the evaluation steps, the lack of proper citation support significantly impacts the score.", "hallucination_score": 0.8423169661128878, "hallucination_error": null, "hallucination_reason": "The Actual Output effectively supports the Input's claim about failures in retrieval and reasoning stages, providing detailed statistics and examples from studies. It accurately reflects the context of RAG systems and their vulnerabilities, particularly in missing crucial documents or misinterpreting paragraphs. However, while it covers the main points well, it could have been more concise in some areas, which slightly detracts from its overall clarity.", "contexts_used": ["GSM8K ( s : Mixtral 8x7B, J : GPT-4o)\n\n-No Issues Detected ( 78 . 4% )\n\n-Mathematical errors in calculations, including rounding and final steps. ( 13 . 2% )\n\n-Incorrect understanding of problem statements leading to flawed reasoning. ( 11 . 8% )\n\n-Failure to fully consider or correctly interpret all given information. ( 5 . 8% )\n\n-Incomplete answers due to missing necessary steps or calculations. ( 5 . 5% )\n\n-Logical errors despite clear reasoning. ( 4 . 3% )\n\n-Misunderstanding or incorrect application of mathematical concepts or methods. ( 3 . 3% )\n\n-Unnecessary complexity or inclusion of irrelevant details. ( 2 . 6% )\n\n-Incorrect handling of units or conversions. ( 0 . 6% )\n\n-Failure to verify or cross-check results. ( 0 . 2% )\n\nTable 1: Issues identified for Mixtral 8x7B over the GSM8K benchmark with a task-specific mode, sorted by decreasing frequency (shown in the parentheses).", "3 Prelimary Study\n\nAlthough R1-like RAG models demonstrate stronger reasoning capabilities than traditional RAG models in QA tasks, questions remain about the faithfulness of their reasoning processes, both in relation to the provided context and in how well their final answers reflect that reasoning. This section focuses on the following central question: In current reasoning-oriented RAG models tackling multi-hop tasks, what is the correlation between the predicted answers, the retrieved documents, and the generated reasoning text?", "| | Category Datasets. | 2Wiki.Corr. | 2Wiki.Incorr. | MuSiQue.Corr. | MuSiQue.Incorr. |\n|---:|:---------------------|--------------:|----------------:|----------------:|------------------:|\n| 0 | Overthinking | 13 | 25 | 13 | 31 |\n| 1 | Good thinking | 232 | 89 | 80 | 96 |\n| 2 | Underthinking | 16 | 125 | 0 | 280 |\n\nTable 1: Distribution of correct (Corr.) and incorrect (Incorr.) predictions across different reasoning categories on 2Wiki and MuSiQue datasets.", "3.1 Preliminary Analysis\n\nWe trained a reasoning-capable RAG model using the GRPO algorithm from DeepSeek-R1, following the data setup of R1-Searcher. We used Qwen-2.5-3B-Instruct with a local retriever and evaluated on in-domain 2Wikimultihopqa (2Wiki) and out-of-domain Musique test sets.", "In what follows, we will denote an example as E = ( Q,RD,A ) , where Q is the query, RD the combined reasoning steps and retrieved documents, and A the predicted or gold answer. Regarding reasoning behavior, we categorize the model's reasoning into three cases: overthinking (gold answer could be inferred before the final step), good thinking (gold answer emerges exactly at the final step), and underthinking (model stops without sufficient content). Following prior work (Joren et al., 2025), we use an LLM to assess whether Q and RD are sufficient to derive the gold answer, but unlike (Joren et al., 2025), we also provide the gold answer to reduce misjudgments. Details of fine-tuning setup and experimental analysis settings are all provided in Appendix J.", "We report sufficiency judgment results separately based on whether the model's prediction is correct. The results are shown in Table 1. We can observe that among correctly predicted examples, overthinking and underthinking account for very small proportions, while good thinking accounts for 86% in Musique and 89% in 2Wiki. This indicates that for correctly predicted examples, the model's reasoning process is generally just right and sufficient. Among incorrectly predicted examples, underthinking accounts for a large proportion. This is unsurprising, as the model did not retrieve relevant documents and thus lacked enough information to answer the question, so answering incorrectly is expected. Surprisingly, overthinking and good thinking together account for 48% in Musique and 31% in 2Wiki. This shows that the reasoning process was sufficient, but the model still failed to", "| | Context Sufficiency | Error type | Case | Ratio |\n|---:|:----------------------|:------------------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:--------|\n| 0 | Insufficient | Knowledge retrieved incorrect | Question: What is the place of birth of the founder of Leister Technologies? Model output: ... <search> founder of Leister Technologies </search> ... <think> Louis Leitz (founder of Esselte Leitz) ... therefore answer Ingersheim </think> <answer> Ingersheim </answer> Gold Answer: Wilhelmshaven | 30.2% |\n| 1 | Insufficient | Thinking incorrect | Question: Where was the place of burial of John De Ferrers, 1st Baron Ferrers of Chartley's father? Model output: ... <search> John De Ferrers, 1st Baron Ferrers of Chartley father's burial place </search> ... <think> Linked to Sir Humphrey Ferrers of Tamworth Castle and concluded burial at Tamworth Church </think> <answer> Tamworth Church </answer> Gold Answer: Staffordshire | 22.1% |\n| 2 | Sufficient | Thinking process mismatch evi- dence | Question: Who is Gaius Rubellius Blandus's father-in-law? Model output: ... <search> Gaius Rubellius Blandus father-in-law </search> ... <think> Mar- ried Julia (granddaughter of Tiberius) \u21d2 therefore father-in-law is Tiberius </think><answer> Tiberius </answer> Gold Answer: Drusus Julius Caesar | 16.4% |\n| 3 | Sufficient | Thinking correct but incorrect an- swer | Question: Which film has the director died earlier, Girls School Screamers or Lucky Numbers? Model output: ... <search> director of Girls School Screamers </search> ... <search> director of Lucky Numbers </search> ... <think> Found John P. Finnegan (died 2007) and Nora Ephron (died 2012) \u21d2 therefore the director of Lucky Numbers director died earlier </think> <answer>Girls School Screamers</answer> Gold Answer: Lucky Numbers | 5.7% |\n| 4 | Sufficient | Thinking process match evidence but thinking process is incorrect | Question: Which film has the director who was born first, Nasib Si Labu Labi or Solo Per Te Lucia? Model output: ... <search> director of Nasib Si Labu Labi birth year </search> ... <search> director of Solo Per Te Lucia birth year </search> ... <think> P. Ramlee (born 1936) vs Franco Rossi (born 1923) \u21d2 therefore Nasib Si Labu Labi director was born first </think><answer> Nasib Si Labu Labi </answer> Gold Answer: Solo Per Te Lucia | 25.6% |", "Table 2: Fine-grained error analysis with examples.\n\nproduce the correct answer.", "3.2 Fine-grained Analysis\n\nSince overthinking accounts for a small proportion among predictions, we further analyzed the incorrectly predicted examples belonging to good thinking and overthinking to determine specific error causes. We manually reviewed these examples and classified them into five error types, as shown in Table 2.", "We observe the following. When the reasoning process was insufficient to fully answer the question, we found two error types: (i) the model mistakenly believes the retrieved content matches the question entity and therefore stops further retrieval and outputs an answer, while in fact the retrieved content is irrelevant; (ii) the model only obtained partial content relevant to the question and ended the reasoning, which is a reasoning omission. When the reasoning process was sufficient to answer the question, a prominent problem was that the model failed to follow evidence, indicating difficulty in understanding retrieved content and integrating it into reasoning (Shi et al., 2025; Li et al., 2025a). Another issue was that the reasoning process was correct but the final answer was wrong, or the model showed correct evidence but failed to reason correctly from it.", "Based on these findings, we categorize the failure cases of RAG reasoning on part of the test sets into three major types: retrieval failure , reasoning failure , and answer failure . Motivated by this observation, we propose TIRESRAG-R1 , which aims to enhance the model's awareness of retrieval sufficiency while improving both its reasoning chain and final answer quality.", "Introdction\n\nInformation retrieval (IR) (Voorhees 1999) plays a fundamental role in many natural language processing (NLP) tasks (Hambarde and Proenc \u00b8a 2023; Wang et al. 2024), especially in retrieval-augmented generation (RAG) systems (Xia et al. 2025; Liu et al. 2025). These systems retrieve relevant passages to support the generation process, improving the factual accuracy and reducing hallucinations (Lewis et al. 2020). In typical RAG pipelines, a fast retriever is often used to select a candidate set of passages from a large corpus, which are then re-scored by a reranker to prioritize passages that are more contextually relevant to the input query.", "Traditional reranking strategies, whether they operate at the pointwise (Nogueira et al. 2020a; Liang et al. 2023), pairwise (Yu et al. 2018; Qin et al. 2024), or listwise (Thonet et al. 2022) level, typically follow a two-step routine: score\n\n* Corresponding author.", "Copyright \u00a9 2026, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\n\nFigure 1: Impact of Top-K passage retrieval on multi-hop QA performance. F1, EM, and Recall scores are plotted for HotpotQA and MuSiQue datasets.\n\nTop2 Top3 Top5 Top8 Top10 TopK 0.0 0.2 0.4 0.6 0.8 Score HotpotQA F1 EM Recall 0.1 0.3 MuSiQue", "each passage on its relevance to the query, and then pick the top K candidates as the input for the generator (Li et al. 2023; Meng et al. 2024). Although effective for singlehop retrieval, this approach overlooks the interdependence among passages-a crucial factor in multi-hop and complex reasoning scenarios, including QFS, long-context processing, and high-level query reasoning (Edge et al. 2024; Qian et al. 2024). In many cases, the information required to answer a question is distributed across multiple sources, some of which might not be highly ranked. Consequently, traditional reranking methods face an inherent trade-off: if K is too small, essential but lower-ranked evidence may be omitted; if K is too large, irrelevant or distracting content may overwhelm the generator, leading to decreased performance.", "To further illustrate this point, we conduct a preliminary analysis on two widely used multi-hop question answering benchmarks, HotpotQA (Yang et al. 2018) and MuSiQue (Trivedi et al. 2022). Our findings highlight the key limitation of traditional reranking approaches: A fixed Top-K retrieval strategy limits the ability of RAG systems to handle multi-hop and complex queries As illustrated in Figures 1, the optimal value of K is highly querydependent, K =5 for HotpotQA and K =3 for MuSiQue. Increasing K from 2 to 8 improves recall by up to 13 . 21% and 53 . 26% on HotpotQA and MuSiQue, respectively, demonstrating the benefit of incorporating more contextual information. However, the F1 score drops sharply beyond K = 5 on HotpotQA and K = 3 on MuSiQue, indicating diminishing or even negative returns as irrelevant content begins to dilute the useful evidence.", "To address this problem, we propose to reframe reranking as a passage selection learning problem, where large language models are fine-tuned through supervised learning (SFT) to identify the minimal set of passages that collectively support answering a query. Rather than assigning scalar relevance scores to each passage, the proposed framework, Dynamic Passage Selector (DPS) , learns to reason about the combinatorial relevance. This formulation allows the model to explicitly capture the inter-passage dependencies critical for multi-hop question answering, understanding when a single passage would suffice or when multiple passages must be selected together.", "As illustrated in Figure 2, the number of passages required to answer the query varies between query A and query B. Traditional reranking approaches retrieve a fixed Top-K set of passages, which introduces redundant information that may compromise answer quality or lead to incorrect responses. In contrast, DPS dynamically selects the minimal yet sufficient subset of passages based on queryspecific needs, thereby ensuring the quality of generated content. Additionally, recent methods attempt to align retrieval and ranking in RAG pipelines (Li et al. 2024; Zhang et al. 2025a; Zuccon, Zhuang, and Ma 2025), they often rely on costly multi-stage API calls or require large-scale finetuning. By contrast, the proposed DPS can be seen as a plugand-play component that integrates seamlessly into existing RAG workflows without fine-tuning process or pipeline modifications.", "We validate our approach on a range of question answering benchmarks with varying levels of difficulty, including three popular multi-hop question answer datasets HotpotQA (Yang et al. 2018), MuSiQue (Trivedi et al. 2022), 2WikiMQA (Ho et al. 2020), as well as two out-of-domain datasets Legal and CS (Qian et al. 2024). Experimental results show that our dynamic passage selector consistently outperforms traditional point-wise rerankers and state-ofthe-art LLM-based list-wise rerankers, especially on complex multi-hop queries. The results demonstrate that DPS can dynamically identify contextually relevant passages, surpassing the limitations of fixed top-K retrieval. By more effectively aligning retrieval with generation, DPS leads to more robust end-to-end performance. Our main contributions are as follows:\n\nTo the best of our knowledge, this is the first work that identifies and addresses a critical limitation in RAG systems: the fixed Top-K retrieval strategy severely impairs RAG system's performance on multi-hop and complex queries.", "We treat reranking as a passage selection problem, finetune LLMs to dynamically select a minimal set of sufficient passages by modeling their interdependencies, enabling accurate retrieval for multi-hop and complex query, while remaining fully plug-and-play with existing RAG systems.", "Through extensive evaluations across five diverse question answering benchmarks, we demonstrate that DPS achieves state-of-the-art performance on both challeng-\n\ning multi-hop and out-of-domain datasets (e.g., 30.06% F1 increase over Qwen3-reranker on MuSiQue), validating its capabilities and robustness for RAG systems.", "5 Concluding Discussion\n\nA central finding of this work is that query framing and retrieval context interact to affect how well LLMs handle health misinformation. The results consistently show that user queries and the type of evidence retrieved have a significant impact on model behavior, with the content distribution of the retrieval pool shaping outcomes alongside the quality of individual documents. In particular, the inclusion of helpful evidence, even in the presence of adversarial or harmful content, can protect against misalignment.", "The empirical analysis demonstrates that ground-truth alignment is most severely harmed by Liar and FSAP documents, with effects that are consistent across both open-source and closed-source model families. Our observations are consistent with previous studies Chaudhari et al. [10], Wang and Yu [73], Xue et al. [80], Zou et al. [89] showing that RAG systems are sensitive to the information provided as context, and when this context is malicious or adversarially manipulated, models can be misled into producing incorrect or biased output. This reinforces the broader evidence that the reliability of RAG pipelines is dependent not only on retrieval accuracy but also on the integrity of the documents used to ground generation. However, adding helpful documents often makes performance superior to the Non-RAG baseline. Query framing is also a key factor in shaping model behavior, with consistent user queries mostly improving alignment and conflicting queries reducing it.", "In addition, our results reveal a consistent robustness gap between COVID-19 queries in TREC 2020 and general health queries in TREC 2021, likely due to extensive exposure to pandemic-related misinformation during training that may have reinforced model defenses. In contrast, broader health topics remain more vulnerable, which suggests a potential imbalance in how current LLMs are safeguarded across medical domains. These outcomes indicate a more extensive asymmetry in alignment practices, wherein robustness is shown to be enhanced in response to prominent crises but is insufficiently applied to other medically significant domains.", "Our study provides important evidence on the vulnerabilities and unintended risks associated with generative AI in potentially high-stakes domains such as healthcare. Our findings demonstrate that adversarial documents can significantly disrupt model behavior, undermining the reliability of even state-of-the-art RAG systems. These failures occur regardless of the size or architecture of the model and underscore the retrieval stage as the primary point of\n\nManuscript submitted to ACM", "vulnerability. Once malicious content is promoted to the top ranks by the retriever or reranker, LLMs rarely recover, even when supported by consistent user queries. This highlights a broader concern: generative AI systems are not only prone to adversarial manipulation but can also amplify harmful content when such information is injected into their context. The results emphasize that enhancing the generation component alone is insufficient to ensure trustworthiness. Robust defenses must also address vulnerabilities in the retrieval pipeline to prevent harmful content from reaching the generation stage.", "F Retrieval Pattern Analysis\n\nWeanalyzed representative cases from MMLongBench-Doc to understand DocsRay's retrieval patterns and identify common scenarios in document question answering. Table 7 presents 11 selected cases that illustrate different retrieval scenarios and the source sections consulted by the system.", "Our analysis reveals three distinct patterns in document retrieval that highlight both the strengths and limitations of current document QA systems.", "The table presents retrieval patterns across 11 representative cases from MMLongBench-Doc illustrating three scenarios:\n\nPattern 1: Clear Single-Source Evidence. Simple factual questions where answers reside on specific pages, such as 'What year is the report for?' (mmlongbench 21), demonstrate DocsRay's effectiveness in precise fact retrieval where evidence localization is unambiguous.", "Pattern 2: Multi-Page Evidence Synthesis. Complex queries requiring information aggregation across multiple pages, such as counting exterior photos of organizations (mmlongbench 26) which requires examining pages 10-20. These cases reveal the importance of complete document coverage, as missing even one relevant page leads to incorrect counts.", "Pattern 3: Insufficient Source Information. Questions where the document lacks sufficient information, such as mmlongbench 18 which asks about continent-wise participant distribution but the document only provides total counts without geographic breakdown. These cases underscore the importance of systems acknowledging when retrieved content is insufficient for answering queries.", "Statistical Evidence Requirements. Questions involving statistical comparisons pose unique challenges. Case mmlongbench 10 asks which group is greater between 'Republican Hispanic' (7%) and 'no leans male' (55%), requiring retrieval from pages 3 and 22. The distributed nature of statistical evidence, where denominators and percentages may appear on different pages, demands sophisticated cross-page reasoning. DocsRay's pseudo-TOC organization helps by grouping related statistical content within sections, reducing the likelihood of missing critical context.", "Visual Evidence Interpretation. Several cases involve visual content interpretation. The Indian Space Programme map (mmlongbench 20) and organizational photos (mmlongbench 26) require the system to process images and generate textual descriptions. While DocsRay identifies and describes these visual elements, the conversion to text-based representations limits detailed visual analysis, a deliberate trade-off prioritizing semantic retrieval over pixel-level processing.", "These patterns demonstrate that effective document QA requires more than accurate retrieval; it benefits from transparent source attribution allowing users to understand which sections were consulted. The cases where DocsRay provides section-level references (Pattern 1 and 2) offer transparency, while acknowledgment of missing information (Pattern 3) prevents misinformation. This analysis reinforces our recommendation for future systems to implement fine-grained evidence grounding mechanisms that link specific claims to exact source passages, for high-stakes applications in legal, medical, and financial domains where detailed answer verification is mandatory."]}
{"type": "entry", "index": 42, "question": "how do i stop retrieval from drifting during a multi-turn convo", "answer": "To keep retrieval from drifting in a multi-turn conversation, you need to control what query actually goes into the retriever and how much past context it depends on.\n\n1) Rewrite each turn as a stand\u2011alone query\nA \"Standalone Query\" (SAQ) module rewrites the user\u2019s current message plus history into a fully self\u2011contained query before retrieval [3]. It:\n- Resolves pronouns and vague phrases (\"it\", \"this one\", \"the second one\") into explicit entities or products [3].\n- Encodes all needed context so downstream retrieval doesn\u2019t have to see the whole dialogue [3].\n\nThis prevents the retriever from latching onto stale topics or ambiguous references and keeps it focused on the current target.\n\n2) Explicitly track topic / entity shifts\nThe SAQ examples show how they distinguish \u201cfollow\u2011up on same product\u201d vs \u201cswitch to a new product,\u201d changing the rewritten query accordingly (e.g., from iPhone 13 to iPhone 14) [3]. You can adopt the same pattern for people, cases, projects, etc., so retrieval doesn\u2019t stay stuck on an earlier entity when the user has moved on.\n\n3) Use routing to decide when to retrieve\nA lightweight router can classify each question as needing external info or not, and skip retrieval when the answer is already in the local context or image [1]. This limits unnecessary retrieval that can drag in off\u2011topic documents.\n\n4) Evaluate specifically for multi\u2011turn drift\nBenchmarks like mtRAG (multi\u2011turn RAG) and similar multi\u2011turn QA tasks explicitly test whether systems maintain coherent, stateful conversations over several turns [1][2]. Using such evaluations helps you detect and correct retrieval drift in practice.\n", "tags": ["_untagged"], "full_faithfulness": 1.0, "full_answer_relevancy": 0.9233755784000001, "seed_faithfulness": 0.23529411760000002, "seed_answer_relevancy": 0.9233755784000001, "context_relevance": 0.2997460651783234, "context_relevance_error": null, "context_relevance_reason": "The context passages provide some relevant information about challenges in multi-turn conversations and retrieval-augmented generation, but they do not directly address the user's question about stopping retrieval from drifting. While there are mentions of maintaining conversational context and coherence, the specific strategies or solutions to prevent retrieval drift are not clearly articulated, leading to a low relevance score.", "structure_citation": 0.5511103924557081, "structure_error": null, "structure_reason": "The Actual Output includes claims supported by citations, but there are issues with citation accuracy. While it references [1], [2], and [3], the context snippets do not provide direct support for all claims made, particularly regarding the effectiveness of the proposed methods. Additionally, the output lacks a citation for the claim about evaluating multi-turn drift, which is critical for the context of the conversation. This results in partial alignment with the evaluation steps.", "hallucination_score": 0.7826434574007173, "hallucination_error": null, "hallucination_reason": "The response provides a clear and relevant strategy for preventing retrieval drift in multi-turn conversations, aligning well with the input question. It effectively outlines methods such as rewriting queries as standalone, tracking topic shifts, and using routing to optimize retrieval. However, while the claims are generally supported by the context, the response could benefit from more explicit connections to the challenges of factual accuracy and grounding mentioned in the context, which would strengthen its alignment with the evaluation steps.", "contexts_used": ["1 Introduction\n\nRecent progress in Vision-Language Models (VLMs) has enabled richer multimodal reasoning, allowing systems to interpret and generate responses from visual and textual inputs. These models have demonstrated impressive capabilities in tasks such as visual question answering, image captioning. However, despite these advances, VLMs still struggle with factual accuracy, frequently producing hallucinations, confident but incorrect or fabricated outputs that are not grounded in the input image or external knowledge [6]. This challenge is particularly acute for real-world applications like smart glasses, which process a continuous stream of dynamic, egocentric, and often low-quality visual data. From this first-person perspective, user queries are not only complex but also frequently vague or ambiguous. They often require real-time information retrieval (e.g., \"what is the cost of this product on Amazon?\"), involve pronouns that need visual grounding (e.g., \"does this contain sodium?\"), and depend on external knowledge for verification. These factors compound the risk of hallucination, demanding systems that are not only capable but fundamentally reliable.", "The Retrieval-Augmented Generation (RAG) paradigm offers a promising solution by grounding model responses in external knowledge [4]. However, extending RAG to the multi-modal setting (MM-RAG) introduces significant challenges, including efficient retrieval, deciding whether and when to query external sources, multi-source synthesis, and maintaining conversational context. These challenges arise from the increased complexity of integrating heterogeneous modalities such as images, user queries, retrieved web passages, and structured knowledge graph entries. Unlike traditional text-based RAG, MM-RAG must reconcile visual inputs with both unstructured and structured textual sources, while maintaining referential grounding and contextual coherence across modalities. To evaluate these challenges in real-world conditions, the KDD Cup 2025 Meta CRAG-MM (Comprehensive RAG Benchmark for Multi-modal Multi-turn Question Answering) introduces a rigorous benchmark designed to foster innovation in MM-RAG systems. To address the compounded challenges of factual grounding and response reliability in MM-RAG, we propose a verification-centric framework designed to prioritize accuracy over completeness. Our\n\n; \u2192 .", "approach incorporates a lightweight query router to identify questions requiring real-time or external information, a multi-source retrieval module combining web and knowledge graph search with reranking and dynamic thresholding, a dual-path generation strategy to balance prior knowledge and grounded context, and a structured verification mechanism that enforces factual consistency through self-consistency and chain-of-verification protocols. In summary, our key contributions are as follows:\n\nWepropose a multi-stage, verification-centric framework for MM-RAG that explicitly prioritizes factual accuracy.", "We detail the implementation of key components, including a lightweight query router for efficiency, a query-aware retrieval pipeline with reranking and dynamic thresholding for relevance, and a novel dual-path generation with a self-consistency check and chain-of-verification for robust answer validation.", "We demonstrate the effectiveness of our system on the KDD Cup 2025 Meta CRAG-MM challenge, and release our implementation to support reproducibility and future research on building reliable MM-RAG systems that reduce hallucination in real-world, egocentric scenarios.", "2 Benchmark and Task Definition\n\n2.1 Benchmark Overview\n\n(2) QA Set : Includes over 5,000 question-answer pairs spanning the same 13 domains. Questions are categorized into four types:\n\nSimple Questions : Require either direct visual recognition or a single piece of external knowledge.", "Multi-hop Questions : Involve reasoning over multiple pieces of information to derive an answer.\n\nComparison and Aggregation Questions : Require synthesis and comparison of several data points.", "Reasoning Questions : Target implicit knowledge or logical inference beyond direct retrieval.\n\nQuestions appear in both single-turn and multi-turn formats, enabling assessment of stateful contextual reasoning.", "2.2 Task Definition\n\nThe competition is structured into three complex tasks:\n\nTask 1: Single-source Augmentation : Tests the basic RAG capability using a provided image-based mock Knowledge Graph (KG) as the external source.", "Task 2: Multi-source Augmentation : Increases complexity by adding a web search API, requiring systems to synthesize information from both the KG and noisy web pages.", "Task 3: Multi-turn QA : Focuses on context understanding and maintaining coherent, stateful conversations over 2-6 turns, where later turns may or may not depend on the initial image.", "Together, the three tasks progressively evaluate different dimensions of multimodal RAG systems under realistic, open-ended QA scenarios. Task 1 serves as a foundation, assessing the system's ability to retrieve and ground answers using a single, structured source (KG) tied to the input image. Task 2 introduces additional challenges by requiring systems to retrieve from multiple heterogeneous sources, thereby testing their robustness to noisy or redundant information. Task 3 emphasizes conversational consistency and memory, evaluating whether a system can maintain contextual grounding and answer coherently across multiple turns. This tiered structure enables a holistic assessment of factuality, retrieval quality, and dialogue-level reasoning in multimodal QA.", "2.3 Evaluation\n\nThe challenge's most defining feature is its rigorous evaluation protocol, which combines automated metrics with manual review and is subject to strict performance constraints. Scoring Metric. Responses are scored on a four-level scale: Perfect (1.0) for fully correct and factual answers; Acceptable (0.5) for useful answers with minor, non-harmful errors; Missing (0.0) for abstentions (e.g., 'I don't know'); and Incorrect (-1.0) for factually wrong or hallucinatory answers. This scoring system imposes a strong incentive for systems to avoid speculation, as a single hallucinated answer cancels out the benefit of a correct one. Our framework is therefore fundamentally guided by the goal of maximizing this truthfulness-focused score. Evaluation Process and Constraints. During the competition, automated evaluation is used for leaderboard scoring, with only the first 75 BPE tokens of a response being considered. However, final rankings are determined by manual review of the top-10 teams, where human annotators assess the quality and factuality of the full response. This process is governed by strict performance constraints: a 10-second timeout is enforced per conversational turn, and any submission exceeding this limit is considered a failure. Furthermore, systems must handle time-sensitive queries, as the ground truth reflects correctness at the time of data collection.", "Figure 1: The overall pipeline of our solution.\n\nSummarization Data Sources Image-Based Mock KG Web Search Mock API Query Router Dual-Path Generation Query-Aware Retrieval Image Query Answer Verification and Finalization\n\nrouter explicitly predicts two binary attributes for each question \ud835\udc5e \ud835\udc56 \u2208 \ud835\udc44 :\n\nNeeds External Info \ud835\udc52 \u2208 { 0 , 1 } : whether answering \ud835\udc5e \ud835\udc56 requires knowledge beyond the input image. This helps avoid hallucinations caused by over-reliance on model priors when the necessary information is not visible.", "Is Real-Time \ud835\udc5f \u2208 { 0 , 1 } : whether \ud835\udc5e \ud835\udc56 is time-sensitive (e.g., involving \"today\", \"latest\", etc.). This is aimed at reducing the risk of outdated model knowledge when responding to temporal queries.", "The classification is performed by prompting the router with a structured template, and the resulting decision \ud835\udc37 = { \ud835\udc52, \ud835\udc5f } informs subsequent processing steps. If \ud835\udc52 = 0, the retrieval stage is skipped, conserving computational resources and improving latency. Full prompt details are provided in Appendix A.1.\n\nFigure 2: The detail of Query-Aware Retrieval module.\n\nRecalled Docs Paragraph Chunker Reranker K Docs MAD Dynamic Scored Docs Threshold Thresholding Top", "4 Year-by-Year Progress in RAG\n\n4.2 Major Milestones (2020-2024)\n\nAdaptive retrieval: Self-RAG[5] trains a single language model to generate special 'reflection' tokens that trigger on-demand retrieval and self-critique. In experiments, 7B and 13B Self-RAG models substantially outperformed ChatGPT and a RAG-augmented Llama-2-chat baseline on open-domain QA, reasoning, and fact-verification tasks, yielding much higher factual accuracy and citation precision.", "Knowledge-grounded dialogue: Retrieval augments dialogue systems to improve consistency and informativeness. Kumari et al.[47] incorporate retrieved persona and context snippets in long conversation modeling, showing that adding relevant knowledge improves response quality. Similarly, Kang et al.[43] propose SURGE , which retrieves relevant subgraphs from a knowledge graph and uses them to bias the response generation. SURGE produces more coherent, factual responses grounded in the retrieved", "knowledge.", "Summarization and explanation: RAG has been applied to summarization and explanation tasks. By retrieving source documents or evidence passages, RAG-augmented summarizers produce more accurate and detailed summaries than closed-book models. Likewise, in fact-checking pipelines, retrieving evidence before verification leads to more reliable verdicts and explanations. These applications extend RAG's grounding advantages beyond QA to a broader range of generative tasks.", "4.3 2025 - The Current Direction\n\nThe community is exploring how to marry graph knowledge with text retrieval. A comprehensive survey formalised the GraphRAG paradigm and mapped design choices for graph-aware retrievers and generators [29]. A companion study compared vanilla RAG and GraphRAG across QA and summarisation, showing complementary strengths and proposing hybrid fusion strategies [28].", "Security and robustness. SafeRAG introduced the first security benchmark for RAG pipelines, cataloguing four attack classes (silver noise, context conflict, soft-ad, DoS) and demonstrating that 14 representative systems fail even simple manipulations [55]. Findings sparked interest in provenance tracking and adversarially trained retrievers.", "Agentic and selective retrieval. A survey on Agentic RAG synthesised emerging patterns-reflection, planning, tool use-and argued that autonomous agents can orchestrate multi-hop retrieval more effectively than static pipelines [83]. Concrete instantiations followed:\n\nSIM-RAG learns a self-skeptic critic that decides when to stop multi-round search, cutting redundant calls and boosting exact-match on five QA sets by up to 4pp[98].", "Self-Routing RAG jointly trains an LLM to choose between internal knowledge and external retrieval, reducing retrieval volume 29% while raising accuracy 5pp[96].", "Conversational evaluation at scale. IBM's mtRAG benchmark is filling a gap in multi-turn assessment: 110 human-written conversations (7.7 turns each) across four domains, plus synthetic variants, revealed that state-of-the-art systems struggle with unanswerable and follow-up questions citep katsis2025mtrag. Alongside automatic judges such as RL-F and RB-LLM, mtRAG enables holistic measurement of retrieval, generation, and turn-level faithfulness.", "Emerging directions. Workshops and surveys highlighted three open fronts. First, agentic planning -letting models reason over retrieval actions-promises better long-horizon reasoning but raises cost and safety questions. Second, structured retrieval (graphs, tables, multimodal stores) demands new embedding spaces and fusion operators. Third, secure and privacy-preserving RAG is gaining urgency, with SafeRAG prompting work on attack-aware retrievers and watermarking of retrieved evidence.", "Outlook Mid-2025 results suggest that modest-sized, security-hardened, agent-controlled RAG systems can rival much larger closed-book LMs while offering provenance. Key challenges ahead include trusted evidence ranking at trillion-document scale, automatic detection of retrieval-based attacks, and seamless integration of non-text modalities. Nevertheless, 2025 firmly positions RAG not merely as a booster of accuracy but as an essential framework for reliable , updatable , and auditable language agents.", "6 Evaluation of RAG Systems\n\n6.1 Evaluation Dimensions and Metrics\n\nRAG performance is commonly assessed along the following dimensions:\n\nRetrieval Accuracy. Key metrics include:\n\nRecall@ k - Measures the proportion of queries where a relevant document appears among the topk retrieved results.", "Mean Reciprocal Rank (MRR) - Captures the average inverse rank of the first relevant document, rewarding early placement.", "Mean Average Precision (MAP) -Evaluates the quality of ranked retrieval across relevant items.", "6.2 Benchmarks and Datasets\n\nSeveral benchmarks are widely used to evaluate RAG systems:\n\nNatural Questions (NQ) , TriviaQA , and WebQuestions for open-domain QA [52].", "FEVER and AveriTeC for fact-checking, emphasizing verifiability.\n\nKILT benchmark suite integrates QA, dialog, slot filling, and entity linking over Wikipedia.", "BEIR evaluates retrieval across 31 zero-shot tasks in domains like biomedicine and finance.\n\nMTRAG targets multi-turn conversations requiring sequential retrieval and reasoning.", "TREC RAG Track (2024-) defines unified evaluation of retrieval, generation, and support quality over MS MARCO with metrics like nugget recall and citation coverage [71].", "6.3 Retrieval-Augmented Generation Assessment System\n\nRAGAS (Retrieval-Augmented Generation Assessment System) is an evaluation framework specifically designed for assessing and improving the factuality and grounding of RAG systems. Unlike conventional metrics that measure superficial linguistic overlap, RAGAS emphasizes the alignment between generated content and retrieved documents, providing explicit signals regarding factual correctness and attribution quality. By systematically measuring how well the generated outputs are supported by the retrieved evidence, RAGAS helps identify and penalize hallucinations-instances where the model generates plausible but unsupported statements. Consequently, employing RAGAS during model training or iterative fine-tuning guides RAG systems toward producing outputs firmly grounded in verifiable sources, substantially improving factual accuracy and reducing the incidence of hallucinated information.", "| | Category | Metric/Tool | Description |\n|---:|:-------------------|:---------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 0 | Retrieval Accuracy | Recall@ k MRR MAP | Proportion of queries with a relevant doc in top- k results Average inverse rank of the first relevant document Mean precision across all relevant retrieved documents |\n| 1 | Generation Quality | Exact Match (EM), F1 BLEU, ROUGE Faithfulness Rate | Measures overlap with ground-truth answers N-gram based overlap metrics for generation Consistency with retrieved context (manual/automated) |\n| 2 | Efficiency | Latency (retrieval/generation) Memory/Compute Usage | Time required for each processing step Resource demands for model operation |\n| 3 | Scalability | Index Size vs. Accuracy Adaptability | Impact of increasing corpus size on performance Ability to incorporate new data without retraining |\n| 4 | Benchmarks | Natural Questions, TriviaQA FEVER, AveriTeC KILT BEIR MTRAG TREC RAG Track | Open-domain QA evaluation Fact-checking datasets Multi-task benchmark over Wikipedia 31-task zero-shot retrieval benchmark Multi-turn reasoning benchmark Unified eval. of retrieval, generation, citation |\n| 5 | Tooling | RAGAS | Evaluates factual consistency and grounding |", "Table 1: Evaluation Dimensions, Metrics, Benchmarks, and Tools for RAG Systems", "6.4 Impact of Architecture on Evaluation\n\nEmpirical studies demonstrate that architectural components in RAG-chunking, embedding, and re-ranking-directly affect performance across tasks and benchmarks.", "4 PROPOSED ARCHITECTURE\n\nOur proposed context-aware framework, illustrated in Figure 1, processes user queries through a modular pipeline that integrates conversational history, product identification, intent understanding, and targeted retrieval.", "The pipeline begins with a Standalone Query (SAQ) Module, which transforms the raw user query \ud835\udc5e ( \ud835\udc56 ) along with its conversational history \ud835\udc3b ( \ud835\udc56 -1 ) into a self-contained, contextualized query \ud835\udc5e \u2032( \ud835\udc56 ) . This is passed to a Catalog Search Model that detects product mentions and maps them to unique product IDs \ud835\udc43 ( \ud835\udc56 ) \ud835\udc3c \ud835\udc37\ud835\udc60 from the catalog P .", "Next, a unified Intent Model predicts a distribution over predefined intent classes, including a Non-Decision category for out-ofdomain queries and multiple Decision intents such as specifications, offers, or payment options. If the Non-Decision intent is not dominant, the system assesses entropy over the Decision intents to guide retrieval strategy: low entropy indicates a clear dominant intent, triggering focused retrieval, while high entropy reflects ambiguity, prompting retrieval for the top-N (e.g., top three) intents to maximize coverage.", "For each identified intent, the system invokes relevant backend APIs to retrieve structured attributes ( \ud835\udc46 \ud835\udc5d ), unstructured reviews ( \ud835\udc48 \ud835\udc5d ), semi-structured ( \ud835\udc40 \ud835\udc5d ), and other product-related data. To manage the potentially large volume of retrieved content, a Retrieval and Re-ranking module selects and prioritizes context snippets based on semantic relevance to \ud835\udc5e \u2032( \ud835\udc56 ) , yielding a concise set C ( \ud835\udc56 ) \ud835\udc5f\ud835\udc52\ud835\udc59 .", "Finally, the refined context C ( \ud835\udc56 ) \ud835\udc5f\ud835\udc52\ud835\udc59 and query \ud835\udc5e \u2032( \ud835\udc56 ) are composed into a tailored prompt for a Large Language Model (LLM). The LLM synthesizes this information to generate a coherent, fluent, and contextually faithful response \ud835\udc66 ( \ud835\udc56 ) , with built-in mechanisms to acknowledge any information gaps.", "4.1 Standalone Query (SAQ)\n\nThe Standalone Query (SAQ) module is the crucial first component in our pipeline, responsible for enabling multi-turn conversation. Its primary function is to rewrite the current user query ( \ud835\udc5e ( \ud835\udc56 ) ) into a\n\nFigure 1: Diagram illustrating the proposed query processing architecture.\n\nOther Data Policy Data Product Data Seryices Semce Service Uscr Pronile Mctadata Gencratlon System Response Identificd Into Gaps (Log) Deciston Intentis) User Qucry Product Catalog SAQ Generation Catalog Scarch Entropy) Mon-Dccision External Handler/ General Agent", "self-contained, contextualized query ( \ud835\udc5e \u2032( \ud835\udc56 ) ) by incorporating the conversational history ( \ud835\udc3b ( \ud835\udc56 -1 ) ). This process serves two key purposes: (1) resolving co-references (e.g., pronouns like \"it\") and (2) performing product disambiguation by mapping ambiguous references (e.g., \"the second one\") to their canonical product names based on the preceding context. The resulting query, \ud835\udc5e \u2032( \ud835\udc56 ) , encapsulates all necessary information, allowing downstream components to operate without needing access to the full conversation history. Table 1 illustrates how the SAQ module handles diverse conversational scenarios.", "4.2 Catalog Search\n\nThe Catalog Search module maps product mentions within the contextualized query ( \ud835\udc5e \u2032( \ud835\udc56 ) ) to unique platform Product IDs. It employs a hierarchical strategy, prioritizing an (1) exact match of extracted product names against the conversational history ( \ud835\udc3b ( \ud835\udc56 -1 ) ). If this fails, it proceeds to (2) fuzzy matching against both the history and the broader platform catalog to handle variations or typos. As a final fallback, (3) the most salient name from \ud835\udc5e \u2032( \ud835\udc56 ) is used to query external and platform search APIs to retrieve a canonical product name, which is then mapped to its ID. This cascaded approach ensures robust disambiguation by prioritizing in-session context before broadening the search.", "4.3 Intent Model\n\nUser query intent is determined by a unified BERT-based multiclass classification model , which categorizes the contextualized query ( \ud835\udc5e \u2032( \ud835\udc56 ) ) into predefined classes. The two primary categories", "| | Scenario | Conversation Snippet | SAQ Output |\n|---:|:----------------------------|:------------------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------|\n| 0 | Follow-up on same product | On iPhone 13 product page U: Battery size? B: <answer> U: Display size? | What is the display size of iPhone 13? |\n| 1 | Switch to a new prod- uct | On iPhone 13 product page U: Battery size? B: <answer> U: How about iPhone | What is the battery size of iPhone 14? |\n| 2 | Product \u2192 Accessory search | On iPhone 13 product page U: Battery size? B: <answer> U: Show me cases for this phone. | Show me cases for iPhone 13. |\n| 3 | Browsing \u2192 Specific product | On Browse page U: Show 2 door refrig- erators. B: <shows multiple fridges including LG 242 L Frost Free 2 Star> U: Capacity of LG fridge? | What is the capacity of LG 242 L Frost Free 2 Star? |\n\nTable 1: Illustrative SAQ Scenarios and Outputs (User and Bot turns are abbreviated for space)", "are: non_decision for out-of-scope queries (e.g., general search like queries) that are routed outside the PQA pipeline, and fine-grained decision intents for queries seeking specific product information (e.g., specifications, offers).", "2 Related Works\n\n2.1 Definitions of Hallucination\n\nThe term hallucination has been used with varying scope across natural language generation tasks. Some studies emphasize factuality , describing hallucinations as outputs that contradict established facts, i.e., inconsistencies with world knowledge or external ground truth [31, 13]. Others highlight faithfulness , where hallucinations occur when generated responses deviate from the user instruction or a reference text, often producing plausible but ungrounded statements particularly in source-conditioned tasks such as summarization or question answering [26]. Beyond these two dimensions, researchers also note cases of incoherent or nonsensical text that cannot be clearly attributed to factuality or faithfulness criteria [14, 23].", "Alternative terms have also been introduced. Confabulation draws on psychology to describe fluent but fabricated content arising from model priors [9], while fabrication is preferred by some to avoid anthropomorphic connotations [1, 21]. More recently, Chakraborty et al. [3] propose a flexible definition tailored to deployment settings, defining a hallucination as a generated output that conflicts with constraints or deviates from desired behavior in actual deployment, while remaining syntactically plausible under the circumstance.", "2.2 Hallucination Detection in LLMs\n\nBuilding on these definitions, hallucinations have been recognized as a major challenge in text generation. Early work in machine translation and abstractive summarization described them as outputs that are not grounded in the input source [23, 16, 15], motivating the development of evaluation metrics and detection methods for faithfulness and factual consistency across natural language generation tasks.", "More recent reference-free (unsupervised or zero-reference) methods aim to detect hallucinations without gold-standard labels by analyzing the model's own outputs. A prominent method is SelfCheckGPT [22], a zero-resource, black-box approach that queries the LLM multiple times with the same prompt and measures semantic consistency across responses. The intuition is that hallucinated content often leads to instability under stochastic re-generation; true facts remain stable, while fabricated ones diverge. Manakul et al. show that SelfCheckGPT achieves strong performance in sentence-level hallucination detection compared to gray-box methods, and emphasize that it requires no external database or access to model internals [22]. However, SelfCheckGPT may struggle when deterministic decoding or high model confidence leads to repeating the same incorrect output.", "2.3 Metamorphic Testing\n\nMetamorphic Testing (MT) [4] was originally proposed in software engineering to address the oracle problem in which the correct output is unknown. MT relies on metamorphic relations (MRs): transformations of the input with predictable effects on outputs, enabling error detection without access to ground truth [28]. In machine learning, MThas been applied to validate models in computer vision [5] (e.g., rotating an image should not change its predicted class) and NLP [27]\n\nIn hallucination detection for LLMs, MetaQA [32] leverages MRs by generating paraphrased or antonym-based question variants and verifying whether answers satisfy expected semantic or logical constraints. Relying purely on prompt mutations and consistency checks, MetaQA achieves higher precision and recall than SelfCheckGPT on open-domain QA.", "Researchers have also adapted MT for more complex conversational and reasoning settings. MORTAR [12] applies dialogue-level perturbations and knowledge-graph-based inference to multi-turn systems, detecting up to four times more unique bugs than singleturn MT. Drowzee [18] uses logic programming to construct temporal and logical rules from Wikipedia, generating fact-conflicting test cases and revealing rates of 24.7% to 59.8% across six LLMs in nine domains [32].", "These works highlight the promise of MT for hallucination detection, but they primarily target open-book QA or multi-turn dialogue, often over short, single-sentence outputs. Prior studies have not addressed hallucination detection in retrieval-augmented generation (RAG) scenarios over proprietary corpora, a setting in which ground-truth references are unavailable and model internals are inaccessible. MetaRAG builds on MT by decomposing answers into factoids and designing MRs tailored to factual consistency against retrieved evidence in a zero-resource, black-box setting."]}
{"type": "entry", "index": 43, "question": "how do i evaluate multi-hop questions without manually labeling the chain", "answer": "You can evaluate multi\u2011hop questions without ever labeling the reasoning chain by focusing on outcome\u2011based metrics and datasets that already provide the final answer and, optionally, supporting facts.\n\n1. Use existing multi\u2011hop QA benchmarks\nBenchmarks like HotpotQA and 2WikiMultihopQA are designed for multi\u2011hop reasoning over multiple documents. They provide gold answers and, for some, human\u2011annotated supporting facts. Standard evaluation on these datasets uses answer Exact Match (EM) and F1, and optionally supporting\u2011fact EM/F1 and a joint metric that requires both to be correct [2]. Other work also evaluates multi\u2011hop QA on HotpotQA and 2WikiMultihopQA using only answer\u2011level EM/F1, without inspecting chains [3], or using simple accuracy on multi\u2011hop question types (compare, bridge, inference, compositional) [4].\n\nIn practice: feed your questions and retrieval context to the model, collect its answers, and compare them to the gold answers using EM/F1 or accuracy. You never need to label intermediate steps.\n\n2. Ignore chain supervision and use supporting\u2011fact metrics only if you want\nIf you don\u2019t want to label or rely on explicit chains, you can:\n- Evaluate only answer EM/F1 or accuracy on multi\u2011hop datasets [2][3][4].\n- Optionally track supporting\u2011fact metrics where they already exist (e.g., HotpotQA) as a proxy for whether the model is looking at the right evidence, but you don\u2019t have to create or enforce a step\u2011by\u2011step chain yourself [2].\n\n3. Use higher\u2011level evaluation frameworks\nBroader QA evaluations often rely on these core metrics (EM/F1, accuracy) even for complex reasoning tasks, and may add LLM\u2011as\u2011judge style assessments like MT\u2011bench to score answer quality holistically without access to chains [2]. This gives you a way to rate multi\u2011hop performance based purely on final answers and overall quality.", "tags": ["_untagged"], "full_faithfulness": 1.0, "full_answer_relevancy": 0.9323801497, "seed_faithfulness": 1.0, "seed_answer_relevancy": 0.9323686446, "context_relevance": 0.8542151466928466, "context_relevance_error": null, "context_relevance_reason": "The context passages provide relevant information about evaluating multi-hop questions, particularly discussing methods like question decomposition and structured retrieval strategies. These directly address the user's query about evaluating multi-hop questions without manual labeling. However, while the passages are informative, they could be more concise and focused on practical steps for evaluation, which slightly detracts from the overall clarity and directness of the response.", "structure_citation": 0.6336799412753875, "structure_error": null, "structure_reason": "The Actual Output provides a comprehensive approach to evaluating multi-hop questions without manual labeling, referencing existing benchmarks and metrics effectively. It includes citations [2], [3], and [4], which correspond to relevant snippets in the Context. However, while the claims are generally supported, the response could benefit from clearer connections to specific snippets in the Context, particularly regarding the methodologies discussed. Overall, it aligns well with the evaluation steps but lacks some explicit detail in citation support.", "hallucination_score": 0.8300055834935621, "hallucination_error": null, "hallucination_reason": "The response effectively addresses the input question by providing multiple strategies for evaluating multi-hop questions without manual labeling. It supports the claims made in the input by referencing established benchmarks like HotpotQA and 2WikiMultihopQA, and discusses outcome-based metrics such as Exact Match (EM) and F1 scores. However, while the output is informative and relevant, it could benefit from a more explicit connection to the context provided, particularly regarding the challenges of multi-hop reasoning in RAG systems.", "contexts_used": ["1 Introduction\n\nLarge language models (LLMs) are impressive in what they can do, but they're not perfect-especially when it comes to answering questions that require external facts. RetrievalAugmented Generation (RAG) tries to close this gap by bringing in relevant documents during inference. Since it was first introduced by (Lewis et al. 2020), RAG has been used in all sorts of tasks like open-domain question answering, fact-checking, and summarization, where grounding responses in real-world information really matters.", "But there's a catch: most RAG systems still rely on embedding the entire user question into a single dense vector and using that to retrieve documents. This approach works reasonably well for straightforward questions. But when it comes to multi-hop questions-those that involve piecing together facts from multiple sources-it often falls short. The model may retrieve something vaguely related but miss key steps along the way.", "In response to this challenge, various studies have explored structured strategies for improving retrieval. For example, some methods use graphs to represent how pieces of information are connected across documents-such as GRAG (Dong et al. 2024), CUE-RAG (Su et al. 2025), and HLG (Ghassel et al. 2025). Others, such as HiRAG and\n\nDeepRAG (Zhang et al. 2024; Ji et al. 2025), use multilayered retrieval pipelines that narrow down the search space in stages.", "These systems bring real gains, but they also bring complexity. We were interested in whether a simpler approach could work just as well. One promising idea is to break down a complex question into smaller, more focused subquestions. This makes it easier to retrieve relevant information for each step and gives the model a clearer path to the answer.", "That's where our method comes in. Instead of trying to handle the full complexity of a multi-hop question all at once, we decompose it into single-hop subquestions, retrieve evidence for each, and then use a reranking model to choose the passages that best match the original question. It's a way to stay focused on what matters, without over-engineering the pipeline.", "Still, even with decomposition and reranking, there's an important issue we had to address: the format mismatch between user queries and document content. Questions are usually short, focused, and interrogative. Documents, on the other hand, are longer, narrative, and descriptive(Furnas et al. 1987). Even if the content is semantically aligned, the structural differences often confuse embedding-based retrieval systems (Zhao and Callan 2010; Zahhar, Mellouli, and Rodrigues 2024).", "To address this mismatch, we take a different approach: instead of forcing a direct match between question and document as-is, we reshape both. On the query side, we decompose the complex multi-hop question into single-hop parts. On the document side, we generate answerable questions (AQs) from each chunk using a large language model.", "This isn't just a structural trick-it enriches the document representation itself. For an LLM to ask meaningful, answerable questions about a chunk of text, it has to process that chunk from multiple angles: what's stated explicitly, what's implied, and what could reasonably be asked about. In doing so, it effectively distills and reorganizes the content in a way that makes it more semantically accessible for retrieval. The result is a document that becomes not just a source of information, but a map of the questions it can answer.", "To address this, we propose a question-centric RAG framework that aligns both sides of the retrieval process through question formatting. Our main contributions are as follows:", "We present a novel RAG pipeline for multihop question answering that leverages LLM-based question decomposition to transform complex questions into semantically focused single-hop subquestions, enabling more accurate and targeted retrieval without requiring task-specific supervision.", "We replace standard chunk embeddings with answerable question (AQ) representations, generated from each document chunk using Qwen3-8B (Yang et al. 2025) without any fine-tuning. This approach aligns the semantic and syntactic structure of document and query embeddings, reducing structural mismatch and improving retrieval quality through question-to-question similarity.", "Indexing and Inference Workflow\n\nOur system is composed of two main workflows: an offline indexing phase where document representations are constructed, and an online inference phase where multihop queries are processed and answers are generated.", "Offline Indexing. Given a corpus of documents, we first segment them into overlapping chunks using a fixed window size. Each chunk d i is passed through the Qwen38B model to generate a set of m answerable questions { a i 1 , a i 2 , . . . , a im } , which can be answered based solely on the content of d i . These questions are then embedded using the same retriever model employed during inference (e.g., E5). We index these AQG embeddings in a vector database, each entry pointing back to its originating chunk. This index serves as the document-side retrieval base and is computed entirely offline.", "Online Inference. At inference time, a user submits a multihop question q . A decomposition model (e.g., GPT4o, Gemini-1.5-pro, or LLaMA-3.1-8B-inst) segments q into single-hop subquestions { q 1 , . . . , q n } . Each q i is embedded and used to query the AQG index for topk 1 matches, which are mapped back to their source document chunks. These candidate chunks are then reranked using a cross-encoder that compares each chunk d to the original complex query q . The topk 2 reranked chunks are selected and passed to a generator model, which produces the final answer.", "This separation of indexing and inference enables efficient, reusable retrieval while ensuring semantically aligned generation without online paraphrasing or summarization.", "Experimental Setup\n\nWe evaluate our approach on three challenging multihop question answering benchmarks from LongBench (Bai et al. 2024): HotpotQA (Yang et al. 2018), 2WikiMultiHopQa (Ho et al. 2020), and MuSiQue (Trivedi et al. 2022). These datasets are designed to test a system's ability to retrieve and reason across disjoint evidence sources, posing challenges such as the 'Lost in the Middle' effect and semantic drift. To simulate open-domain settings, we use the full-wiki setup for all tasks.", "Models and Evaluations. Our evaluations employ three recent LLMs: GPT-4o (Hurst et al. 2024), Gemini-1.5pro (Team et al. 2024), and the open-source LLaMA-3.18B-instruct (Dubey et al. 2024). Performance is measured using F1-score. We compare our model primarily against the following baselines:\n\nNo-RAG LLMs: Each LLM directly answers questions without retrieval.", "RAG with reasoning-oriented transformations (ROT): Self-RAG (Gao and et al. 2023) a generationside control method, enables multi-step reasoning by dynamically deciding when to retrieve and how to evaluate retrieved content using reflection tokens. Query Rewriting (Ma et al. 2023), a question-side transformation method, reformulates queries to better\n\nmatch retrievable content. ReSP (Jiang et al. 2025), a document-side transformation approach, iteratively summarizes retrieved passages and plans the next retrieval step. Each method supports multi-step reasoning through distinct transformation strategies across the retrieval and generation pipeline.", "RAG with reranking: Standard dense retrieval pipelines with cross-encoder reranking, including RAPTOR (Sarthi et al. 2024) and LongRAG (Jiang, Ma, and Chen 2024) and MacRAG (Lim et al. 2025), which serves as a strong recent baseline. MacRAG adopts the same LLM and evaluates on the same three datasets as used in our work, enabling a direct and fair comparison. Importantly, MacRAG also conducts experiments with LongRAG under the identical setting, and we incorporate those results for direct comparison in this paper.", "We evaluate performance using the standard F1 score, averaged across all datasets. For each subquestion, we retrieve the top k 1 = 100 AQG candidates and rerank them to select the top k 2 = 7 document chunks for final answer generation. We use ms-macro-MiniLM-L-12-v2 , the same crossencoder reranker used in MacRAG, throughout our system.", "Unlike MacRAG and LongRAG, which compares multiple generation modes (e.g., R&L, Full E&F), we adopt a single-generation scheme: topk 2 reranked chunks are passed directly to the LLM for single-step generation (R&B). We adopt this setup to better isolate the impact of retrieval and reranking, without the added variability introduced by complex generation strategies.", "RAG + Graphs in QA\n\nTo address these limitations of traditional RAG, particularly in retrieving contextually precise and semantically rich information, research has tried to integrate structured knowledge sources and graphbased reasoning into the retrieval pipeline. One notable direction of work is LLM-based retrieval, which incorporates knowledge graphs information into the generation process of LLMs. The LLMs are augmented using the retrieved facts from the KG [26], leading to a clear dependence on the quality of extracted graph information, using it to generate responses. Research has also been done towards augmenting knowledge graph information, retrieved through some semantic or some other similarity index, in the prompts given to the LLM [27], to help the model do zero-shot question-answering. Some researchers have tried a different approach to fact retrieval, where the model tries different queries, using structured query languages, until the desired information comes through [28]. All of these approaches have used KGs as an external source to retrieve information from and answer the questions [29]. Then, aligning the retrieval process with LLM even more closely, some researchers have proposed methods which use LLMs in intermediate steps as well to plan the retrieval and judge whether the retrieved information is relevant or not [29,30], continuing the process until the desired output emerges.", "Another interesting direction of work is integrating GNNs with LLMs, which leverages graph neural networks to enhance retrieval and re-ranking using learned graph representations, along with generation capabilities of LLMs. There have been approaches such as GNN-RAG [31], which have tried combining language understanding abilities of LLMs with the reasoning abilities of GNNs in a retrieval-augmented generation (RAG) style. Other methods of GNN-LLM alignment, have been classified into symmetric and asymmetric alignment [32]. Symmetric alignment refers to the equal treatment of the graph and text modalities during the alignment process[33, 34, 35]. Asymmetric alignment focuses on allowing one modality to assist or enhance the other, here leveraging the capabilities of GNNs to improve the LLMs [36, 37, 38].", "The continual evolution of such hybrid architectures reflects the dynamic nature of the QA landscape and its responsiveness to complex information needs. Given the diversity of methods explored to enhance QA systems-from rule-based techniques to advanced neural and hybrid models-it becomes essential to establish robust mechanisms for evaluating their effectiveness. It is thus important to discuss the evaluation paradigms that compare and benchmark these systems. Extractive QA benchmarks almost universally use Exact Match (EM)-the strict proportion of predictions that character-for-character match a ground-truth answer-and F1 score, the harmonic mean of token-level precision and recall, to evaluate answer quality. On SQuAD v2.0 [39], human annotators achieve around 86.831 EM and 89.452 F1 on the test set, whereas state-of-the-art models now exceed 90 EM and 93 F1. Natural Questions [40] extends this paradigm to long-answer (paragraph) and short-answer (span or yes/no) annotations drawn from real Google search queries. Meanwhile, multiple-choice datasets like OpenBookQA [41]-designed to mimic open-book science exams-use simple accuracy. Complex reasoning benchmarks push beyond single-span extraction. HotpotQA [42], a multi-hop dataset built on pairs of Wikipedia articles, evaluates both answer-span EM/F1 and supporting-fact EM/F1, plus a joint metric requiring both to be correct; even top models achieve only ~72 joint F1[43] under the full-wiki setting, far below human performance of ~82 average F1 and ~96 upper bound F1. These core metrics and datasets underpin broader QA evaluation: multi-task suites like GLUE/MMLU [44, 45] include QA subtasks to probe general language understanding, while specialized frameworks such as MT-bench [46] ('LLM as judge') and automated platforms like Scale Evaluation layer on top to assess conversational and retrieval-augmented QA in real-world scenarios.", "This review has highlighted the broad spectrum of methodologies and evaluation strategies used in enhancing and assessing question answering systems. With this foundation in place, we now transition to the specific tools and frameworks employed in our research for graph-based question answering. In the following section, we briefly introduce and contextualize spaCy, Stanford CoreNLP, and GraphRAG-three diverse and widely-used tools we have utilized for knowledge graph construction and integration with LLMs.", "4 Experiment\n4.1 Experimental Setup\n\nTo implement the MoT framework, we default to GPT-4o-mini as the LLM \ud835\udc3f . To ensure consistency with prior work [23], the dataset chunk size is set to 1200. We use the nano vector database for vector database operations. The hyperparameter settings for MoT are detailed in the Numerical Analysis subsection of this chapter. All experiments are conducted on a Linux server equipped with six NVIDIA A6000 GPUs.", "4.2 24-Point Game\n\nThe 24-Point Game is a classic mathematical puzzle task. The game rules are as follows: the player manipulates four integers, ranging from 1 to 9, using addition, subtraction, multiplication, and division operations to achieve a final result of 24. The order of operations can be flexibly adjusted as needed, and each of the four numbers can only be used once. This task serves as a benchmark for LLMs' mathematical reasoning abilities[12, 33]. It tests the LLM's arithmetic operations and strategic planning abilities, requiring exploration of various combinations and operation sequences to find the optimal solution. We tested the model on 100 randomly selected problems from the 4nums dataset. For each model, a result is labeled as successful if it includes an equation that complies with the rules. The baseline RATT, which uses the external knowledge base from RAG, is fully consistent with MoT.", "| | Method | Success Rate |\n|---:|:---------|---------------:|\n| 0 | IO | 0.17 |\n| 1 | CoT | 0.28 |\n| 2 | ToT | 0.36 |\n| 3 | RAT | 0.34 |\n| 4 | RATT-RAG | 0.42 |\n| 5 | RATT | 0.45 |\n| 6 | MoT-RAG | 0.52 |\n| 7 | MoT | 0.66 |\n\nTable 1: Success rates of different methods.", "As shown in Table 1, the success rate of MoT reached a maximum of 66%, while the baseline methods, such as RATT, did not exceed", "45%. Interestingly, compared to the RATT method with the RAG module, our MoT-RAG method, which excludes the RAG module, still maintains a leading success rate, while the RATT-RAG method, without the RAG module, did not experience a significant drop in success rate. We believe that in logical and numerical reasoning tasks, the framework's own reasoning and thinking ability is key to solving the problem. If the reasoning ability is insufficient, even with sufficient input data, there remains the possibility of errors.", "4.3 Question Answering Evaluation\n\nIn this experiment, we use NaturalQuestions [14] for single-hop QA evaluation, and HotpotQA [29] together with 2WikiMultihopQA (2WMQA) [9] for multi-hop QA evaluation. To ensure consistency with prior work, we sample 500 instances from each dataset. We use the F1 score and Exact Match (EM) as evaluation metrics for all three datasets. We compare MoT with GPT-4o-mini under direct prompting, Zero-shot CoT[13], Few-shot CoT[26], and ToT, and against RAG-based baselines including vanila RAG[15], ReAct[31], IRCoT[23], FLARE[11], Self-Rag[1], SearChain[28], Rowen[3], SlimPLM[22], LightRAG[8], CoA[34], RAT[25], and RATT[32]. For RATT, we set its internal RAG knowledge base to be the same as that of MoT.", "Results Analysis: We compared MoT with the aforementioned baselines, as shown in Table 1. Based on the comparison metrics, we reached the following conclusions: First, MoT outperforms all baselines. Compared to non-RAG baseline methods, our approach improves F1 by 5.3% to 9.1% and EM by 6.2% to 8.9%. Furthermore, when compared to RAG-based and thought-structure-based methods, we still maintain a lead of at least 3% in both F1 and EM scores. This is particularly evident on multi-hop question-answering datasets such as HotpotQA and 2WMQA, demonstrating the effectiveness of the proposed MoT paradigm and knowledge units. The column-cell communication mechanism of MoT focuses on exploring multibranch thinking and enhancing the breadth of responses to optimize answer quality. Second, we found that some RAG-based methods did not fully outperform the non-RAG baselines, possibly due to inefficient retrieval mechanisms that introduce noise or erroneous information, misleading the LLM's responses.", "4.4 Proposition Writing\n\nTo address the issues in Multi-metric performance evaluation, we build on the method of Guo et al. [6] and design a simple yet effective protocol to evaluate, in a multidimensional manner, the response quality of QA models for open-domain QA tasks. We selected datasets from four domains-biology, law, computer science, and physics-from the UltraDomain dataset[17] to construct the external knowledge base, with each dataset containing millions of tokens. Following the method of Edge et al[6], we retrieved open-ended questions from the relevant domains. For each question, we guided GPT-4o to evaluate MoT and the baseline models in four dimensions: Comprehensiveness, Accuracy, Empowerment, and Overall performance. The results were used to determine the winning rate and provide the rationale for the judgment.", "Results Analysis: As shown in Table 3, we compare the win rates between MoT and the aforementioned baselines pairwise. It is evident that our method achieves a win rate of over 80% across most dimensions and in the overall assessment, further validating the effectiveness of the MoT approach. Among them, the non-reasoningstructure method (LightRAG) achieves a win rate of no more than 10% across the four datasets. This further supports our viewpoint that, under similar knowledge enhancement, reasoning structures play a critical role in the utilization of knowledge and the logical reasoning ability, significantly influencing the overall quality of the answers. The chain-based reasoning structure methods (CoA, RAT) achieve win rates ranging from 9.6% to 25.2%, with the win rate for comprehensiveness assessment consistently lower than that of other dimensions. This indicates that while chain-based reasoning structures have a positive effect on answer generation, their lack of multi-branch reasoning capability results in a noticeable disadvantage in comprehensiveness compared to tree-based structures (e.g., RATT) and matrix-based structures (e.g., MoT). The win rates for the tree-based reasoning structure method (RATT) range from 12% to 34.4%. We attribute this advantage to the column-cell communication mechanism employed by MoT, which better stimulates the LLMs' multi-branch reasoning strategies for answering the questions, thereby yielding higher-quality answers.", "4.5 Ablation Study\n\nSubsequently, we conducted an ablation study and developed four variants of MoT to demonstrate the effectiveness of each mechanism within MoT. The comparison is performed using the single-hop QA dataset NQ and the multi-hop QA dataset HotpotQA. The experimental results are shown in Table 4.", "The \"-RAG\" variant removes the RAG correction module, resulting in significant performance degradation on two datasets due to knowledge boundaries and LLM hallucinations. However, it outperforms non-RAG methods, demonstrating the superiority of the Matrix of Thought (MoT) over other thought structures. The \"origin\" variant removes the use of original text in knowledge units, leading to some performance loss, especially on multi-hop datasets, where the performance gap between \"-origin\" and \"-KG\" is more pronounced, highlighting the advantage of KG-based data in multihop tasks. The \"-KG\" variant removes knowledge graph triples from the knowledge unit, showing noticeable performance loss due to the lack of structured multi-hop information in the retrieval process. The \"-comm\" variant, which disables column-cell communication by setting the communication weight matrix to 0, effectively reduces MoT to a tree structure. The results show that removing the communication mechanism leads to substantial performance loss, even exceeding the performance decline seen in the \"-KG\" and \"-origin\" variants, emphasizing the dominant role of the thought structure in LLM reasoning, beyond just enhanced knowledge.", "4 Experiments\n4.1 Setup\n\nDatasets Following the experimental setups of [10, 54, 25], we evaluate our approach across following datasets: (1) Natural Questions (NQ) [32], a large-scale QA dataset based on real Google search queries. (2) RGB [7], a RAG benchmark that evaluates models' ability to utilize retrieved information, focusing on noise robustness, negative rejection, information integration, and counterfactual robustness. We use its English test set for evaluation. (3) HotpotQA (HQA) [60], which requires multi-hop reasoning over multiple documents, featuring both compare and bridge question types: compare questions involve contrasting information from multiple sources, while bridge questions require connecting intermediate facts to reach the answer. We evaluate all methods on their dev set, reporting results under the categories Compare, Bridge, and Total in the main results table. (4) 2WikiMultihopQA (2WQA) [22], which presents a more challenging multi-hop QA scenario with four distinct task types: comparison (comparing information), bridge comparison (connecting intermediate facts for comparison), inference (deriving conclusions), and compositional (integrating multiple facts). We use its dev set for evaluation, reporting separate performance for each question type (denoted as Comapre, Bridge, Inf, Compose and Total) in the experimental results.", "Baselines To demonstrate the broad effectiveness, we evaluate it on four widely used language models: Llama-2-7B-Chat-hf (Llama2-7B) [52], Mistral-7B-v0.1 (Mistral-7B) [27], DeepSeek-llm7B-base (DeepSeek-7B) [4], and Qwen3-8B [51]. We evaluate LFD against three decoding strategies: (1) Greedy Decoding (GD) is the standard autoregressive decoding method that selects the highestprobability token at each generation step. To further examine the effect of noise, we augment the GD strategy with varying numbers of irrelevant documents (4, 8, and 12) added to the prompt context. The abbreviations GD (0), GD (4), GD (8) and GD (12), as used in the main results table, refer to greedy decoding with 0, 4, 8, 12 noise documents added to the prompt context, respectively (2) Contrastive Search (CS) [48] promotes more comprehensive outputs by balancing response quality and diversity. (3) Decoding by Contrasting Layers (DoLA) [9] is a contrastive decoding approach that reduces hallucinations by comparing predictions across different model layers. (4) LFD (Random) is a variant that randomly selects an intermediate layer to fuse during the final decoding stage.", "Evaluation Metrics We use accuracy as our primary evaluation metric. For the NQ and RGB datasets, which include samples with multiple acceptable answer variants (e.g., alternative phrasings of the same concept), we follow the evaluation protocol established in [10, 30, 38]. A model's response is marked correct if it matches any annotated ground-truth answer.", "Experimental Setting Both LFD and DoLA require the implementation of dynamic layer selection strategies. For LFD, we prioritize layers in the latter half of the architecture (e.g., layers 16-32 for 32-layer models like Llama2-7B and Mistral-7B). In contrast, DoLA selects layers across the entire depth (layers 0-32) based on divergence from the final layer's predictions. Similar configurations apply to DeepSeek-7B (30 layers: LFD uses 15-30; DoLA uses 0-30) and Qwen3-8B (36 layers: LFD uses 18-36; DoLA uses 0-36). Following DoLA's convention, we restrict candidates to evennumbered layers within these ranges for efficiency. Contrastive Search uses a degeneration penalty \u03b1 = 0 . 6 and top-k candidate size k = 5 , adopting the parameters from [48]. Since each sample", "| | | NQ.NQ | RGB.RGB | HotpotQA.Compare | HotpotQA.Bridge | HotpotQA.Total | 2WikiMultihopQA.Compare | 2WikiMultihopQA.Bridge | 2WikiMultihopQA.Inf | 2WikiMultihopQA.Compose | 2WikiMultihopQA.Total |\n|---:|:-------------|--------:|----------:|-------------------:|------------------:|-----------------:|:--------------------------|-------------------------:|----------------------:|--------------------------:|------------------------:|\n| 0 | GD (0) | 0.5745 | 0.89 | 0.4755 | 0.4108 | 0.4237 | 0.3095 | 0.2815 | 0.2607 | 0.2928 | 0.2904 |\n| 1 | GD (4) | 0.5559 | 0.8267 | 0.5084 | 0.4049 | 0.4257 | 0.2559 | 0.2564 | 0.2048 | 0.2405 | 0.2433 |\n| 2 | GD (8) | 0.5965 | 0.83 | 0.5091 | 0.4096 | 0.4317 | 0.3257 | 0.2983 | 0.2789 | 0.2995 | 0.303 |\n| 3 | GD (12) | 0.6383 | 0.82 | 0.5286 | 0.4236 | 0.4447 | 0.3204 | 0.3202 | 0.2646 | 0.2943 | 0.3027 |\n| 4 | CS | 0.5775 | 0.8567 | 0.4284 | 0.4049 | 0.4096 | 0.2800 | 0.2462 | 0.2464 | 0.2866 | 0.2712 |\n| 5 | DoLA | 0.5809 | 0.8733 | 0.4438 | 0.3883 | 0.3995 | 0.2738 | 0.2411 | 0.2289 | 0.2592 | 0.255 |\n| 6 | LFD (Random) | 0.5928 | 0.89 | 0.4385 | 0.4295 | 0.4313 | 0.3065 | 0.2845 | 0.2744 | 0.3066 | 0.2978 |\n| 7 | LFD | 0.5949 | 0.9067 | 0.5453 | 0.4295 | 0.4528 | 0.3174 | 0.3005 | 0.3043 | 0.3232 | 0.3144 |\n| 8 | GD (0) | 0.613 | 0.89 | 0.5864 | 0.5889 | 0.5884 | 0.5518 | 0.5204 | 0.5605 | 0.5337 | 0.5385 |\n| 9 | GD (4) | 0.5667 | 0.8033 | 0.5494 | 0.5348 | 0.5377 | 0.3681 | 0.3443 | 0.3173 | 0.3296 | 0.3406 |\n| 10 | GD (8) | 0.5678 | 0.77 | 0.5326 | 0.526 | 0.5273 | 0.2963 | 0.2874 | 0.2503 | 0.2582 | 0.2728 |\n| 11 | GD (12) | 0.5814 | 0.8267 | 0.544 | 0.5461 | 0.5457 | 0.3098 | 0.2888 | 0.2503 | 0.2655 | 0.2795 |\n| 12 | CS | 0.5598 | 0.7433 | 0.4149 | 0.5117 | 0.4922 | 0.4346 | 0.3873 | 0.4486 | 0.474 | 0.4423 |\n| 13 | DoLA | 0.6142 | 0.89 | 0.5931 | 0.587 | 0.5883 | 0.5366 | 0.5018 | 0.5572 | 0.5239 | 0.5262 |\n| 14 | LFD (Random) | 0.627 | 0.9133 | 0.5985 | 0.6016 | 0.6009 | 0.5687 | 0.5317 | 0.5767 | 0.5506 | 0.5541 |\n| 15 | LFD | 0.6357 | 0.9367 | 0.6026 | 0.6093 | 0.6079 | 0.5813 | 0.5434 | 0.5982 | 0.5645 | 0.568 |\n| 16 | GD (0) | 0.525 | 0.8233 | 0.3282 | 0.4033 | 0.3883 | 0.2625 | 0.2414 | 0.2663 | 0.2609 | 0.2609 |\n| 17 | GD (4) | 0.5216 | 0.83 | 0.3968 | 0.4437 | 0.4343 | 0.2807 | 0.2939 | 0.2744 | 0.273 | 0.2796 |\n| 18 | GD (8) | 0.5226 | 0.88 | 0.4506 | 0.4579 | 0.4564 | 0.3578 | 0.3603 | 0.3407 | 0.3463 | 0.3515 |\n| 19 | GD (12) | 0.5204 | 0.8933 | 0.4573 | 0.4618 | 0.4609 | 0.3701 | 0.3705 | 0.3349 | 0.3614 | 0.3622 |\n| 20 | CS | 0.5322 | 0.8 | 0.4028 | 0.4584 | 0.4472 | 0.3929 | 0.3833 | 0.3888 | 0.4077 | 0.3964 |\n| 21 | DoLA | 0.3755 | 0.5033 | 0.349 | 0.3003 | 0.31 | 0.3039 | 0.3118 | 0.2484 | 0.2594 | 0.2803 |\n| 22 | LFD (Random) | 0.4858 | 0.67 | 0.3847 | 0.3734 | 0.3757 | 0.3466 | 0.3563 | 0.3277 | 0.332 | 0.3403 |\n| 23 | LFD | 0.5412 | 0.8267 | 0.4801 | 0.4466 | 0.4533 | 0.4492 | 0.4227 | 0.4194 | 0.4223 | 0.4285 |\n| 24 | GD (0) | 0.7318 | 0.9571 | 0.696 | 0.6708 | 0.6759 | 0.6637 | 0.6335 | 0.5897 | 0.6085 | 0.625 |\n| 25 | GD (4) | 0.7213 | 0.9467 | 0.6658 | 0.6544 | 0.6567 | 0.6044 | 0.5729 | 0.4948 | 0.5268 | 0.5517 |\n| 26 | GD (8) | 0.7233 | 0.95 | 0.6584 | 0.6568 | 0.6571 | 0.6117 | 0.578 | 0.5078 | 0.537 | 0.5605 |\n| 27 | GD (12) | 0.7133 | 0.9533 | 0.653 | 0.6582 | 0.6571 | 0.6146 | 0.5722 | 0.5052 | 0.5462 | 0.5634 |\n| 28 | CS | 0.7204 | 0.9533 | 0.7081 | 0.6762 | 0.6826 | 0.6468 | 0.6204 | 0.5754 | 0.6015 | 0.6134 |\n| 29 | DoLA | 0.7168 | 0.9533 | 0.7014 | 0.6703 | 0.6766 | 0.6485 | 0.6058 | 0.565 | 0.5929 | 0.6057 |\n| 30 | LFD (Random) | 0.7357 | 0.9567 | 0.7108 | 0.6935 | 0.697 | | 0.6334 | 0.6014 | 0.6136 | 0.6289 |\n| 31 | LFD | 0.738 | 0.96 | 0.7182 | 0.6974 | 0.7016 | 0.6627 0.6663 | 0.6342 | 0.6034 | 0.6148 | 0.6301 |", "Table 1: The accuracy performance comparison of different methods on four datasets. Bold values indicate the best performance, while underlined values represent the second-best.", "in the aforementioned benchmark datasets is accompanied by multiple retrieved documents, we construct the input context using these documents, supplemented with golden documents, i.e., the ground-truth passages that contain the information necessary to answer the question. This setup, following prior work [33, 10, 31, 41], allows us to evaluate the model's ability to effectively leverage external knowledge when it is explicitly provided in the input context.", "4.2 Main Results\n\nTable 1 summarizes the accuracy of RAG based on four different models across four QA datasets. From the table, we have the following observations: (1) LFD matches or exceeds the performance of noise-injection strategies. We can see our method demonstrates consistent performance gains, ranging from minimal 0.29% (Qwen3-8B, RGB) to maximal 16.76% (DeepSeek-7B, 2WikiMultihopQA) improvement. Injecting noise, i.e., GD (12), shows performance gains on some datasets and models, with the highest improvement being 10.13% (DeepSeek-7B, 2WikiMultihopQA). However, it significantly degrades performance for Mistral-7B and Qwen3-8B across all datasets, leading to a notable reduction in accuracy (maximum \u2206 = -26 . 57 % on Mistral-7B). (2) LFD outperforms decoding strategies without noise injection. Compared to alternative decoding methods, LFD consistently delivers superior performance. While methods like DoLA and CS show strong results in specific cases, e.g., achieving 1.94% and 13.55% gains on DeepSeek-7B with the 2WikiMultihopQA dataset, they occasionally underperform even relative to the greedy decoding baseline. In particular, DoLA shows the largest decline of 30% on DeepSeek-7B with the RGB dataset, while CS exhibits a maximum drop of 14.57% on Mistral-7B with the RGB dataset. These results indicate that both strategies are sensitive to specific model architectures or data characteristics. Additional comparisons of these methods under different noise levels are provided in Appendix C. (3) The fusion layer selection plays a critical role for RAG. Our dynamic layer selection strategy consistently outperforms the random approach, with an average improvement of 3.11%, particularly achieving a 15.67% gain on DeepSeek-7B/RGB, demonstrating its efficacy.", "Figure 5: Comparison between LFD and LFD (Fixed) on the NQ (a) and HotpotQA (b) datasets. (c) illustrates the layer selection distribution in LFD compared to the optimal fixed layer selection.\n\n20 22 24 26 28 30\n\nLayer 0.57 0.58 0.59 0.60 0.61 Accuracy LFD GD(0) LFD (Fixed)\n\n(a) LFD vs. LFD (Fixed) on NQ dataset.\n\n0.36 0.38 0.40 0.42 0.44\n\n(b) LFD vs. LFD (Fixed) on HotpotQA dataset.\n\n8 1012141618202224262830 0.0 0.1 0.2 0.3 0.4 Probability Density Frequency Normal Distribution ( =26.7, =2.3)\n\nMean Layer = 26.7\n\n(c) Histgram of layer selection in LFD.", "4.3 Analysis Experiments\n\n| | | | NQ | RGB. | HotpotQA.Compare | HotpotQA.Bridge | HotpotQA.Total | 2WikiMultihopQA.Compare | 2WikiMultihopQA.Bridge | 2WikiMultihopQA.Inf | 2WikiMultihopQA.Compose | 2WikiMultihopQA.Total |\n|---:|:-----------|:------------|-------:|-------:|-------------------:|------------------:|-----------------:|--------------------------:|-------------------------:|----------------------:|--------------------------:|------------------------:|\n| 0 | | GD (0) | 0.5745 | 0.89 | 0.4755 | 0.4108 | 0.4237 | 0.3095 | 0.2815 | 0.2607 | 0.2928 | 0.2904 |\n| 1 | | LFD[0, 16) | 0.5758 | 0.8833 | 0.4371 | 0.4027 | 0.4096 | 0.2956 | 0.2582 | 0.2529 | 0.2739 | 0.2731 |\n| 2 | | LFD[16, 32) | 0.5949 | 0.9067 | 0.5454 | 0.4295 | 0.4528 | 0.3174 | 0.3005 | 0.3043 | 0.3232 | 0.3144 |\n| 3 | | GD (0) | 0.613 | 0.89 | 0.5864 | 0.5889 | 0.5884 | 0.5518 | 0.5204 | 0.5605 | 0.5337 | 0.5385 |\n| 4 | Mistral-7B | LFD[0, 16) | 0.6081 | 0.8867 | 0.5864 | 0.5919 | 0.5908 | 0.5551 | 0.5218 | 0.565 | 0.5402 | 0.5429 |\n| 5 | Mistral-7B | LFD[16, 32) | 0.6357 | 0.9367 | 0.6026 | 0.6093 | 0.6079 | 0.5813 | 0.5434 | 0.5982 | 0.5645 | 0.568 |", "Table 2: Comparison of accuracy between different layer selection ranges under dynamic layer selection strategy. Bold indicate the best performance, while underline represent the second-best. LFD[0, 16) and LFD[16, 32) mean selecting layers from the earlier and later half respectively."]}
{"type": "entry", "index": 44, "question": "what\u2019s a good way to score faithfulness when the answer pulls from multiple papers", "answer": "A practical way to score faithfulness when the answer draws on multiple papers is to evaluate it at the level of individual facts and check each one against the retrieved sources.\n\n1. Define faithfulness\nFaithfulness is how consistent the answer is with the retrieved evidence, not whether it sounds good or is globally true [1][2][4]. In many works it is tied directly to hallucination: ungrounded or contradicting statements are hallucinations, grounded ones are faithful [3][4].\n\n2. Break the answer into fact units\nDecompose the answer into short, self-contained statements (\"factoids\"). MetaRAG does this to test each fact against evidence [4].\n\n3. Match each factoid to supporting passages\nFor each factoid, check whether at least one retrieved chunk from any of the papers supports it. If a fact is unsupported or contradicted, mark it as unfaithful (hallucinated) [3][4]. This is similar in spirit to \"Grounded Accuracy\" in e\u2011commerce QA, which only counts answers that are both factually correct and grounded in context [3].\n\n4. Aggregate into a faithfulness score\nYou can compute a faithfulness score like:\n- Faithful facts / total facts (high is better), and/or\n- Hallucination rate = ungrounded or incorrect facts / total facts (lower is better) [3].\n\n5. Use an LLM evaluator if needed\nSeveral RAG evaluations already use LLM-based judges to score faithfulness to retrieved evidence and detect hallucinations [1][2]. You can prompt such a judge to:\n- See all retrieved passages from all papers,\n- See the answer,\n- Label each factoid as supported, contradicted, or unsupported,\n- Then output an overall faithfulness score.\n\nThis approach naturally handles answers that weave together multiple papers, because any fact can be justified by any one of the retrieved sources.", "tags": ["benchmarks_eval"], "full_faithfulness": 1.0, "full_answer_relevancy": 0.9494953497, "seed_faithfulness": 0.8148148148000001, "seed_answer_relevancy": 0.9445032462, "context_relevance": 0.49854802440590273, "context_relevance_error": null, "context_relevance_reason": "The context passages provide relevant information about evaluating faithfulness in generated answers, particularly in relation to the metrics used for assessing answer quality and consistency with retrieved knowledge. However, the response does not directly address the user's question about scoring faithfulness when answers pull from multiple papers, leading to a lack of specific guidance on that aspect.", "structure_citation": 0.744673764061005, "structure_error": null, "structure_reason": "The Actual Output effectively outlines a method for scoring faithfulness by breaking down the answer into fact units and matching them to supporting passages, which aligns well with the evaluation steps. It includes citations [1], [2], [3], and [4], which correspond to relevant snippets in the Context. However, while the claims are generally supported, the response could benefit from clearer connections to specific snippets for each citation, as some claims may not be directly backed by the provided Context, leading to potential unsupported claims.", "hallucination_score": 0.7738525679703241, "hallucination_error": null, "hallucination_reason": "The response provides a clear and structured approach to scoring faithfulness, aligning well with the evaluation steps. It defines faithfulness, breaks down the answer into fact units, and suggests methods for matching factoids to supporting passages, which are relevant to the context provided. However, while the claims made are generally supported by the context, there is a lack of direct references to specific studies or metrics mentioned in the context, which slightly detracts from the overall alignment.", "contexts_used": ["Experiment Setup\n\nWe evaluate GridCodex against two baselines:\n\nGrid code compliance with general LLMs: Queries are directly submitted to general-purpose LLMs with minimal prompt engineering and region-specific metadata. Guardrails are included to suppress hallucinations when the model encounters queries outside its knowledge.", "Grid code compliance with vanilla RAG: This baseline incorporates RAPTOR-enabled knowledge bases (Sarthi et al. 2024). Questions are used to retrieve relevant chunks, which are appended to the user queries before being processed by a general LLM. Prompt-level guardrails are added to mitigate hallucinations when retrieval fails.\n\nWe evaluate system performance on three key metrics:\n\nAnswer Quality: It evaluates accuracy (correctness), completeness (coverage of essential points), and usefulness (relevance to the query). Accuracy is prioritized, with completeness and usefulness downweighted when accuracy is low.", "Faithfulness: It measures consistency between the generated answers and the retrieved knowledge chunks.\n\nRecall@30: It assesses whether the information necessary to answer a query is contained within the top 30 retrieved chunks.", "To ensure fairness and objectivity, we adopt automated scoring using LLM-based evaluators. Carefully designed prompts and expert-provided reference answers from the electricity industry guide the evaluation.", "Models\n\nThe GridCodex pipeline integrates carefully selected LLMs and embedding models, chosen to balance retrieval accuracy, reasoning ability, answer quality, and computational efficiency:\n\nEmbedding model: Linq-Embed-Mistral (Choi et al. 2024) is used to construct vector embeddings of knowledge chunks. Trained on high-quality synthetic data, it delivers strong precision and reliability in semantic search.", "RAPTOR summarization: DeepSeek-R1-0528 (Guo et al. 2025) is employed for hierarchical document summarization, leveraging strong reasoning capabilities for semantic clustering.", "Query refinement and translation: For terminology expansion and translation, we adopt Qwen3-32B-AWQ (Yang et al. 2025), which balances refinement quality and hardware efficiency.", "Answer synthesis and scoring: Qwen3-235B-A22B (Yang et al. 2025), a mixture-of-experts (MoE) model, is used for final answer generation and automated scoring, providing concise, faithful responses.", "Model Size: Large vs. Mid-size\n\nTo study the role of scale, we compare: (1) Qwen3-235B-A22B (Yang et al. 2025), the flagship model with 235 billion total parameters and 22 billion active, optimized for long-context reasoning, and (2) Qwen3-30B-A3B (Yang et al. 2025), a 30B-parameter variant with 3B active, designed for faster inference and reduced hardware costs.", "Figure 4: Overall performance of different systems across regions. (Subject to aesthetic changes.)\n\nAnswer Quality Faithfulness Recall@30 0.0 0.2 0.4 0.6 0.8 1.0 1.2 Score 69.6% N/A 68.2% 97.0% 9.5% 88.0% 96.9% 93.2% Overall Performance General LLMs Vanilla RAG GridCodex\n\n| | Model | Answer Quality | Faithfulness |\n|---:|:----------|-----------------:|---------------:|\n| 0 | 235B-A22B | 0.852 | 0.968 |\n| 1 | 30B-A3B | 0.74 | 0.995 |\n\nTable 3: Model size impact (Netherlands dataset).", "As shown in Table 3, scaling from 30B to 235B parameters improves answer quality by 11.2%. The larger model demonstrates stronger multi-hop reasoning, cross-clause referencing, and ambiguity resolution due to greater representational capacity and expert routing. However, this gain comes at a cost: faithfulness drops slightly (0.968 vs. 0.995), as the 235B model integrates retrieved context more aggressively, occasionally introducing inferred connections not explicitly present in the documents. This highlights a key trade-off-larger models enable richer reasoning but may lean toward speculative integration, whereas smaller models remain more conservative and strictly grounded in retrieved evidence.", "6 Datasets and Evaluation Benchmarks\n\nThe evaluation of IR and RAG systems relies on diverse datasets that test different aspects of retrieval and answer generation capabilities, including the retrieval, ranking, and answer quality.", "6.1 IR Specific\n\nInformation retrieval has a long history of evaluation campaigns, including those from TREC, CLEF, NTCIR, and FIRE. The queries used in these collections are often developed to strike a balance between having too many relevant documents in the target collection (which can be too easy to retrieve and too difficult to properly annotate (Voorhees et al., 2022)) and too few relevant documents. Sometimes challenging topics are also developed deliberately (Voorhees, 2005). The topic development process often involves manual reformulation of queries to ensure good coverage of relevance assessments. Mirroring the annotation process itself, it can be beneficial for automated retrieval systems to also perform various forms of query understanding (expansion or rewriting) to help ensure high recall. Hence, the approaches described in Section 3 show performance gains.", "Comprehensive benchmarks like BEIR (Thakur et al., 2021) offer heterogeneous evaluation across 17 diverse datasets spanning multiple domains and retrieval tasks, enabling zero-shot generalization assessment. In addition, evaluation sets such as TREC DL (Deep Learning) and its variants (Craswell et al., 2020, 2021, 2022, 2023) are used to evaluate IR systems.", "6.2 Question Answering\n\nQuestion Answering datasets for retrieval evaluation fall into two primary categories based on complexity. Single-hop QA benchmarks include Natural Questions (NQ) (Kwiatkowski et al., 2019), TriviaQA (Joshi et al., 2017), SQuAD (Rajpurkar et al., 2016), and PopQA (Mallen et al., 2023) test the RAG system's ability to retrieve and utilize information to generate the final answer. Multi-hop QA benchmarks comprise 2WikiMultiHopQA (Ho et al., 2020) (questions requiring evidence from exactly two Wikipedia articles via bridging/comparison), HotpotQA (Yang et al., 2018), and MuSiQue (Trivedi et al., 2022), which features compositionally complex questions constructed from interconnected sub-questions. Due to the dependency on intermediate answers, such questions cannot be answered through isolated single-step retrieval. Therefore, this design naturally motivates the RAG systems to decompose the original query and new queries based on the intermediate answers help in the retrieval of better context.", "6.3 Fact Checking/Verification\n\nThese datasets assess models' ability to verify claims against retrieved evidence. FEVER (Fact Extraction and VERification) (Thorne et al., 2018) is an open-domain fact-checking benchmark that requires retrieval over a large collection of Wikipedia articles and training NLI models to classify claims as supported, refuted, or not having enough information based on Wikipedia evidence. While FEVER deals with simple claims, HoVeR (Jiang et al., 2020) proposes a fact-checking benchmark with claims that require multi-hop reasoning and multi-turn retrieval. QuanTemp (V et al., 2024) is the first to propose a large open-domain benchmark for fact-checking numerical claims. It comprises claims that require interleaving claim decomposition and retrieval based on the verification output of sub-claims. This relates to query-level feedback and generation-time feedback. AveriTeC (Schlichtkrull et al., 2023) is a real-world claim verification dataset that includes diverse claim types with supporting or refuting evidence gathered through web search, making it particularly valuable for evaluating RAG systems on authentic fact-checking scenarios that require verification against potentially noisy or contradictory web sources.", "6.4 Complex Reasoning\n\nEmerging benchmarks introduce novel relevance criteria requiring awareness of reasoning structure. BRIGHT (SU et al., 2025) defines document relevance not by topical alignment but by whether passages contain logical constructs (deductive steps, analogies, constraints) necessary to derive answers. This challenges lexical retrievers that lack inference-awareness. However, reasoningaugmented retrieval methods, such as CoT (Chainof-Thought), have shown performance gains. Further, there exist reasoning/agentic tasks like Deep Research (Wu et al., 2025), GPQA (Rein et al., 2023), MATH500 (Cobbe et al., 2021), which involve access to the web search engine, have been used in the RAG setting (Li et al., 2025a; Wei et al., 2025). However, due to the absence of the cor- pus, it is hard to evaluate the retrieval performance. Recent benchmarks like BrowseComp-Plus (Chen et al., 2025b) provide a curated corpus for Deep Research tasks and enable the evaluation of retrieval performance.", "6.5 Evaluation Metrics\n\nEvaluating retrieval systems and RetrievalAugmented Generation (RAG) pipelines is critical for ensuring the accuracy, relevance, and reliability of generated responses. Retrieval evaluation typically focuses on metrics such as recall@k, precision, mean reciprocal rank (MRR), and hit rate, which assess how effectively the system retrieves pertinent documents or passages given a query. In contrast, RAG evaluation is more holistic, combining retrieval quality with generation fidelity and coherence. Common approaches include measuring answer correctness using exact match (EM) or F1 score, assessing faithfulness to retrieved evidence to detect hallucinations, and evaluating relevance and fluency through human or automated scoring (e.g., BLEU, ROUGE, or BERTScore). Recent frameworks like RAGAS (ExplodingGradients, 2024), ARES (Saad-Falcon et al., 2024), and CRUX (Ju et al., 2025) also emphasize end-to-end evaluation, where the interplay between retrieval accuracy and generation quality is analyzed to identify bottlenecks-such as irrelevant documents leading to incorrect answers-making comprehensive evaluation essential for diagnosing and improving RAG system performance.", "Challenges . The current RAG evaluation methods mainly focus on the retrieval and final answer performance. However, Reasoning RAG systems are highly dependent on intermediate reasoning steps and retrieval rounds. Therefore, it is also important to consider additional evaluation dimensions such as computational cost, efficiency, or number of retrieval rounds.", "II. Answer Evaluation Metrics\n\nWe evaluate generated answers based on four distinct scenarios: (S1) context is sufficient and an answer is provided; (S2) context is insufficient and the system responds with 'IDK'; (S3) context is insufficient but the system attempts a factual answer (not IDK); and (S4) context is sufficient but the answer is 'IDK'. Key metrics include Factuality/Faithfulness , which measures whether answers are factually correct and grounded in the retrieved context, computed as Grounded Accuracy = \ud835\udc41 \ud835\udc46 1 ,\ud835\udc39\ud835\udc36 \ud835\udc41 \ud835\udc46 1 . Answer Completeness assesses if all sub-questions are answered, given by Completeness = \ud835\udc41 \ud835\udc46 1 ,\ud835\udc36\ud835\udc5c\ud835\udc5a\ud835\udc5d \ud835\udc41 \ud835\udc46 1 . Precision quantifies the fraction of valid factual answers among all attempted (S1 and S3): Precision = \ud835\udc41 \ud835\udc46 1 ,\ud835\udc3a\ud835\udc5c\ud835\udc5c\ud835\udc51 \ud835\udc41 \ud835\udc46 1 + \ud835\udc41 \ud835\udc46 3 . Recall measures the fraction of correctly answered, truly answerable queries (S1 and S4): Recall = \ud835\udc41 \ud835\udc46 1 ,\ud835\udc3a\ud835\udc5c\ud835\udc5c\ud835\udc51 \ud835\udc41 \ud835\udc46 1 + \ud835\udc41 \ud835\udc46 4 . Accuracy reflects overall correctness, rewarding both valid S1 answers and correct S2 IDK responses: Accuracy = \ud835\udc41 \ud835\udc46 1 ,\ud835\udc3a\ud835\udc5c\ud835\udc5c\ud835\udc51 + \ud835\udc41 \ud835\udc46 2 \ud835\udc40 , where \ud835\udc40 is the total number of queries.\"Good\" requires both factual correctness AND completeness. Finally, the Hallucination Rate captures the proportion of unsupported or incorrect answers (S3 and incorrect S1), calculated as Hallucination Rate = \ud835\udc41 \ud835\udc46 3 + \ud835\udc41 \ud835\udc46 1 ,\ud835\udc35\ud835\udc4e\ud835\udc51 \ud835\udc40 .", "Table 5 summarizes generation performance across these metrics and user intent types, highlighting the system's strengths in managing contextual ambiguity and mitigating hallucinations.", "| | Intent | Coverage (%) | Precision (%) | Recall (%) | Hallucination (%) |\n|---:|:---------------------|:---------------|:----------------|:-------------|:--------------------|\n| 0 | authenticity | 94.0% | 98.2% | 98.4% | 1.69% |\n| 1 | checkout | 85.9% | 98.0% | 99.5% | 1.32% |\n| 2 | delivery_sla | 92.4% | 96.6% | 98.4% | 3.11% |\n| 3 | offers_and_discounts | 90.85% | 96.42% | 98.62% | 2.90% |\n| 4 | payment_options | 95.9% | 96.5% | 97.6% | 3.36% |\n| 5 | product_exchange | 79.4% | 91.3% | 92.9% | 6.89% |\n| 6 | product_spec | 86.4% | 97.2% | 98.7% | 1.93% |\n| 7 | return_policy | 84.0% | 96.7% | 96.9% | 2.72% |\n| 8 | size_and_fit | 35.2% | 90.3% | 94.8% | 3.42% |\n| 9 | stock_availability | 51.4% | 95.1% | 97.1% | 2.50% |\n| 10 | variant | 77.6% | 94.4% | 95.8% | 4.38% |\n| 11 | warranty | 90.0% | 97.7% | 98.7% | 2.00% |\n\nTable 5: Answer Generation Performance Across Intents: Coverage, Precision, Recall, and Hallucination Rate", "2 Related Works\n\n2.1 Definitions of Hallucination\n\nThe term hallucination has been used with varying scope across natural language generation tasks. Some studies emphasize factuality , describing hallucinations as outputs that contradict established facts, i.e., inconsistencies with world knowledge or external ground truth [31, 13]. Others highlight faithfulness , where hallucinations occur when generated responses deviate from the user instruction or a reference text, often producing plausible but ungrounded statements particularly in source-conditioned tasks such as summarization or question answering [26]. Beyond these two dimensions, researchers also note cases of incoherent or nonsensical text that cannot be clearly attributed to factuality or faithfulness criteria [14, 23].", "Alternative terms have also been introduced. Confabulation draws on psychology to describe fluent but fabricated content arising from model priors [9], while fabrication is preferred by some to avoid anthropomorphic connotations [1, 21]. More recently, Chakraborty et al. [3] propose a flexible definition tailored to deployment settings, defining a hallucination as a generated output that conflicts with constraints or deviates from desired behavior in actual deployment, while remaining syntactically plausible under the circumstance.", "2.2 Hallucination Detection in LLMs\n\nBuilding on these definitions, hallucinations have been recognized as a major challenge in text generation. Early work in machine translation and abstractive summarization described them as outputs that are not grounded in the input source [23, 16, 15], motivating the development of evaluation metrics and detection methods for faithfulness and factual consistency across natural language generation tasks.", "More recent reference-free (unsupervised or zero-reference) methods aim to detect hallucinations without gold-standard labels by analyzing the model's own outputs. A prominent method is SelfCheckGPT [22], a zero-resource, black-box approach that queries the LLM multiple times with the same prompt and measures semantic consistency across responses. The intuition is that hallucinated content often leads to instability under stochastic re-generation; true facts remain stable, while fabricated ones diverge. Manakul et al. show that SelfCheckGPT achieves strong performance in sentence-level hallucination detection compared to gray-box methods, and emphasize that it requires no external database or access to model internals [22]. However, SelfCheckGPT may struggle when deterministic decoding or high model confidence leads to repeating the same incorrect output.", "2.3 Metamorphic Testing\n\nMetamorphic Testing (MT) [4] was originally proposed in software engineering to address the oracle problem in which the correct output is unknown. MT relies on metamorphic relations (MRs): transformations of the input with predictable effects on outputs, enabling error detection without access to ground truth [28]. In machine learning, MThas been applied to validate models in computer vision [5] (e.g., rotating an image should not change its predicted class) and NLP [27]\n\nIn hallucination detection for LLMs, MetaQA [32] leverages MRs by generating paraphrased or antonym-based question variants and verifying whether answers satisfy expected semantic or logical constraints. Relying purely on prompt mutations and consistency checks, MetaQA achieves higher precision and recall than SelfCheckGPT on open-domain QA.", "Researchers have also adapted MT for more complex conversational and reasoning settings. MORTAR [12] applies dialogue-level perturbations and knowledge-graph-based inference to multi-turn systems, detecting up to four times more unique bugs than singleturn MT. Drowzee [18] uses logic programming to construct temporal and logical rules from Wikipedia, generating fact-conflicting test cases and revealing rates of 24.7% to 59.8% across six LLMs in nine domains [32].", "These works highlight the promise of MT for hallucination detection, but they primarily target open-book QA or multi-turn dialogue, often over short, single-sentence outputs. Prior studies have not addressed hallucination detection in retrieval-augmented generation (RAG) scenarios over proprietary corpora, a setting in which ground-truth references are unavailable and model internals are inaccessible. MetaRAG builds on MT by decomposing answers into factoids and designing MRs tailored to factual consistency against retrieved evidence in a zero-resource, black-box setting."]}
{"type": "entry", "index": 45, "question": "how do i test it on \u201cnew paper contradicts old paper\u201d situations", "answer": "To test a system on \u201cnew paper contradicts old paper\u201d cases, you want an evaluation where two separate texts about the same topic disagree, and the model must (a) notice the conflict and (b) pinpoint where it is.\n\nThe MAGIC benchmark is designed exactly for inter\u2011document conflicts like this. It builds pairs of contexts from a knowledge graph, then slightly perturbs one of them so that the two texts end up making incompatible claims\u2014for example, different release orders of songs or different genres for the same work.[1] These are then converted into natural text passages, giving you two realistic, human\u2011like documents that disagree in subtle ways, often requiring multi\u2011step reasoning to see the contradiction.[1]\n\nTo test your system in a \u201cnew vs. old paper\u201d setting using this style:\n1. Treat each MAGIC instance as two papers: Context A = older paper, Context B = newer paper.[1]\n2. Feed both texts to your model via your usual RAG or reading pipeline.\n3. Ask it explicitly: (a) Is there any conflict between these two documents? and (b) If yes, quote or summarize the conflicting statements. MAGIC\u2019s own evaluation measures both conflict detection (Identification score) and precise localization of the conflicting spans (Localization score).[1]\n4. Score your system the same way: it gets credit only if it correctly says there is a conflict and points to the exact conflicting claims.[1]\n\nMAGIC includes both simple single\u2011hop and harder multi\u2011hop conflicts, so you can see how well your system handles straightforward and subtle contradictions, which closely mimics real \u201cnew paper overturns old paper\u201d situations.[1]", "tags": ["_untagged"], "full_faithfulness": 0.5833333333, "full_answer_relevancy": 0.8812546640000001, "seed_faithfulness": 0.7916666667000001, "seed_answer_relevancy": 0.8812546640000001, "context_relevance": 0.8129051644428735, "context_relevance_error": null, "context_relevance_reason": "The context passages provide substantial information about knowledge conflict in retrieval-augmented generation systems, which directly relates to the user's question about testing scenarios where new papers contradict old ones. The passages discuss the challenges of detecting conflicts and the proposed framework for evaluating inter-context knowledge conflicts, which aligns well with the user's inquiry. However, while the context is relevant, it could be more explicitly focused on practical testing methods for the specific scenario mentioned, which slightly limits the score.", "structure_citation": 0.8314206979239959, "structure_error": null, "structure_reason": "The Actual Output effectively addresses the evaluation steps by discussing the MAGIC benchmark, which is relevant to testing systems on inter-document conflicts. It includes multiple [1] markers that correspond to specific snippets in the Context, demonstrating a clear understanding of the task. However, while the claims made are generally supported by the Context, there is a slight lack of explicit citations for some claims, which could lead to minor deductions in the score.", "hallucination_score": 0.9108062771886773, "hallucination_error": null, "hallucination_reason": "The response effectively addresses the input by outlining a clear methodology for testing systems in scenarios where new papers contradict old ones. It accurately describes the MAGIC benchmark's purpose and how it generates inter-document conflicts, aligning well with the context provided. The steps for testing, including conflict detection and localization, are detailed and relevant, demonstrating a strong understanding of the evaluation criteria. However, a minor shortcoming is the lack of explicit mention of how to handle specific types of contradictions beyond the general framework, which could enhance clarity.", "contexts_used": ["Abstract\n\nKnowledge conflict often arises in retrievalaugmented generation (RAG) systems, where retrieved documents may be inconsistent with one another or contradict the model's parametric knowledge. Existing benchmarks for investigating the phenomenon have notable limitations, including a narrow focus on the question answering setup, heavy reliance on entity substitution techniques, and a restricted range of conflict types. To address these issues, we propose a knowledge graph (KG)-based framework that generates varied and subtle conflicts between two similar yet distinct contexts, while ensuring interpretability through the explicit relational structure of KGs. Experimental results on our benchmark, MAGIC, provide intriguing insights into the inner workings of LLMs regarding knowledge conflict: both open-source and proprietary models struggle with conflict detection-especially when multi-hop reasoning is required-and often fail to pinpoint the exact source of contradictions. Finally, we present in-depth analyses that serve as a foundation for improving LLMs in integrating diverse, sometimes even conflicting, information.", "1 Introduction\n\nRetrieval-augmented generation (RAG) has become the de facto standard for enhancing the performance of large language models (LLMs) by enabling updates to outdated knowledge and facilitating adaptation to specialized domains (Lewis et al., 2020). While effective, RAG's heavy reliance on retrieval quality introduces inherent risks. For instance, knowledge obtained from external sources may conflict with the model's parametric knowledge or contain inconsistencies within the retrieved documents themselves.", "Knowledge conflict (KC) is a recent research topic that covers issues related to the aforemen-\n\n\u2020 Equal contribution. * Corresponding author.", "Figure 1: Example of a three-hop conflict from our benchmark, MAGIC. Even advanced LLMs struggle to detect subtle inconsistencies across two contexts, such as conflicting release orders of two songs.\n\nKnowledge Graph A Context A \u22ef Shine was released after Bloom\n\nand included in the album\n\n+\n\n, which was followed by\n\nRepack\nKnowledge Graph B\npart of\n\nNo conflicts are detected!\n\nIs there a\nknowledge conflict\n\nbetween the two contexts?\n\nContext B\nwas followed by\n\n.\n\nThe song is part of , and \ud83d\udca5 (Imply \u00e0 )\n\n(The model fails to detect the 3-hop conflict.)\n\nfollowed\nby\n\ntioned scenarios and has been receiving attention in the field (Xu et al., 2024). An ideal LLM-based system should be robust against knowledge conflict, capable of integrating information from multiple sources-including those that may be contradictory-and ultimately generating reliable responses. However, its implementation is largely hindered by the challenge of detecting whether disagreements exist across different knowledge sources and, if so, pinpointing exactly where they occur.", "Numerous benchmarks have been introduced to evaluate the performance of LLMs in knowledge conflict detection (Hsu et al., 2021; Li et al., 2024; Jiayang et al., 2024; Hou et al., 2024). Nonetheless, we emphasize that existing research in this area has notable limitations. First, previous studies primarily focus on the question answering (QA) task, where conflicts occur only among multiple answer candidates for a given question (Chen et al., 2022; Xie et al., 2024; Marjanovic et al., 2024). Second, prior research often relies on overly simplistic techniques for dataset construction, e.g., entity substitution (Longpre et al., 2021; Chen et al., 2022), which are insufficient to capture the complex and subtle nature of knowledge conflicts. Third, while some studies attempt to categorize types of knowledge conflict (Hou et al., 2024; Marjanovic et al., 2024), systematic analysis distinguishing between forms, such as single-hop vs. multi-hop conflicts, is still lacking. Finally, existing benchmarks are largely concerned with exploring conflicts between parametric and external knowledge, while conflicts among multiple input documents remain underexplored (Jiayang et al., 2024; Hou et al., 2024). 1", "To alleviate these issues, we propose a framework for constructing a benchmark targeting intercontext knowledge conflict . It leverages knowledge graphs (KGs) as the primary source, from which subgraphs are extracted to serve as the basis for distinct knowledge chunks. These subgraphs are then perturbed by modifying nodes and edges to introduce conflicts. Finally, both original and altered graphs are converted into corresponding text passages using KG-to-text generation algorithms.", "By design, the proposed framework offers several advantages. Leveraging the relational structure of KGs enables greater diversity, complexity, and control in inducing conflicts within documents. Moreover, compared to text-based strategies, our approach improves interpretability and supports structured analysis by representing conflicting entities and relations in graph form (see Figure 1).", "Lastly, we conduct extensive analyses using MAGIC (A M ulti-Hop A nd G raph-Based Benchmark for I nterC ontext Conflicts), a novel dataset constructed through our framework. 2 MAGIC features complex inter-context conflict patterns, including simultaneous and multi-hop cases rarely seen in existing benchmarks. Experiments on MAGIC yield key insights into how LLMs perceive knowledge conflicts: (1) most models remain imperfect at detecting conflicts, especially when multi-hop reasoning is required; and (2) even when contradictions are detected, models often fail to localize the exact point of conflict.", "1 Knowledge conflict is commonly studied in three setups (Xu et al., 2024): (1) context-memory conflict , involving inconsistencies between parametric and external knowledge; (2) inter-context conflict , where contradictions exist among multiple input documents; and (3) intra-memory conflict , where inconsistent responses reflect variation in an LLM's training data. In this work, we focus on inter-context conflict .\n\n2 Available at: https://github.com/HYU-NLP/MAGIC.", "2 Related Work\n\nKnowledge conflict benchmarks Early studies on synthetically inducing sentences with knowledge conflicts (KC) primarily rely on entity substitution (Chen et al., 2022), which is often subsequently polished and paraphrased using LLMs (Xie et al., 2024; Gokul et al., 2025). Regarding domains and tasks, most prior work focuses on potential inconsistencies arising in QA, such as contradictory answers provided for a given question (Longpre et al., 2021). However, this paradigm has a clear limitation: most conflicts must appear near answer-related contexts, which strictly limits the range of possible variations. To overcome this, we adopt question-free, KG-based methods, which inherently support more diverse problem types. 3", "Inter-context knowledge conflict detection evaluates an LLM's ability to identify contradictions either across multiple input contexts (Jiayang et al., 2024; Hou et al., 2024; Marjanovic et al., 2024) or within a single document (Li et al., 2024). Existing benchmarks follow two main strategies: collecting conflicts curated from Wikipedia (Hou et al., 2024) and generating artificial ones using LLMs (Jiayang et al., 2024). The former offers realism but limited coverage, while the latter is scalable yet often less natural and less representative of real-world scenarios. We aim to combine the strengths of both by leveraging the factual grounding of KGs and the generative fluency of LLMs, ensuring both high quality and scalability. Finally, we highlight that little work has examined performance with respect to fine-grained conflict types. Departing from the common convention of using only two conflict categories, 4 we organize our dataset by the number of conflicts and the reasoning hops required for resolution, enabling more systematic analysis.", "KG-based dataset creation KGs play a crucial role in diverse tasks, e.g., fact verification (Kim et al., 2023), QA (Chen et al., 2024), and RAG (Sanmartin, 2024), by providing structured representations of knowledge. In addition, KGs can serve as valuable resources for dataset construction. For instance, Meng et al. (2022) introduce COUNTERFACT, a dataset designed to evaluate factual consistency and modifications in LLMs. In this paper, we also utilize KGs as a foundation\n\n3 Concurrently, Gokul et al. (2025) propose a query-free method but rely solely on LLMs, unlike our KG-based one.", "4 Jiayang et al. (2024): answer & factoid; Hou et al. (2024): explicit & implicit; Marjanovic et al. (2024): static & dynamic.", "Figure 2: Overview of the proposed KG-based framework for benchmarking inter-context knowledge conflict detection. It comprises three steps: (1) Subgraph Extraction, (2) Knowledge Conflict Generation, and (3) KG-to-Text Conversion, with details listed in \u00a73.\n\n(Still Losing You, genre,\n\nRock music)\n<Original Triplet>\n\nStep 2. Knowledge Conflict Generation\n\n\u2026\n\nStep 3. KG-to-Text Conversion\n\n<Original Context>\n\n\u22ef Still Losing You belongs\n\nto rock music genre \u22ef\n\nStep 1. Subgraph\nExtraction\n<Perturbed Context>\n\n\u22ef Still Losing You is a song of\n\nC&W and\ndistinct from\nthe\n\nalbum She Loves Car. It has\n\nmusic genre rock music \u22ef\n\n<Perturbed Triplet>\n\n(Still Losing You, instance of, C&W)\n\n(C&W,\n\n, She Loves Car)\n\n(She Loves Car, genre, Rock music)\n\nfor generating realistic and nuanced conflict statements. Bi et al. (2024) shares similarities with our work, as it also employs Wikidata triplets to induce knowledge conflicts. However, their approach is limited to retrieving seed entities for substitution rather than leveraging the full structure of KGs.", "4 Experimental Setups\n\nWith the MAGIC benchmark as a foundation, we conduct experiments to examine how LLMs handle inter-context knowledge conflicts. We evaluate various open-source and proprietary LLMs without task-specific training, instead prompting them to identify potential contradictions. In the following, we outline the LLMs, datasets, prompting strategies, and metrics used in our experiments.", "LLMs We use 5 LLMs: Llama 3.1 70B Instruct (Dubey et al., 2024), o1 (OpenAI, 2024b), Mixtral8x7B Instruct (MistralAI, 2023), Claude 3.5 Haiku (Anthropic, 2024), GPT-4o-mini (OpenAI, 2024a).", "Datasets Alongside MAGIC, we employ two existing benchmarks to highlight its strengths.\n\nECON (Jiayang et al., 2024): A dataset created by introducing evidence conflicts through two methods-answer conflicts and factoid conflicts-highlighting contradictions in supporting evidence. It contains 168 data instances.", "WikiContradict (Hou et al., 2024): A humanannotated QA benchmark utilizing Wikipedia's contradiction tags to capture real-world knowledge conflicts. It categorizes contradictions into explicit and implicit types. After deduplication, it comprises 103 data samples.", "MAGIC : It is constructed atop KGs, where conflicts are systematically induced from the un-", "| | Models / Datasets | ECON | WikiContradict | MAGIC |\n|---:|:--------------------|-------:|-----------------:|--------:|\n| 0 | Mixtral 8x7B | 46.43 | 52.43 | 37.92 |\n| 1 | Llama 3.1 70B | 81.41 | 78.79 | 72.86 |\n| 2 | Claude 3.5 Haiku | 83.33 | 61.17 | 60.28 |\n| 3 | GPT-4o-mini | 88.1 | 82.52 | 83.61 |\n| 4 | o1 | 74.4 | 74.76 | 68.06 |\n| 5 | Average | 74.73 | 69.93 | 64.54 |\n\nTable 2: ID scores (%) on three KC detection datasets. Lower scores indicate higher task complexity. The lowest value in each row is shown in bold .", "derlying relational structure. It encompasses both single-hop and multi-hop contradictions, with the number of conflicts dynamically varying across context pairs. In total, MAGIC consists of 1,080 carefully curated examples, with comprehensive statistics presented in Table 1. Remarkably, the scale of MAGIC surpasses that of existing benchmarks by a significant margin, offering a richer and more challenging resource for evaluating inter-context conflict detection.", "Prompting strategy Prior work (Jiayang et al., 2024; Hou et al., 2024) typically frames the task as a binary classification problem, relying on minimal prompts (see Appendix B). In contrast, we introduce a stepwise prompting strategy (refer to Figure 17 for details) designed to more fully probe the capabilities of LLMs for inter-context conflict detection.", "Metrics To account for the stochasticity of LLMs, all models perform three independent inference runs. We use two metrics for fine-grained evaluation, with scores averaged across all instances in each dataset. These metrics are manually computed by participating researchers, as automatic methods-such as LLM-as-a-judge-remain insufficiently reliable for this task. 13\n\nIdentification ( ID ) score: If a model fails to detect a conflict in any of the three attempts, it receives a score of 0; otherwise, it receives 1.", "Localization ( LOC ) score: We further evaluate LLMs' performance on conflict localization. A full score (1) is awarded only if all conflicting locations are correctly identified; otherwise, the score is 0. Note that this fine-grained evaluation has not been considered in previous work.", "13 If they were, further investigation into knowledge conflict detection would be unnecessary. See Appendix H for details.", "6 Further Analysis and Discussion\n\nTaxonomy-based analysis To systematically analyze conflict patterns, we apply our proposed conflict typology to ECON and WikiContradict. Rep-\n\n16 Still, exceptions exist depending on the used model.", "Figure 10: Comparison of ID and LOC scores on 1Single-Hop conflicts across three datasets.\n\n74.02 81.12 70.91 63.14 67.82 62.99 ECON WikiContradict MAGIC Score (%) ID Score LOC Score\n\nresenting these datasets as graphs allows us to highlight their characteristics. A key challenge is the lack of predefined ontologies or domain structures, which hinders the use of traditional ontology-based methods (van Cauter and Yakovets, 2024). To address this, we use LangChain (Chase, 2022) to construct reliable, schema-free KGs that support structured conflict representation.", "Figure 3 shows that 1-Single-Hop conflicts-the typically easiest case-are the most prevalent in prior datasets, accounting for 78% in ECON and 76% in WikiContradict. In contrast, MAGIC exhibits a balanced distribution across conflict types, with substantial proportions of 1-Multi-Hop (28%) and N-Multi-Hop (27%), underscoring its robustness as a benchmark.", "Length-based analysis In conflict detection, a reasonable hypothesis is that context length positively correlates with dataset difficulty, as longer documents tend to involve more complex linguistic structures. To validate this, we present lengthoriented anlysis in Figure 9. The left panel visualizes the total context length distribution among the three datasets, with MAGIC containing more longcontext examples. We suspect MAGIC's multi-hop and description-rich design contributes to this.", "Further, we group MAGIC into four bins based on context length and report their respective performance in the right panel of Figure 9. To balance group sizes, we use quantile-based binning: Q1 contains the shortest contexts, and Q4 the longest. As context length increases, both scores decline, with LOC dropping more sharply-indicating the growing difficulty of pinpointing conflicting spans in longer inputs.", "Comparison with existing KC datasets In addition to the results discussed in \u00a75, we compare MAGICwith existing datasets by focusing on the 1Single-Hop type-the only category shared across\n\nFigure 11: Comparison of difficulty between challenging subsets of WikiContradict and MAGIC, with scores averaged over all five LLMs.\n\n29.25 54.34 45.87 61.37 Accuracy (%)\n\nFigure 12: Comparison of ID scores for binary and multi-step prompts, tested on two models.\n\n55.61 59.22 78.57 83.61 82.52 88.1 GPT-4o-mini 34.42 45.63 50.6 73.83 78.79 81.41 Llama 3.1 70B Binary Multi-step (Ours)\n\nall datasets-to ensure a fair and consistent comparison. In ECON and WikiContradict, conflict categories are manually annotated. Figure 10 shows that even on relatively simple 1-Single-Hop conflicts, models perform worse on MAGIC than on ECON and WikiContradict, suggesting that our dataset poses challenges beyond surface-level contradiction.", "We further conduct a comparative study between the challenging subset of WikiContradict (Implicit) and the corresponding subset from our dataset (Multi-Hop in MAGIC). 17 Figure 11 reports that MAGIC proves more difficult for LLMs than WikiContradict. MAGIC yields an ID score up to 9% lower and a LOC score up to 16% lower compared to WikiContradict. This highlights MAGIC's intrinsic complexity, setting it apart from existing datasets.", "Impact of prompts While prior work (Jiayang et al., 2024; Hou et al., 2024) typically employs binary (yes/no) prompts for conflict detection-often oversimplifying the task-we use a multi-step technique that helps improve performance. To compare the effectiveness of these two distinct prompting strategies for conflict detection, we conduct experiments, with results reported in Figure 12.", "17 WikiContradict consists of explicit and implicit conflicts; the latter are considered more challenging due to their subtlety.", "Figure 13: ID scores of two LLMs on known (O) vs. unknown (X) instances, based on their parametric knowledge. Models show slightly stronger conflict detection when the relevant knowledge is already embedded.\n\nX\nO\n\n(Estimated) Existence of Parametric Knowledge\n\nLlama 4o-mini 69.39 69.51 72.55 79.71 ID Score (%)\n\nAcross all cases, the results show that our multistep prompting approach outperforms the na\u00efve prompt, achieving improvement up to 39.41%. This implies that although we employ a method superior to those commonly used in the literature, there remains substantial room for addressing the challenges posed by MAGIC.", "Impact of parametric knowledge In the literature on inter-context conflict, research has primarily focused on conflicts between two input documents, overlooking the influence of parametric knowledge which can significantly affect the performance of LLMs in knowledge conflict detection.", "To address this, we explore an underexamined setup by splitting a subset of MAGIC-instances with 1- and 2-conflicts-into two groups based on the estimated presence of parametric knowledge in LLMs. Concretely, we approximate the existence of parametric knowledge for a given triplet by posing a converted verification question-for example, 'Is Barack Obama Sr. the father of Barack Obama?'. We label a triplet as known if the model provides the correct answer in at least 4 out of 5 attempts, and as unknown if it succeeds in no more than 1.", "As shown in Figure 13, both GPT-4o-mini and Llama 3.1 achieve slightly higher ID scores on instances with prior knowledge. These findings suggest that models may detect conflicts more effectively when they already possess the relevant factual knowledge, although a more systematic investigation is left for future work.", "Impact of context order To probe whether context order influences model behavior, we evaluate both the original MAGIC and a variant in which the two contexts in each instance are reversed.", "As reported in Table 4, the ID scores remain largely consistent across models, with Mixtral 8x7B showing a slight increase and others nearly unchanged. These results indicate that context or-\n\n| | Models | Original | Swap | \u2206 (Swap-Orig) |\n|---:|:-----------------|-----------:|-------:|----------------:|\n| 0 | Mixtral 8x7B | 37.92 | 46.45 | 8.53 |\n| 1 | Llama 3.1 70B | 73.83 | 75.64 | 1.81 |\n| 2 | Claude 3.5 Haiku | 61.11 | 61.01 | -0.1 |\n| 3 | GPT-4o-mini | 83.61 | 81.86 | -1.75 |\n\nTable 4: Effect of context order on ID scores in MAGIC.", "| | Metric | ECON | WikiContradict | MAGIC |\n|---:|:------------|-------:|-----------------:|--------:|\n| 0 | Naturalness | 4.36 | 4.39 | 4.08 |\n| 1 | Realism | 4 | 4.72 | 4.26 |\n\nTable 5: Naturalness and realism scores for three KC datasets, evaluated by Claude 3.7 Sonnet.", "der does not significantly affect model performance on MAGIC, suggesting that our findings are robust to input ordering.", "Linguistic quality of MAGIC An essential requirement for MAGIC is that its conflict data be both realistic and stylistically similar to humanwritten text. To validate this, we use an LLM judge (Claude 3.7 Sonnet) to evaluate two aspects: (i) naturalness -whether the text is fluent and humanlike-and (ii) realism -whether the scenario appears plausible in real-world contexts. We randomly sample 155 contexts from three datasets to match overall length distributions. Each context is rated on a 0-5 scale, with the detailed evaluation prompts provided in Figures 18 and 19.", "As shown in Table 5, MAGIC achieves naturalness scores close to WikiContradict (-0.31), despite WikiContradict relying on real Wikipedia text. It further surpasses ECON in realism, showing that MAGIC generates scenarios that are both plausible and human-like. These results underscore MAGIC's reliability as a benchmark for advancing research in knowledge conflict detection."]}
{"type": "entry", "index": 46, "question": "does reranking bias toward certain writing styles/venues/authors or am i paranoid", "answer": "You\u2019re not paranoid\u2014reranking can absolutely introduce biases, including toward certain venues, authors, and indirectly toward particular writing styles.\n\n1. Bias from what features you use\nIn some systems, papers aren\u2019t just ranked by topical relevance, but also by \"quality\" signals like citation counts, author h\u2011index, and venue reputation.[2] This is explicit in QUAL-SG: papers are re-ranked using citation performance, author influence, and venue reputation.[2] That naturally favors:\n- Well\u2011cited, older or mainstream topics\n- Famous authors with high h\u2011indexes\n- Prestigious or \u201ccore\u201d venues\nLess-cited but innovative or niche work\u2014and authors from less-known venues\u2014will be pushed down.\n\n2. Bias from training data\nRerankers are trained on large collections of query\u2013document pairs; if those datasets over-represent certain domains, article structures, or writing styles, the model will implicitly learn to favor them.[3][4] The GeoGPT reranker emphasizes cleaning out \u201cnoisy or biased examples\u201d precisely because even a small number can \u201csignificantly degrade reranking performance,\u201d which includes skewing what it prefers.[3]\n\n3. Bias acknowledged but often not measured\nOne health-RAG study explicitly notes its limitation: it assumes relevant documents are already retrieved and \u201comits retrieval-stage factors such as ranking errors and bias,\u201d and calls for future work to study retrievers and rerankers under real-world conditions.[1] That\u2019s essentially an admission that reranking bias is real but under-explored.\n\nSo yes, reranking can systematically favor certain venues/authors and, via those signals, particular academic styles. It\u2019s not just in your head; it\u2019s a design and data issue.", "tags": ["rerankers"], "full_faithfulness": 0.7000000000000001, "full_answer_relevancy": 0.8758143842, "seed_faithfulness": 0.8, "seed_answer_relevancy": 0.8758135454, "context_relevance": 0.15555671730722426, "context_relevance_error": null, "context_relevance_reason": "The context passages primarily focus on methodologies and experimental setups related to reranking and evaluation metrics, but they do not directly address the user's question about whether reranking introduces bias toward certain writing styles, venues, or authors. While there is mention of bias-controlled pooling setups, the overall relevance to the user's specific concern is minimal, leading to a low score.", "structure_citation": 0.46602165449373983, "structure_error": null, "structure_reason": "The Actual Output includes claims supported by citations, but there are issues with citation matching. While citations [2], [3], and [4] are referenced, they do not correspond to any snippets in the provided Context, leading to unsupported claims. Additionally, the claim about bias in reranking is acknowledged but lacks direct support from the Context, which affects the overall reliability of the response.", "hallucination_score": 0.7409667366685813, "hallucination_error": null, "hallucination_reason": "The response accurately addresses the user's concern about reranking biases, providing specific examples of how biases can arise from feature selection and training data. It effectively references the context by discussing the implications of biased evidence pools and acknowledges the limitations of current studies, aligning well with the factual claims made in the input. However, while the response is strong, it could benefit from more explicit connections to the context provided, particularly regarding the experimental setups mentioned.", "contexts_used": ["3 Methodology\n3.3 Experimental Setup\n\nBased on the dataset statistics in Table 1, the number of possible document pairs quickly grows large. For example, with 193 helpful and 121 harmful documents in TREC 2020, the helpful-harmful condition alone would generate over 23,000 possible pairs; when combined with six adversarial variants per query in both TREC 2020 and 2021, the number of pairs becomes even larger. To keep computation feasible, we only sample up to ten document pairs for each condition per query. If there are fewer than ten valid pairs, all available combinations are included. Each document pair is then combined with user query framings to assess how sequencing and user assumptions affect the model outputs together.", "This setup mainly contributes to RQ3 by testing the influence of evidence order and combinations, while also providing insight into RQ2 by extending the single-document analysis to multi-document contexts, and RQ4 by evaluating how different query framings shape ground-truth alignment outcomes.", "3.3.3 Passage-Based Pooling Setup. The passage-based pooling setup moves beyond controlled pairings. In this setup, all documents in the dataset are first segmented into smaller chunks to enable segment-level retrieval. We segment each Manuscript submitted to ACM", "document with a sliding window of 512 words and a 256-word overlap. These segments, regardless of their original classification as helpful, harmful, or adversarial, are subsequently reranked using the MonoT5 reranker [54]. For each query, the ten highest-ranked segments are retained to form the retrieval pool, which is then provided to the language model as context in combination with one of three user query formulations. This setup thus reflects how LLMs would behave under conventional retrieval conditions, where retrieved evidence is shaped by ranking models rather than manually controlled.", "This setup primarily addresses RQ3 by examining the impact of various evidence types on model behavior, while simultaneously enhancing RQ2 by evaluating the persistence of document type effects observed in controlled environments within a conventional retrieval pipeline. This setup also examines RQ4 by analysing the interplay between query formulations and non-uniform retrieval pools in establishing ground-truth alignment results.", "3.3.4 Bias-Controlled Pooling Setup. The bias-controlled pooling setup examines how context pools that are biased towards a certain type of evidence (helpful or harmful) affect how models respond. This setup is inspired by the methodological framework of Pogacar et al. [61], who showed that imbalances in evidence presentation can affect human judgment. We apply this concept to the RAG context by creating two skewed retrieval pools: one biased towards helpful evidence and the other biased towards harmful evidence . We used the MonoT5 reranker to process each category of document segments (helpful, harmful, and adversarial) independently for every query, producing the top-10 helpful, top-10 harmful, and top-10 adversarial results for each attack type. These segments are then grouped into retrieval pools of fixed size (ten segments). In the helpful-biased condition, the eight highest-ranked segments from the top-10 helpful list are placed at the top of the pool, while the remaining two highest-ranked segments are selected from the top-10 harmful or adversarial lists. In the harmful-biased condition, the eight highest-ranked segments from the top-10 harmful or adversarial list are placed first, followed by the two highest-ranked segments from the top-10 helpful list. This setup contributes to RQ3 by testing imbalanced evidence and RQ4 by examining how user query formulations interact with skewed pools to affect alignment.", "3.4 Evaluation\n\nTo evaluate model performance across different experimental setups, we use a single metric, Ground-Truth Alignment (i.e., accuracy), which measures whether the stance of a generated response matches the ground-truth stance label for the query. A response is considered aligned if its predicted stance is identical to the ground-truth stance.", "Prior work has shown that LLMs can perform reliably as automatic evaluators [16, 29], often approaching humanlevel agreement. Following this line of work, we employ gemini-2.0-flash as an external stance classifier. To further validate our evaluation pipeline, we conduct a secondary classification of all responses using gpt-4o-mini and measure inter-rater agreement with gemini-2.0-flash and compute Cohen's Kappa score [26]. Agreement was 0.90 for TREC 2020 and 0.82 for TREC 2021. These values indicate what is conventionally characterized as 'almost perfect' agreement between the two classifiers and indicate the reliability of automated stance detection. We use gemini-2.0-flash as the only stance classifier for all of the experiments to avoid relying on the same model family for both generation and evaluation.", "For each experimental setup, we compute the Ground-Truth Alignment Rate, defined as the proportion of aligned responses relative to the total number of generated responses. The prompt used for stance classification of LLM responses is publicly available in the project's github repository.", "TREC 2021 Non-RAG Helpful Harmful Rewriter Paraphraser Fact-Inversion FSAP-IntraQ FSAP-InterQ Liar Ground-Truth Alignment (%) TREC 2020 GPT-4.1 Consistent Query Neutral Query Inconsistent Query", "Fig. 1. Results of the single-document setup with GPT-4.1 on TREC2020 and TREC 2021 . The x-axis shows the different document types provided to the LLMs, as well as the Non-RAG baseline, and the y-axis reports ground-truth alignment rate (%). Error bars indicate 95% confidence intervals estimated via bootstrapping. Across both datasets, providing helpful documents leads to the highest alignment rate, while presenting Liar documents results in the lowest. In addition, the results reveal a consistent user query framing trend in which the alignment rate is highest with consistent queries, lower with neutral queries, and lowest with inconsistent queries.", "6 Limitations\n\nOur study advances the understanding of RAG robustness under adversarial evidence, but it has limitations. First, we rely on ground-truth alignment as the primary evaluation metric, which does not fully capture other important aspects of response quality, such as completeness, clarity, or potential user trust effects. Future work should employ multidimensional evaluations to provide a more comprehensive assessment of system performance in sensitive domains.", "Second, our design assumes that relevant documents are already retrieved. This lets us isolate how document content and query framing influence model behavior, but omits retrieval-stage factors such as ranking errors and bias. Future work should extend this analysis to full RAG pipelines to examine how adversarial documents interact with retrievers and rerankers under real-world conditions.", "Finally, our evaluation relies on automated stance classification with gemini-2.0-flash, validated using gpt-4o-mini. While prior work shows LLMs can align well with human judgments [16, 29], automated classifiers may miss subtle inaccuracies or nuanced risks. Future work should include human-in-the-loop or multi-annotator studies to strengthen validity in safety-critical domains.", "2 Approach\n\n2.2 SurveyGen : Dataset Construction\n\nSince title-based filtering may still include nonsurvey articles, we applied an additional filtering step using abstracts to further refine the candidate set. Specifically, inspired by previous work that LLMs are effective as NLI models for evaluating factual consistency (Gubelmann et al., 2023; Chiang et al., 2024), we prompted three LLMs to clas-\n\n2 Data collected from S2ORC up to December 2024.", "sify whether a candidate paper is a survey-type article based on its title and abstracts, following three criteria: (1) Explicit declaration of survey intent (e.g., 'conducts a survey' or 'provides a survey' ). (2) Focus on survey papers, rather than proposing novel methodologies or experimental results. (3) Discussion of field-specific trends, challenges, or future directions. Papers without abstracts were excluded at this step. Based on these criteria, 6,851 out of 8,676 papers were identified as survey articles by a majority vote of the LLMs.", "We then retrieved the full-text data of these surveys using their paper IDs from the S2ORC bulks 3 . Here, the full-text data includes the full body of the survey with section divisions , as well as the citation locations of each reference within the survey. This allows us to obtain the structural outline of each survey paper and map references to their corresponding sections, which serve as the key input for Task 3. At this point, we removed papers that had fewer than 30 references or fewer than three top-level sections, as they are too short to serve as meaningful surveys. Finally, we obtained 4,205 papers suitable for survey generation and constructed the SurveyGen dataset, which includes 115,376 sections, 242,143 references directly cited within the surveys, and 5,062,596 references cited by these cited papers.", "The data format is outlined in Appendix H. Table 1 compares SurveyGen with other datasets for scientific document summarization.", "2.3 Quality-Related Indicators Supplement\n\nTo facilitate citation-based evaluation, we first supplemented all survey papers and their directly cited references with basic metadata (e.g., abstract, DOI,\n\n3 https://api.semanticscholar.org/api-docs/", "Figure 2: Overview of the QUAL-SG framework, which comprises two main stages: paper retrieval and survey generation . The retrieval stage includes three steps: (1) retrieving topic-relevant papers, (2) expanding with frequently co-cited papers, and (3) enriching them with quality-related metadata. Based on the retrieved set, the generation stage first re-ranks the papers from three key aspects, then prompts LLMs to perform tasks under different input conditions. Finally, we evaluate the generated surveys against human-written ones across multiple dimensions.\n\nS2ORC Survey topic Co-citation papers \u2026 h-index, i10-index, 2-year citedness, core journal h-index, publication count, total citation count citation count, influential citation count Citation performance Venue reputation Author influence Related papers Step 1:Paper Retrieval Step 2:Survey Generation Relevance Quality Diversity (1) Paper re-ranking (2) Task design | Quality indicators (3) Evaluation\n\nCitation:\n\n\u2606\n\nContent:\n\nStructural:\n\nLLM Generate VS Human Written Task1\n\n:\n\nSurvey fully generated by LLMs Task2 Survey generated from retrieved references Task3 human-curated references LLMs Topic\n\npublication venue, date, and research fields) from S2ORC, linked via corpus IDs. However, S2ORC does not provide sufficient metadata to measure the impact of academic papers.", "To address this limitation, we used DOIs of the involved papers to retrieve their corresponding metadata from the OpenAlex 4 (Priem et al., 2022a) database and enriched them with additional quality-related signals. Specifically, we incorporated three well-known bibliometric indicators to measure the quality of scientific publications: (1) citation performance : citation count and influential citation count 5 ; (2) author influence : h-index, publication count, and total citation count; and (3) venue reputation : h-index, i10-index, and CORE status of the publication venue (journal or conference) (Hicks et al., 2015; Donthu et al., 2021). As a result, each survey is now paired with its full text, complete with section divisions, and linked to its directly cited references. These references are further enriched with comprehensive multi-level quality indicators retrieved from OpenAlex, providing a robust foundation for evaluating generated surveys across multiple dimensions, such as citation accuracy, content quality, and structural consistency.", "Second-Level References Supplement: In some cases, influential works relevant to a survey may not be semantically aligned with its primary topic. For example, when retrieving literature on 'Deep Learning' , seminal works such as the 'Backpropagation algorithm' (Rumelhart et al., 1986) may obtain low semantic similarity scores as their titles and abstracts do not explicitly mention the topic. However, such papers are frequently cited by other retrieved references and are widely rec-\n\n4 https://openalex.org/\n\n5 Citations identified by Semantic Scholar as impactful in context, rather than mere mentions in the bibliography.", "ognized as foundational to the field. To address this issue and support advanced citation network analysis, we further enriched the metadata for 5.06 million references cited by the papers referenced in all surveys. For each of these references, we extracted essential bibliographic details, including the title, abstract, DOI, and citation count.", "2.4 QUAL-SG: QUALity-aware Literature Retrieval for Survey Generation\n\nWe propose QUAL-SG, a quality-aware extension of the naive RAG framework designed to improve the quality of retrieved literature for survey generation (Task 2). The overall framework is illustrated in Figure 2. As described in Section 2.1, in the Naive-RAG framework, the survey topic is used as a query to retrieve relevant papers from external databases. Formally, let q denote the topic derived from the human-written survey. Each candidate paper d i in the external database is represented by its abstract embedding. We define the semantic similarity score as:\n\nwhere v q and v d i are the embedding vectors of the query and the abstract of document d i , respectively.", "2.4 QUAL-SG: QUALity-aware Literature Retrieval for Survey Generation\n\nThe topn with the highest embedding similarity scores are selected to form the initial candidate set D = { d 1 , d 2 , . . . , d n } , where n is set to exceed the number of references in the corresponding human-written survey to ensure sufficient candidate coverage.", "Ethics Statement\n\nData Collection, Ethics, and Licensing. Our SurveyGen dataset is constructed based on S2ORC (Lo et al., 2020), a large corpus of scientific papers released under the CC BY-NC 4.0 11 . The dataset includes metadata extracted from the papers, such as author names, venue names, citation counts, and h-index values. No sensitive personal data (e.g., contact details or affiliations) is included. All metadata was collected in compliance with the terms of their sources and is used strictly for noncommercial academic research. The dataset is not intended for ranking or evaluating individuals or venues. We are committed to handling the data responsibly and ethically and will release our dataset under the same non-commercial license to ensure transparency and responsible data usage.", "Caution about Use of LLMs. While our QUALSG framework leverages LLMs to generate scientific surveys and strives to maintain the factual accuracy of the literature, there remains a concern of factual inconsistencies during the generation process. We advise users to critically evaluate the generated content, especially when using it for subsequent scientific research or practical applications. The LLM-generated survey is for reference only and should not be regarded as a substitute for peerreviewed articles or expert judgment.", "d. Reranker Model Fine-tuning\n\nThe reranker is a crucial component in Retrieval-Augmented Generation (RAG) systems. It refines the initial set of retrieved candidates by reordering them based on their semantic relevance to the query. Unlike embedding models, which encode queries and documents independently into fixed-length", "vectors, the reranker jointly processes the query and document as input and directly predicts a relevance score. By modeling token-level interactions between the query and document, the reranker captures richer semantic relationships, resulting in improved ranking of the top candidates. Given a query-document pair, the model outputs a raw relevance score, which is typically passed through a sigmoid function to produce a normalized value in the range [ 0 , 1 ] .", "Model and Training methods. We adopt an industry-standard cross-encoding scheme for reranking and use the BGE-M3 model (Chen et al., 2024) as our base. BGE-M3 is known for its strong generalization capabilities and is supported by a robust open-source ecosystem, making it well-suited for real-time applications.", "During training, we feed both positive and negative samples corresponding to each query into the model simultaneously. To optimize ranking performance, we apply the Multiple Negative Ranking (MNR) loss, defined as:\n\nwhere \ud835\udc41 is the total number, \ud835\udc36 is the number of negative samples per query, \ud835\udc5d pos is the predicted relevance score for the positive document, and \ud835\udc5d neg \ud835\udc56 is the score for the \ud835\udc56 -th negative document. This loss encourages the model to rank the positive document higher than its negative counterparts, thereby improving retrieval precision in downstream tasks.", "Dataset and Quality Improvement. To maximize the reranker's performance in the geoscience domain, we curated a large training corpus composed of both open-source and domain-specific ranking pairs. The domain-specific data were synthesized using Qwen, conditioned on abstracts from open-access scholarly articles in geography sourced from arXiv.", "After assembling all positive query-document pairs, we applied SimANS (Zhou et al., 2022) to mine hard negative samples from the candidate corpus, thus forming a balanced and challenging training dataset. Since both synthetic and open-source data were generated in batches, individual data points may vary in quality. Even a small number of noisy or biased examples can significantly degrade reranking performance. To address this, we implemented a data quality filtering process to improve the reliability of training examples. For each positive pair, we used the LLaMA3 model to assess answer quality based on a set of predefined criteria. Each sample was assigned a quality score ranging from 0 to 3, following the standards detailed in Table 8. Only samples with sufficient quality were retained for training.Data with a quality label of 0 is excluded from training.", "| | quality label | quality standards |\n|---:|:----------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 0 | 3 2 1 0 | completely correct, detailed, and covers all key information with clear and easy-to-understand expression. mostly correct, covers most key information, and is clearly expressed but with minor shortcomings. generally correct but has some errors or omissions, with mostly clear expression. significant errors or omissions, lacks sufficient detail, and is not clearly expressed. |\n\nTable 8 | data quality standards\n\nWe ultimately utilized diverse types of paired data to train the model. Open-source data included:\n\nQA pairs: DuReader (He et al., 2017), NQ, LCQMC (Liu et al., 2018), HotpotQA, etc.", "Retrieval-oriented pairs: T2Ranking (Xie et al., 2023), mMARCO, MS MARCO, etc.\n\nSimilarity task pairs: NLI (Koreeda and Manning, 2021), NLI-zh, AFQMC (Lee et al., 2022), PAWSX (Yang et al., 2019), etc.\n\nMultilingual data: Mr. TyDi.", "In total, we incorporated over 360,000 open-source data pairs, along with 30,000 synthetically generated QA pairs specifically tailored for geological science applications.", "For training with contrastive objectives, negative sample quality plays a critical role in reranker performance. Whenever available, we retain the original negative samples provided within each dataset. However, many datasets either lack negatives entirely or contain an insufficient number of challenging examples. To ensure training robustness, we construct a set of at least 16 negative samples per query for all datasets.", "When additional negatives are required, we use a weak retriever to select hard negative candidates from a large-scale public Wiki corpus or domain-specific geoscience libraries curated from scholarly literature. We then apply the SimANS sampling strategy (Zhou et al., 2022) to prioritize harder examples from the candidate pool. Each negative sample \ud835\udc5b\ud835\udc52\ud835\udc54 \ud835\udc65 is assigned a sampling probability based on the squared difference between its retriever score and that of the corresponding positive sample:\n\nwhere \ud835\udc5b\ud835\udc52\ud835\udc54 score and \ud835\udc5d\ud835\udc5c\ud835\udc60 score are the scores returned by the weak retriever, and \ud835\udf0e is a task-specific hyperparameter that adjusts the sharpness of the sampling distribution. F Sample ( \ud835\udc5b\ud835\udc52\ud835\udc54 \ud835\udc65 ) represents the probability of selecting \ud835\udc5b\ud835\udc52\ud835\udc54 \ud835\udc65 as a training negative.", "Introdction\n\nInformation retrieval (IR) (Voorhees 1999) plays a fundamental role in many natural language processing (NLP) tasks (Hambarde and Proenc \u00b8a 2023; Wang et al. 2024), especially in retrieval-augmented generation (RAG) systems (Xia et al. 2025; Liu et al. 2025). These systems retrieve relevant passages to support the generation process, improving the factual accuracy and reducing hallucinations (Lewis et al. 2020). In typical RAG pipelines, a fast retriever is often used to select a candidate set of passages from a large corpus, which are then re-scored by a reranker to prioritize passages that are more contextually relevant to the input query.", "Traditional reranking strategies, whether they operate at the pointwise (Nogueira et al. 2020a; Liang et al. 2023), pairwise (Yu et al. 2018; Qin et al. 2024), or listwise (Thonet et al. 2022) level, typically follow a two-step routine: score\n\n* Corresponding author.", "Copyright \u00a9 2026, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\n\nFigure 1: Impact of Top-K passage retrieval on multi-hop QA performance. F1, EM, and Recall scores are plotted for HotpotQA and MuSiQue datasets.\n\nTop2 Top3 Top5 Top8 Top10 TopK 0.0 0.2 0.4 0.6 0.8 Score HotpotQA F1 EM Recall 0.1 0.3 MuSiQue", "each passage on its relevance to the query, and then pick the top K candidates as the input for the generator (Li et al. 2023; Meng et al. 2024). Although effective for singlehop retrieval, this approach overlooks the interdependence among passages-a crucial factor in multi-hop and complex reasoning scenarios, including QFS, long-context processing, and high-level query reasoning (Edge et al. 2024; Qian et al. 2024). In many cases, the information required to answer a question is distributed across multiple sources, some of which might not be highly ranked. Consequently, traditional reranking methods face an inherent trade-off: if K is too small, essential but lower-ranked evidence may be omitted; if K is too large, irrelevant or distracting content may overwhelm the generator, leading to decreased performance.", "To further illustrate this point, we conduct a preliminary analysis on two widely used multi-hop question answering benchmarks, HotpotQA (Yang et al. 2018) and MuSiQue (Trivedi et al. 2022). Our findings highlight the key limitation of traditional reranking approaches: A fixed Top-K retrieval strategy limits the ability of RAG systems to handle multi-hop and complex queries As illustrated in Figures 1, the optimal value of K is highly querydependent, K =5 for HotpotQA and K =3 for MuSiQue. Increasing K from 2 to 8 improves recall by up to 13 . 21% and 53 . 26% on HotpotQA and MuSiQue, respectively, demonstrating the benefit of incorporating more contextual information. However, the F1 score drops sharply beyond K = 5 on HotpotQA and K = 3 on MuSiQue, indicating diminishing or even negative returns as irrelevant content begins to dilute the useful evidence.", "To address this problem, we propose to reframe reranking as a passage selection learning problem, where large language models are fine-tuned through supervised learning (SFT) to identify the minimal set of passages that collectively support answering a query. Rather than assigning scalar relevance scores to each passage, the proposed framework, Dynamic Passage Selector (DPS) , learns to reason about the combinatorial relevance. This formulation allows the model to explicitly capture the inter-passage dependencies critical for multi-hop question answering, understanding when a single passage would suffice or when multiple passages must be selected together.", "As illustrated in Figure 2, the number of passages required to answer the query varies between query A and query B. Traditional reranking approaches retrieve a fixed Top-K set of passages, which introduces redundant information that may compromise answer quality or lead to incorrect responses. In contrast, DPS dynamically selects the minimal yet sufficient subset of passages based on queryspecific needs, thereby ensuring the quality of generated content. Additionally, recent methods attempt to align retrieval and ranking in RAG pipelines (Li et al. 2024; Zhang et al. 2025a; Zuccon, Zhuang, and Ma 2025), they often rely on costly multi-stage API calls or require large-scale finetuning. By contrast, the proposed DPS can be seen as a plugand-play component that integrates seamlessly into existing RAG workflows without fine-tuning process or pipeline modifications.", "We validate our approach on a range of question answering benchmarks with varying levels of difficulty, including three popular multi-hop question answer datasets HotpotQA (Yang et al. 2018), MuSiQue (Trivedi et al. 2022), 2WikiMQA (Ho et al. 2020), as well as two out-of-domain datasets Legal and CS (Qian et al. 2024). Experimental results show that our dynamic passage selector consistently outperforms traditional point-wise rerankers and state-ofthe-art LLM-based list-wise rerankers, especially on complex multi-hop queries. The results demonstrate that DPS can dynamically identify contextually relevant passages, surpassing the limitations of fixed top-K retrieval. By more effectively aligning retrieval with generation, DPS leads to more robust end-to-end performance. Our main contributions are as follows:\n\nTo the best of our knowledge, this is the first work that identifies and addresses a critical limitation in RAG systems: the fixed Top-K retrieval strategy severely impairs RAG system's performance on multi-hop and complex queries.", "We treat reranking as a passage selection problem, finetune LLMs to dynamically select a minimal set of sufficient passages by modeling their interdependencies, enabling accurate retrieval for multi-hop and complex query, while remaining fully plug-and-play with existing RAG systems.", "Through extensive evaluations across five diverse question answering benchmarks, we demonstrate that DPS achieves state-of-the-art performance on both challeng-\n\ning multi-hop and out-of-domain datasets (e.g., 30.06% F1 increase over Qwen3-reranker on MuSiQue), validating its capabilities and robustness for RAG systems."]}
{"type": "entry", "index": 47, "question": "how do i measure retrieval consistency when users paraphrase the same question", "answer": "You can think of retrieval consistency under paraphrasing as: \u201cDo I get essentially the same evidence, and therefore the same answer, when the user asks the same thing in different words?\u201d The cited work suggests a few concrete ways to measure this.\n\n1) Create paraphrased queries\n\u2022 For each original user question, generate several semantically equivalent variants (e.g., 5 paraphrases) using an LLM, as done in paraphrasing defenses that create five variants per query and then evaluate over them [2].\n\n2) Run retrieval for each paraphrase\n\u2022 For each variant, run your retriever and collect the top\u2011k documents.\n\n3) Measure evidence overlap across paraphrases\n\u2022 Compute how often the same documents appear in the top\u2011k for different paraphrases (e.g., average overlap/Jaccard similarity across all pairs of paraphrases). In multi\u2011stage retrieval, repeatedly querying with paraphrases is exactly how they check whether certain documents keep showing up, which signals a systematic bias in retrieval [1].\n\u2022 High overlap among relevant documents suggests good consistency; a specific document that appears disproportionately often (especially if low\u2011quality or adversarial) may indicate a problem [1].\n\n4) Measure task metrics averaged over paraphrases\n\u2022 For evaluation datasets where you know the correct supporting document, track how often that target document is retrieved across paraphrases (a \"target recall@k\" similar to TI Recall@k used for poisoned retrieval [2]).\n\u2022 Also track answer quality metrics (e.g., F1 or accuracy) averaged across paraphrases, as done when reporting attack success under paraphrasing defenses [2]. Large swings across paraphrases imply unstable retrieval.\n\n5) (Optional) Answer consistency\n\u2022 You can also borrow the idea of checking answer consistency across semantically similar variations of the same question: if answers diverge too much, trigger more retrieval or investigation [3].", "tags": ["_untagged"], "full_faithfulness": 1.0, "full_answer_relevancy": 0.9341795610000001, "seed_faithfulness": 1.0, "seed_answer_relevancy": 0.9345725616, "context_relevance": 0.7967191094538355, "context_relevance_error": null, "context_relevance_reason": "The context passages provide relevant information about measuring retrieval consistency, particularly in the section discussing the robustness of AIP against paraphrasing. It highlights how AIP maintains effectiveness even when users paraphrase prompts, which directly addresses the user's question. However, while the context is mostly relevant, some passages diverge into technical details that may not directly relate to the user's specific inquiry about measuring consistency, slightly lowering the overall score.", "structure_citation": 0.47044408597243204, "structure_error": null, "structure_reason": "The Actual Output provides a detailed explanation of measuring retrieval consistency under paraphrasing, outlining several methods. However, it includes citations [1], [2], and [3] that do not correspond to any snippets in the provided Context, leading to unsupported claims. This results in a partial alignment with the evaluation steps, as some claims are not backed by the Context, which affects the overall credibility of the response.", "hallucination_score": 0.8642606120765353, "hallucination_error": null, "hallucination_reason": "The response effectively addresses the input question about measuring retrieval consistency under paraphrasing by outlining a clear methodology that includes generating paraphrased queries, running retrieval for each variant, and measuring evidence overlap. It aligns well with the context provided, particularly in discussing retrieval strategies and their impact on performance. The only minor shortcoming is the lack of specific examples or references to the context data, which could enhance the practical applicability of the suggestions.", "contexts_used": ["5 Evaluation\n5.3.2 Top-k Retrieval\n\nWe investigate the impact of different top-k retrieval in the RAG pipeline. Table 4 presents the proposed AIP results on the MedSquad knowledge base using the cost-effective adversarial instructional prompt. The results indicate that both attack and clean performance drop with top-3 retrieval compared to top-5 retrieval. Moreover, as top-k increases, clean performance improves, while attack performance (ASR) remains consistently high.", "5.3.3 Impact of Language Model\n\nWe examine the transferability of AIP by varying the LLM used in the RAG pipeline. Table 5 presents the results on the MedSquad knowledge base using GPT-3.5-turbo, GPT-4, Llama 3.1, and Gemini. AIP achieves a perfect attack success rate (ASR) of 100% with GPT-3.5-turbo, GPT-4, and Llama 3.1, while slightly lower ASR of 80.95% is observed with Gemini. In terms of clean accuracy (ACA), Gemini achieves the highest score of 71.42%, followed by GPT-3.5-turbo (66.70%), GPT-4 (61.90%) and Llama 3.1 (60.00%). These results demonstrate that AIP is highly transferable and robust across a range of popular LLMs, maintaining strong attack effectiveness without significantly compromising clean performance.", "| | 0 | 1 | 2 |\n|---:|:----------------|:----------|:----------|\n| 0 | Language Models | ASR (%) \u2191 | ACA (%) \u2191 |\n| 1 | GPT-3.5-turbo | 100.0 | 66.70 |\n| 2 | GPT-4 | 100.0 | 61.90 |\n| 3 | Llama3.1 | 100.0 | 60.00 |\n| 4 | Gemini | 80.95 | 71.42 |\n\nTable 5: Performance of AIP using GPT-3.5-turbo, GPT-4, Llama3.1, and Gemini LLMs in RAG's pipeline on MedSquad knowledge base.", "5.3.4 Robustness Analysis\n\nWe assess the robustness of AIP against three standard defenses: (1) Perplexity-based Detection, which flags text that deviates from language model likelihood distributions; (2) Automatic Spamicity Detection, which captures repetitive or spam-like patterns; and (3) Fluency Detection, which evaluates grammaticality and readability. As shown in Table 6, the average detection rates across MedSquad, AmazonQA, and MoviesQA remain low, 26.67% for both Perplexity and Spamicity, and 33.33% for Fluency. These results indicate that adversarial documents produced by AIP are largely indistinguishable from clean ones, effectively evading current defenses. This underscores the need for stronger detection methods, as AIP maintains high fluency and naturalness. Additional details are provided in Appendix A.5.", "5.3.5 Instructional Prompt Rephrasing\n\nSince users may rephrase instructional prompts, we further evaluate the robustness of AIP under this more challenging setting to demonstrate its ability to generalize across prompt variations. We con-", "| | 0 | 1 |\n|---:|:-----------------|:---------------------|\n| 0 | Defense Method | Detection Rate (%) \u2191 |\n| 1 | Perplexity Score | 26.67 |\n| 2 | Spamicity Score | 26.67 |\n| 3 | Fluency Score | 33.33 |\n\nTable 6: Robustness Evaluation of AIP using Perplexity, Spamicity, and Fluency detection defenses.", "|---:|:------------------------------|:--------|\n| 0 | Number of modified words | ASR (%) |\n| 1 | Corpus Poisoning 0 (original) | 28.57 |\n| 2 | Prompt Injection 0 (original) | 23.80 |\n| 3 | PoisonedRAG 0 (original) | 28.57 |\n| 4 | TrojanRAG 0 (original) | 0.00 |\n| 5 | AIP 0 (original) | 95.23 |\n| 6 | AIP 1 | 83.33 |\n| 7 | AIP 2 | 80.95 |\n| 8 | AIP 3 | 80.94 |\n| 9 | AIP 4 | 80.94 |\n| 10 | AIP 5 | 76.29 |\n\nTable 7: Dynamic Prompt Rephrasing of AIP on the MedSquad dataset.", "ducted additional experiments on the MedSquad dataset, where we randomly modified 1 to 5 words in the adversarial instructional prompt and measured the resulting attack success rate (ASR). Table 7 reports the ASR of our proposed AIP attacks with 1-5 word modifications, as well as the baseline attacks (Corpus Poisoning, Prompt Injection, PoisonedRAG, and TrojanRAG). The results show that AIP remains highly effective, significantly outperforming the baselines even under prompt modifications. Although performance degrades slightly as more words are modified, AIP still achieves a robust 76% ASR with 5-word modifications, demonstrating strong resilience to dynamic prompt rephrasing.", "5.3.6 Post-hoc Analysis\n\nTo better understand the strengths and boundaries of AIP, we conducted a post-hoc analysis of the few failure cases across all three knowledge bases. While overall attack performance remains strong, failures are mainly associated with (i) lack of lexical specificity in queries, (ii) indirect or conversational phrasing, and (iii) sensitivity to keyword variants. These cases represent edge scenarios rather than fundamental weaknesses, highlighting opportunities for refinement. Detailed examples and analysis are provided in Appendix A.6.", "5.4 Potential Defense Strategies\n\nExploring additional defense mechanisms is valuable for strengthening system robustness. We outline two possible defenses below, which we will incorporate in the revision.", "(1) Multi-Stage Retrieval. To detect retrieval manipulation, the system could perform multiple consecutive retrieval rounds using slight paraphrases of the user query or higher-level conceptual queries derived from the core topic (e.g., 'What is parasite disease?' or 'Explain the treatment of parasite disease'). If adversarial documents consistently appear across paraphrased queries while clean documents fluctuate, this may indicate retrieval bias and suggest a targeted attack.", "A. Pipeline-Level Defenses\n\nWe primarily evaluate attack performance against (i) paraphrasing defenses and (ii) textual metric checks. We further discuss other pipeline defenses, including cross-encoder reranking and pattern-based filters.", "Paraphrasing Defenses. Since the attacker is agnostic to the exact query prompted by the user, a RAG system can employ an LLM to paraphrase the input query before retrieval. Following previous work [84], [30], [45], [44], we paraphrase each query into five semantically equivalent variants using the GPT-4o mini, and report the average performance across these variants. As shown in Table IX, our method continues to retrieve the target instruction almost perfectly under this defense for all datasets (TI Recall@ k \u00bf 99%). While the retrieval of malicious contexts shows a slight drop, its impact on the final ASR is moderate (ASR \u00bf 81%). These results indicate that paraphrasing the victim query has a limited effect on our attack, as the poisoned retriever's manipulation of the embedding space allows it to robustly match the target instruction despite variations in query wording.", "Textual Metric Checks. We next evaluate whether textual metric checks on returned contexts can filter out the target instruction. We compare our ME-based approach with two textual optimization baselines designed to modify the textual content of the target instruction to pull its embedding closer to the victim query. Specifically, Gaslite appends an optimized sequence to the original instruction, while Hotflip performs lightweight modifications by flipping a limited number of tokens in the original instruction.", "| | Dataset | Metric | with defense | without defense |\n|---:|:----------|:-----------------------|:----------------------------------------|:----------------------------------------|\n| 0 | NQ | ASR MT F1 TI Recall@ k | 82.80 \u00b1 3.27 67.00 \u00b1 0.03 99.40 \u00b1 0.49 | 94.00 \u00b1 0.82 78.00 \u00b1 0.82 100.00 \u00b1 0.00 |\n| 1 | HotpotQA | ASR MT F1 TI Recall@ k | 85.00 \u00b1 1.58 79.25 \u00b1 0.00 100.00 \u00b1 0.00 | 84.33 \u00b1 0.47 80.00 \u00b1 0.00 100.00 \u00b1 0.00 |\n| 2 | Msmarco | ASR MT F1 TI Recall@ k | 81.00 \u00b1 4.19 62.00 \u00b1 0.06 99.20 \u00b1 0.40 | 86.75 \u00b1 1.25 74.00 \u00b1 0.00 100.00 \u00b1 0.00 |\n\nTABLE IX EFFECTIVENESS OF OUR METHOD AGAINST PARAPHRASING DEFENSES ACROSS DATASETS AND METRICS. WE REPORT THE MEAN AND STANDARD DEVIATION OF THE VALUE OVER THE FIVE VARIANTS.", "| | Defenses | Metrics | Hotflip | Gaslite | DisarmRAG |\n|---:|:--------------------|:-----------------------|:-------------|:------------|:--------------|\n| 0 | wo. defense | ASR TI Recall@ k | 54% 34% | 83% 100% | 94% 100% |\n| 1 | w/. perplexity | ASR TI Recall@ k value | 41% 7% 163 | 35% 0% 2818 | 94% 100% 36 |\n| 2 | w/. lexical density | ASR TI Recall@ k value | 53% 33% 0.42 | 37% 0% 0.62 | 94% 100% 0.43 |\n\nTABLE X EFFECTIVENESS UNDER DIFFERENT TEXTUAL METRIC CHECKS.", "We consider two widely used textual metrics for detecting abnormal contexts. Perplexity [31], [5], [21] measures the fluency of the retrieved text with respect to a reference language model. High values indicate potentially unnatural or adversarially generated content. Lexical density [59], [63] computes the ratio of content words to total words, flagging text that is overly dense or sparse. Details of metric computation are provided in Appendix D. In our evaluation, a defense flags a target instruction if its metric value exceeds a threshold derived from benign instructions.", "As shown in Table X, both Hotflip and Gaslite produce instructions with extreme metric values. For example, Gaslite yields a perplexity of 2818 and a lexical density of 0.62. Hotflip also inflates perplexity to 163. Such anomalies make these attacks easily detectable and can drastically reduce ASR. For instance, Gaslite's ASR drops from 83% to 35% under the perplexity check. In contrast, our model editing approach produces target instructions with benign-like values. The perplexity and lexical density of our injected target instructions are 36 and 0.43, respectively, both within the benign range. Under both metric checks, our method consistently retrieves every target instruction while maintaining high ASR. This", "further underscores the necessity of adopting model editing for effective and stealthy retriever poisoning.", "Other Pipeline Defenses. Alternative defenses such as crossencoder re-ranking [55], [61] and pattern-based filters [46], [39] are also limited in practice. Cross-encoder re-ranking relies on using a heavier model to rerank retrieved documents. While this can improve robustness, it introduces substantial latency and computation cost, which makes it impractical at scale. Pattern-based filters attempt to block adversarial instructions based on syntactic rules, but our optimized instruction pool avoids unified structures or keywords, reducing their effectiveness. Taken together, these defenses are either too costly or insufficient against our attack.", "5 Generation-time feedback\n\nRAG systems face two fundamental challenges: determining when to retrieve external knowledge, since not all queries benefit from it, and how to retrieve relevant content effectively (Su et al., 2024b). Classical RAG pipelines rigidly follow a fixed sequence of retrieval, optionally ranking, followed by generation, limiting their ability to adapt to the context or task. To address these limitations, recent work has introduced adaptive RAG , where the retrieval strategy is dynamically adjusted according to the query, the model feedback, or the complexity of the task. We categorize this emerging line of work into three main classes.", "5.1 Rule-Based and Discriminative Approaches\n\nIn-Context RALM (Retrieval-Augmented Language Model) (Ram et al., 2023) proposes retrieving relevant context documents during inference at fixed intervals (every s tokens, known as the retrieval stride), using the last l tokens of the input as the retrieval query. In a similar spirit, IRCoT (Interleaving Retrieval in a CoT) (Trivedi et al., 2023) dynamically retrieves documents if the CoT (Wei et al., 2022) step has not provided the answer. At first, it uses the original question to retrieve the context and then uses the last generated CoT sentence as a query for subsequent retrieval. However, both of these methods retrieve the context regardless of whether the LLM needs external context or not. Hence, the unnecessary retrieval steps add additional latency cost during answer generation. Also, the noisy retrieved context can lead to a wrong answer. CtRLA (Huanshuo et al., 2025) devises a latent space probing-based approach for making decisions regarding retrieval timings for adaptive retrieval augmented generation. The authors extract latent vectors that represent abstract concepts like honesty and confidence and use these dimensions to steer retrieval and control LLM behavior, leading to better performance and robust answers. To overcome the over-retrieval limitation of rule-based dynamic RAG methods, retrieval-ondemand approaches have been proposed. These methods trigger retrieval only when the LLM needs it, based on either external feedback (Section 5.2)or the LLM's own assessment (Section 5.3).", "5.2 Retrieval-on-Demand via Feedback Signals\n\nThe feedback signals can come from different sources, including the answer uncertainty, the model's internal states, or context faithfulness and sufficiency. SKR (Wang et al., 2023b) asks LLM itself if additional information is needed to answer the query. If yes, then the retrieval round is triggered; otherwise, the answer is generated from the", "LLM's internal knowledge. However, the judgment is solely based on LLM, and without context, they try to be overconfident (Xiong et al., 2024). FLARE (Jiang et al., 2023) retrieves the documents only if the token probability is below a predefined threshold and uses the last generated sentence as a query for retrieval (excluding the uncertain tokens) and generates the response until the next uncertain token or completion is done. However, these uncertain tokens are not equally important to trigger a retrieval round. Based on this, DRAD (Su et al., 2024a) uses an external module for hallucination detection on entities in the generated answer; if the answer contains hallucination, the retrieval is triggered. The last generated sentence (without a hallucinated entity) is used as a query for retrieval. However, the choice of the new query for retrieval relies on heuristic strategies. Since the model's information needs may extend beyond the last sentence or CoT, it could require context from a broader span of the generation to effectively build confidence. Based on this motivation, DRAGIN (Su et al., 2024b), similar to FLARE, also considers the token probabilities as a criterion of the retrieval round but does not consider the uncertain tokens as a part of the new query. Further, it also reformulates the query using the keywords based on the model's internal attention weights and reasoning. SeaKR (Yao et al., 2025) computes the self-aware uncertainty using internal states of the LLM. If the uncertainty is above a threshold, then a retrieval round is triggered.", "Other types of works, like Rowen (Ding et al., 2024), consider the LLM's answer consistency as feedback. Rowen considers answer consistency across languages of the same question with semantically similar variations, and the consistency over answers generated by different LLMs. If the total consistency is below a predefined threshold, then the retrieval round is triggered. However, similar to SUNAR (V et al., 2025), the consistency can be toward wrong answers.", "However, these approaches consider all queries equally complex and might end up with noisy context retrieval and hence a wrong answer. Adaptive RAG (Jeong et al., 2024) uses a query routing mechanism that predicts whether the query needs retrieval or not. Further, it also decides on the number of retrieval rounds based on query complexity. However, it assumes that the retrieved context is relevant to the query without assessing its relevancy or sufficiency. Towards the relevancy, CRAG", "(Corrective RAG) (Yan et al., 2024) evaluates the relevance scores using a fine-tuned model, and classifies the retrieved document into correct, incorrect, and ambiguous. If the context is not correct, then a rewritten query is issued to the web search engine. Similar fashion, SIM-RAG (Yang et al., 2025b) focuses on the context sufficiency angle, and trains a lightweight critic model that provides feedback if the retrieved context is sufficient to generate the answer. If the information is not sufficient, then a new query is formulated using the original query and the already retrieved context, and a retrieval round is triggered. Further CoV-RAG (He et al., 2024) identifies errors, including reference and answer correctness, and truthfulness, and then scores them using a trained verifier. Based on the scores, either provide a final or rewrite the query and do a further retrieval round.", "5.3 Self-Triggered Retrieval via Reasoning\n\nIn this section, we discuss works where LLM autonomously makes the decision on when to retrieve and how to retrieve through query decomposition or planning-based approaches without external triggers. These approaches are also termed Reasoning RAG or Agentic RAG . These approaches can be divided into mainly two categories: first, where the instructions for query decomposition, when to retrieve, and what to retrieve are provided in the prompt along with few-shot examples; second, where the language models are trained to decide by themselves whether to decompose the query, when to retrieve, and what to retrieve.", "Prompt-Based Methods. DeComP (Khot et al., 2023) divides a task into granular sub-tasks and delegates them to different components through actions. However, DeComP only acts as a trigger for when to retrieve and employs a BM25 retriever for getting relevant documents in a single shot. It does not subsequently generate reasoning steps to improve retrieval, thus not providing much indication as to how to retrieve. ReAct (Yao et al., 2022) interleaves the generation of verbal reasoning traces with actions that interact with the external environment. The verbal reasoning traces act as indicators of how to retrieve, and the actions themselves serve as triggers (when to retrieve). Similarly, SelfAsk (Press et al., 2023) proposes to decompose the original complex query into simpler sub-questions iteratively interleaved by a retrieval step and intermediate answer generation. At each step, the LLM makes a decision to generate a follow-up question if more information is needed, or it may generate the final answer. Authors observed that this approach helped cover diverse aspects of complex queries and improved search and downstream answering performance.", "However, these approaches do not have provision for correction of the entire reasoning trajectory, and an intermediate error may cause cascading failures. Searchain (Xu et al., 2024) proposes to mitigate this by constructing a global reasoning chain first, where each node comprises a retrievaloriented query, an answer from LLM to the query, and a flag indicating if additional knowledge is needed to arrive at a better answer. SMR (State Machine Reasoning) (Lee et al., 2025) identifies the issues of the CoT-based query decomposition and retrieval methods like ReAct (Yao et al., 2022), where the CoT might result in redundant reasoning (new queries that result in the retrieval of the same documents) and misguided reasoning (new query diverges from the user's intent). To address these limitations, SMR proposes three actions: Refine, Rerank, and Stop. Action Refine updates the query using the feedback from the already retrieved documents, and a retrieval round is triggered. Then the retrieved documents are ranked according to the old query to make sure only the relevant information is used to answer. Finally, the Stop action is called to stop the reasoning if a sufficient retrieval quality is achieved, which helps in token efficiency and prevents overthinking.", "MCTS-RAG (Hu et al., 2025) combines Monte Carlo Tree Search (MCTS) with RetrievalAugmented Generation (RAG) to improve reasoning and retrieval in language models. It guides the search for relevant information using MCTS to explore promising retrieval paths, enhancing answer accuracy. However, it is computationally expensive due to the iterative tree search process and may struggle with highly noisy or irrelevant documents. Search-O1 (Li et al., 2025a) proposes an agentic search workflow for reasoning augmented retrieval by letting the Large Reasoning Models (LRMs) like O1 decide autonomously when to retrieve by detecting the presence of salient uncertain words in their output. Additionally, they augment the workflow with a reason-in-documents step, where LRMs analyze the documents in depth to remove noise and reduce redundancy before employing them to generate the final answer.", "4 Results\n\n4.2 RQ2: To what extent does the type of retrieved context influence robustness?\n\nto 37.7% for GPT-4.1, 43.4% for GPT-5, and 36.0% for Claude-3.5-Haiku. Furthermore, alignment rate is nearly eliminated by strong adversarial documents such as Liar, with GPT-4.1 dropping to 4.4%, GPT-5 to 2.6%, and Claude-3.5-Haiku to 0.9% for the same neutral condition.", "4.2 RQ2: To what extent does the type of retrieved context influence robustness?\n\nOpen-source models follow the same pattern. As shown in Table 4 for TREC 2021, the alignment rate of DeepSeekR1-Distill-Qwen-32B improves from 63.0% in the Non-RAG baseline to 92.6% with helpful documents for neutral user queries, but collapses to just 1.3% under Liar inputs. A similar trend appears for Phi-4, which rises from 66.7% in the Non-RAG baseline to 96.5% with helpful evidence, but drops sharply to 2.2% with Liar documents. LLaMA-3 8B Instruct shows the same pattern, moving from 81.6% without context to 93.9% with helpful inputs, before falling to just 0.4% under Liar documents. Even weaker adversarial documents, such as Rewriter and Paraphraser, substantially reduce alignment rate, often into the 55-70% range across all open-source models. The same pattern is observed for TREC 2020 as well, but with consistently higher rates, as detailed in the following paragraphs. We emphasize neutral user query results here because RQ1-RQ3 focus primarily on the effect of retrieved context, while the role of query framing is analyzed separately in Section 4.4.", "4.2 RQ2: To what extent does the type of retrieved context influence robustness?\n\nAcross both datasets and model families, the relative effectiveness of adversarial strategies follows a consistent ordering: Liar \u226b FSAP-IntraQ > FSAP-InterQ > Fact Inversion > Paraphraser \u2248 Rewriter . This hierarchy is a result of how adversarial documents are made. Liar documents are constructed from scratch based on the query and an incorrect stance, so they are not constrained to the wording or rhetorical framing of pre-existing harmful content. This independence enables more fluent phrasing and broader topic coverage, making misinformation more confident and persuasive, and thus more challenging for LLMs. In contrast, Paraphraser and Rewriter documents inherit limitations from pre-existing harmful material, which can make them appear less natural.", "4.2 RQ2: To what extent does the type of retrieved context influence robustness?\n\nIn cross-model comparison, GPT-4.1 and GPT-5 achieve the highest levels of alignment rate when paired with helpful documents; however, both demonstrate comparable vulnerabilities when confronted with adversarial evidence. Claude-3.5-Haiku has significant performance drops in the presence of harmful or adversarial content; on the other hand, DeepSeek-R1-Distill-Qwen-32B performs well and often obtains levels that are similar to closed-source models when provided with helpful documents. Phi-4 and LLaMA-3 8B Instruct achieves a lower overall alignment rate but follows the same qualitative trends. Since these patterns are consistent across all models, we selected GPT-4.1 as the representative model for subsequent experiments due to its relatively high performance and speed. GPT-5 was not released at the time these experiments were first conducted and was included afterwards.", "4.2 RQ2: To what extent does the type of retrieved context influence robustness?\n\nAnother important finding of this experiment is the difference at the dataset level, where models consistently display a higher alignment rate on TREC 2020 (COVID-19 misinformation) than on TREC 2021 (general health misinformation). For example, TREC 2021 includes queries about misinformation on everyday remedies and niche treatments, such as using duct tape for warts and toothpaste for pimples, which rarely reach the same level of wide public discourse, although the ground truth for these questions is based on evidence. This underscores an unexpected outcome from our research, suggesting that LLMs appear considerably more robust against adversarial manipulation in the COVID-19 context. This difference may be due to post-training alignment placing greater emphasis on topics like COVID-19, which was the subject of widespread misinformation and public attention, while giving limited focus to less prominent areas of health.", "4.2 RQ2: To what extent does the type of retrieved context influence robustness?\n\nOverall, our results address RQ2 by showing that the type and quality of retrieved context substantially affect LLM robustness, by demonstrating that helpful documents consistently improve alignment rate relative to the baseline, whereas harmful and adversarial documents undermine stability and reliability, in some cases reducing robustness to the point of complete failure.", "4.2 RQ2: To what extent does the type of retrieved context influence robustness?\n\nWhile it may appear self-evident that helpful evidence would improve alignment rate and harmful or adversarial evidence would reduce it, our results move beyond speculation by empirically quantifying both the magnitude and the consistency of these effects across datasets and model families. The measurements establish concrete performance bounds. Alignment rate can approach near-maximum levels by providing a single helpful document, yet it can collapse to almost zero under adversarial documents such as Liar. As outlined in Section 1, this systematic analysis offers the empirical evidence required to turn abstract concerns about adversarial susceptibility into evidence-based design principles for RAG systems. In line with RQ1, the findings highlight that context can significantly shape outcomes and must be safeguarded against harmful or adversarial inputs.", "4.3 RQ3: In what ways do orders and combinations of multiple evidence sources shape model behavior?\n\nFindings from RQ1 and RQ2 show that context type strongly shapes ground-truth alignment, which motivates an assessment of more complex scenarios with multiple documents presented in different orders and compositions, leading to RQ3. To address this question, we examine how models respond when exposed to multiple sources of evidence, focusing on two key dimensions: (i) the order in which helpful and misleading documents are presented, and (ii) the composition of the retrieval pool when evidence is naturally or systematically skewed. We investigate these dimensions through three complementary setups: the paired-document setup, the passage-based pooling setup, and the bias-controlled pooling setup, which are analyzed in the following sections.", "4.3 RQ3: In what ways do orders and combinations of multiple evidence sources shape model behavior?\n\n4.3.1 Paired-Document Analysis. Figure 2 shows ground-truth alignment rate when a helpful document is either first (green, 'Helpful First') or second (blue, 'Helpful Second') relative to a harmful/adversarial document, compared with the single-document baseline (red). The results show that the order of the documents has minimal impact on ground-truth alignment. The Helpful First and Helpful Second conditions deliver nearly the same results, with confidence intervals that mostly overlap. For instance, on TREC 2021 under the consistent user query, the alignment rate of LLM responses rises from only 1.7% (CI 0.4-3.5) in the single-document baseline to 39.1% (CI 33.3-45.0) when helpful evidence appears first and 45.0% (CI 39.1-50.8) when it appears second for Liar inputs. Similarly, for FSAP-IntraQ documents, alignment rate improves from 7.4% (CI 4.4-10.9) to 41.1% (CI 35.3-47.3) when helpful content precedes and 48.8% (CI 43.0-55.0) when it follows the adversarial document. On TREC 2020, robustness is consistently higher. For example, under the neutral user query, for FSAP-IntraQ documents, alignment rate rises from 54.9% (CI 48.2-62.2) to 68.3% (CI 61.9-74.8) when helpful material comes first and 76.7% (CI 70.8-82.7) when it is placed second relative to them. These results show that presenting helpful evidence, regardless of position, minimizes the impact of harmful or adversarial content. Additionally, discrepancies among datasets persist, as TREC 2020 (COVID-19) queries demonstrate greater robustness than the more general health queries in TREC 2021, thereby reinforcing the previously identified dataset-level variations.", "4.3 RQ3: In what ways do orders and combinations of multiple evidence sources shape model behavior?\n\n4.3.2 Passage-Based Pooling Analysis. In the passage-based pooling setup, the MonoT5 reranker provides the top-10 passages for each query, obtained by sliding a 512-word window with 256-word overlap across documents and ranking the resulting segments. Since the adversarial documents are designed to rank highly, these pools are heavily skewed toward this adversarial content, with more than nine of the top-10 segments being adversarial. For TREC 2020, the top-10 pools comprise \u223c 92% adversarial, 5% helpful, and 3% harmful passages, whereas for TREC 2021 the distribution is \u223c 94% adversarial, 5% helpful, and 2% harmful passages. As shown in Fig. 3, the dominance of adversarial segments in the passage-based pool leads to consistently low alignment rates. On TREC 2020, GPT-4.1 reaches 43.7% under consistent user queries, but performance drops to 24.9% with neutral user queries and remains low at 24.9% under inconsistent user queries. The degradation is even worse on TREC 2021, where the alignment rate drops to 17.3% with consistent Manuscript submitted to ACM", "4.3 RQ3: In what ways do orders and combinations of multiple evidence sources shape model behavior?\n\nFig. 2. Results of paired-document setup for GPT-4.1 on TREC 2020 and TREC 2021 with consistent, neutral, and inconsistent user queries. The x-axis shows the document type with which the helpful document is paired (either first or second), and the y-axis reports ground-truth alignment rate (%). Error bars indicate 95% confidence intervals estimated via bootstrapping. Bars compare helpful-first, helpful-second, and single-document inputs (harmful/adversarial only). Helpful placement shows only minor differences, while single-document inputs yield substantially lower ground-truth alignment rates.\n\nHarmful Rewriter Paraphraser Fact Inversion FSAP-InterQ FSAP-IntraQ Liar Ground-Truth Alignment (%) TREC2021 Consistent Query TREC2021 Neutral Query TREC2021 Inconsistent Query TREC2020 Consistent Query TREC2020 Neutral Query TREC2020 Inconsistent Query Helpful First Helpful Second Single Document", "4.3 RQ3: In what ways do orders and combinations of multiple evidence sources shape model behavior?\n\nuser queries, 13.0% with neutral user queries, and only 4.3% with inconsistent user queries. These findings emphasize retrieval as the primary bottleneck in RAG pipelines and demonstrate that the composition of the evidence pool can significantly undermine robustness, irrespective of user query framing.", "4.3 RQ3: In what ways do orders and combinations of multiple evidence sources shape model behavior?\n\n4.3.3 Bias-Controlled Pooling Analysis. To examine the impact of systematic skew in retrieval, we construct pools biased toward either helpful or harmful content. In the condition biased toward harmful, the ground-truth alignment rate declines markedly. On TREC 2021, as shown in Fig. 3, GPT-4.1 achieves only a 26.04% alignment rate when the pool is biased toward harmful content with Fact Inversion documents, and 4.35% when biased with Liar documents, even under consistent user queries. The situation worsens under inconsistent user queries, where providing Liar examples leads to a 0% alignment rate, and presenting FSAP-IntraQ documents collapses the rate to 0% as well. By contrast, TREC 2020 demonstrates somewhat greater resilience. Although alignment rate against Rewriter documents remains as high as 93.76% and Paraphraser at 87.52% under consistent user queries, it still collapses for the most adversarial harmful cases, dropping to 24.95% for Liar and 37.4% for FSAP-IntraQ documents.", "4.3 RQ3: In what ways do orders and combinations of multiple evidence sources shape model behavior?\n\nIn the biased-toward-helpful condition, alignment levels are close to the best possible. On both datasets, consistent and neutral user queries frequently sustain a 100% alignment rate, with LLM responses across all attack categories in TREC 2021 reaching a full alignment rate when helpful documents are included in the pool. Even inconsistent user queries remain robust: in TREC 2020, pooled conditions containing Rewriter or FSAP-InterQ documents yield a 100%\n\nManuscript submitted to ACM", "4.3 RQ3: In what ways do orders and combinations of multiple evidence sources shape model behavior?\n\nFig. 3. Results of passage-based pooling and biased-controlled pooling setups for GPT-4.1 on TREC 2020 and TREC 2021 across consistent, neutral, and inconsistent user queries. The x-axis includes the passage-based pooling baseline as well as the different document types used in biased conditions, where helpful documents are paired with harmful or adversarial ones. The y-axis reports ground-truth alignment rate (%). Error bars indicate 95% confidence intervals estimated via bootstrapping. Results show helpful-biased pools yield near-perfect alignment, whereas harmful and passage-based pools sharply reduce it.\n\nPassage-Based\n\nTREC 2021 Consistent Query\n\nTREC 2021 Neutral Query\n\nTREC 2021 Inconsistent Query\n\nTREC 2020 Consistent Query\n\nTREC 2020 Neutral Query\n\nTREC 2020 Inconsistent Query\n\nBiased to Helpful\nBiased to Harmful\nPassage-Based Pooling", "4.3 RQ3: In what ways do orders and combinations of multiple evidence sources shape model behavior?\n\nalignment rate, and Fact Inversion pools reach 95.63% on TREC 2021. Once helpful content dominates the retrieval pool, the influence of user query framing diminishes, as supportive context outweighs misleading presuppositions.", "4.3 RQ3: In what ways do orders and combinations of multiple evidence sources shape model behavior?\n\nOverall, the bias-controlled experiments confirm that the composition of retrieved evidence plays a critical role in RAG systems' robustness. Pools dominated by helpful documents consistently lead to high alignment rates, often exceeding 90-100%, while harmful-biased pools drastically reduce reliability, in some cases collapsing alignment rate to nearly zero. These results underscore that the stability of RAG systems is fundamentally dependent on the distribution of their evidence sources, which highlights the retrieval stage as the key vulnerability in pipeline design.", "4.4 RQ4: How do different query framings affect stance alignment?\n\nAfter discussing the significance of evidence type (RQ2) and evidence order/combination (RQ3), we move on to RQ4, which assesses the impact of user query framing (consistent, neutral, and inconsistent) on the results of ground-truth alignment. Prior work has consistently shown that LLM performance is highly sensitive to prompt formulation. Even minor variations in wording, token placement, or tone can lead to substantially different outcomes, and several studies have proposed methods to stabilize predictions under such alterations [32, 42, 49, 50, 57, 63, 65, 75]. Motivated by these results, we revisit our experimental setups with query sensitivity in mind.", "4.4 RQ4: How do different query framings affect stance alignment?\n\nWe analyze user query formulation effects across all four experimental setups: the single-document setup, the paireddocument setup, the passage-based pooling setup, and the bias-controlled pooling setup. Figures 1-3 and Tables 2-5 summarize results across models, datasets, and retrieval conditions.", "4.4 RQ4: How do different query framings affect stance alignment?\n\nFraming has a persistent impact on ground-truth alignment rate, and the order of user query types remains the same across models and datasets (Consistent > Neutral > Inconsistent), which directly addresses RQ4 by showing that the framing of user queries impacts ground-truth alignment outcomes. The same hierarchy emerges in the single-document setup (Fig.1), persists when multiple sources of evidence are paired (Fig.2), and carries over to more passage-based pooling setups (Fig. 3).", "4.4 RQ4: How do different query framings affect stance alignment?\n\nConsistent user queries, those which align with the ground-truth stance of the answer, reliably produce the highest alignment rates by framing the model's reasoning in a direction that agrees with available evidence; on the other hand, inconsistent user queries embed contradictory presuppositions that actively mislead the model, often overwhelming the corrective signal of helpful documents. While explicit stance bias is eliminated by neutral user queries, models are still able to base their responses on the evidence they have retrieved, resulting in intermediate performance. These results show that models reflect the quality of retrieved evidence (RQ2) and systematically inherit bias from how user queries are phrased. User query formulation, therefore, plays a central role in shaping ground-truth alignment outcomes (RQ4).", "Ablation: Query Decomposition\n\nReranking Scope ( k 2 ). To analyze the effect of reranking scope, we vary the number of topk 2 documents used in the final reranked retrieval step. As shown in Figure 1, performance trends vary across datasets and model backbones.", "GPT-4o performs best with moderate k 2 values, peaking at k 2 = 7 on 2WikiMultiHopQa and k 2 = 10 on HotpotQA, but degrades noticeably with larger k 2 , especially on MuSiQue.", "Gemini-1.5-Pro shows more stable or even improving performance as k 2 increases, particularly on 2WikiMultiHopQa and HotpotQA, peaking around k 2 = 15 .", "LLaMA-3.1-8B-inst on the other hand, consistently suffers from performance drops as k 2 increases, indicating difficulties in leveraging longer context windows effectively.", "These results suggest that the optimal number of reranked passages ( k 2 ) is model- and dataset-dependent. Smaller k 2 values are generally safer when using models that struggle with long context (e.g., LLaMA-3.1-8B-inst), while larger values benefit stronger models like Gemini in scenarios with informative passages.", "Sequential vs. Unified Inference. Our experiments compare sequential answering over decomposed subquestions with unified inference using the original query and full retrieved context.(Table 4) The results demonstrate that unified inference consistently outperforms sequential generation for large-scale models such as GPT-4o and Gemini. The performance gains are particularly pronounced on datasets with high inter-document dependency, such as 2WikiMultiHopQa and MuSiQue, indicating that reasoning over a fully integrated context enables more effective multi-hop answer synthesis.", "These findings underscore the importance of postretrieval reasoning strategy and suggest that unified inference provides a more robust mechanism for information integration than aggregating intermediate answers. Interestingly, our results align with observations made in prior work such as LongRAG, which reports strong performance by leveraging long-context models to reason over retrieved document blocks as a whole. While our setup differs in architecture and retrieval granularity, the underlying conclusion remains consistent: models benefit from reasoning holistically over relevant information when capacity permits.", "Summary and Paraphrase Embedding RAG. To assess the utility of transformed document content for retrieval, we compare four types of representations: the original document, summaries, paraphrased versions, and AQG-derived questions. For summarization and paraphrasing, we prompt an LLM to generate summaries or paraphrased versions of each document chunk individually.", "Figure 1: Effect of reranking scope ( k 2 ) on multihop QA performance across datasets.\n\n2wikimultihopqa hotpotqa musique kz (reranked passages) GPT-40 Gemini-1.5-pro LLaMA-3.1-8B\n\n| | Model | Dataset | Sequential | Unified |\n|---:|:------------------|:---------------------------------|:------------------|:------------------|\n| 0 | GPT-4o | 2WikiMultiHopQa HotpotQA MuSiQue | 64.42 66.23 47.55 | 69.19 67.50 54.99 |\n| 1 | Gemini-1.5-pro | 2WikiMultiHopQa HotpotQA MuSiQue | 65.56 63.26 48.32 | 66.85 63.61 48.44 |\n| 2 | LLaMA-3.1-8B-inst | 2WikiMultiHopQa HotpotQA MuSiQue | 52.07 54.79 33.16 | 50.49 56.42 33.81 |\n\nTable 4: Comparison of sequential vs. unified inference methods over decomposed questions.", "As shown in Figure 2, both summarization and paraphrasing yield only marginal improvements over the raw document representation, and in some cases even result in performance degradation, depending on the model and dataset. Paraphrasing tends to slightly outperform summarization, which may be attributed to the fact that summarization often omits or compresses original information, while paraphrasing preserves the full content by merely rephrasing it. We provide all prompt templates used for AQ generation, summarization, and paraphrasing in Appendix C to support reproducibility.", "In contrast, AQG-derived queries lead to consistent and substantial improvements across all models and datasets. This suggests that query-oriented transformations offer stronger retrieval signals than document-side reformulations. The benefit becomes especially clear when using large-scale models capable of aligning the query and context semantically during generation."]}
{"type": "entry", "index": 48, "question": "top-k: is \u201cmore context\u201d helping or just adding noise and confusion", "answer": "\u201cMore context\u201d (a larger top\u2011k) helps up to a point, then often starts to hurt.\n\nAcross retrieval systems, increasing top\u2011k almost always boosts recall: the chance that at least one retrieved item contains the needed information rises steadily as k grows [1][4][5]. For example, in technical-document search, recall improves from k=5 to k=20 for all embedding models [5]. Similarly, in multimodal RAG, going from k=10 to k=20 increases recall for both models studied [4].\n\nHowever, generation quality (accuracy/F1) does not simply keep improving. In multi\u2011hop QA, raising k from 2 to 8 greatly boosts recall, but F1 drops sharply once k exceeds the dataset\u2011specific sweet spot (around 5 for HotpotQA, 3 for MuSiQue) because extra, less relevant passages dilute the useful evidence and confuse the model [1]. This is exactly the core limitation of fixed top\u2011k: too small misses evidence; too large floods the model with noise [1]. A survey on RAG frames this as both a selection and a filtering problem, stressing that irrelevant context can lead to incorrect outputs and must be suppressed, not just added [2].\n\nOther studies see mixed effects: in multimodal RAG, k=20 increases recall but slightly *reduces* accuracy for one model while improving both for another [4]. In an adversarial RAG setting, higher k improves clean accuracy but keeps attack success very high, so more context does not guarantee safety [3]. Another work notes that adding more retrieved passages increases uncertainty in the system\u2019s behavior [6].\n\nSo, more context is beneficial only up to a query\u2011 and model\u2011dependent optimum. Beyond that, it increasingly adds noise and confusion unless you dynamically select and filter what the model actually sees [1][2].", "tags": ["retrieval_tuning"], "full_faithfulness": 0.7727272727000001, "full_answer_relevancy": 0.0, "seed_faithfulness": 0.6956521739, "seed_answer_relevancy": 0.0, "context_relevance": 0.8080899775376486, "context_relevance_error": null, "context_relevance_reason": "The context passages provide substantial information regarding the implications of top-k retrieval in RAG systems, directly addressing the user's question about whether 'more context' is beneficial or detrimental. The passages discuss the trade-offs of increasing K, highlighting how too much irrelevant content can overwhelm the generator, which aligns well with the user's concern about noise and confusion. However, while the response is relevant, it could be more explicitly tied to the user's phrasing about 'noise and confusion' for a higher score.", "structure_citation": 0.8291363656743324, "structure_error": null, "structure_reason": "The Actual Output effectively utilizes multiple citations [1][2][4][5][6] that correspond to relevant snippets in the Context, supporting its claims about the impact of top-k retrieval on recall and generation quality. However, while most claims are well-supported, there are minor inconsistencies in the depth of explanation regarding the limitations of fixed top-k strategies, which could have been more explicitly tied to the cited studies. Overall, the response demonstrates strong alignment with the evaluation steps.", "hallucination_score": 0.859266659995407, "hallucination_error": null, "hallucination_reason": "The response effectively addresses the input question about the impact of 'more context' in retrieval systems, providing a nuanced analysis that aligns well with the claims made. It supports the assertion that increasing top-k can enhance recall but may also introduce noise, which is consistent with the context provided. The output includes specific examples and studies that illustrate the trade-offs involved, demonstrating a strong understanding of the topic. However, a minor shortcoming is the lack of explicit mention of the 'sweet spot' for top-k values in the summary, which could have further strengthened the response.", "contexts_used": ["Introdction\n\nInformation retrieval (IR) (Voorhees 1999) plays a fundamental role in many natural language processing (NLP) tasks (Hambarde and Proenc \u00b8a 2023; Wang et al. 2024), especially in retrieval-augmented generation (RAG) systems (Xia et al. 2025; Liu et al. 2025). These systems retrieve relevant passages to support the generation process, improving the factual accuracy and reducing hallucinations (Lewis et al. 2020). In typical RAG pipelines, a fast retriever is often used to select a candidate set of passages from a large corpus, which are then re-scored by a reranker to prioritize passages that are more contextually relevant to the input query.", "Traditional reranking strategies, whether they operate at the pointwise (Nogueira et al. 2020a; Liang et al. 2023), pairwise (Yu et al. 2018; Qin et al. 2024), or listwise (Thonet et al. 2022) level, typically follow a two-step routine: score\n\n* Corresponding author.", "Copyright \u00a9 2026, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\n\nFigure 1: Impact of Top-K passage retrieval on multi-hop QA performance. F1, EM, and Recall scores are plotted for HotpotQA and MuSiQue datasets.\n\nTop2 Top3 Top5 Top8 Top10 TopK 0.0 0.2 0.4 0.6 0.8 Score HotpotQA F1 EM Recall 0.1 0.3 MuSiQue", "each passage on its relevance to the query, and then pick the top K candidates as the input for the generator (Li et al. 2023; Meng et al. 2024). Although effective for singlehop retrieval, this approach overlooks the interdependence among passages-a crucial factor in multi-hop and complex reasoning scenarios, including QFS, long-context processing, and high-level query reasoning (Edge et al. 2024; Qian et al. 2024). In many cases, the information required to answer a question is distributed across multiple sources, some of which might not be highly ranked. Consequently, traditional reranking methods face an inherent trade-off: if K is too small, essential but lower-ranked evidence may be omitted; if K is too large, irrelevant or distracting content may overwhelm the generator, leading to decreased performance.", "To further illustrate this point, we conduct a preliminary analysis on two widely used multi-hop question answering benchmarks, HotpotQA (Yang et al. 2018) and MuSiQue (Trivedi et al. 2022). Our findings highlight the key limitation of traditional reranking approaches: A fixed Top-K retrieval strategy limits the ability of RAG systems to handle multi-hop and complex queries As illustrated in Figures 1, the optimal value of K is highly querydependent, K =5 for HotpotQA and K =3 for MuSiQue. Increasing K from 2 to 8 improves recall by up to 13 . 21% and 53 . 26% on HotpotQA and MuSiQue, respectively, demonstrating the benefit of incorporating more contextual information. However, the F1 score drops sharply beyond K = 5 on HotpotQA and K = 3 on MuSiQue, indicating diminishing or even negative returns as irrelevant content begins to dilute the useful evidence.", "To address this problem, we propose to reframe reranking as a passage selection learning problem, where large language models are fine-tuned through supervised learning (SFT) to identify the minimal set of passages that collectively support answering a query. Rather than assigning scalar relevance scores to each passage, the proposed framework, Dynamic Passage Selector (DPS) , learns to reason about the combinatorial relevance. This formulation allows the model to explicitly capture the inter-passage dependencies critical for multi-hop question answering, understanding when a single passage would suffice or when multiple passages must be selected together.", "As illustrated in Figure 2, the number of passages required to answer the query varies between query A and query B. Traditional reranking approaches retrieve a fixed Top-K set of passages, which introduces redundant information that may compromise answer quality or lead to incorrect responses. In contrast, DPS dynamically selects the minimal yet sufficient subset of passages based on queryspecific needs, thereby ensuring the quality of generated content. Additionally, recent methods attempt to align retrieval and ranking in RAG pipelines (Li et al. 2024; Zhang et al. 2025a; Zuccon, Zhuang, and Ma 2025), they often rely on costly multi-stage API calls or require large-scale finetuning. By contrast, the proposed DPS can be seen as a plugand-play component that integrates seamlessly into existing RAG workflows without fine-tuning process or pipeline modifications.", "We validate our approach on a range of question answering benchmarks with varying levels of difficulty, including three popular multi-hop question answer datasets HotpotQA (Yang et al. 2018), MuSiQue (Trivedi et al. 2022), 2WikiMQA (Ho et al. 2020), as well as two out-of-domain datasets Legal and CS (Qian et al. 2024). Experimental results show that our dynamic passage selector consistently outperforms traditional point-wise rerankers and state-ofthe-art LLM-based list-wise rerankers, especially on complex multi-hop queries. The results demonstrate that DPS can dynamically identify contextually relevant passages, surpassing the limitations of fixed top-K retrieval. By more effectively aligning retrieval with generation, DPS leads to more robust end-to-end performance. Our main contributions are as follows:\n\nTo the best of our knowledge, this is the first work that identifies and addresses a critical limitation in RAG systems: the fixed Top-K retrieval strategy severely impairs RAG system's performance on multi-hop and complex queries.", "We treat reranking as a passage selection problem, finetune LLMs to dynamically select a minimal set of sufficient passages by modeling their interdependencies, enabling accurate retrieval for multi-hop and complex query, while remaining fully plug-and-play with existing RAG systems.", "Through extensive evaluations across five diverse question answering benchmarks, we demonstrate that DPS achieves state-of-the-art performance on both challeng-\n\ning multi-hop and out-of-domain datasets (e.g., 30.06% F1 increase over Qwen3-reranker on MuSiQue), validating its capabilities and robustness for RAG systems.", "2 Preliminaries\n2.1 Retrieval System\n\nThe core objective of a retrieval system is to identify and rank a subset of documents ( d 1 , d 2 , ..., d k ) from a large corpus C based on their estimated relevance to a query q . Classical retrieval approaches, such as BM25 (Robertson et al., 1995), rely on exact term matching and produce sparse relevance scores. In contrast, dense retrieval methods employ neural encoders to project queries and documents into a shared embedding space, enabling semantic similarity matching (Karpukhin et al., 2020). Since first-stage retrievers often produce noisy candidates, modern pipelines incorporate a second-stage re-ranking step using more expressive models. This includes LLM-based rankers (Pradeep et al., 2023b;\n\nMa et al., 2024; Sun et al., 2023) and reasoningaugmented models such as ReasonIR (Shao et al., 2025), Rank-1 (Weller et al., 2025), and RankR1 (Zhuang et al., 2025), which refine the initial rankings by modeling deeper interactions between the query and candidate documents.", "2.2 Retrieval-Augmented Generation\n\nRetrieval-Augmented Generation (RAG) (Lewis et al., 2020b) is a hybrid paradigm that enhances the generative capabilities of large language models (LLMs) by incorporating non-parametric external knowledge during inference. This design mitigates well-documented limitations of standalone LLMs, including hallucinations, confident but incorrect outputs, and inability to reflect up-to-date or domain-specific information due to static pretraining (Hurst et al., 2024; Grattafiori et al., 2024; Yang et al., 2025a).", "RAG introduces an explicit retrieval step: for a query q , a retriever selects a set of topk documents { d 1 , . . . , d k } from an external corpus. A generator G then conditions on both q and the retrieved context to produce the output a = G ( q, d 1 , . . . , d k ) where G is typically an encoderdecoder or decoder-only LLM fine-tuned to integrate retrieved evidence into its generation process.", "2.3 Challenges in RAG\n\nA central challenge in RAG is that generation quality is tightly coupled with retrieval quality. If relevant (i.e., answer-containing) documents are not retrieved, or if irrelevant context is included, the generator is prone to producing incorrect or misleading outputs (Cuconasu et al., 2024, 2025; Liu et al., 2024).", "Consequently, improving the topk retrieval results is crucial. This can be viewed both as a selection problem (how to retrieve the most relevant documents) and a filtering problem (how to suppress distracting or noisy context). To this end, several methods have been proposed that incorporate various forms of feedback , ranging from simple lexical overlap to more sophisticated agentic or reasoning-based signals, to guide and refine the retrieval process.", "In this survey, we systematically categorize these feedback mechanisms and analyze their effectiveness across different components of the RAG pipeline. We focus on how feedback is acquired, represented, and integrated into retrieval, with the", "Figure 1: Illustration of feedback signals across the RAG pipeline. Feedback can modify the query (e.g., rewriting), the retrieved pool (e.g., ranker-based expansion), or the generation loop (e.g., retrieval triggers based on uncertainty).\n\nRetrieval Reranker results Generation Query Encoder representation new results Ranker Relevance Dynamic Understanding Generated Output Feedback levels Query Level Retrieval Level Generation Time Query Understanding Ranker Relevance Dynamic Retrieval - Query Expansion - Query Rewriting\n\n- Query Vector Adaption\n\n- Query Decomposition\n- Result refinement\n\n- When to retrieve\n\n- What (new query) to retrieve\n\naim of providing a comprehensive taxonomy and highlighting open research challenges.", "5 Evaluation\n5.3.1 Naturalness Analysis\n\nWe assess the naturalness of adversarial documents from AIP and baseline methods to evaluate stealthiness. Following (Zhang et al., 2024), we use GPT-4o to answer prompts such as 'Is this text meaningless?', 'Is this text unintelligible?', and 'Is this text gibberish?'. Each 'yes' response adds one point, yielding a naturalness score from 0 to 3 (higher is better). Table 3 reports naturalness scores and average ASR across MedSquad, AmazonQA,", "| | Attack | Naturalness Score \u2191 | GRUEN Score \u2191 | ASR (%) \u2191 |\n|---:|:------------------------|----------------------:|----------------:|------------:|\n| 0 | Corpus Poisoning | 0 | 0.446 | 12.69 |\n| 1 | Prompt Injection Attack | 0 | 0.446 | 57.13 |\n| 2 | PoisonedRAG | 3 | 0.711 | 33.34 |\n| 3 | TrojanRAG | 3 | 0.837 | 34.89 |\n| 4 | AIP | 3 | 0.883 | 90.06 |\n\nTable 3: Comparison of AIP with existing attacks based on Naturalness Score, GRUEN Score, and ASR. The naturalness score ranges from 0 to 3, the GRUEN score ranges from 0 to 1, and higher values in all metrics indicate better performance.", "| | 0 | 1 | 2 |\n|---:|:----------------|:----------|:----------|\n| 0 | Top-k Retrieval | ASR (%) \u2191 | ACA (%) \u2191 |\n| 1 | Top-3 | 90.47 | 61.90 |\n| 2 | Top-5 | 100.0 | 66.70 |\n| 3 | Top-10 | 100.0 | 80.95 |\n| 4 | Top-20 | 100.0 | 90.47 |\n\nTable 4: Performance of AIP for different top-k retrieval on MedSquad knowledge base for cost-effective adversarial instructional prompt.", "and MoviesQA. AIP outperforms Corpus Poisoning and Prompt Injection in naturalness. Although PoisonedRAG and TrojanRAG score higher on naturalness, their ASR remains limited to 33.34% and 34.89%, respectively.", "We acknowledge the limitations of relying solely on LLM-based judgments for evaluating naturalness, as illustrated in Table 3. To address this, we supplement the GPT-based scores with GRUEN (Zhu and Bhat, 2020), a well-established NLP quality metric. Our results show that AIP achieves an average GRUEN score of 0.883, outperforming Corpus Poisoning (0.446), Prompt Injection (0.446), PoisonedRAG (0.711), and TrojanRAG (0.837). These findings reinforce that AIP-generated content maintains superior linguistic quality and fluency, further supporting its stealthiness.", "5.3.2 Top-k Retrieval\n\nWe investigate the impact of different top-k retrieval in the RAG pipeline. Table 4 presents the proposed AIP results on the MedSquad knowledge base using the cost-effective adversarial instructional prompt. The results indicate that both attack and clean performance drop with top-3 retrieval compared to top-5 retrieval. Moreover, as top-k increases, clean performance improves, while attack performance (ASR) remains consistently high.", "5.3.3 Impact of Language Model\n\nWe examine the transferability of AIP by varying the LLM used in the RAG pipeline. Table 5 presents the results on the MedSquad knowledge base using GPT-3.5-turbo, GPT-4, Llama 3.1, and Gemini. AIP achieves a perfect attack success rate (ASR) of 100% with GPT-3.5-turbo, GPT-4, and Llama 3.1, while slightly lower ASR of 80.95% is observed with Gemini. In terms of clean accuracy (ACA), Gemini achieves the highest score of 71.42%, followed by GPT-3.5-turbo (66.70%), GPT-4 (61.90%) and Llama 3.1 (60.00%). These results demonstrate that AIP is highly transferable and robust across a range of popular LLMs, maintaining strong attack effectiveness without significantly compromising clean performance.", "| 0 | Language Models | ASR (%) \u2191 | ACA (%) \u2191 |\n| 1 | GPT-3.5-turbo | 100.0 | 66.70 |\n| 2 | GPT-4 | 100.0 | 61.90 |\n| 3 | Llama3.1 | 100.0 | 60.00 |\n| 4 | Gemini | 80.95 | 71.42 |\n\nTable 5: Performance of AIP using GPT-3.5-turbo, GPT-4, Llama3.1, and Gemini LLMs in RAG's pipeline on MedSquad knowledge base.", "5.3.4 Robustness Analysis\n\nWe assess the robustness of AIP against three standard defenses: (1) Perplexity-based Detection, which flags text that deviates from language model likelihood distributions; (2) Automatic Spamicity Detection, which captures repetitive or spam-like patterns; and (3) Fluency Detection, which evaluates grammaticality and readability. As shown in Table 6, the average detection rates across MedSquad, AmazonQA, and MoviesQA remain low, 26.67% for both Perplexity and Spamicity, and 33.33% for Fluency. These results indicate that adversarial documents produced by AIP are largely indistinguishable from clean ones, effectively evading current defenses. This underscores the need for stronger detection methods, as AIP maintains high fluency and naturalness. Additional details are provided in Appendix A.5.", "5.3.5 Instructional Prompt Rephrasing\n\nSince users may rephrase instructional prompts, we further evaluate the robustness of AIP under this more challenging setting to demonstrate its ability to generalize across prompt variations. We con-", "| | 0 | 1 |\n|---:|:-----------------|:---------------------|\n| 0 | Defense Method | Detection Rate (%) \u2191 |\n| 1 | Perplexity Score | 26.67 |\n| 2 | Spamicity Score | 26.67 |\n| 3 | Fluency Score | 33.33 |\n\nTable 6: Robustness Evaluation of AIP using Perplexity, Spamicity, and Fluency detection defenses.", "6 ANALYSIS\n\n6.1 ANALYSIS OF ITERATIVE PERFORMANCE\n\nFigure 2: Accuracy and recall of MI-RAG on InfoSeek subset across 9 iterations.\n\nImpact of Iterations on MIRAG Performance g Metric Accuracy Recall Iteration", "6.2 SCALING RETRIEVERS\n\nBuilding upon the established benefits of scaling MLLMs, we now investigate the impact of scaling the retriever. We investigated this by employing a larger vision encoder (SigLIP2-g) and a text embedding model (Qwen3-Embedding-0.6B). On the InfoSeek validation split, when using Gemini2.5-Flash for MI-RAG, this enhanced retrieval improves accuracy to 66.7% (+6.4%) and cumulative recall to 95.2% (+3.1%). Similarly, on the Encyclopedic VQA test split, the scaled retriever paired with the Gemma-3-4B enhances accuracy to 50.2% (+1.9%) and cumulative recall to 65.7%\n\nWe analyze how iterative refinement impacts MI-RAG's performance by measuring accuracy and recall across iterations. As shown in Figure 2, performance improves consistently with each step. The initial iterations deliver significant gains. Although the rate of improvement moderates in later steps, the model continues to achieve substantial gains in accuracy and recall. This performance improvement comes with an associated computational cost for each iteration, which we detail in Appendix F.", "(+12.1%). These findings confirm that improving retrieval capability provides significant benefits for MI-RAG's performance, validating the synergistic design.", "6.3 TOP-K SENSITIVITY ANALYSIS\n\n| | Model | Metric | k = 10 | k = 20 |\n|---:|:-----------------|:-----------|:------------|:------------|\n| 0 | Gemma-3-4B | Acc Recall | 52.66 90.82 | 48.95 94.52 |\n| 1 | Gemini-2.5-Flash | Acc | 61.84 94.36 | 65.70 |\n| 2 | | Recall | | 95.97 |\n\nTable 9: Top-k sensitivity analysis on the InfoSeek validation subset. k indicates the number of retrieved image-text pairs.\n\nidence.", "C. Analysis of Top-k Recall\n\nTable III presents the performance metrics of various models in semantic search using text embeddings on the RAG-EDA dataset. The results, reported as recall at varying topk values (R = k), offer valuable insights into each model's effectiveness. The Technical-Embeddings model achieves the highest recall rates across all thresholds: 0.571 at R=5, 0.677 at R=10, 0.733 at R=15, and 0.764 at R=20. These results not only highlight its superior capability in retrieving relevant documents but also underscore its effectiveness in capturing the nuanced relationships within technical content.", "| | Model | R=5 | R=10 | R=15 | R=20 |\n|---:|:-----------------------|------:|-------:|-------:|-------:|\n| 0 | text-embedding-ada-002 | 0.447 | 0.534 | 0.609 | 0.634 |\n| 1 | bge-large-en-v1.5 | 0.503 | 0.596 | 0.634 | 0.66 |\n| 2 | RAG-EDA | 0.547 | 0.658 | 0.702 | 0.733 |\n| 3 | Technical-Embeddings | 0.571 | 0.677 | 0.733 | 0.764 |\n\nTABLE III PERFORMANCE OF THE SEMANTIC SEARCH FOR TEXT-EMBEDDING AT DIFFERENT TOP-K ON THE RAG-EDA DATASET.", "Comparatively, the bge-large-en-v1.5 model [17], known for its high precision applications, shows commendable recall scores but falls short with 0.503 at R=5 and 0.660 at R=20. Although it demonstrates consistent improvement, it does not match the higher recall achieved by the Technical-Embeddings model, especially as the threshold increases. Similarly, the textembedding-ada-002 model [18] performs well, achieving a recall of 0.447 at R=5 and 0.634 at R=20. However, these scores indicate that while it is capable of identifying relevant documents, it lacks the robustness necessary to compete with the advanced capabilities of the Technical-Embeddings model.", "Additionally, the RAG-EDA model [13], which has been noted for its effectiveness in retrieval-augmented generation techniques, also presents strong performance metrics with a recall of 0.547 at R=5 and 0.733 at R=20. Despite this, it still trails behind the Technical-Embeddings model in terms of overall recall, particularly at higher thresholds.", "|---:|:-------------------|------:|-------:|-------:|-------:|\n| 0 | Ours w/o tuning | 0.596 | 0.708 | 0.745 | 0.776 |\n| 1 | Ours w/o queries | 0.603 | 0.689 | 0.733 | 0.776 |\n| 2 | Ours w/o summaries | 0.54 | 0.677 | 0.714 | 0.758 |\n| 3 | Ours (Full Model) | 0.603 | 0.708 | 0.764 | 0.795 |\n\nTABLE IV ABLATION STUDIES ON RAG-EDA DATASET.", "VI. DISCUSSION\n\nCosting. One practical consideration of our SMA framework is the inference cost associated with blackbox access. Since SMA operates in a RAG black-box setting and relies on perturbation-based zero-gradient attribution, each query potentially requires multiple forward passes to complete attribution across tokens. When accessed via APIs, this leads to increased token consumption and corresponding monetary cost. The total token usage for each inference in SMA can be approximated as ( TokenSMA = TokenOutput From Taret +60) . The actual cost depends on the maximum token limits and pricing model of the deployed server. Fortunately, with the emergence of low-cost model providers such as DeepSeek, the economic overhead is increasingly manageable-for example, 1M tokens currently cost as little as $0.07. To further mitigate operational costs and avoid triggering abnormal API usage alerts, a promising future direction is to integrate shadow model inference [52] [53]. This approach would replicate the queried model locally, allowing efficient large-scale offline testing of the SMA framework without incurring external token charges or raising API rate-limiting alarms. Such optimizations could substantially improve the deployment practicality of SMA in high-volume audit scenarios.", "Parameter Sensitivity. Another aspect of our analysis involves understanding how the SMA attack behaves under different parameter settings in both RAG&LLM and MRAG&MLLM System scenarios.", "In Figure 6, we evaluate two key hyperparameters, the RAG number of results top k , and the number of perturbation queries used in zero-gradient blackbox attribution. In Fig.6a , we vary top k from 1 to 8, which controls how many documents are retrieved from the RAG system and appended to the prompt. Despite the increasing uncertainty introduced by adding more retrieved passages, our SMA method maintains a consistently high ACC across different top k values. This suggests that SMA is robust to varying retrieval depths and can effectively filter out non-member content from diverse inputs. Fig. 6b explores the effect of different perturbation counts, ranging from 1 to 100. Perturbations influence the precision of attribution in zero-gradient settings: too few queries may yield noisy gradients, while excessive queries increase computational load. Our results show that SMA achieves stable and high accuracy around 6080 perturbations, with diminishing returns beyond that point. The number of perturbations can thus be flexibly tuned based on the attacker's computational budget and accuracy requirement.\n\n(a) Adaptation to RAG\n\ntop\nk\n\nAdaptation to RAG Top_k\n\n0.6 ACC (Smooth) Coverage (Smooth) ACC Regression 95% CI (ACC)\n\nBlack-box Perturbations vs. ACC\n\n0.80 0.75 0.70 0.65 0,60 Perturbation Count\n\n(b) Black-box Perturbations vs. ACC", "Fig. 6: (a) Varying RAG retrieval depth shows that SMA sustains high ACC and graceful coverage degradation even as top k grows. (b) Varying the number of zerogradient perturbations reveals an optimal budget that maximizes ACC in black-box MIA.", "Limitations. While SMA demonstrates strong performance under RAG scenarios, it has several practical limitations. First, as a strictly semi-black-box technique,", "SMA is constrained by the language model's maximum token limit and sampling temperature. Specifically, if the temperature is set above typical operational ranges such as temperature bigger than 5.0 generated outputs can become overly stochastic, degrading attribution consistency and attack accuracy. Similarly, overly restrictive max_tokens settings truncate the context window, preventing sufficient perturbation analysis and impairing the attack's success rate. Second, SMA's reliance on repeated perturbation-based queries leads to high CPU and API usage. Conducting dozens to hundreds of semi-black-box attribution passes incurs significant computational overhead and latency, resulting in longer overall execution times compared to whitebox or single-shot methods. Future work may explore optimizations such as batched perturbations or early stopping criteria to alleviate these resource constraints."]}
{"type": "entry", "index": 49, "question": "how do i catch when the model ignores retrieved context and answers from memory", "answer": "You can\u2019t perfectly \u201csee inside\u201d the model, but you can set up checks that reveal when it is not using the retrieved context and is instead relying on its own memory.\n\n1. Ask the model to judge the context\u2019s sufficiency\nUse an extra step where the model (or a smaller helper model) is prompted only with the query plus retrieved passages and must answer: \u201cIs this context sufficient to answer the question?\u201d and justify its decision. This is like a Pre\u2011Check/Post\u2011Check that decides if retrieval is needed or sufficient before answering [2]. If it says the context is sufficient but its final answer contradicts that context, it is likely answering from memory.\n\n2. Compare answers with and without context\nRun the model twice: closed\u2011book (no retrieval) and with retrieval. When the closed\u2011book answer is confident and correct but the RAG answer changes in a way that contradicts the given passages, you\u2019re seeing a context\u2011memory conflict, where the model is overriding or ignoring context [1]. Tracking a \u201cResilience\u201d metric (how often previously correct closed\u2011book answers stay correct when you add context) helps quantify this [1]. Large drops indicate the model is not reliably grounded in the retrieved text.\n\n3. Add an explicit context\u2011relevance check\nBefore answering, have a helper model score whether the retrieved context actually covers the question (a context coverage check) [3]. If coverage is high but the final answer uses facts not present in the passages, you can flag that response as likely coming from parametric memory rather than the retrieval.\n\nCombining these: (a) measure context coverage, (b) log closed\u2011book vs RAG answer changes, and (c) add a small \u201cdetector\u201d step that decides whether to trust or ignore retrieval [1][2][3].", "tags": ["_untagged"], "full_faithfulness": 0.8823529412000001, "full_answer_relevancy": 0.0, "seed_faithfulness": 0.9375, "seed_answer_relevancy": 0.0, "context_relevance": 0.735627097589058, "context_relevance_error": null, "context_relevance_reason": "The context passages provide substantial information about the challenges of context-memory conflict in retrieval-augmented generation (RAG) systems, which directly relates to the user's question about catching when a model ignores retrieved context. The passages discuss various strategies to mitigate these conflicts, such as adaptive retrieval and robust training, which are relevant to understanding how to address the issue. However, while the context is informative, it could be more explicitly focused on practical methods for detecting when a model relies on memory over retrieved context, which slightly limits the overall relevance.", "structure_citation": 0.5866427796520451, "structure_error": null, "structure_reason": "The Actual Output provides a structured approach to identifying when a model relies on memory instead of retrieved context, but it lacks proper citations for the claims made. While it references methods and concepts that align with the Context, it does not include the necessary [n] citations to substantiate its claims, which is a significant shortcoming according to the evaluation steps. Specifically, the output mentions techniques like context sufficiency checks and resilience metrics but fails to link these to the corresponding snippets in the Context, leading to unsupported claims.", "hallucination_score": 0.8226107523005671, "hallucination_error": null, "hallucination_reason": "The response effectively outlines methods to detect when a model relies on memory instead of retrieved context, aligning well with the evaluation steps. It identifies key strategies such as context sufficiency checks and comparing answers with and without context, which are supported by the context provided. However, while the claims are mostly accurate, the response could benefit from more explicit references to the context's details, particularly regarding the implications of context-memory conflict, which would strengthen the overall alignment.", "contexts_used": ["1 Introduction\n\nRetrieval-augmented generation (RAG) serves as an effective strategy for enhancing large language models (LLMs) by grounding them in external information (Lewis et al., 2020; Gao et al., 2023; Huang and Huang, 2024). However, when the retrieved context contradicts the LLM's internal knowledge, it causes a critical vulnerability: the challenge of resolving the conflict between incorrect external context and correct internal knowledge, known as context-memory conflict .", "This challenge is exacerbated when incorrect yet highly ranked contexts serve as hard negatives. Conventional RAG, i.e. , simply appending\n\n* Corresponding author\n\n1 https://github.com/eunseongc/CARE\n\nFigure 1: LLMs struggle to resolve context-memory conflict. Green bars show the number of questions correctly answered without retrieval in a closed-book setting. Blue and yellow bars show performance when provided with a positive or negative context, respectively.\n\nClosed-book w/ Positive Context W/ Negative Context 8k 25.1% 49.1% 39.6% 47.5% 6k 4k 2k Mistral-7b LLaMA3-8b GPT-4o-mini Claude-3.5\n\nretrieved context to the prompt, struggles to discriminate between incorrect external context and correct parametric knowledge (Ren et al., 2025). This misalignment leads to overriding correct internal representations, resulting in substantial performance degradation on questions that the model initially answered correctly. As shown in Figure 1, we observed significant performance drops of 25.149.1% across state-of-the-art LLMs when negative contexts were added to questions for which the LLM had already generated the correct answer without external context.", "To mitigate context-memory conflict, existing studies such as adaptive retrieval (Ren et al., 2025; Baek et al., 2025) and the decoding strategies (Zhao et al., 2024; Han et al., 2025) adjust the influence of external context either before or during answer generation. However, due to the LLM's limited capacity in detecting conflicts, it is susceptible to misleading contextual inputs that contradict the LLM's parametric knowledge. Recently, robust training has equipped LLMs, enabling them to identify conflicts (Asai et al., 2024; Wang et al., 2024). As shown in Figure 2(a), it enables the LLM to dis-", "Negative Context Misleading RAG\n\nFigure 2: (a) Vanilla RAG fails with negative context despite having the correct information in parametric knowledge. (b) Robust training can cause the LLM to disregard instructions and suffer from catastrophic forgetting, particularly when transitioning between tasks like question answering and fact verification.\n\nQuestion: how many\nmedals did\n(16\"\n\naustralia win in the 2000 olympics?\n\nLLM\n(Correct)\n\nAustralia at the 2000 Summer Paralympics\n\nVanilla RAG\n\nAustralia finished at the top of the medal\n\n(63\"\n\n39 silver and 47 bronze\n\n(Incorrect)\n\nRAG prompt wl a negative\n\nGwlRobust\nTraining\nFine-tuned\n\nInstruction Neglect in Fine-tuned LLM\n\nVerify the claim with True or False\n\n\"Adare Manor\n\nClaim: Adare Manor is in Miami\n\nHotel & Golf Adare Adare Manor is mansion located Resort\" on\n\nlands on the banks of the River Maigue .\n\ngold\ngold,\npair\n\ncern conflicts and assess the confidence of external contexts, i.e. , whether to rely on them during generation. Although it demonstrates promising in-domain performance gains, it incurs the catastrophic forgetting (Luo et al., 2023; Yang et al., 2024) problem, which significantly impairs the generalization performance of the LLM. As shown in Figure 2(b), the robust fine-tuning of QA datasets results in the model forgetting knowledge related to tasks beyond the specific datasets.", "This motivates our central research question: How can we incorporate conflict awareness into LLMs while preserving their general capabilities? To this end, we propose C onflictA ware RE trievalAugmented Generation ( CARE ) , comprising two components: a context assessor and a base LLM . The context assessor, instantiated from the base LLM itself, is designed to identify knowledge conflicts. Inspired by soft prompting (Ge et al., 2024), the context assessor encodes external context into compact, trainable memory tokens, referred to as soft context embeddings .", "Specifically, the context assessor is trained using a conflict-aware training strategy. First, we perform reconstruction pre-training to enable the context assessor to encode memory embeddings from raw context. Next, we fine-tune the assessor to discern between accurate and conflicting knowledge via grounded/adversarial soft prompting . When the base LLM produces an incorrect answer relying solely on its parametric knowledge, we provide a positively retrieved passage to supervise the assessor. Conversely, when the LLM answers correctly without retrieval, we pair it with a hard negative context to discourage unnecessary reliance on re-\n\n| | Approach | Soft Decision | Conflict Awareness | Generality |\n|---:|:-----------------------------------------------------|:----------------|:---------------------|:-------------|\n| 0 | Adaptive Retrieval Decoding Strategy Robust Training | \u2717 \u2713 \u2713 | \u2717 \u2717 \u2713 | \u2713 \u2713 \u2717 |\n| 1 | CARE (Ours) | \u2713 | \u2713 | \u2713 |\n\nTable 1: Comparisons for existing studies to resolve knowledge conflict. Our method achieves comprehensive coverage across all criteria.", "trieved context. This training strategy enables the assessor to guide the LLM along the appropriate reasoning path, balancing retrieved and parametric knowledge. Notably, CARE does not require any fine-tuning of the base LLM, preserving the general-purpose ability of the base LLM.\n\nWe summarize our key contributions as follows.", "We examine how the conflict between external context and parametric memory hinders the conventional RAG system.\n\nWe propose CARE via grounded/adversarial soft prompting to learn context embeddings that encode both the context and its implicit confidence, thereby preventing the LLM from being misled by conflicting knowledge.", "Experimental results show that CARE significantly improves the robustness of RAG systems, increasing the overall performance by up to 5.0% over existing methods on QA and fact-checking benchmarks.", "2 Related Work\n2.1 Context-Memory Conflict\n\nRecent studies have focused on improving the conflict between external context and internal knowledge. Specifically, they are categorized into three directions: (i) Adaptive Retrieval, (ii) Decoding Strategies, and (iii) Robust Training.", "Adaptive Retrieval. It aims to selectively incorporate external context only when the LLM lacks sufficient knowledge, mitigating the conflict between retrieved and parametric knowledge. The decision is generally made through prompting the LLM to judge its uncertainty (Ren et al., 2025), estimating confidence from hidden states (Su et al., 2024; Yao et al., 2024), or using an external module trained to predict retrieval necessity (Wang et al., 2023; Jeong et al., 2024; Baek et al., 2025). However, it is inherently difficult for the LLM to accurately assess the boundaries of its knowledge in the process of making discrete retrieval decisions (Xiong et al., 2024).", "Decoding Strategy. A representative work, CAD (Shi et al., 2024) adjusts the model's output distribution by contrasting output probability distributions with and without the context. Subsequent studies use additional information to dynamically adjust the weights of the contrastive decoding distribution according to the strength of the knowledge conflict (Yuan et al., 2024; Han et al., 2025). Since these methods combine distributions that already reflect conflicting information, it is crucial to incorporate conflict-awareness within the model itself rather than relying solely on the decoding stage.", "Robust Training. It trains the LLM to assess the reliability of retrieved documents based on its internal knowledge. A representative method performs adversarial training, where negative documents are introduced during fine-tuning to help the model recognize contradictions and assess context reliability (Yoran et al., 2024; Fang et al., 2024). Another line of work explicitly trains the LLM to acquire new capabilities, such as evaluating the relevance of retrieved documents and deciding whether retrieval is necessary (Wang et al., 2024; Asai et al., 2024). However, fine-tuning LLMs risks catastrophic forgetting, which can degrade their generalization performance and erode parametric knowledge. In contrast, our method addresses the knowledge conflict by leveraging conflict-aware context representations, thereby preserving the generality of LLMs. Table 1 presents a comparison between CARE and existing approaches across the three criteria.", "2.2 Soft Prompting\n\nSoft prompting aims to encode lengthy text into a few trainable continuous embeddings (Chevalier et al., 2023; Mu et al., 2023; Qin and Durme, 2023). Ge et al. (2024) introduces memory slots appended to the input context, leveraging their final hidden states to reconstruct the original context. Recently, Cheng et al. (2024) introduced a lightweight MLP to transform retrieval embeddings into documentlevel soft prompt vectors. While previous studies have primarily used soft prompts to compress retrieved contexts or instructions, we repurpose them to balance the influence of external information against internal knowledge. Specifically, we encode soft context embeddings that reflect the reliability of the retrieved content. Our approach offers a straightforward solution to the context-memory conflict by improving the context representation directly, rather than relying on an additional mechanism at the retrieval or decoding stage.", "5 Results and Analysis\n\n5.1 Main Results\n\nTable 2 compares CARE with baselines across tasks using two recent LLMs. CARE achieves the best overall performance, outperforming standard RAG by 5.01% and 6.13% with Mistral and LLaMA, respectively, demonstrating the effectiveness of our approach. Key results are as follows: (i) Directly fine-tuning the LLMs, such as RetRobust (Yoran et al., 2024) or Direct FT , has proven effective on the short-form QA tasks they were trained on. However, these methods struggle to generalize to out-of-domain tasks such as longform QA and fact-checking, exhibiting substantial performance decrements (see Appendix D for a case study). In contrast, CARE avoids directly fine-tuning the LLM, thereby maintaining generalization across tasks while achieving competitive performance. (ii) Our method benefits from soft context embeddings that allow the LLM to fall back on its internal knowledge when retrieval is unreliable. This leads to a 5.29% performance gain in Mistral-7B over the Adaptive Retrieval approach, which relies on hard decisions, highlighting the advantage of soft decision-making in fully leveraging parametric knowledge. (iii) CARE aims to detect conflicts and assesses context reliability to balance parametric and retrieved knowledge. In contrast, relying solely on retrieved context in response to conflict, as in CAD (Shi et al., 2024) and AdaCAD (Han et al., 2025), leads to poor performance when the context is noisy. This highlights CARE as a more viable approach for real-world scenarios. (iv) CARE remains robust by effectively leveraging the LLM's internal knowledge, as shown on WebQA. In this dataset, the closed-book setting with LLaMA yields the best performance, indicating that parametric knowledge is crucial for the task. See Table 9 for retrieval quality and Appendix B for additional experiments with Qwen.", "5.2 Fine-grained Evaluation\n\nFigure 4 presents a fine-grained evaluation on NQ dataset. To effectively evaluate how the methods handle context-memory conflicts, we define two evaluation regimes: Resilience , measuring the preservation of correct answers, and Boost , mea-\n\nFigure 4: Fine-grained evaluation on TriviaQA using Mistral-7B. Resilience measures the accuracy on questions that were correctly answered in the closed-book setting, while Boost measures the accuracy on those that were initially incorrect. Numbers in parentheses indicate the number of samples.\n\nRAG 0.8 ADACAD SKR-kNN Priori Judgment 0.6 CARE (Ours) 2 0.4 0.2 0.0 AlI (11,313) Resilience (6,523) Boost (4,790)", "| | C pos. | C neg. | Criteria. | NQ dev (Span EM).All | NQ dev (Span EM).Res. | NQ dev (Span EM).Boost |\n|---:|:-----------------|:-----------------|:-----------------|-----------------------:|------------------------:|-------------------------:|\n| 0 | \u2713 | \u2713 | Correct | 0.438 | 0.766 | 0.291 |\n| 1 | \u2713 | - \u2713 \u2713 | - | 0.414 | 0.696 | 0.287 |\n| 2 | - | | - | 0.29 | 0.776 | 0.071 |\n| 3 | \u2713 | | Random | 0.431 | 0.792 | 0.269 |\n| 4 | \u2713 | Random | Correct | 0.414 | 0.727 | 0.273 |\n| 5 | w/o Pre-training | w/o Pre-training | w/o Pre-training | 0.347 | 0.804 | 0.14 |\n| 6 | w/o L LM | w/o L LM | w/o L LM | 0.405 | 0.726 | 0.26 |\n| 7 | w/o L KD | w/o L KD | w/o L KD | 0.403 | 0.695 | 0.272 |", "Table 3: Ablation study for CARE on the validation subset of Natural Questions (NQ). The best performance is marked in bold . Each subset, i.e. , Res. and Boost, contains 6,058 and 2,734 evaluation samples, respectively.", "suring improvements on initially incorrect answers. (i) CARE achieves the highest Resilience performance, demonstrating strong robustness to contextmemory conflicts. This indicates the ability to assess the reliability of retrieved passages and selectively rely on external context versus internal knowledge. (ii) Adaptive Retrieval methods, which filter out questions unlikely to benefit from retrieval yield a high Resilience score. However, their Boost scores are substantially lower, likely due to overconfidence on the LLM's prior knowledge (Xiong et al., 2024). (iii) Dynamic decoding strategies show a modest Boost score by prioritizing retrieved context, but significantly reduce Resilience score. This suggests that blindly favoring retrieved content may harm performance, particularly in settings that potentially contain misleading context.", "Figure 5: t-SNE visualization of context embeddings generated by SFR and CARE. Red and blue points correspond to positive and negative labels, respectively, based on whether the context contains the answer span.\n\nSFR-Embedding-Mistral CARE (ours Positive Negative", "5.3 Ablation Study\n\nTable 3 thoroughly breaks the proposed method to evaluate the contribution of each component. In this experiment, we report the results on the validation set of NQ with the Resilience and Boost scores.", "Conflict-aware Fine-tuning. Using only C pos or C neg significantly reduces Resilience and Boost score, respectively. This highlights the mutual benefit of their roles in helping the context assessor balance parametric and external knowledge. In addition, when the conflict is not simulated with the correct signals in a closed-book setting, the Boost score drops substantially, suggesting the importance of exposing knowledge gaps for learning informative context embeddings.", "Random Negatives. Replacing hard negatives with randomly sampled context from the corpus results in a 5.48% decrease. Notably, the decline appears across both subsets, highlighting the importance of training with meaningful conflict signals, as easy negatives fail to expose substantive conflict.", "Reconstruction Pretraining. Removing pretraining leads to a 20.9% performance drop, as the context assessor fails to learn compact and meaningful memory embeddings. Note that it also significantly affects the fine-tuning phase, where context must be conveyed in a scenario-dependent manner.", "Loss Ablation. L LM and L KD play the complementary roles in conflict-aware fine-tuning stage. Removing the LM loss degrades Boost rate from 0.291 to 0.260, while removing the KD loss reduces Resilience rate from 0.766 to 0.695. This confirms that LM loss ensures capturing important information for answer generation, while KD loss teaches context selectivity through scenariospecific teacher supervision.", "1. INTRODUCTION\n\nRetrieval-Augmented Generation (RAG) seeks to enhance Large Language Models (LLMs) by integrating external knowledge bases, thereby mitigating factual inaccuracies and hallucinations [1, 2]. This approach has garnered significant interest for its potential in domain-specific applications. However, the performance of RAG systems is heavily dependent on the quality of retrieved information, and existing methods often face critical challenges that degrade their effectiveness, as illustrated in Figure 1.", "One primary issue is Semantic Discrepancy, where retrievers, optimized for semantic similarity, fail to capture the nuanced 'relevance' required by complex queries. For instance, a query about 'callus formation' might retrieve documents discussing 'rigid immobilization' due to superficial semantic overlap, even though 'movement at the fracture site' is the more relevant concept for answering the question. This discrepancy between similarity and relevance often leads to the retrieval of suboptimal or misleading information.", "Amore subtle and pernicious problem, which we identify as Retrieval Hallucination, occurs particularly with LLMs that have been fine-tuned on specialized domain data. These\n\n1 Code is available: https://anonymous.4open.science/r/AC-RAG/\n\nFig. 1 : AC-RAG overcomes key failures of vanilla RAG: (1) Semantic Discrepancy (retrieving irrelevant content) and (2) Retrieval Hallucination (overconfidence in accepting poor results). Our framework addresses both through adversarial collaboration.\n\nWhich of the following increases callus formation?", "A. Rigid immobilization B. Movement at fracture site\n\nC. Compression plating D. Intraosseous nailing\n\nWhat is\nRigid immobilization\n\n?\n\nrefers to \u2026 KG Immobilization techniques such as \u2026\n\nNot sufficient for problem-solving.\n\ncallus formation\n\nCallus formation results from\n\nSufficient for problem-solving.\n\nB. Movement at fracture site\n\nWhich of the following increases\n\nA.\n\nB.\n\nMovement at fracture site\n\nC.\n\nCompression plating\n\nD.\n\nIntraosseous nailing\n\nrespiratory distress syndrome\n\nin adults may be produced by any\n\nof a number of causes resulting in\n\nsimilar pathophysiologic changes\n\nand having identical therapeutic\n\nimplications. the most important\n\nfactors in treatment are early\n\nrecognition and early institution\n\nof therapy\n\nAlthough some concepts still require\n\nexplanation, my existing knowledge is\n\nalready sufficient for problem-solving.\n\nA. Rigid immobilization \u2460 Semantic Discrepancy \u2461 Retrieval Hallucination Self-Reflect \u2713 \u2717", "expert models can exhibit overconfidence in their own parametric knowledge, causing them to either bypass the retrieval process entirely or uncritically accept irrelevant search results during self-reflection phases. As shown in Figure 1, even when retrieval provides a low-quality document about 'respiratory distress syndrome' for a question on 'callus formation', a fine-tuned model might erroneously conclude its existing knowledge is sufficient and proceed to generate an incorrect answer. This overconfidence prevents the model from initiating further, more targeted searches, effectively trapping it in a state of hallucination supported by poor retrieval. This challenge is particularly acute in systems that utilize self-correction or reflection mechanisms but are hindered by an overconfident generator [3, 4].", "To address these interconnected challenges, we propose the Adversarial Collaborative RAG (AC-RAG), a novel framework designed to foster more rigorous and accurate knowledge acquisition. AC-RAG orchestrates a dynamic interaction between two heterogeneous agents: a generalist, non-fine-tuned Detector and a domain-expert, fine-tuned Re-", "Fig. 2 : Overview of our framework. We employ two different LLMs in our Adversarial Collaboration process (Detector + Retriever). A Neutral Moderator supervises the entire operation. For each task Q , our workflow is divided into four phases, involving between 0 and N retrievals. The process concludes at an optimal point through Adaptive Retrieval. The memory M from each interaction is used as a reference for subsequent interactions and for generating the final response.\n\nPart2: Challenge Dissection\n\nPart3: Retrieval & Integration\n\nList the professional terms needed for further explanation.\n\nSaltatory conduction\n\nExplain the definition of the professional terms.\n\nWalk me through the context below and write a summary.\n\nSaltatory conduction:\n\nSaltatory conduction is a process\n\nby which nerve impulses travel along myelinated axons,\n\njumping from one node of Ranvier to the next.", "In neuroscience, saltatory conduction (from Latin saltus 'leap, jump') is the\n\npropagation of action potentials along myelinated axons from one node of\n\nRanvier to the next node, increasing the conduction velocity of action potentials.", "The uninsulated nodes of Ranvier are the only places along the axon where ions\n\nare exchanged across the axon membrane, regenerating the action potential\n\nbetween regions of the axon that are insulated by myelin, unlike electrical\n\nconduction in a simple circuit.\n\n: Neutral Moderator\n: Resolver\n: Detector\n\nWhich of the following is\n\nnot true for myelinated\n\nnerve fibers?\n\nA. Impulse through\n\nmyelinated fibers is slower\n\nthan non-myelinated fibers\n\nB. Membrane currents are\n\ngenerated at nodes of\n\nRanvier\n\nC. Saltatory conduction of\n\nimpulses is seen\n\nD. Local anesthesia can be\n\neffective only when the\n\nnerve is not covered by\n\nmyelin sheath Instruction Tuning on Domain-specific Data Part1: Pre-Check\n\nAre there any medical terms in this\n\nquestion that you do not understand?\n\nAnalysis: Myelinated nerve fibers as local anesthesia\n\ncan be less effective on them compared to non-\n\nmyelinated fibers,\n\nAnswer: A. Impulse through myelinated fibers is\n\nslower than non-myelinated fibers.\n\nAnswer the question directly.\n\nNo\nYes\n\nMyelinated Fibers:\n\nMyelinated fibers are nerve fibers\n\ncovered with myelin sheath, enhancing the speed and\n\nefficiency of nerve impulse transmission.\n\nMembrane currents:\n\nMembrane currents refer to the\n\nflow of ions across the cell membrane, driven by\n\ndifferences in ion concentration and electrical potential.\n\nMemory\nPart4: Post-Check\n\nDo you think the context is sufficient\n\nfor answering?\n\nAnalysis: Myelinated nerve fibers conduct impulses\n\nfaster than non-myelinated fibers due to the", "presence of the myelin sheath,\n\nAnswer the question given the\n\nreference context.\n\nTask\n\n:\n\nMyelinated nerve fibers have a myelin sheath,\n\nenabling faster impulse conduction via saltatory\n\nconduction at nodes of Ranvier, unlike non-\n\nmyelinated fibers. Local anesthesia blocks nerve\n\nconduction.", "solver. The Detector's lack of specialized knowledge makes it less prone to overconfidence, positioning it as an ideal agent for identifying potential knowledge gaps and dissecting complex problems into simpler, more retrievable sub-questions. In contrast, the expert Resolver is tasked with answering these sub-questions, leveraging its deep domain knowledge to generate precise queries and synthesize retrieved information. This interaction follows a 'Dissect-Retrieve-Reflect' workflow, where the Detector's persistent questioning and the Resolver's expert synthesis create a productive adversarial tension. This process, guided by a neutral moderator, systematically filters out noise, probes for deeper understanding, and converges on a well-supported, collaborative solution, effectively mitigating both semantic discrepancies and retrieval hallucinations.", "2. METHODS\n\nOur AC-RAG framework is built upon an adversarial collaboration between two heterogeneous agents: a general, nonfine-tuned Detector ( F D ) and a domain-expert, fine-tuned Resolver ( F R ). The Detector identifies knowledge gaps, while the Resolver provides expert solutions. This process is orchestrated by a neutral moderator through a multi-turn 'Dissect-Retrieve-Reflect' workflow, as depicted in Figure 2. The entire workflow, including the pseudo-code (Algorithm 1), is detailed in the Appendix.", "Pre-Check. The workflow begins with a Pre-Check, where the Detector assesses if retrieval is necessary. This step prevents the indiscriminate use of RAG, which can sometimes be counterproductive.", "Challenge Dissection. If retrieval is needed, the process enters an iterative loop. In each round k +1 , the Detector analyzes the original query Q and the accumulated memory M k to formulate a new sub-question t k +1 = F D ( Q,M k ) . The memory M k stores pairs of terms and their summarized explanations from previous rounds, i.e., M k = \u22c3 k i =1 ( t i , s i ) , with M 0 = \u2205 .", "Retrieval & Integration. Next, we first generate a preliminary explanation e k +1 = F R ( t k +1 ) for the sub-question. We then use this explanation as an enhanced query for the retriever R , a technique known to improve performance by aligning the query more closely with the language of the source documents [5]. This yields retrieved documents r k +1 = R ( e k +1 ) . Subsequently, the Resolver summarizes this content to produce a concise explanation s k +1 = F R ( r k +1 ) , filtering out noise. The memory is then updated: M k +1 = M k \u22c3 ( t k +1 , s k +1 ) .", "Post-Check. The final stage of each loop is the Post-Check, where the Detector determines if the current knowledge in memory M k +1 is sufficient. If not, and if the maximum number of iterations N has not been reached, a new round begins. Otherwise, the loop terminates, and the Resolver generates the final answer based on the full context: A = F R ( Q,M n ) .", "Adaptive Retrieval. Decisions in the Pre-Check and PostCheck phases are governed by the Detector's confidence. We measure this by calculating the log probability of it generating an affirmative token (e.g., 'yes'). The confidence score is defined as score k = 1 L \u2211 L \u2113 =1 log P ( y \u2113 \u2208 S | y <\u2113 , Q, M k , F D ) , where S is the set of affirmative tokens. Retrieval is triggered if score 0 > \u03b4 1 (Pre-Check) or if another iteration is requested with score k > \u03b4 4 (Post-Check). For efficiency, we set L = 1 .", "I. Context Quality Metric\n\nContext Coverage (CCov): Assesses whether the retrieved context provides sufficient information to fully or partially answer the query. Metric: Percentage of queries at least partially answerable from the retrieved context."]}
{"type": "entry", "index": 50, "question": "how do i evaluate quote-level grounding, like \u201cshow me the exact sentence\u201d", "answer": "To evaluate quote\u2011level grounding (\u201cshow me the exact sentence\u201d), you need two things: gold labels at the sentence/span level, and metrics that compare the model\u2019s cited spans to those gold labels.\n\n1. Create fine\u2011grained gold data\n- Break documents into sentences or short spans.\n- For each answer (or each claim in an answer), have annotators mark the exact supporting sentence(s) in the source document.\n- This is what current systems like DocsRay explicitly lack: they only track section\u2011level sources, not paragraph or sentence\u2011level links; future work they propose includes \u201chighlighting exact text spans \u2026 or inline citations that map claims to specific source passages\u201d [1]. Your annotation step is filling this gap.\n\n2. Define grounding metrics\n- Span\u2011level EM/F1: Treat each cited sentence or span like an \u201canswer span.\u201d Use Exact Match (did the model select exactly the gold span?) and F1 based on token overlap, just as QA benchmarks do for answer spans [2][3]. HotpotQA goes further and evaluates \u201csupporting\u2011fact EM/F1\u201d separately from answer EM/F1 [3]; you can mirror that: one score for answer correctness, one for supporting\u2011sentence correctness.\n- Grounded vs ungrounded classification: For each claim, decide whether the cited span truly supports it. You can then measure Precision, Recall, F1 and accuracy at this binary level, similar to how MetaRAG converts fine\u2011grained hallucination scores into a response\u2011level classifier with Precision/Recall/F1/accuracy [5].\n\n3. Reporting\n- Report: (a) answer correctness (EM/F1), (b) quote\u2011level grounding scores (supporting\u2011fact EM/F1), and (c) hallucination or mis\u2011grounding rates using the binary classifier view [3][5].\n\nThis combination tells you not just if the answer is right, but whether every quoted claim is backed by the exact sentence in the source.", "tags": ["_untagged"], "full_faithfulness": 1.0, "full_answer_relevancy": 0.9070964744000001, "seed_faithfulness": 1.0, "seed_answer_relevancy": 0.9069446107, "context_relevance": 0.2506810457321161, "context_relevance_error": null, "context_relevance_reason": "The context passages do not directly address the user's question about evaluating quote-level grounding or providing exact sentences. While some passages discuss evidence grounding and retrieval capabilities, they lack specific information on how to evaluate quote-level grounding, which is the core of the user's inquiry.", "structure_citation": 0.38399467729814113, "structure_error": null, "structure_reason": "The Actual Output provides a detailed explanation of how to evaluate quote-level grounding, but it lacks specific citations marked with [n] that correspond to the Context snippets. While it discusses the importance of fine-grained gold data and grounding metrics, it does not reference any specific snippets from the Context, which is a critical requirement for supporting the claims made. This results in a partial alignment with the evaluation steps, leading to a moderate score.", "hallucination_score": 0.6615117424174828, "hallucination_error": null, "hallucination_reason": "The response accurately identifies the key components necessary for evaluating quote-level grounding, such as the need for fine-grained gold data and specific metrics. It effectively outlines the process of creating gold labels and defining grounding metrics, which aligns well with the context provided. However, it does not explicitly reference any specific claims or examples from the context, which could enhance its grounding in the provided material.", "contexts_used": ["4 Experiments\n\n4.1 Results on MMLongBench-Doc\n\nThe performance differences across DocsRay variants illustrate the impact of model scale. All variants use the Gemma-3 family (Team et al. 2025a) without additional training: Pro (27B), Base (12B), and Lite (4B). Notably, even the Lite variant surpasses several leading LVLMs, confirming the strength of our system design.", "Furthermore, DocsRay-Pro's accuracy (64.7%) closely approaches the human expert baseline (65.8%), demonstrating its capability to capture essential human document comprehension skills: structured organization, retrieval-based attention, and multimodal understanding.", "| | Pattern | Description | Example | Key Model Behavior (DocsRay-Pro) |\n|---:|:------------------------------|:---------------------------------------------------------------------------|:---------------|:---------------------------------------------------------------------------|\n| 0 | Single-Source Fact Retrieval | Tasks where the answer is con- tained entirely on a single page | mmlongbench 13 | Correctly retrieves page 2 and extracts the precise location |\n| 1 | Multi-Page Evidence Synthesis | Tasks requiring aggregation of facts across multiple, non-contiguous pages | mmlongbench 36 | Correctly identifies all 5 quotes and their respective page numbers |\n| 2 | Evidence Attribution Failures | Tasks where the document lacks sufficient information to form an answer | mmlongbench 8 | Correctly identifies that data for 2024 is absent and avoids hallucination |\n\nTable 3: Analysis of Evidence Grounding and Scaling Behavior. Hierarchical search ensures systematic coverage, preventing missed evidence common in flat retrieval.", "4.2 Qualitative Analysis of Evidence Grounding\n\nWe conducted a detailed analysis of representative cases from MMLongBench-Doc to understand DocsRay's evidence grounding capabilities and scaling behavior. Throughout this section, we refer to specific test cases (e.g., mmlongbench 13), and the corresponding evidence summaries are presented in Appendix F.", "Single-Source Fact Retrieval For straightforward factual questions like 'Where was Gestalt psychology conceived?' (mmlongbench 13), DocsRay effectively retrieves the specific page containing 'Berlin School of Experimental Psychology.' This demonstrates the system's precision in localized fact retrieval where evidence is unambiguous.", "Multi-Page Evidence Synthesis More challenging cases require aggregating information across multiple pages. The question 'How many human quotes with sources?' (mmlongbench 36) necessitates scanning pages 14, 19, 20, 33, and 37 to identify all quoted individuals. DocsRay's hierarchical retrieval excels here by ensuring systematic section coverage rather than relying on keyword matching alone, preventing the missed evidence common in flat retrieval approaches.", "Evidence Attribution Failures The most instructive cases involve questions where evidence is absent. When asked about 'Democrats voting percentage in 2024?' (mmlongbench 8), the document only contains data through 2018. DocsRay correctly identifies this limitation rather than hallucinating an answer, demonstrating robustness against unanswerable questions, a key feature for trustworthy AI.", "Our scaling analysis across model sizes reveals that while all models handle simple fact retrieval effectively, complex multi-page synthesis and visual understanding capabilities strongly correlate with model scale. The Pro model maintains coherent tracking across extended contexts, while smaller models struggle with evidence aggregation tasks. For a comprehensive analysis of additional evidence grounding patterns including statistical evidence requirements (mmlongbench 10), visual evidence interpretation (mmlongbench 20, mmlongbench 26), and cases with ambiguous evidence (mmlongbench 9, mmlongbench 18), please refer to Table 7 in the Appendix F.", "| | Task Type | Pro | Base | Lite |\n|---:|:------------------------------------------|:------|:-------|:-------|\n| 0 | Tier 1: Simple Fact Retrieval | \u221a | \u221a | \u221a |\n| 1 | Where was Gestalt psychology con- ceived? | | | |\n| 2 | What year is the report for? | \u221a \u221a | \u221a \u221a | \u221a \u221a |\n| 3 | Republican Hispanic vs no leans male | | | |\n| 4 | Tier 2: Complex Reasoning | \u221a | \u221a | |\n| 5 | Define law of good gestalt (technical) | \u221a | \u221a | \u00d7 |\n| 6 | Count human quotes (multi-page) | \u221a | \u221a | \u00d7 |\n| 7 | 5% support rate analysis | | | \u00d7 |\n| 8 | Tier 3: Advanced Synthesis | \u221a | | |\n| 9 | Count exterior photos (10+ pages) | \u221a | \u00d7 | \u00d7 |\n| 10 | Count hand-drawn cartoons (visual) | \u221a | \u00d7 | \u00d7 |\n| 11 | Identify closure principle shapes | | \u00d7 | \u00d7 |\n\nTable 4: Performance across model scales reveals three capability tiers. All models handle simple retrieval, but only larger models excel at complex multi-page synthesis and visual understanding tasks.", "4.3 Comparative Analysis Across Model Scales\n\nTo better understand the impact of model scaling on document understanding capabilities, we conducted a detailed comparative analysis using three variants of DocsRay. By examining performance differences across simple fact retrieval, multi-page synthesis, and visual understanding tasks, we can identify which capabilities emerge or improve with increased model scale.", "Simple Fact Retrieval All model sizes successfully handle straightforward factual questions requiring single-page evidence. Even the Lite model correctly retrieves specific facts like 'Berlin School of Experimental Psychology,' validating our hierarchical retrieval architecture's effectiveness across scales.", "G.1 Limitations of Docsray\n\nDependency on Backbone LLM Choice The quality and structure of the generated pseudo-TOC depends on the specific choice of the backbone LLM. Different models may produce varying segmentation boundaries and title qualities based on their training data and inherent biases. While our experiments with Gemini-1.5 Pro show robust performance, the pseudo-TOC generation approach may require prompt engineering adjustments when adapting to other LLMs. This dependency underscores that our contribution lies not in a universal algorithm but in demonstrating how to leverage LLM capabilities for document structuring.", "Performance on Documents with Multiple Images SlideVQA and similar benchmarks requiring reasoning over multiple images per page expose expected limitations given our design choices. DocsRay achieves only 17.1% EM (Exact Match) for Pro, 16.1% for Base, and 9.91% for Lite variants. The relatively low score on SlideVQA confirms that quantitative reasoning over multiple concurrent images is a distinct challenge, one that we deliberately exclude from the primary research scope of DocsRay in order to concentrate on document-level semantic retrieval. The core issue stems from our text-based retrieval approach: we convert images to textual descriptions (alt-text) for embedding and retrieval. While this enables efficient text-based search, it cannot preserve pixel-level visual relationships crucial for multi-image comparison tasks. Our text-centric pipeline underperforms on SlideVQA-style tasks that demand pixel-level comparison across multiple images. We regard such tasks as future extensions rather than shortcomings of the proposed retrieval architecture.", "Absence of Semantic Retrieval Benchmarks The document understanding community lacks benchmarks designed to evaluate semantic retrieval quality in document contexts. Existing benchmarks focus on end-to-end question answering accuracy but do not isolate retrieval performance. This limitation prevented us from quantitatively validating our core technical contribution, which lies in demonstrating the superiority of hierarchical semantic retrieval over flat retrieval methods. We conducted qualitative evaluations with domain experts who confirmed that our retrieved chunks showed higher topical coherence and completeness compared to baseline systems. However, the absence of standardized metrics and datasets for document-centric semantic retrieval represents a significant gap. We are currently developing a benchmark dataset with manually annotated relevance judgments to enable future quantitative evaluation of retrieval quality.", "Dependency on Multimodal LLMs for Document Processing While leveraging Gemma-3's (Team et al. 2025a) multimodal capabilities enables state-of-the-art performance on MMLongBench-Doc, this approach may not generalize optimally to all document understanding tasks. Documents requiring precise layout understanding, such as forms, invoices, or complex tables with spatial relationships, may benefit from specialized layout-aware models. Our current approach treats layout implicitly through the multimodal LLM's understanding rather than explicitly modeling spatial relationships. For instance, in documents where the relative position of text blocks carries semantic meaning (e.g., organizational charts, complex forms), our text-extraction approach may miss crucial structural information. Future research should investigate hybrid approaches that combine our semantic understanding with explicit layout modeling for documents where structure is paramount to meaning.", "Lack of Fine-Grained Evidence Grounding While DocsRay provides section-level source attribution by listing the context sections used for generation (see Section 4.6), it currently lacks fine-grained, statement-level evidence grounding. The generated 'References' list indicates the general source of information but does not link specific claims within the answer to the exact sentences or visual elements that support them. As demonstrated in our retrieval analysis (see Table 7), fine-grained evidence grounding helps build user trust, in high-stakes domains like financial reporting and medical records where detailed audit trails are mandatory. The current pipeline maintains source metadata at the section level but does not preserve paragraph or sentencelevel attribution in the output. Implementing this more granular form of citation to enhance verifiability is a key direction for future work, such as highlighting exact text spans, bounding box identification for visual elements, or inline citations that map claims to specific source passages.", "Limited Multilingual Evaluation Our quantitative evaluation focuses exclusively on English technical and business documents, leaving the system's multilingual capabilities empirically unvalidated despite theoretical support. A benchmark for multilingual, multi-page document question answering remains, to our knowledge, absent from the research landscape. This gap is concerning given the global nature of document processing needs in multinational corporations, international organizations, and cross-border collaborations.", "While we selected the Multilingual-E5-Large and BGEM3 embedding combination for their advertised multilingual capabilities, with BGE-M3 specifically trained on over 100 languages and Multilingual-E5-Large supporting 94 languages, we lack empirical evidence of their effectiveness in our dual-embedding architecture across different language families. Critical questions remain unanswered: Does the concatenation approach maintain its advantages when processing non-Latin scripts? How does the pseudo-TOC generation perform with languages that have different discourse structures than English? Do our prompt-based boundary detection methods generalize to languages with different paragraph and section conventions? These questions demand investigation across diverse language families including East Asian languages with their unique character systems, right-to-left scripts like Arabic and Hebrew, and morphologically rich languages like Finnish or Turkish.", "Comprehensiveness of Architectural Exploration While our experiments demonstrate the clear superiority of concatenation over simple addition, we acknowledge that the space of possible fusion mechanisms extends far beyond these two approaches. We did not evaluate alternative fusion strategies such as Hadamard product, learned linear projections, gated fusion mechanisms, or attention-based combination methods. This stems from our prioritization of efficiency for production deployment over exhaustive architectural search.", "Our preliminary trials with more complex fusion mechanisms provide tantalizing hints of unexplored potential. A learnable MLP layer for combining embeddings showed marginal accuracy improvements of 1-2 percentage points but at the cost of 3x higher inference latency due to the additional neural network forward pass. Attention-based fusion, where the query dynamically weights the contribution of each embedding model, demonstrated promise for handling domain-specific queries but required engineering to integrate with our caching infrastructure. These results suggest that while concatenation provides a balance of simplicity and performance, task-specific applications might benefit from more sophisticated fusion strategies. A comprehensive benchmark comparing fusion mechanisms across different document types, query categories, and computational budgets remains valuable future work.", "Out-of-Scope Tasks Complex Layout Understanding represents a fundamental boundary of our approach. Documents featuring intricate spatial layouts, where the relative positioning of elements carries semantic meaning, require capabilities beyond our text-centric architecture. Consider a complex government form where checkboxes and their labels are spatially distributed without explicit textual connections, or technical flowcharts and organizational diagrams where relationships are expressed through alignment, directionality, or visual grouping.", "Our system effectively extracts and understands textual content but inevitably loses critical structural information encoded in spatial relationships. Humans intuitively grasp hierarchical and peer relationships from visual positioning alone, whereas our method flattens two-dimensional layouts into a linear text stream. This limitation arises from our deliberate emphasis on deep semantic understanding of textual content rather than shallow capture of spatial relations.", "Experiment setup\n\nEvaluations datasets. We evaluate the effectiveness of the proposed method on open domain QA datasets: (1) NaturalQA (Kwiatkowski et al. 2019), (2) WebQuestions (Berant et al. 2013), (3) SQuAD (Rajpurkar et al. 2016), and (4) TriviaQA (Joshi et al. 2017). In addition, we further test the method using two complex multi-hop QA datasets, i.e., HotpotQA (Yang et al. 2018) and 2WikiMultiHopQA (Ho et al. 2020). For the open domain datasets, we use the 21M English Wikipedia dump as the corpus for retrieval, while for the multi-hop datasets, we use their original corpus. The statistics of all datasets are listed in Tab. 1.", "| | Datasets | #Questions | #Documents |\n|---:|:----------------|:-------------|:-------------|\n| 0 | NaturalQA | 3,610 | 21M |\n| 1 | WebQuestions | 2,032 | 21M |\n| 2 | SQuAD | 10,570 | 21M |\n| 3 | TriviaQA | 11,313 | 21M |\n| 4 | HotpotQA | 7,405 | 5M |\n| 5 | 2WikiMultiHopQA | 12,576 | 431 K |\n\nTable 1: Statistics of evaluation datasets.", "| | Method.Method | HotpotQA.EM | HotpotQA.F1 | 2WikiQA.EM | 2WikiQA.F1 | NaturalQA.EM | NaturalQA.F1 | WebQuestions.EM | WebQuestions.F1 | SQuAQ.EM | SQuAQ.F1 | TriviaQA.EM | TriviaQA.F1 | Average.EM | Average.F1 |\n|---:|:----------------|:--------------|:--------------|:-------------|:-------------|:---------------|:---------------|:------------------|:------------------|:-----------|:-----------|:--------------|:--------------|:-------------|:-------------|\n| 0 | No Retrieval | 17.27 | 23.92 | 21.55 | 24.97 | 15.54 | 18.85 | 17.67 | 24.42 | 10.54 | 16.96 | 38.40 | 20.25 | 21.16 | 21.56 |\n| 1 | Standard RAG | 34.84 | 44.75 | 29.19 | 34.17 | 34.29 | 36.06 | 20.08 | 27.68 | 28.99 | 34.41 | 54.69 | 30.89 | 33.68 | 34.66 |\n| 2 | HyDE | 32.74 | 41.93 | 23.47 | 28.17 | 34.60 | 36.38 | 22.69 | 30.74 | 25.53 | 31.03 | 56.19 | 31.26 | 32.54 | 33.25 |\n| 3 | Q2D | 33.71 | 43.41 | 24.25 | 29.07 | 35.68 | 37.36 | 22.79 | 30.77 | 29.20 | 34.79 | 58.12 | 32.56 | 33.96 | 34.66 |\n| 4 | CoT | 32.96 | 42.50 | 24.05 | 28.88 | 35.67 | 37.36 | 22.15 | 30.52 | 28.38 | 34.23 | 58.32 | 32.39 | 33.59 | 34.13 |\n| 5 | PAIRS | 35.14 | 45.20 | 26.99 | 31.73 | 36.87 | 38.17 | 23.08 | 31.15 | 28.85 | 34.90 | 59.23 | 32.92 | 35.03 | 35.68 |\n| 6 | RA ratio | (74.4%) | (74.4%) | (69.1%) | (69.1%) | (82.8%) | (82.8%) | (78.6%) | (78.6%) | (86.3%) | (86.3%) | (61.6%) | (61.6%) | (75.5%) | (75.5%) |\n| 7 | DPR-AIS | 36.61 | 47.02 | 28.48 | 33.48 | 37.42 | 38.95 | 22.00 | 30.63 | 30.00 | 35.87 | 59.09 | 33.32 | 35.60 | 36.55 |\n| 8 | Rerank | 38.56 | 49.48 | 31.02 | 36.04 | 33.60 | 35.12 | 20.47 | 28.83 | 26.20 | 31.48 | 57.58 | 32.64 | 34.57 | 35.60 |\n| 9 | DPR-AIS-rerank | 39.42 | 50.21 | 30.34 | 35.10 | 36.12 | 37.36 | 21.85 | 30.30 | 29.99 | 35.81 | 59.78 | 33.88 | 36.25 | 37.11 |", "Table 2: Main results across six QA datasets. RA ratio indicates the proportion of queries for which the retriever was activated by PAIRS. Bold and underlined values represent the best and second-best scores, respectively.", "Baselines. We compare the proposed method with other training-free baselines. (1) No Retrieval leverages the parametric knowledge of LLM to directly generate response to the given query without retrieval. (2) Standard RAG incorporates the retrieved topk documents with the query as the input of LLM through prompting. (3) HyDE (Gao et al. 2023a) leverages the LLM-generated passage to enhance the retrieval. (4) Q2D (Wang, Yang, and Wei 2023) concatenates multiple queries and the LLM-generated pseudodocument to perform the retrieval. (5) CoT (Jagerman et al. 2023) first prompts the LLM to generate the answer as well as the rationale to the given query, and then combines multiple queries and the LLM outputs into the retrieval signal. In addition, we also compare DPR-AIS-powered reranking (denoted as DPR-AIS-rerank ) with the traditional reranking mechanism. Hence, we have the last baseline: (6) Rerank (Glass et al. 2022; Chang et al. 2025) selects the topk documents from retrievals using a reranker.", "Evaluation metrics. To comprehensively assess the quality of generated answers, we adopt two widely used metrics in open-domain question answering: Exact Match (EM) and F1 score. The EM metric quantifies the proportion of predictions that exactly match any one of the ground-truth answers, serving as a strict measure of correctness. On the contrary, the F1 score provides a more forgiving evaluation by computing the token-level overlap between the predicted and reference answers. We normalize the predicted and ground truth answers following the implementation of Fang, Meng, and Macdonald (2025).", "Implementation details. We implement our PAIRS framework using Qwen2.5-7B-Instruct (QwenTeam 2024) as the backbone LLM for answer and pseudo-context generation. To ensure deterministic outputs and eliminate variability due to random sampling, we set the temperature to 0.0 during decoding (Kim et al. 2024). For dense retrieval, we employ the bge-large-en-v1.5 embedding model (Xiao et al. 2023) with a hidden dimension of 768, using inner product (IP) as the similarity metric. In the dual-path retrieval step, we retrieve the top 5 most relevant documents independently for both the query and the generated pseudocontext, resulting in a combined candidate pool of 10 documents ( 2 n = 10 ). For reranking, we adopt the bge-reranker- base model (Xiao et al. 2023) to reorder the top-10 documents retrieved by the query-only baseline. Across all experiments, we pass the top 3 documents to the LLM for final answer generation, except No Retrieval . All remaining experimental configurations of baselines follow the implementations reported in their respective original papers. We present the prompts used in this study in the appendix.", "RAG + Graphs in QA\n\nTo address these limitations of traditional RAG, particularly in retrieving contextually precise and semantically rich information, research has tried to integrate structured knowledge sources and graphbased reasoning into the retrieval pipeline. One notable direction of work is LLM-based retrieval, which incorporates knowledge graphs information into the generation process of LLMs. The LLMs are augmented using the retrieved facts from the KG [26], leading to a clear dependence on the quality of extracted graph information, using it to generate responses. Research has also been done towards augmenting knowledge graph information, retrieved through some semantic or some other similarity index, in the prompts given to the LLM [27], to help the model do zero-shot question-answering. Some researchers have tried a different approach to fact retrieval, where the model tries different queries, using structured query languages, until the desired information comes through [28]. All of these approaches have used KGs as an external source to retrieve information from and answer the questions [29]. Then, aligning the retrieval process with LLM even more closely, some researchers have proposed methods which use LLMs in intermediate steps as well to plan the retrieval and judge whether the retrieved information is relevant or not [29,30], continuing the process until the desired output emerges.", "Another interesting direction of work is integrating GNNs with LLMs, which leverages graph neural networks to enhance retrieval and re-ranking using learned graph representations, along with generation capabilities of LLMs. There have been approaches such as GNN-RAG [31], which have tried combining language understanding abilities of LLMs with the reasoning abilities of GNNs in a retrieval-augmented generation (RAG) style. Other methods of GNN-LLM alignment, have been classified into symmetric and asymmetric alignment [32]. Symmetric alignment refers to the equal treatment of the graph and text modalities during the alignment process[33, 34, 35]. Asymmetric alignment focuses on allowing one modality to assist or enhance the other, here leveraging the capabilities of GNNs to improve the LLMs [36, 37, 38].", "The continual evolution of such hybrid architectures reflects the dynamic nature of the QA landscape and its responsiveness to complex information needs. Given the diversity of methods explored to enhance QA systems-from rule-based techniques to advanced neural and hybrid models-it becomes essential to establish robust mechanisms for evaluating their effectiveness. It is thus important to discuss the evaluation paradigms that compare and benchmark these systems. Extractive QA benchmarks almost universally use Exact Match (EM)-the strict proportion of predictions that character-for-character match a ground-truth answer-and F1 score, the harmonic mean of token-level precision and recall, to evaluate answer quality. On SQuAD v2.0 [39], human annotators achieve around 86.831 EM and 89.452 F1 on the test set, whereas state-of-the-art models now exceed 90 EM and 93 F1. Natural Questions [40] extends this paradigm to long-answer (paragraph) and short-answer (span or yes/no) annotations drawn from real Google search queries. Meanwhile, multiple-choice datasets like OpenBookQA [41]-designed to mimic open-book science exams-use simple accuracy. Complex reasoning benchmarks push beyond single-span extraction. HotpotQA [42], a multi-hop dataset built on pairs of Wikipedia articles, evaluates both answer-span EM/F1 and supporting-fact EM/F1, plus a joint metric requiring both to be correct; even top models achieve only ~72 joint F1[43] under the full-wiki setting, far below human performance of ~82 average F1 and ~96 upper bound F1. These core metrics and datasets underpin broader QA evaluation: multi-task suites like GLUE/MMLU [44, 45] include QA subtasks to probe general language understanding, while specialized frameworks such as MT-bench [46] ('LLM as judge') and automated platforms like Scale Evaluation layer on top to assess conversational and retrieval-augmented QA in real-world scenarios.", "This review has highlighted the broad spectrum of methodologies and evaluation strategies used in enhancing and assessing question answering systems. With this foundation in place, we now transition to the specific tools and frameworks employed in our research for graph-based question answering. In the following section, we briefly introduce and contextualize spaCy, Stanford CoreNLP, and GraphRAG-three diverse and widely-used tools we have utilized for knowledge graph construction and integration with LLMs.", "3 Methodology\n3.3 Experimental Setup\n\nBased on the dataset statistics in Table 1, the number of possible document pairs quickly grows large. For example, with 193 helpful and 121 harmful documents in TREC 2020, the helpful-harmful condition alone would generate over 23,000 possible pairs; when combined with six adversarial variants per query in both TREC 2020 and 2021, the number of pairs becomes even larger. To keep computation feasible, we only sample up to ten document pairs for each condition per query. If there are fewer than ten valid pairs, all available combinations are included. Each document pair is then combined with user query framings to assess how sequencing and user assumptions affect the model outputs together.", "This setup mainly contributes to RQ3 by testing the influence of evidence order and combinations, while also providing insight into RQ2 by extending the single-document analysis to multi-document contexts, and RQ4 by evaluating how different query framings shape ground-truth alignment outcomes.", "3.3.3 Passage-Based Pooling Setup. The passage-based pooling setup moves beyond controlled pairings. In this setup, all documents in the dataset are first segmented into smaller chunks to enable segment-level retrieval. We segment each Manuscript submitted to ACM", "document with a sliding window of 512 words and a 256-word overlap. These segments, regardless of their original classification as helpful, harmful, or adversarial, are subsequently reranked using the MonoT5 reranker [54]. For each query, the ten highest-ranked segments are retained to form the retrieval pool, which is then provided to the language model as context in combination with one of three user query formulations. This setup thus reflects how LLMs would behave under conventional retrieval conditions, where retrieved evidence is shaped by ranking models rather than manually controlled.", "This setup primarily addresses RQ3 by examining the impact of various evidence types on model behavior, while simultaneously enhancing RQ2 by evaluating the persistence of document type effects observed in controlled environments within a conventional retrieval pipeline. This setup also examines RQ4 by analysing the interplay between query formulations and non-uniform retrieval pools in establishing ground-truth alignment results.", "3.3.4 Bias-Controlled Pooling Setup. The bias-controlled pooling setup examines how context pools that are biased towards a certain type of evidence (helpful or harmful) affect how models respond. This setup is inspired by the methodological framework of Pogacar et al. [61], who showed that imbalances in evidence presentation can affect human judgment. We apply this concept to the RAG context by creating two skewed retrieval pools: one biased towards helpful evidence and the other biased towards harmful evidence . We used the MonoT5 reranker to process each category of document segments (helpful, harmful, and adversarial) independently for every query, producing the top-10 helpful, top-10 harmful, and top-10 adversarial results for each attack type. These segments are then grouped into retrieval pools of fixed size (ten segments). In the helpful-biased condition, the eight highest-ranked segments from the top-10 helpful list are placed at the top of the pool, while the remaining two highest-ranked segments are selected from the top-10 harmful or adversarial lists. In the harmful-biased condition, the eight highest-ranked segments from the top-10 harmful or adversarial list are placed first, followed by the two highest-ranked segments from the top-10 helpful list. This setup contributes to RQ3 by testing imbalanced evidence and RQ4 by examining how user query formulations interact with skewed pools to affect alignment.", "3.4 Evaluation\n\nTo evaluate model performance across different experimental setups, we use a single metric, Ground-Truth Alignment (i.e., accuracy), which measures whether the stance of a generated response matches the ground-truth stance label for the query. A response is considered aligned if its predicted stance is identical to the ground-truth stance.", "Prior work has shown that LLMs can perform reliably as automatic evaluators [16, 29], often approaching humanlevel agreement. Following this line of work, we employ gemini-2.0-flash as an external stance classifier. To further validate our evaluation pipeline, we conduct a secondary classification of all responses using gpt-4o-mini and measure inter-rater agreement with gemini-2.0-flash and compute Cohen's Kappa score [26]. Agreement was 0.90 for TREC 2020 and 0.82 for TREC 2021. These values indicate what is conventionally characterized as 'almost perfect' agreement between the two classifiers and indicate the reliability of automated stance detection. We use gemini-2.0-flash as the only stance classifier for all of the experiments to avoid relying on the same model family for both generation and evaluation.", "For each experimental setup, we compute the Ground-Truth Alignment Rate, defined as the proportion of aligned responses relative to the total number of generated responses. The prompt used for stance classification of LLM responses is publicly available in the project's github repository.", "TREC 2021 Non-RAG Helpful Harmful Rewriter Paraphraser Fact-Inversion FSAP-IntraQ FSAP-InterQ Liar Ground-Truth Alignment (%) TREC 2020 GPT-4.1 Consistent Query Neutral Query Inconsistent Query", "Fig. 1. Results of the single-document setup with GPT-4.1 on TREC2020 and TREC 2021 . The x-axis shows the different document types provided to the LLMs, as well as the Non-RAG baseline, and the y-axis reports ground-truth alignment rate (%). Error bars indicate 95% confidence intervals estimated via bootstrapping. Across both datasets, providing helpful documents leads to the highest alignment rate, while presenting Liar documents results in the lowest. In addition, the results reveal a consistent user query framing trend in which the alignment rate is highest with consistent queries, lower with neutral queries, and lowest with inconsistent queries.", "4 Experiments\n\nWe conducted experiments to evaluate MetaRAG on its ability to detect hallucinations in retrieval-augmented generation (RAG). The evaluation simulates a realistic enterprise deployment setting, in which a chatbot serves responses generated from internal documentation. Our focus is on the detection stage, that is, identifying when an answer contains unsupported (hallucination) or fabricated information. Prevention and mitigation are important but they are outside the scope of this work.", "4.1 Dataset\n\nThe evaluation dataset is a proprietary collection of 23 internal enterprise documents , including policy manuals, procedural guidelines, and analytical reports, none of which were seen during LLM training. Each document was segmented into chunks of a few hundred tokens, and retrieval used cosine similarity over text-embedding-3-large , with the topk = 3 chunks appended to each query.", "We then collected a set of user queries and corresponding chatbot answers. Each response was labeled by human annotators as either hallucinated or not, using the retrieved context as the reference. The final evaluation set contains 67 responses , of which 36 are labeled as not hallucinated and 31 as hallucinated .", "To preserve confidentiality, we do not release the full annotated dataset. However, the complete annotation guidelines are included in the supplementary material.", "4.2 Evaluation Protocol\n\nMetaRAG produces fine-grained, factoid-level hallucination scores , whereas the available ground truth labels are at the response level . To align with these existing labels, we evaluate MetaRAG as a binary classifier by thresholding the hallucination score H ( Q,A,C ) at \u03c4 = 0 . 5 . We report standard classification metrics: Precision, Recall, F1 score and accuracy. Latency is also recorded to assess feasibility for real-time deployment.", "4.2.1 Case Studies in Identity-Sensitive Domains\n\nBeyond quantitative evaluation, we also provide qualitative illustrations of MetaRAG in identity-sensitive scenarios. To illustrate how MetaRAG's span-level scores can enable identity-aware safeguards without inferring protected attributes, we present two stylized examples. These are not part of the quantitative evaluation in Section 4, but highlight potential deployment scenarios.", "Healthcare (pregnancy). Auser asks: 'Can pregnant women take ibuprofen for back pain?' The model answers: 'Yes, ibuprofen is safe throughout pregnancy .' However, the retrieved context specifies that ibuprofen is contraindicated in the third trimester. MetaRAG flags the span 'safe throughout pregnancy' with a high factoid score ( S i = 0 . 92 ), yielding a response-level score H = 0 . 92 . Under the policy hooks described in Section 3.6, the topic tag pregnancy triggers a stricter threshold ( \u03c4 identity = 0 . 3 , lower than the general case), span highlighting, a forced citation requirement, and possible escalation to human review.", "Migration (refugee rights). A user asks: 'Do LGBTQ+ refugees automatically receive protection in country X?' The model claims that such protections are automatic , but the retrieved legal text provides no evidence of this. MetaRAG flags the unsupported claim 'automatically receive protection' with a moderate score ( S i = 0 . 5 ), yielding a response-level score H = 0 . 5 . Although this score would sit at the decision boundary under a general threshold ( \u03c4 general = 0 . 5 ), the stricter identity-aware threshold ( \u03c4 identity = 0 . 3 ) ensures it is flagged for this case. Under the policy hooks, the topic tag asylum/refugee enforces citation and may escalate the response to a human reviewer. In a chatbot deployment, the system would abstain from returning the unsupported answer and instead notify the user that expert verification is required.", "These qualitative vignettes complement our quantitative evaluation by showing how MetaRAG's flagged spans can be turned into concrete safeguards in identity-sensitive deployments."]}
{"type": "entry", "index": 51, "question": "how much do negatives matter when training an embedder for scientific text", "answer": "Negatives matter a lot for training a good embedder, especially on scientific text.\n\nHow the loss works. Dense retrievers like DPR train encoders to pull true query\u2013document pairs close together in embedding space while pushing negatives away, using a contrastive (cross\u2011entropy) loss over one positive and many negatives.[1] If the negatives are weak or random, the task is too easy and the model does not learn sharp boundaries between truly relevant and only loosely related texts.\n\nScientific text is subtle. In domains like geoscience, many papers share similar jargon and topics, so the model must learn to distinguish fine\u2011grained differences (e.g., \"groundwater recharge in arid basins\" vs. \"groundwater quality in arid basins\"). GeoEmbedding explicitly mines hard negatives from in\u2011domain corpora using SimANS so the model sees very similar but incorrect passages, forcing it to focus on precise meaning.[2] This is done both for general datasets lacking negatives and for synthetic geoscience datasets, again emphasizing domain\u2011hard negatives.[2]\n\nQuantity and difficulty. Training is done with in\u2011batch negatives and InfoNCE, so every other document in the batch is a negative unless it is the positive for another query.[2] For reranking, they insist on at least 16 negatives per query and prioritize harder negatives chosen by a weak retriever plus SimANS, because negative sample quality is \u201ccritical\u201d for performance.[2]\n\nEffect of random/easy negatives. When hard negatives are replaced with random ones, performance drops by 5.48%, showing that easy negatives fail to teach useful distinctions.[3]\n\nSo for scientific embedders, you should (1) use many negatives per query, and (2) mine in\u2011domain, hard negatives rather than relying on random or trivially irrelevant text.[2][3]", "tags": ["_untagged"], "full_faithfulness": 1.0, "full_answer_relevancy": 0.9265846595, "seed_faithfulness": 0.4782608696, "seed_answer_relevancy": 0.9265846595, "context_relevance": 0.8309124576615176, "context_relevance_error": null, "context_relevance_reason": "The context passages provide substantial information about the role of negative samples in training embedding models, particularly in the context of contrastive learning and the importance of distinguishing between positive and negative examples. This directly relates to the user's question about the significance of negatives in training an embedder for scientific text. However, while the passages are relevant, they could be more explicitly connected to the specific implications for scientific text, which slightly reduces the score.", "structure_citation": 0.6077586216744619, "structure_error": null, "structure_reason": "The Actual Output makes several claims about the importance of negatives in training embedders for scientific text, but it includes citations [1], [2], and [3] that do not correspond to any snippets in the provided Context. While the claims are relevant and well-articulated, the lack of matching citations significantly undermines the support for these claims, leading to a moderate score.", "hallucination_score": 0.9398675796889041, "hallucination_error": null, "hallucination_reason": "The response effectively supports the claim that negatives are crucial for training an embedder for scientific text. It provides detailed explanations of how negatives influence the training process, particularly in the context of scientific text, and aligns well with the context provided. The mention of specific techniques like mining hard negatives and the impact of random negatives on performance further strengthens the argument. However, while the response is comprehensive, it could be slightly more concise in some areas.", "contexts_used": ["Retriever Module (Dense Passage Retrieval).\n\nThe retriever's job is to efficiently identify which pieces of text in a large corpus are relevant to the input query x . Modern RAG implementations typically use dense retrievers like Dense Passage Retrieval (DPR) [45] in lieu of traditional keyword search. In DPR, a bi-encoder architecture is employed: a question encoder E q ( x ) maps the query x to a d -dimensional vector, and a passage encoder E p ( d ) maps each candidate document (or passage) d in the corpus to a d -dimensional vector in the same space. Relevance is assessed via a similarity score, usually the dot product of these vectors:\n\nAt query time, the retriever computes v q = E q ( x ) and then finds the topK documents whose vector E p ( d ) has highest inner product with v q . This search for maximum inner product can be implemented efficiently using Approximate Nearest Neighbor techniques (e.g., FAISS) to handle millions of documents in sub-linear time. The output of the retriever is the set Z = { z 1 , . . . , z K } of top-ranked documents and potentially their similarity scores. One can view the retriever as defining a probability distribution P ret ( z | x ) over documents z in the corpus, such that:\n\nwith the normalization \u2211 d \u2208C exp( E q ( x ) \u22a4 E p ( d )) (in practice approximated by summing over retrieved candidates rather than all of C ). In other words, the retriever assigns higher probability (or score)", "to documents whose embedding is most similar to the query's embedding. The DPR retriever is usually first trained on pairs of questions and relevant passages to ensure E q and E p produce representations that maximize dot-products for true Q-A pairs and minimize them for irrelevant pairs. The training objective is often a contrastive loss : for a given question q with a gold relevant passage p + and a set of negative passages { p -1 , . . . , p -N } , the encoder is trained to maximize sim ( q, p + ) while minimizing sim ( q, p -) for negatives. This can be formulated as a cross-entropy loss treating the positive passage as the correct class among one positive and N negatives:\n\nwhich encourages E q and E p to embed true pairs closer together than any negative pair [45]. After training, the retriever can generalize to new queries: it embeds the query and efficiently finds the nearest neighbor passages in the index. This retrieval step is crucial because it narrows down the evidence from potentially billions of tokens to a manageable subset that the generator will actually consider.", "Generator Module (Conditional Seq2Seq Model). The generator in a RAG pipeline is typically a sequence-to-sequence language model that produces the final answer or output text, given the input query and retrieved documents. Formally, the generator defines a conditional distribution P gen ( y | x, Z ) over output sequences, where Z = { z 1 , . . . , z K } are the retrieved passages. The generator is often initialized from a pre-trained transformer-based seq2seq model (such as BART or T5) to leverage rich language generation capabilities [52]. During generation, the model is provided with the question (or prompt) as well as the content of the retrieved documents. There are multiple ways to feed the retrieved context to the generator:\n\nIn an early fusion approach, one can concatenate the query x with the text of all retrieved passages into a single extended input sequence (perhaps with special separators) and have the seq2seq model attend to all of it at once. This is straightforward: the generator effectively treats the combined text as context and learns to pick out the relevant bits when producing the answer. However, this can be difficult if K is large, as the input may become very long.", "In the late fusion approach adopted by RAG [52], the generator considers one retrieved document at a time as a context and then marginalizes over the document choices (as in Eq. 1). Specifically, the RAG-Sequence variant fixes a single document z i as context for generating the entire output and computes P ( y | x ) by summing the probabilities P ( y | x, z i ) weighted by the retriever's confidence P ( z i | x ) . Another variant, RAG-Token, allows the generator to switch between documents at the token level, effectively marginalizing over the document choice for each generated token [52]. In both cases, the generator P gen ( y | x, z ) itself works like a standard seq2seq model: it factorizes over the output tokens y 1 , . . . , y T as \u220f T t =1 P \u03b8 ( y t | x, z, y <t ) , i.e. it generates one token at a time, attending to the input query x and the content of z (or multiple z 's if early fusion). The generator's architecture typically uses an encoder-decoder Transformer: the encoder encodes the combination of x and z , and the decoder produces y autoregressively.", "Because the generator is conditioning on retrieved evidence, it tends to produce outputs that are supported by that evidence. For example, if the query asks for a specific factual answer, the generator can copy or rephrase the needed information from one of the retrieved passages. This is in contrast to a vanilla language model which would attempt to rely on parametric knowledge (which might be outdated or incomplete). In sum, the generator module in RAG is responsible for fluent and coherent text generation, but crucially it is grounded by the retrieval context, which guides it toward accurate content.", "c. Embedding Model Fine-tuning\n\nEmbedding models are fundamental components in modern language systems, transforming text into dense, continuous vector representations. These representations support a wide range of downstream tasks, including retrieval, reranking, classification, clustering, and semantic textual similarity. Within RAG frameworks, embedding models play a central role by enabling the retrieval of semantically relevant documents from large external knowledge bases.", "Historically, encoder-based architectures such as BERT and T5 have dominated the landscape of text embedding models. Recent advances in decoder-only transformer models have demonstrated competitive-and in many cases superior-performance for embedding tasks (Li et al., 2024; Meng et al., 2024). However, studies (Meng* et al., 2024) show that converting decode-only transformer models to encode-only architectures further improves performance. In parallel, several studies (Lee et al., 2024; Wang et al., 2023) have shown that large language models can be leveraged to generate high-quality synthetic training data, enabling effective fine-tuning with fewer training steps.", "In this work, we introduce GeoEmbedding , a domain-adapted embedding model fine-tuned from a state-of-the-art encode-only transformer converted from a decode-only transformer. To improve its in-domain retrieval capabilities, we trained GeoEmbedding using synthetic geoscientific data, specifically tailored to the linguistic and semantic patterns found in academic literature from the geography and earth sciences domains.", "Datasets. To train our GeoEmbedding model, we used a combination of publicly available datasets spanning a range of NLP tasks, including retrieval, classification, clustering, reranking, and semantic textual similarity (STS). This diverse set of tasks enables the model to learn general-purpose em-", "beddings while being robust to domain-specific requirements. We sample about 360k data from datasets including SciFact (Wadden et al., 2020), S2ORC (Lo et al., 2020), PAQ (Lewis et al., 2021), Specter (Cohan et al., 2020), and FiQA (Maia et al., 2018), etc. Additionally, we incorporate some datasets from the MTEB benchmark, such as AmazonCounterfactual, Banking77, Emotion, MassiveIntent, MassiveScenario, and ToxicConversations, etc.", "Following standard practice in prior work such as SFR-Embedding-Mistral and NV-Embed, we train using only the training splits from each dataset. Where datasets include annotated negatives, we use them directly for contrastive learning. For datasets without sufficient negative samples-or none at all-we apply SimANS (Zhou et al., 2022) to mine hard negatives from the corresponding corpus.", "In addition to general-purpose datasets, we generate domain-specific data for geoscience using Qwen. These synthetic examples are based on abstracts from open-access scholarly articles collected from arXiv under CC BY license, with proper attribution. We compile these into a dedicated training set, and apply SimANS to mine hard negatives from the domain corpus, ensuring the model learns to distinguish subtle differences in geoscientific content.", "Training. Given the diversity of tasks across datasets, we prepend a task-specific instruction to each query to guide the model during training. For each query-document pair, we construct an instructed query by combining the task instruction \ud835\udc3c with the query \ud835\udc5e in the following format:\n\nWe apply this formatting only to the query; no instruction is added to the document side. To fine-tune the embedding model, we adopt instruction tuning with in-batch negatives using the InfoNCE loss function:\n\nwhere \ud835\udc5e inst is the instructed query, \ud835\udc51 + is the corresponding positive document, and \ud835\udc5b\ud835\udc56 denotes the \ud835\udc56 -th negative document in the batch. The similarity function \ud835\udf19 (\u00b7 , \u00b7) is defined using a temperature-scaled cosine similarity:\n\nwhere h \ud835\udc5e and h \ud835\udc51 are the vector embeddings of the query and document, respectively, and \ud835\udf0f is a temperature hyperparameter controlling the sharpness of the similarity distribution.", "To enable parameter-efficient training, we apply Low-Rank Adaptation (LoRA) (Hu et al., 2021), setting the rank \ud835\udc5f = 8 and scaling factor \ud835\udefc = 16. The model is fine-tuned from Mistral-7B-v0.1 (Jiang et al., 2023), a 7B decoder-only transformer pretrained model that achieved state-of-the-art performance in 2023. We fine-tune the model over 4096 gradient accumulation steps with a learning rate of 1 \u00d7 10 -5 .", "We convert the Mistral model into an encode-only architecture by removing its causal attention masks. Additionally, we apply Matryoshka representation learning during training to enhance the quality of low-dimensional embeddings.", "Evaluation. We evaluate our embedding model on both general-purpose and domain-specific benchmarks to assess its performance across retrieval tasks.", "For general evaluation, we use the MTEB(eng) benchmark, which includes a diverse set of retrieval datasets. Table 7 summarizes our results. We compare our model against two strong baselines-SFREmbedding-Mistral and NV-Embed-v2-using scores replicated from the official MTEB leaderboard 1\n\n1 https://huggingface.co/spaces/mteb/leaderboard", "| | | Classification | Clustering | PairClassification | Reranking | Retrieval | STS | Summarization | Overall |\n|---:|:----------------------|-----------------:|-------------:|---------------------:|------------:|------------:|------:|----------------:|----------:|\n| 0 | SFR-Embedding-Mistral | 80.47 | 54.93 | 88.59 | 50.15 | 59.33 | 84.77 | 36.32 | 69.31 |\n| 1 | NV-Embed-V2 | 87.19 | 47.66 | 88.69 | 49.61 | 62.84 | 83.82 | 35.21 | 69.81 |\n| 2 | GeoEmbedding | 89.67 | 56.5 | 82.51 | 48.32 | 60.91 | 80.6 | 30.43 | 70.21 |\n\nTable 7 | Evaluation on MTEB tasks\n\nFigure 9 | MTEB (eng) benchmark as of April 22, 2025.\n\nMemory Zero Number Embedding Max Mean Pair Hodel Usage Classification Clustering shot Paraneters Dinensions Tokens (Task, (TaskType) (HB) RELLE 27125 32768 73.59 67.95 90.06 58.20 88.74 95% Unknonn Unknown 73.30 67.67 90.05 59.39 87.70 Jasper 131072 71.41 66.65 90.27 60.52 88.14 29040 70.72 65.77 88.52 58.97 85.90 13563 69.82 65.31 90.54 88.09 56% 14975 69.81 65.00 87.19 47.66 88.69 69.80 65.29 83.00 54.07 88.44 enbedding-005 69.60 64.77 86.03 51.91 69.53 64.82", "51.52 87.65 stella en 1.5B 69.43 65.32 89.38 57.06 88.02\n\n(visualized in Figure 9). Despite being fine-tuned on synthetic in-domain data, our model retains competitive performance across general retrieval benchmarks, demonstrating robust generalization ability .", "To evaluate in-domain performance, we constructed a dedicated geoscience retrieval benchmark using GeoRAG-QA . This benchmark includes synthetic query-document pairs generated using Qwen110B and a retrieval corpus of GeoGPT library. As shown in Figure 10, our fine-tuned model achieves substantial gains in retrieval accuracy across all top\ud835\udc58 settings-from Recall@1 through Recall@10-demonstrating its effectiveness in domain-specific information retrieval.\n\nFigure 10 | Top\ud835\udc58 recall on GeoRAG-QA benchmark\n\n.\n\nTop- \ud835\udc58 Recall SFR-Embedding-Misral GeoEmbedding", "d. Reranker Model Fine-tuning\n\nDataset and Quality Improvement. To maximize the reranker's performance in the geoscience domain, we curated a large training corpus composed of both open-source and domain-specific ranking pairs. The domain-specific data were synthesized using Qwen, conditioned on abstracts from open-access scholarly articles in geography sourced from arXiv.", "After assembling all positive query-document pairs, we applied SimANS (Zhou et al., 2022) to mine hard negative samples from the candidate corpus, thus forming a balanced and challenging training dataset. Since both synthetic and open-source data were generated in batches, individual data points may vary in quality. Even a small number of noisy or biased examples can significantly degrade reranking performance. To address this, we implemented a data quality filtering process to improve the reliability of training examples. For each positive pair, we used the LLaMA3 model to assess answer quality based on a set of predefined criteria. Each sample was assigned a quality score ranging from 0 to 3, following the standards detailed in Table 8. Only samples with sufficient quality were retained for training.Data with a quality label of 0 is excluded from training.", "| | quality label | quality standards |\n|---:|:----------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 0 | 3 2 1 0 | completely correct, detailed, and covers all key information with clear and easy-to-understand expression. mostly correct, covers most key information, and is clearly expressed but with minor shortcomings. generally correct but has some errors or omissions, with mostly clear expression. significant errors or omissions, lacks sufficient detail, and is not clearly expressed. |\n\nTable 8 | data quality standards\n\nWe ultimately utilized diverse types of paired data to train the model. Open-source data included:\n\nQA pairs: DuReader (He et al., 2017), NQ, LCQMC (Liu et al., 2018), HotpotQA, etc.", "Retrieval-oriented pairs: T2Ranking (Xie et al., 2023), mMARCO, MS MARCO, etc.\n\nSimilarity task pairs: NLI (Koreeda and Manning, 2021), NLI-zh, AFQMC (Lee et al., 2022), PAWSX (Yang et al., 2019), etc.\n\nMultilingual data: Mr. TyDi.", "In total, we incorporated over 360,000 open-source data pairs, along with 30,000 synthetically generated QA pairs specifically tailored for geological science applications.", "For training with contrastive objectives, negative sample quality plays a critical role in reranker performance. Whenever available, we retain the original negative samples provided within each dataset. However, many datasets either lack negatives entirely or contain an insufficient number of challenging examples. To ensure training robustness, we construct a set of at least 16 negative samples per query for all datasets.", "When additional negatives are required, we use a weak retriever to select hard negative candidates from a large-scale public Wiki corpus or domain-specific geoscience libraries curated from scholarly literature. We then apply the SimANS sampling strategy (Zhou et al., 2022) to prioritize harder examples from the candidate pool. Each negative sample \ud835\udc5b\ud835\udc52\ud835\udc54 \ud835\udc65 is assigned a sampling probability based on the squared difference between its retriever score and that of the corresponding positive sample:\n\nwhere \ud835\udc5b\ud835\udc52\ud835\udc54 score and \ud835\udc5d\ud835\udc5c\ud835\udc60 score are the scores returned by the weak retriever, and \ud835\udf0e is a task-specific hyperparameter that adjusts the sharpness of the sampling distribution. F Sample ( \ud835\udc5b\ud835\udc52\ud835\udc54 \ud835\udc65 ) represents the probability of selecting \ud835\udc5b\ud835\udc52\ud835\udc54 \ud835\udc65 as a training negative.", "We adopt different \ud835\udf0e values to reflect the difficulty characteristics of various dataset types. For question-answering datasets-where semantic precision is critical-we use a sharper distribution with \ud835\udf0e = 1, focusing the model on distinguishing subtle contextual differences. For reranking tasks and semantic similarity datasets, where broader generalization is beneficial, we use a more relaxed sampling distribution with \ud835\udf0e = 3.", "By ensuring a consistent quantity of informative and diverse negative samples, this strategy strengthens the model's ability to discriminate between highly similar documents and improves ranking accuracy in both general and domain-specific settings.", "Evaluation. We adopted a dual evaluation approach assessing both domain-specific and general capabilities. For domain-specific performance, we conducted top-k recall tests using domain-specific datasets, with the results presented in Table 2. For general capabilities, we evaluated the model on the BEIR(Thakur et al., 2021) benchmark dataset, which include includes 16 rank evaluation datasets from different domains. Under the condition that all retrievers used BM25, our model outperformed open-source state-of-the-art model in general retrieval tasks as well.", "| | BEIR metric | bge-reranker-v2-m3 | GeoReranker |\n|---:|:----------------|---------------------:|--------------:|\n| 0 | Map@3 average | 0.31295 | 0.317111 |\n| 1 | NDCG@10 average | 0.468298 | 0.477823 |\n\nTable 9 | BEIR General Benchmark metric", "5 Results and Analysis\n\n5.2 Fine-grained Evaluation\n\nTable 3: Ablation study for CARE on the validation subset of Natural Questions (NQ). The best performance is marked in bold . Each subset, i.e. , Res. and Boost, contains 6,058 and 2,734 evaluation samples, respectively.", "suring improvements on initially incorrect answers. (i) CARE achieves the highest Resilience performance, demonstrating strong robustness to contextmemory conflicts. This indicates the ability to assess the reliability of retrieved passages and selectively rely on external context versus internal knowledge. (ii) Adaptive Retrieval methods, which filter out questions unlikely to benefit from retrieval yield a high Resilience score. However, their Boost scores are substantially lower, likely due to overconfidence on the LLM's prior knowledge (Xiong et al., 2024). (iii) Dynamic decoding strategies show a modest Boost score by prioritizing retrieved context, but significantly reduce Resilience score. This suggests that blindly favoring retrieved content may harm performance, particularly in settings that potentially contain misleading context.", "Figure 5: t-SNE visualization of context embeddings generated by SFR and CARE. Red and blue points correspond to positive and negative labels, respectively, based on whether the context contains the answer span.\n\nSFR-Embedding-Mistral CARE (ours Positive Negative", "5.3 Ablation Study\n\nTable 3 thoroughly breaks the proposed method to evaluate the contribution of each component. In this experiment, we report the results on the validation set of NQ with the Resilience and Boost scores.", "Conflict-aware Fine-tuning. Using only C pos or C neg significantly reduces Resilience and Boost score, respectively. This highlights the mutual benefit of their roles in helping the context assessor balance parametric and external knowledge. In addition, when the conflict is not simulated with the correct signals in a closed-book setting, the Boost score drops substantially, suggesting the importance of exposing knowledge gaps for learning informative context embeddings.", "Random Negatives. Replacing hard negatives with randomly sampled context from the corpus results in a 5.48% decrease. Notably, the decline appears across both subsets, highlighting the importance of training with meaningful conflict signals, as easy negatives fail to expose substantive conflict.", "Reconstruction Pretraining. Removing pretraining leads to a 20.9% performance drop, as the context assessor fails to learn compact and meaningful memory embeddings. Note that it also significantly affects the fine-tuning phase, where context must be conveyed in a scenario-dependent manner.", "Loss Ablation. L LM and L KD play the complementary roles in conflict-aware fine-tuning stage. Removing the LM loss degrades Boost rate from 0.291 to 0.260, while removing the KD loss reduces Resilience rate from 0.766 to 0.695. This confirms that LM loss ensures capturing important information for answer generation, while KD loss teaches context selectivity through scenariospecific teacher supervision.", "| | Methods | Latency (Preprocessing + Gen.) | Span EM (NQ) |\n|---:|:--------------|:---------------------------------|---------------:|\n| 0 | RAG | 1.07s | 0.419 |\n| 1 | ADACAD | 1.54s | 0.417 |\n| 2 | Priori Judge. | 2.10s (1.02s + 1.08s) | 0.422 |\n| 3 | CARE | 1.19s (0.06s + 1.13s) | 0.447 |\n\nTable 4: Latency comparison using Mistral-7B-Instructv0.2 (Jiang et al., 2023), measured as average time per query on the NQ test set. Preprocessing includes retrieval decision or soft prompting.", "5.4 Visualization\n\nFigure 5 provides a qualitative analysis of context embeddings using two-dimensional t-SNE, comparing representations produced by SFR (Meng et al., 2024) and CARE on 100 passages retrieved by ColBERTv2 (Santhanam et al., 2022). Each point is labeled as positive or negative depending on whether the passage contains an answer to the question. To ensure a fair comparison, both methods take the same question-context pairs as input. We apply mean pooling over memory embeddings E mem for CARE. While SFR shows limited separation, CARE clearly distinguishes positive from negative contexts by modeling conflict. It is worth noting that proposed method still spreads embeddings widely, preserving contextual information.", "5.5 Efficiency Analysis\n\nWe analyze the efficiency of the proposed method by breaking it down into preprocessing, e.g. , retrieval decision or encoding with the context assessor, and generation. We set the maximum number of generation tokens to 30, and disable FlashAttention (Dao et al., 2022). All latency is measured using a single A100 GPU, except for ADACAD (Han et al., 2025), as its original implementation requires two GPUs. As shown in Table 4, CARE incurs slightly more total latency than standard RAG due to the additional step for computing soft context embeddings. However, it remains significantly more efficient than other methods, as encoding memory token embeddings requires only a single forward pass, which is much lighter than autoregressive generation. This suggests that incorporating soft context embeddings offers a promising direction for RAG, enabling more adaptive behavior with minimal impact on run-time efficiency."]}
{"type": "entry", "index": 52, "question": "does hybrid retrieval help more for methods questions or results questions", "answer": "From the provided documents, we can\u2019t directly tell whether hybrid retrieval helps more for \u201cmethods\u201d questions (how something was done) or \u201cresults\u201d questions (what was found), because none of the studies break down performance by question type.\n\nWhat we can say is how hybrid retrieval behaves with different kinds of queries:\n\n1. Hybrid sparse+dense retrieval generally improves the relevance of retrieved passages over using a single method, especially when the query is clearly specified and well-matched to content in the corpus.[2][3]\n   \u2013 In medical document retrieval, combining TF\u2011IDF (sparse) with dense SPECTER2 embeddings significantly improved precision at various ranks compared to dense-only retrieval.[2]\n   \u2013 In literacy-controlled counterspeech, a keyword+semantic hybrid retriever is chosen specifically because it \u201cperforms better than a single method\u201d when pulling evidence for answers.[3]\n\n2. Hybrid methods still struggle with vague or underspecified questions: they can pull in extra, somewhat-related material, making it harder to find the key information (\u201cneedle in a haystack\u201d).[2]\n\n3. Hybrid or multi-view setups (e.g., combining vector similarity with graph relationships) are especially good when the query concept doesn\u2019t match corpus items directly but is indirectly related; this boosts semantic generalization rather than a particular question type.[4]\n\nApplied to your distinction:\n- If a methods question is very specific (e.g., asking about a particular technique or pipeline), hybrid retrieval is likely to help by catching both keyword overlaps and semantic paraphrases.[2][3]\n- If a results question is vague (e.g., \u201cwhat did they find about X in general?\u201d), hybrid retrieval may bring in more noise.[2]\n\nBut the documents do not provide empirical evidence that one question type (methods vs results) benefits more overall.", "tags": ["hybrid_vs_kg"], "full_faithfulness": 1.0, "full_answer_relevancy": 0.0, "seed_faithfulness": 0.7692307692, "seed_answer_relevancy": 0.0, "context_relevance": 0.4969386825305667, "context_relevance_error": null, "context_relevance_reason": "The context passages provide some relevant information about hybrid retrieval methods and their effectiveness, particularly in relation to well-defined queries. However, they do not directly address the user's question about whether hybrid retrieval helps more for methods questions or results questions. The passages discuss performance metrics and comparisons but lack a clear distinction or analysis between methods and results questions, leading to a moderate relevance score.", "structure_citation": 0.6252554867295019, "structure_error": null, "structure_reason": "The Actual Output includes claims supported by citations [2] and [3], which correspond to snippets in the Context. However, it also contains an unsupported claim regarding the performance of hybrid retrieval methods for vague questions, which is not directly backed by the provided Context. Additionally, citation [4] is mentioned but does not have a corresponding snippet in the Context, leading to a deduction in points for unmatched markers. Overall, while there are strengths in the supported claims, the unsupported assertions and missing citations affect the overall alignment.", "hallucination_score": 0.7493840710144641, "hallucination_error": null, "hallucination_reason": "The response accurately addresses the input question about the effectiveness of hybrid retrieval for methods versus results questions. It clearly states that the documents do not provide empirical evidence to support a definitive conclusion, which aligns with the input's inquiry. The output also discusses the performance of hybrid retrieval in different contexts, highlighting its strengths and weaknesses, particularly in relation to specific question types. However, it could have provided more direct comparisons or examples to strengthen its claims further.", "contexts_used": ["C. RQ3: Exploration on Retreived Results\n\nDue to the superior performance of similarity-based RAG, we conduct an exploratory analysis of retrieved results from lexical and semantic retrieval techniques. The results reveal minimal overlap between BM25 and semantic techniques: out of 100 test examples in our benchmark, there are 76, 74, and 64 completely distinct retrieved samples when comparing BM25 with UniXcoder, CoCoSoDa, and GTE-Qwen, respectively. This difference in retrieval distributions motivates us to explore the combination of these techniques.", "As shown in Table III, the effectiveness of combining BM25 with different semantic retrieval techniques becomes more pronounced as model size increases. In the 200B+ scale, the BM25+GTE-Qwen combination achieves CB/ES metrics of 63.62/75.26 for DeepSeek-V3, substantially outperforming both individual techniques. Similarly, other semantic retrieval techniques also benefit from the combination of BM25. DeepSeek-V2.5 improves CB/ES metrics from 48.16/63.85 to 56.68/68.20 when combined with BM25. Notably, the advantage of BM25+GTE-Qwen combination is particularly striking for Qwen2.5-32B, which achieves impressive CB/ES scores of 63.73/72.25, rivaling or even surpassing models with significantly larger parameters such as DeepSeek-V2.5 and DeepSeek-V3. However, for smaller models (below 7B), the combination shows limited or even negative impact, suggesting that the complementary benefits of hybrid retrieval methods are more effectively leveraged by larger models.", "Finding 3: Lexical and semantic retrieval techniques exhibit distinct retrieval results distribution and demonstrate complementary characteristics in larger-scale models (7B+). With their combination, especially BM25+GTEQwen, similarity-based RAG achieves optimal performance in most LLMs.", "2 Related Works\n\nSeveral studies have explored and implemented advanced RAG (Retrieval-Augmented Generation) systems to overcome the limitations of the naive RAG approach discussed in the introduction. A significant focus of these works lies in developing novel methods to enhance vector database-centric retrieval systems, evaluated across a diverse set of metrics pertinent to vector-based retrieval. Additionally, some studies assess the performance of RAG solutions leveraging Knowledge Graphs [11], while one investigation examines the outcomes of naively combining Knowledge Graph-based and semantic search results for question-answering tasks. These related works also highlight certain limitations, ranging from increased resource demands to the need for novel strategies or enhancements to improve system performance.", "Addressing the limitations of naive Retrieval-Augmented Generation (RAG) pipelines has led to the integration of hybrid retrieval methods that combine sparse (keyword-based) and dense (semantic) retrievers.", "Spare and Dense Retriever RAG methods: Hybrid approaches, such as the \"Blended RAG\" framework proposed by Sawarkar et al. [12], demonstrate that combining multiple retrievers can improve the retrieval of relevant passages for clearly defined queries. This framework integrates six distinct retrievers-including traditional sparse methods (e.g., BM25) and dense semantic search techniques-selected based on their performance across benchmark datasets like Natural Questions (NQ) [13], TREC-COVID [14], SqUAD [15], and HotPotQA [16].", "A key strength of this hybrid search technique is its ability to return relevant passages, aligning well with Research Goal 3 for well-specified queries. However, these methods do not fully address Research Goal 3 when handling vaguer questions. In such cases, the hybrid approach can retrieve extraneous or unnecessary information, thus failing to eliminate the \"needle in a haystack\" problem. This shortcoming is evident in evaluations on the CoQA dataset [17], where retrieval accuracies ranged only from 45% to 49%. The reliance on document metadata during retrieval-a common feature in these systems-further complicates matters, as such metadata is often minimal in real-world scenarios (e.g., limited to document names and creation dates), thereby increasing both the creation time of the information store (contradicting Research Research Goal 1) and the risk of returning irrelevant passages (contradicting Research Research Goal 3) leading to longer contexts (contradicting Research Goal 2).", "Figure 3: Results of Priyanka Mandikal et al. on 2 example queries [18].\n\nQuery: Do fibroblasts from CF patients grow at a\n\nQuery: What is the prognosis for patients who have\n\nnormal rate?\n\nhad an episode of respiratory failure?\n\nMethod P@1 P@3 P@5 P@10 VSR 33.33 SPECTERZ Hybrid 66.67 Sample Retrieval This paper analyzes the cell cycle of cystic\n\nThe diagnosis, management and prognosis of 17 infants\n\nfibrosis-derived fibroblasts and statistically\n\nwith cystic fibrosis (CF) and severe respiratory disease\n\ndemonstrates their failure support normal\n\nwere reviewed for the\n\nperiod 1968 to 1972.\n\nThe clinical\n\npopulation doubling time in vitro and indicates that this\n\ncourse these infants was characterized by is due largely\n\nto a failure of the\n\ncystic fibrosis-derived\n\nfailure to thrive and\n\ncells to enter the cell cycle in normal numbers.\n\nWe have malnutrition Despite vigorous therapy including no explanation for significant difference antibiotics there was\n\n60% mortality:\n\nA delay in the\n\nbehavior of these cells from patients with cystic fibrosis\n\ndiagnosis of CF from the onset of respiratory symptoms\n\nvis-a-vis either disease control fibroblasts, with a\n\nmean of six weeks was considered an important\n\nsave to conclude that proceeds from an intrinsic\n\nfactor affecting survival. This data supports the need for\n\nproperty of the cells themselves.", "developing a reliable screen test for CF at birth.", "In contrast, recent work by Mandikal et al. [18] has advanced a hybrid retrieval model that integrates a sparse TF-IDF retriever with a dense retriever based on fine-tuned SPECTER2 embeddings [19]. Their approach employs a weighted scoring mechanism, controlled by a hyperparameter \u03bb , to balance the contributions of the two retrieval methods. Empirical evaluations on a medical database concerning Cystic Fibrosis indicate that while dense retrieval alone underperforms, incorporating approximately 20% of the sparse score (i.e., setting \u03bb = 0 . 2 for the sparse component) significantly improves document retrieval accuracies across different retrieval ranks [Figure 3]. Crucially, this method indexes documents solely based on their content, thereby avoiding the pitfalls of metadata dependency. This contributes to reducing information store creation times (Research Goal 1) and partially mitigates the retrieval of unnecessary information (Research Goal 3), although it does not entirely resolve the challenge of handling vaguer queries.", "In summary, while hybrid retrieval approaches represent a significant advancement by returning more relevant passages for well-defined queries complying with Research Goal 3. However they remain imperfect in filtering out unnecessary information for vaguer questions which can lead to longer contexts. This limitation underscores the need for further refinement to minimize the \"needle in a haystack\" problem in large-scale, real-world applications and make a pipeline which is compliant with Research Goal 2 as well.", "Reranker RAG methods: To address the limitation which Hybrid-Search methods have in adhering to Research Goal 2 for vaguer questions, Reranker methods have been developed to reduce the size and improve the quality of the final context.", "Figure 4: A high level view of what a RAG pipeline with a reranker looks like.\n\nIst Stage 2nd Stage Knowledge Embedding-based LLM-based Docs Corpus retrieval reranking\n\nUsually this is a process called 2-step retrieval where the first step involves retrieving documents such as through sparse or dense retriever techniques and then a second stage where an LLM based reranker comes into play to decide which of the documents retrieved are most relevant to the query and sorts them in order [Figure 4] before passing it to an LLM for generation.", "Prior to the formal conceptualization of Retrieval-Augmented Generation (RAG), information retrieval research explored the use of document rerankers. For example, Noguiera et al.'s 2020 study, Document Ranking with a Pretrained Sequence-to-Sequence Model [20], proposed employing a sequence-to-sequence model as a document reranker. This work evaluated multiple fine-tuned T5 models of varying sizes, as well as a fine-tuned BERT model, on documents initially retrieved using the sparse BM25 keyword method. The incorporation of these rerankers led to improvements in retrieval performance across the Robust04, Core17, and Core18 datasets. However, a critical limitation of this framework was its reliance on large models, leaving unresolved whether the observed performance gains were due to the reranking strategy or simply the use of more powerful models.", "Subsequent advancements in RAG have integrated BERT-based rerankers into new pipelines. Michael Glass et al.'s Re 2 G framework [21] embeds a BERT-based reranker-similar to that of Noguiera et al.-within a RAG system. This integration has shown notable improvements in metrics such as Recall (for 5 documents), overall recall, precision, and accuracy on several datasets, as indicated by performance on the KILT leaderboard. Nevertheless, a significant drawback of this approach is the necessity to host an additional model, which directly conflicts with Goal 4: alleviating the monetary and computational costs associated with deploying full-scale LLMs.", "Further refinement in reranking approaches is demonstrated by Xuegang Ma et al., who fine-tuned a LLaMA-2-7B model for reranking in their RankLLaMA system [22]. Although RankLLaMA achieves superior reranking performance, it again relies on a large model comparable to LLaMA-2-7B. Such reliance on resource-intensive models undermines the overarching objective of Goal 4, as the additional computational and financial burden associated with hosting these rerankers contradicts the need for efficiency and cost reduction.", "In summary, while reranker models can mitigate the 'needle in a haystack' problem (Goal 2) by filtering out least relevant information, they are not a sustainable solution. Their requirement for additional large-scale models significantly increases computational and monetary costs, thereby violating Goal 4. Consequently, alternative strategies that achieve comparable retrieval performance without incurring such overhead are essential for the advancement of practical and efficient RAG systems.", "Knowledge Graph RAG methods: Knowledge graph-based solutions have been proposed as a promising alternative for enhancing the RAG process. The core idea behind employing knowledge graphs is to address semantic search challenges by organizing data according to intrinsic relationships. For example, although not originally applied within a RAG context, Dawei Cheng et al.'s 2020 work on constructing knowledge graphs for a financial price prediction tool [23] illustrates how grouping semantically related data into distinct sub-networks enables a query-such as 'who quit Apple'-to retrieve only the pertinent subset of information, rather than returning all semantically similar sentences (e.g., both 'Steve Jobs quits Apple' and 'David Peter leaves Starbucks'). This targeted retrieval helps mitigate the \"needle in a haystack\" problem (Goal 2) and increases the rate of retrieving relevant passages (Goal 3).", "Building on these insights, recent research has directly integrated knowledge graphs within RAG frameworks. For instance, Zhentao Xu et al. [24] developed a system for LinkedIn's customer question answering service by first", "creating a custom knowledge graph and then retrieving related nodes. The retrieved information is subsequently refined using GPT-4 to produce coherent responses. When compared with a baseline semantic search system, this pipeline significantly improved retrieval metrics-including Mean Reciprocal Rank, Recall, and Normalized Discounted Cumulative Gain-as well as evaluation measures such as BLEU, METEOR, and ROUGE, thereby supporting Goals 2 and 3.", "However, despite these promising advances, knowledge graph-based approaches fall short of meeting Goal 1. The process of constructing and training knowledge graphs is computationally intensive and time-consuming, which directly conflicts with the objective of reducing information store creation times.", "| | Model | EM | Fl Score | Accuracy | Hallucination |\n|---:|:--------------|-----:|:-----------|:-----------|:----------------|\n| 0 | Human | 63 | | | |\n| 1 | MHQA-GRN | 33.2 | | | |\n| 2 | Embedding-RAG | 28 | 37 | 46 | 30 |\n| 3 | KG-RAG | 19 | | 32 | 15 |\n\nFigure 5: Results comparing knowledge graph framework with semantic search methods [25].", "For example, Diego Sanmart\u00edn's 2024 work [25] demonstrated a Knowledge Graph RAG system that leverages LLMs to extract entity-relationship triples from unstructured text and employs a 'Chain of Explorations' retrieval strategy. While this approach shows potential for handling vague queries, the high training times required to build and maintain the knowledge graph significantly increase the overall overhead. Moreover, the compressed storage format-using entity-relationship triples-can lead to a loss of detailed contextual information, thereby limiting the system's ability to fully expand upon relevant topics during retrieval hence resulting in even lower EM, F1, Accuracy scores with only an improvement in Hallucination compared to a standard Semantic Search method [Figure 5].", "In summary, while knowledge graph-based solutions offer clear benefits in terms of improving semantic search and enhancing the retrieval of relevant passages (addressing Goals 2 and 3), they do not currently satisfy Goal 1 due to their high training times and associated computational costs. Addressing these inefficiencies is essential for realizing the full potential of knowledge graphs within efficient and practical RAG frameworks.", "Question Generation System Prompt\n\nInstruction: Your task is to create a small question out of the below information.\n\nInformation: chunk from knowledge graph results\n\nAnswer:\n\nThe sub-question is then used to query the dense and sparse vector databases using the hybrid retriever with \u03bb = 0 . 8 [18] and k = 10 (4) [Figure 8(D)] where an embeddings model is used when querying the dense vector database [Figure 8(c)].", "We then use the quantized LLM with the results from the hybrid retriever as context to generate the answer to the sub-question and then store into a list containing all the sub-answers. Once completed for each chunk, we then concatenate all the sub-answers and use the Longformer model to summarize the results so that we can reduce the size of the context [Figure 8(D)]. Finally, the summarized sub-answers and the summarized knowledge graph results are concatenated and passed as context to the quantized LLM to generate the final answer to the question [Figure 8(E)].", "3 Methodology\n3.1 Task Definition\n\nGiven the complex definition of health literacy, measuring health literacy in its entirety remains a methodological challenge, especially at scale and in automated settings. In our framework, we first adopt the Flesch-Kincaid Reading Ease (FKRE) score as a practical, scalable proxy for estimating the functional dimension of health literacy. FKRE quantifies text difficulty based on syntactic features, which directly influence comprehension. This makes it an approximate tool for distinguishing materials accessible to users with varying levels of health literacy. For example, texts with higher FKRE scores (e.g., 80-100) are generally easier to read and more appropriate for individuals with lower health literacy. Referring to the previous study (Rooein et al., 2023), we collapse the FKRE score into three categories, easy (80-100), medium (60-79), and hard (0-59).", "However, FKRE alone does not capture users' ability to critically interpret or act upon health information, dimensions that fall under interactive and critical health literacy. FKRE also fails to reflect whether users perceive the content as helpful, trustworthy, or respectful. To address this limitation, our approach augments FKRE with simulated user preference ratings, derived from LLMs conditioned to emulate users at different health literacy levels. These simulated raters assess counterspeech based on perceived accessibility, clarity, and helpfulness, providing a subjective complement to the FKRE score. Together, these signals enable our model to generate content that aligns with the syntactic simplicity suitable for a target group and resonates with the group's communication expectations and information-processing styles.", "The task is formulated in the following: given a piece of health misinformation m and relevant retrieved knowledge k , the function f generates a counterspeech response c that is tailored to match the health literacy level l of a target user group.", "3.2 Controlled-Literacy Pipeline\n\nThe Controlled-Literacy Pipeline includes three sections (See Figure 2): Knowledge Base Construction, Health Literacy-Adaptive Evidence Retrieval, and Controlled Literacy RL Generation.", "Knowledge Base Construction Users with different levels of health literacy get information from different sources and prefer content with varying complexity and detail (Chen et al., 2018). We collect knowledge from a diverse set of reliable sources to ensure the inclusiveness and representativeness of knowledge across user groups. These include federal health agencies such as the Centers for Disease Control and Prevention (CDC) and academic and research databases like the Johns Hopkins Children's Center. In addition to diverse sources, we employ FKRE to evaluate the readability of each document, categorized as easy (80-100), medium (60-79), or hard (0-59), to ensure that materials are at different readability levels.", "Health Literacy-Adaptive Evidence Retrieval We utilize a hybrid retrieval method to get evidence from the knowledge base, which has been proven to perform better than a single method (Sawarkar et al., 2024). Our retrieval module integrates two retrieval methods: keyword-based ( R k ) and semantic retrieval ( R s ). The hybrid retriever ( R h ) integrates the strengths of two methods: R h = R k \u222a R s . When retrieving the topN documents ( R h = { d 1 , d 2 , . . . , d N } ), these documents are concatenated into a single context: C = concat ( d 1 , d 2 , . . . , d N ) .The concatenated context C is then paired with the input query q to construct the prompt for the LLM to generate responses r .", "After retrieving knowledge, we filter the evidence using both the FKRE score and LLMsimulated user preference ratings. We design a 1-5 Likert-style scale to evaluate their preference for the generated counterspeech, where a rating of 3 or higher indicate that users with a certain health literacy level find the content to be at least acceptable. We retain evidence that falls within the target FKRE range and receives a preference rating of 3 or higher, leaving out content users find unhelpful or confusing (rated 1 or 2). This ensures the selected content is both syntactically accessible and aligned with users' communication expectations.", "Controlled Literacy RL Generation A counterspeech generator is trained to adapt to the target audience's health literacy level. It balances a readability-based reward with a user preference reward to optimize outputs, enabling the generation to be more accessible to a user group. The FKRE score assesses the difficulty of texts. Rather than enforcing strict constraints, we consider a response desirable if its FKRE score falls within the corresponding target range. This design allows for greater flexibility during optimization and encourages more natural counterspeech genera- tion (Ibrahim et al., 2024). We adopt a doublesigmoid function to optimize outputs into the target range of readability:\n\nF is the FKRE score. L and R are the left and right boundaries of the target FKRE range. s is a scaling factor controlling the transition smoothness.", "User preference measures whether the generated counterspeech is accessible and easy for users with the target health literacy level to comprehend and apply. Since no publicly available datasets exist for this task and employing human annotators is costly, we leverage LLMs with customized instructions to simulate users from different health literacy levels.", "Weultimately incorporate both evaluations to optimize generation, ensuring that the counterspeech objectively fits the target readability level and subjectively aligns with user preferences. We define the final reward as a weighted combination of two components: the readability reward and the user preference reward. Specifically,\n\nwhere \u03b1 \u2208 [0 , 1] is a hyperparameter that controls the balance between promoting text that fits the target readability range and ensuring subjective accessibility for users across different health literacy levels. r ( x, y ) is the composite reward. r read is the readability reward based on the FKRE score, The preference reward r pref is derived from LLMbased Likert-style scoring (1-5) simulated for a given health literacy level.", "After confirming the reward function, we use Group Relative Policy Optimization (GRPO) (Shao et al., 2024) to optimize the counterspeech. GRPO generates n responses for each prompt and computes their relative advantages based on an aggregated reward signal. The policy is then updated to increase the likelihood of higher-ranked responses while constraining divergence from the reference policy \u03c0 \u03d5 using KL regularization. The optimization objective is:\n\nwhere r ( x, y ) is the composite reward function, and \u03b2 controls the regularization strength.", "4. Graph-Augmented Vector Retrieval\n\n4.3. Hybrid Scoring and Retrieval\n\nTo bridge the gap between local embedding similarity and global semantic relationships, we introduce a hybrid scoring framework that integrates vector-based relevance with graphbased propagation. Given a semantic graph G = ( V , E ) , and a query embedding q \u2208 R d , we first retrieve a candidate pool V q \u2282 V using standard ANN search in the vector space. Our objective is to refine the ranking of nodes in V q \u222aN ( V q ) , where N ( \u00b7 ) denotes neighbors in the graph G , by considering both geometric proximity and graph-induced connectivity.", "Vector-Based Relevance. The local relevance of a node v \u2208 V is defined via cosine similarity between its embedding v \u2208 R d and the query vector:\n\nGraph-Based Influence. To capture semantic signals that are not evident from local proximity, we compute a relevance diffusion score S graph ( v, q ) by propagating query affinity through the graph structure. One effective method is Personalized PageRank (PPR) , which computes a stationary distribution over nodes biased towards the query neighborhood:\n\nwhere s is a one-hot or soft seed vector centered on V q , A is the normalized adjacency matrix of G , and \u03b1 \u2208 (0 , 1) controls the restart probability. The score S graph ( v, q ) = r [ v ] then reflects how reachable v is from the query region under random walks.", "Hybrid Scoring Function. We define the final relevance score for each node v \u2208 V q \u222a N ( V q ) as a convex combination of the vector and graph-based components:\n\nwhere \u03b2 \u2208 [0 , 1] modulates the influence of structural context. This formulation smoothly interpolates between pure ANN retrieval ( \u03b2 = 0 ) and purely graph-based exploration ( \u03b2 = 1 ).", "Figure 3. Left: Hybrid graph constructed by combining symbolic edges (black) and k -nearest neighbor edges (gray). This enriched structure supports semantic exploration through Personalized PageRank. Right: Retrieved points for a fixed query (shown in red) across three strategies: Topk ANN (blue), Semantic Compression (purple diamonds), and Graph-Augmented Retrieval (green). While ANN focuses on local neighborhoods and Semantic Compression samples from diverse clusters, the graph-based method retrieves highly relevant points concentrated in a semantic region.\n\nGraph with Symbolic + kNN Edges Symbolic k-NN Query Node\n\nClusters and Retrieved Points\n\n10.0 7.5 5.0 2.5 0.0 Cluster Top-k ANN (top-10) Semantic Compression (top-5)\n\nGraph-Augmented (PPR) (top-10)\n\nQuery\n\nThe hybrid approach can be viewed as an instance of multiview retrieval , where the vector space and graph topology represent two complementary manifolds. The weighted fusion ensures robustness to deficiencies in either view-for example, embedding collapse or graph sparsity.\n\nTo ensure scalability:\n\nThe graph G can be restricted to the kNN subgraph over V q , significantly reducing memory and compute.", "S graph can be precomputed for frequent queries or efficiently approximated via truncated random walks.\n\nThe scores R ( v | q ) support batching and GPU acceleration, making them deployable in production retrieval stacks.", "This hybrid retrieval mechanism significantly enhances semantic generalization, particularly in cases where the query concept has no direct match in the corpus but is indirectly connected via ontological or contextual relationships.", "4.4. Experiment : Graph-Augmented Retrieval with Sparse Symbolic Cluster Connectivity\n\nWe evaluate the three retrieval strategies on a synthetic 2D dataset with clustered structure: (i) Topk Approximate Nearest Neighbors (ANN), (ii) Semantic Compression using cluster centers, and (iii) Graph-Augmented Retrieval based on Personalized PageRank (PPR) over a symbolic k -NN graph.", "4.4. Experiment : Graph-Augmented Retrieval with Sparse Symbolic Cluster Connectivity\n\nTo assess retrieval quality, we compute two metrics: Relevance , the average cosine similarity between retrieved items and the query, and Diversity , defined as one minus the average pairwise cosine similarity among the retrieved items. Each method retrieves the top-10 results for a fixed query.", "4.4. Experiment : Graph-Augmented Retrieval with Sparse Symbolic Cluster Connectivity\n\n| | Method | Relevance \u2191 | Diversity \u2191 |\n|---:|:----------------------|--------------:|--------------:|\n| 0 | Top- k ANN | 0.5174 | 0.8068 |\n| 1 | Semantic Compression | 0.4798 | 0.5718 |\n| 2 | Graph-Augmented (PPR) | 0.9688 | 0.0671 |\n\nTable 1. Comparison of relevance and diversity across retrieval methods.", "4.4. Experiment : Graph-Augmented Retrieval with Sparse Symbolic Cluster Connectivity\n\nGraph-Augmented Retrieval yields the highest relevance by leveraging both symbolic and similarity-based structure. PPR walks tend to remain within semantically consistent regions, which explains the lower diversity. In contrast, Topk ANN, based on geometric proximity, offers a better balance between relevance and diversity. Semantic Compression provides moderate scores by returning cluster centroids, which capture representative semantics but lose finer granularity.", "4.4. Experiment : Graph-Augmented Retrieval with Sparse Symbolic Cluster Connectivity\n\nFigure 3 offers a visual comparison. The left subfigure shows the hybrid symbolic+ k NN graph used by the PPRbased retrieval, while the right subfigure overlays the retrieved points from each strategy on the original clustered space. We observe distinct retrieval patterns for each method due to their inherent mechanisms. Topk ANN retrieves items tightly around the query because it directly selects the nearest neighbors in the embedding space based purely on"]}
{"type": "entry", "index": 53, "question": "when does using titles/abstracts beat full text retrieval", "answer": "The provided documents do not report a clear case where using only titles/abstracts decisively outperforms full\u2011text retrieval in terms of answer quality.\n\nInstead, they mostly highlight trade\u2011offs:\n\n1. In scientific survey generation, relying only on abstracts and metadata is explicitly described as a *limitation*: it \u201csignificantly limits the information coverage\u201d and makes surveys lack \u201cdepth and completeness\u201d compared to using full text [2]. This suggests full text is better for rich, in\u2011depth tasks, not worse.\n\n2. In biomedical QA (BioASQ), the retrieval system indexes PubMed titles and abstracts and gets good results, but this is framed as a *practical design choice* (PubMed abstracts are standardized and easy to index) rather than as being superior to using full articles [4]. The paper does not compare against full\u2011text retrieval; it only says that even using full abstracts required reranking to get adequate QA performance [4].\n\n3. For automatic survey generation, authors note that copyright restrictions forced them to use abstracts only, again implying that abstracts are used for legal/efficiency reasons, not because they are inherently better than full texts for quality [2].\n\nSo, within these documents, titles/abstracts are preferred mainly for efficiency, standardization, and legal constraints, while full text is expected to give *better* coverage and depth when it is available and computationally manageable [2][4]. There is no evidence here that titles/abstracts systematically \u201cbeat\u201d full text in effectiveness; they mainly win on cost and feasibility, not on information quality.", "tags": ["_untagged"], "full_faithfulness": 0.6666666667000001, "full_answer_relevancy": 0.0, "seed_faithfulness": 0.6666666667000001, "seed_answer_relevancy": 0.0, "context_relevance": 0.32672400966666454, "context_relevance_error": null, "context_relevance_reason": "The context passages provide some relevant information about retrieval methods and their effectiveness, but they do not directly address the specific question of when using titles/abstracts is preferable to full text retrieval. While there are mentions of limitations related to using abstracts, the overall relevance to the user's question is limited, resulting in a low score.", "structure_citation": 0.7463109367772365, "structure_error": null, "structure_reason": "The Actual Output effectively discusses the limitations of using titles/abstracts compared to full text, referencing specific snippets from the Context with [2] and [4] markers. It accurately highlights that the documents do not support the claim that titles/abstracts outperform full text in quality, aligning well with the evaluation steps. However, while the claims are well-supported, there is a slight lack of clarity in directly linking all claims to the corresponding snippets, which prevents a perfect score.", "hallucination_score": 0.5295564447751956, "hallucination_error": null, "hallucination_reason": "The response accurately identifies that the documents do not provide evidence that titles/abstracts outperform full-text retrieval, highlighting trade-offs and limitations. However, it fails to fully address the context provided, which discusses the challenges of using abstracts due to copyright restrictions and the potential for reduced depth and completeness. While it mentions efficiency and legal constraints, it does not sufficiently connect these points to the broader implications of retrieval methods discussed in the context.", "contexts_used": ["Abstract\n\nElectronic health records (EHRs) are long, noisy, and often redundant, posing a major challenge for the clinicians who must navigate them. Large language models (LLMs) offer a promising solution for extracting and reasoning over this unstructured text, but the length of clinical notes often exceeds even stateof-the-art models' extended context windows. Retrieval-augmented generation (RAG) offers an alternative by retrieving task-relevant passages from across the entire EHR, potentially reducing the amount of required input tokens. In this work, we propose three clinical tasks designed to be replicable across health systems with minimal effort: 1) extracting imaging procedures, 2) generating timelines of antibiotic use, and 3) identifying key diagnoses. Using EHRs from actual hospitalized patients, we test three state-of-the-art LLMs with varying amounts of provided context, using either targeted text retrieval or the most recent clinical notes. We find that RAG closely matches or exceeds the performance of using recent notes, and approaches the performance of using the models' full context while requiring drastically fewer input tokens. Our results suggest that RAG remains a competitive and efficient approach even as newer models become capable of handling increasingly longer amounts of text.", "5 Discussion and Future Directions\n\nLLMfor Automatic Survey Generation: Are We There Yet? The results in Section 4.1 indicate that neither Fully LLM-based nor RAG-based approaches have achieved human-level performance. As highlighted in (Liang et al., 2025; Tang et al., 2025), hallucinated information, such as fabricated references and factual inaccuracies, remains a critical challenge in LLM-generated surveys. Although RAG-based methods reduce hallucinations by retrieving external sources, the retrieved papers are often only topically relevant and misaligned with human preferences. While LLMs have demonstrated efficiency and the ability to generate content considered useful by human evaluators (Wang et al., 2024), our human evaluation results (\u00a74.5) reveal that, despite strong topical relevance, LLMgenerated surveys exhibit limited coverage and indepth analysis, both essential for high-quality scientific surveys. Therefore, while LLMs can assist in survey generation, they are still unable to independently craft surveys that meet academic standards at the current stage.", "Future Directions for Enhancing Survey Generation As shown in Section 4.1, quality-based ranking of candidate references effectively improves the citation performance of generated surveys. This can be further enhanced through several strategies. For example, citation network analysis can be employed to capture the global relationships among papers and identify influential studies. Additionally, analyzing human citation behavior-such as citation intent, frequency, and location in the textual context-can inform better reference selection mechanisms. Training reference selection models on human-annotated datasets is also a potential option for collecting literature suitable for survey generation. On the generation stage, relying solely on abstracts as input significantly limits the information coverage, as it fails to fully capture the paper's broader details. Future work could leverage full-text information to enable more comprehensive contextual understanding, as well as explore human-in-the-loop discourse control, factual consistency verification, and advanced long-document modeling to improve survey quality.", "Real-World Applicability and Deployment Our framework is designed with modular components, including embedding-based retrieval, cocitation expansion, and re-ranking, which can be parallelized or extended. For the retrieval stage, we choose S2ORC (Lo et al., 2020) as the external database because its papers are peer-reviewed and have full dataset downloads, which can be stored locally and used for a one-time embedding computation. In practice, it can be replaced with other sources such as arXiv 7 or PubMed 8 , depending on user needs. Additionally, numerous wellestablished embedding models are available on the MTEB leaderboard 9 , offering a range of trade-offs between accuracy, model size, and computational efficiency. For the co-citation expansion module, we rely on the OpenAlex (Priem et al., 2022b) database for citation analysis. OpenAlex also provides free APIs and allows bulk download of citation data. Similarly, users can replace OpenAlex with other citation databases, such as Scopus (Elsevier, 2025) and SciSciNet (Lin et al., 2023). As for the re-ranking, we assume it is highly adaptable to different downstream needs. Since we will release the SurveyGen , users can customize re-ranking strategies according to their specific preferences. In the generation stage, the results (\u00a74.1) show that open-source models (e.g., LLaMA-3.1-70B) can achieve competitive performance compared to closed-source commercial LLMs such as GPT-4.1. This offers users greater flexibility based on their budget, deployment needs, and infrastructure.", "Limitations\n\nWe acknowledge several limitations in our work.", "Input Limitation. For copyright reasons, our approach is restricted to using only abstracts and bibliographic metadata of the retrieved papers, without access to full-text content. This limitation may hinder the LLM's ability to capture finer-grained details and structural elements that are often present in full-length papers. Hence, the generated surveys may lack depth and completeness compared to human-written surveys that draw on the entire papers.", "Post-generation Refinement. To reduce API call costs, we did not perform post-generation refinement to the LLM outputs, such as language polishing, citation formatting, or structural adjustments. These post-processing steps could further improve the personalization and overall quality of the generated surveys. Also, our work focuses on generating textual survey content and does not include visual elements such as figures, tables, or diagrams, which are often present in published scientific surveys. Lastly, for longer surveys, models like Claude-3Haiku 10 may offer superior performance due to their extended context handling capabilities.", "Data Contamination. We acknowledge the possibility of data contamination, as some surveys or key references (\u00a73.1) used are open access and may have been included in the training data of the LLMs, potentially leading to slightly different performance estimates. Although we do not explicitly control for this factor in our evaluation process, such contamination is a general challenge in benchmarking LLMs on open-domain generation tasks (Xu et al., 2024). Moreover, since all baselines in our study are based on mainstream LLMs, any potential contamination would be shared and thus unlikely to impact the relative comparison.", "Evaluation Sample Scope. While our empirical evaluation focuses on a subset of 120 relatively short surveys spanning multiple disciplines-selected to balance cost and feasibility in academic settings -weexpect similar performance trends to hold across the full dataset. We encourage the broader research community to further benchmark models using our dataset and framework to extend our findings across broader contexts.\n\nhttps://www.anthropic.com/news/ claude-3-haiku", "C. Contextual Summary\n\nContextual summaries are generated to provide concise, relevant information about technical documents, facilitating more accurate and focused retrieval. For a document \ud835\udc51\ud835\udc57 , the summary \ud835\udc46 \ud835\udc57 is extracted using an attention mechanism that focuses on key themes and contextually important parts:\n\nHere, \ud835\udc4a\ud835\udc60 represents the learned parameters for the summarization task. This approach ensures that the model captures essential details from \ud835\udc51\ud835\udc57 , enhancing comprehension and retrieval accuracy.", "3. Our methodology\n\n3.1. Information Retrieval Pipeline\n\nTo support high-quality RAG for Phase A+, we developed an IR pipeline that integrates traditional lexical search with LLM-based query generation and semantic reranking (Fig. 1).", "If the initial query returns fewer than five documents, we invoke Gemini 2.5 Pro Preview (05-06) to automatically revise the query. The model is prompted to enhance retrieval recall by enabling approximate matching and omitting overly rare or domain-specific terms. This refinement step is done to improve the query coverage while maintaining relevance. Our experiments have shown that this process is required in less than 5% of the queries in the BioASQ 13 test set.", "We index all PubMed article titles and abstracts in an Elasticsearch instance, using BM25 retrieval as the ranking function. For each input question, we use Gemini 2.0 Flash to generate a structured Elasticsearch query that captures the semantic intent of the question using synonyms, related terms, and full boolean query string syntax rules supported by Elasticsearch. This query is validated using regular expressions and then is used to retrieve up to 10,000 documents.", "Following document retrieval, we apply a semantic reranking model (Google semantic-ranker-default004) to reduce the number of candidate documents [11]. This model re-scores the initially retrieved documents based on semantic similarity to the original question, allowing us to select the top 300 most relevant documents. This reranked subset is used for downstream RAG-based QA, since despite really long context supported by modern Transformer architectures [12, 13], we could not get adequate QA results on full article abstracts without this step.", "Finally, we have added additional IR searches to handle the cases where a QA step does not return a response based on the evidence retrieved from Elasticsearch. We have observed that Elasticsearch context might not provide sufficient evidence for QA in 3-7% of test cases for Phase A+, depending on the batch. An automated process is used to expand IR sources to address these cases. First, we are using a Google search restricted to PubMed sources to attempt to find new matches. If that fails, we extend our sources to include Home of the Office of Health Promotion and Disease Prevention, WebMD,\n\nThis multi-stage retrieval approach, combining LLM-generated queries, a traditional BM25 search, and semantic reranking, enables flexible, high-recall, and high-precision document selection tailored to complex biomedical queries.", "Figure 1: IR process\n\nPubMed corpus in Elasticsearch Query Generation (Gemini 2.0 Flash) Query Valida- tion and IR (BM25, \u2264 10,000 docs) Results < Refinement 2.5 Pro) Reranking (semantic- reranker-4) Top 300 Articles for RAG No Yes Refine\n\nHealthline, and Wikipedia. This ensures that we have an answer candidate for all questions in Phase A+ test sets.", "3.2. Question Answering Pipeline\n\nWe adopt a unified, zero-shot QA framework for both Phase A+ and Phase B of the challenge. While the core QA procedure remains consistent across phases, Phase A+ incorporates an additional IR step to verify the presence of candidate answers within relevant documents (described at the end of Section 3.1). This ensures that selected documents contain sufficient information to support answer generation.", "To generate candidate answers, we leverage several large language models (LLMs): Gemini 2.0 Flash, Gemini 2.5 Flash Preview (2025-04-17), and Claude 3.7 Sonnet (2025-02-19). Prompts are adjusted using examples derived from the BioASQ 11 test set, improving the response structure and quality.", "The system uses zero-shot prompting, tailored to the question type: Yes/No, Factoid, or List. We experiment with multiple types of input context: (1) IR-derived results from Phase A+, (2) curated snippets provided in Phase B, and (3) full abstracts of articles selected during Phase B. This allows us to examine the influence of context granularity on answer accuracy and completeness.", "To consolidate candidate answers, we perform a secondary synthesis step using Gemini 2.0 Flash. This model is prompted to resolve any contradictions, select the most precise and specific answer components, and integrate complementary information into a single, unified response. As part of this step, the model also returns a confidence score estimating the reliability of the synthesized answer. If the score is below a predefined threshold (0.5, determined empirically), the synthesis is re-run with reduced sampling temperature (from 0.1 to 0.0) to improve determinism. This synthesis process is evaluated using the BioASQ 12 dataset to ensure consistency with benchmark standards.", "Table 1\n\nResults of our runs on BioASQ 13 Phase A+, Yes/No questions.", "4 Experiments\n4.2 Performance Comparison\n\nTable 1 and Table 2 report the Recall scores on E-VQA and InfoSeek, respectively. QM-Retriever consistently outperforms all baseline methods, achieving average improvements of 9.9% (E-VQA) and 7.0% (InfoSeek) over the second-best approach. The strong recall performance ensures that mKG-RAG operates on highly relevant knowledge graphs constructed in the fine-grained retrieval phase, as further supported by our ablation studies. Additionally, the results reveal that V \u2192 Vretrieval consistently beats other unimodal and cross-modal configurations, underscoring the critical role of visual content in VQA tasks.", "| | MLLM.MLLM | E-VQA.E-VQA | InternVL3.8B | LLaMA-3.2.11B | LLaVA-v1.5.7B | LLaVA-v1.5.13B | DeepSeek-VL2.3B | DeepSeek-VL2.16B | Qwen2.5-VL.3B | Qwen2.5-VL.7B | Qwen2.5-VL.32B |\n|---:|:------------|:--------------|:---------------|:----------------|:----------------|:-----------------|:------------------|:-------------------|:----------------|:----------------|:-----------------|\n| 0 | Zero-shot | Single-Hop | 22.4 | 27.0 | 15.8 | 16.1 | 22.0 | 22.4 | 19.1 | 21.0 | 27.1 |\n| 1 | Zero-shot | All | 23.0 | 28.9 | 16.2 | 16.6 | 21.6 | 22.3 | 18.9 | 20.8 | 27.3 |\n| 2 | mKG-RAG | Single-Hop | 32.7 | 37.2 | 25.0 | 27.7 | 28.4 | 31.1 | 28.9 | 30.4 | 36.5 |\n| 3 | mKG-RAG | | \u2191 10.3 | \u2191 10.2 | \u2191 9.2 | \u2191 11.6 | \u2191 6.4 | \u2191 8.7 | \u2191 9.8 | \u2191 9.4 | \u2191 9.4 |\n| 4 | | All | 32.7 | 38.5 | 24.6 | 27.8 | 27.4 | 29.9 | 28.2 | 29.6 | 36.5 |\n| 5 | | | \u2191 9.7 | \u2191 9.6 | \u2191 8.4 | \u2191 11.2 | \u2191 5.8 | \u2191 7.6 | \u2191 9.3 | \u2191 8.8 | \u2191 9.2 |", "Table 4: VQA accuracy on E-VQA across different MLLM architectures with varying sizes\n\nQwen2-VL: Jozef KUBA\n\nGPT-4o: Matej Walch mKG-RAG: J. Langer\n\nQ : Who designed this museum?\n\nQwen2-VL \uff1a Indonesian\n\nGPT-4o: Indo-Saracenic mKG-RAG: Javanese\n\nQ : What is the architectural style of this mosque?\n\nQwen2-VL: 400\n\nGPT-4o:\n\nI can't answer without specific details.\n\nmKG-RAG: 250-metre\n\nQ : How long is the track at this velodrome?\n\nQwen2-VL: PARK\n\nGPT-4o: New York City Department of Parks and Recreation mKG-RAG: Abingdon Square Conservancy\n\nQ : Who is in charge of maintaining this park?\n\nQwen2-VL: 1000000\n\nGPT-4o: 26.04 km 2\n\nmKG-RAG: 3.82 km 2\n\nQ\n\n: What is the surface area of this lake?\n\nQwen2-VL: air space\n\nGPT-4o: a ring of dark spots mKG-RAG: brownish spots\n\nQ : What forms a ring on the larger end of the egg of this bird?", "Figure 4: Qualitative results of Qwen2-VL-7B, GPT-4o and mKG-RAG on E-VQA dataset.", "Results on E-VQA and InfoSeek. In this section, we compare mKG-RAG with Zero-shot MLLMs and RAG-based approaches on the benchmarks mentioned above. The results in Table 3 demonstrate that zero-shot MLLMs struggle with knowledge-based VQA tasks, particularly on the InfoSeek dataset. These limitations underscore the critical need for external knowledge integration. By augmenting LLaVA-More with mKG-RAG, we achieve substantial improvements, over 20.3% on E-VQA and 31.9% on InfoSeek, highlighting the value of retrieval augmentation.", "Furthermore, our method achieves state-of-the-art performance on both datasets. Under the finetuning setting, mKG-RAG * surpasses both mR2AG * and ReflectiVA * . Even without fine-tuning, mKG-RAG outperforms EchoSight by 1.2% and 1.7%, respectively. These results highlight the advantages of integrating RAG with multimodal KGs and demonstrate the effectiveness of our QMRetriever. Table 3 also includes two mKG-RAG variants that replace QM-Retriever with text-only and vision-only CLIP for entity/relationship retrieval, while still using documents retrieved by QMRetriever to construct multimodal KGs. In the text-only variant, both questions and image captions are used as queries to provide more context, explaining its better performance over the vision-only version. However, they both remain less effective than our full approach with QM-Retriever.", "Consistency across Architectures. In Table 4, we provide a detailed comparison of VQA scores across MLLMs of varying parameter sizes, including InternVL3 [58], LLaMA-3.2-Vision 2 , LLaVAv1.5 [34], DeepSeek-VL2 [52], and Qwen2.5-VL [3]. When enhanced with our mKG-RAG framework, these models achieve average performance gains of 9.4% on single-hop queries and 8.7% on overall scenarios, demonstrating the method's strong generalization across different architectures and scales.", "Qualitative Results. Figure 4 shows a qualitative comparison of mKG-RAG with zero-shot Qwen2VL and GPT-4o. While the latter two tend to produce plausible but incorrect or evasive responses, mKG-RAG consistently handles knowledge-intensive queries, especially those involving precise numerical and temporal reasoning.\n\n2 https://huggingface.co/meta-llama/Llama-3.2-11B-Vision", "| | Method. | E-VQA.Single-Hop | E-VQA.All | InfoSeek.Un-Q | InfoSeek.Un-E | InfoSeek.All |\n|---:|:--------------------|-------------------:|------------:|----------------:|----------------:|---------------:|\n| 0 | mKG-RAG | 38.4 | 36.3 | 41.4 | 39.6 | 40.5 |\n| 1 | w/o QM-Retriever | 34.2 | 31.6 | 38.9 | 37.9 | 38.4 |\n| 2 | w/o Graph Retrieval | 30.1 | 28.2 | 33.3 | 32.7 | 33 |\n| 3 | w/o Graph Expansion | 37.2 | 35 | 40.8 | 39.4 | 40.1 |\n\nTable 5: The ablation study of the design of mKG-RAG\n\n| | Model | Ret. Mode | K g = 1 | K g = 5 | K g = 10 | K g = 20 |\n|---:|:---------|:------------|----------:|----------:|-----------:|-----------:|\n| 0 | mKG-RAG\u2020 | Textual | 29.1 | 33.9 | 34.9 | 35.9 |\n| 1 | mKG-RAG\u2021 | Visual | 23 | 29.6 | 31 | 32 |\n| 2 | mKG-RAG | Multimodal | 29.2 | 35.1 | 36.3 | 36.9 |\n\nTable 6: The ablation study of how the retrieval number of entities/relationships affects the VQA accuracy on E-VQA.", "4.3 Ablation Study\n\nImpact of Coarse-grained Retrieval. To quantify the impact of coarse-grained document retrieval, we conduct an ablation experiment replacing QM-Retriever with visual-only CLIP (ViT-L/14@336) for topK d document selection. The results in Table 5 reveal significant performance drops: overall VQA accuracy of mKG-RAG decreases by 4.7% on E-VQA and 2.1% on InfoSeek. This ablation conclusively demonstrates the critical role of first-stage retrieval and QM-Retriever's superiority over unimodal alternatives.", "Effectiveness of Graph-based Retrieval. In our method, the entities and relationships extracted from documents form a distilled knowledge graph, reducing noise and enabling more effective retrieval than direct text chunk matching. To validate this insight, we replace graph-based retrieval with a naive chunk-based alternative. Specifically, we segment retrieved documents into fixed-size chunks and select those relevant to the given question and image description. As shown in Table 5, chunk-based retrieval leads to a substantial accuracy drop, 8.1% on E-VQA and 7.5% on InfoSeek."]}
{"type": "entry", "index": 54, "question": "how do i stop duplicate evidence from inflating confidence in the answer", "answer": "To stop duplicate evidence from falsely boosting confidence, you need to (1) detect redundancy and (2) down\u2011weight or remove it when scoring the evidence.\n\n1. Detect redundancy at the chunk level\nVERIRAG treats each piece of evidence (a text chunk) as potentially redundant with earlier chunks. It measures how similar a chunk is to any earlier chunk using text similarity (like cosine similarity over TF\u2011IDF vectors). If a chunk is almost identical to something already seen, it gets a high redundancy score. [5]\n\n2. Penalize duplicates when aggregating evidence\nFor each document, VERIRAG computes a redundancy penalty as the average redundancy of its chunks and converts that into an \u201cinformation\u2011gain weight\u201d w = 1 \u2212 redundancy. Highly redundant documents therefore get weights close to zero, so they add almost no extra support or refutation to the final verdict. [5]\n\nBy design, adding a semantically identical chunk barely changes the overall evidence tallies\u2014this is their \u201credundancy immunity\u201d property. [5] In practice, this means 10 copies of the same study should not count as 10 times the evidence.\n\n3. Filter duplicates during retrieval\nComoRAG explicitly removes duplicated evidence in its iterative retrieval loop, so later cycles focus on new information rather than re\u2011finding the same passages. This improves retrieval efficiency and avoids over\u2011counting the same facts. [4]\n\nPutting it together, a robust approach is:\n- During retrieval: drop or de\u2011prioritize near\u2011duplicate passages. [4]\n- During scoring: compute a novelty weight per document or chunk and strongly discount highly similar repeats. [5]\n\nThis combination prevents duplicate or copy\u2011pasted evidence from inflating your confidence in an answer.", "tags": ["summarization_conflict"], "full_faithfulness": 1.0, "full_answer_relevancy": 0.9201597198, "seed_faithfulness": 1.0, "seed_answer_relevancy": 0.9270275312, "context_relevance": 0.825053250455424, "context_relevance_error": null, "context_relevance_reason": "The context passages provide substantial information on mechanisms to mitigate issues related to duplicate evidence inflating confidence, particularly through the discussion of the dual-path generation strategy and the verification-driven finalization step. These elements directly address the user's concern about managing redundancy in evidence. However, while the context is relevant, it could be more explicitly tied to the user's question, which slightly reduces the overall score.", "structure_citation": 0.6420922508076038, "structure_error": null, "structure_reason": "The Actual Output effectively addresses the query about stopping duplicate evidence from inflating confidence by outlining specific strategies such as detecting redundancy, penalizing duplicates, and filtering during retrieval. It includes citations [4] and [5], which correspond to relevant snippets in the Context. However, while the claims are well-supported, the response could have benefited from additional citations to strengthen its grounding further, particularly for the detailed mechanisms described.", "hallucination_score": 0.9458848691888937, "hallucination_error": null, "hallucination_reason": "The response effectively addresses the input question by outlining a clear methodology to prevent duplicate evidence from inflating confidence in answers. It details the processes of detecting redundancy, penalizing duplicates, and filtering during retrieval, which directly supports the claim made in the input. The explanation aligns well with the context provided, particularly regarding the mechanisms of the RAG pipeline and the importance of maintaining evidence integrity. However, while comprehensive, a slight lack of emphasis on the practical implications of these methods could enhance clarity.", "contexts_used": ["Retrieval-Augmented Generation (RAG) Pipeline\n\nThe RAG pipeline is engineered to ensure responses are strictly grounded in a verified knowledge base, mitigating the risk of LLM hallucination (Figure 1).", "Knowledge Corpus: The retrieval corpus is constructed from publicly available and accessible UK clinical guidelines and resources from authoritative bodies (e.g., endorsed by NICE, SIGN, Royal Colleges). Ad-hoc scripts monitor these sources daily for updates to ensure content currency.", "Ingestion and Indexing: Documents undergo a proprietary cleaning pipeline and are segmented into semantically coherent text chunks (average 500 tokens, maximum ~3000 tokens). Optimization of this chunking is critical for retrieval relevance [14]. These chunks are converted into numerical vectors using advanced embedding models (GPT embedding family) and indexed in a vector database (Pinecone) for efficient semantic search.", "Retrieval and Confidence Scoring: When a user submits a query, a proprietary algorithmic search engine executes a semantic search, retrieving up to 15 of the most relevant text passages. The algorithm assesses the semantic similarity between the query and the retrieved passages to determine a confidence score.", "Dynamic Scope Extension: If the initial retrieval from the core guidelines yields low confidence scores (indicating potential gaps or ambiguity), the system dynamically extends its scope by triangulating findings with a search of publicly available peer-", "reviewed research. This secondary search algorithmically ranks evidence by hierarchical strength (e.g., prioritizing meta-analyses over case studies). 5. Safety Threshold (Refusal to Answer): If the confidence score remains below a predefined safety threshold after scope extension, the system declines to answer the query. This mechanism is a critical safety feature to prevent the dissemination of information not robustly supported by the evidence base.", "6. Synthesis: If the confidence threshold is met, the retrieved passages and the original query are passed as context to the generator model. The platform utilizes a cascade of LLMs, including models from the Google Gemini and OpenAI families, alongside a proprietary, post-trained model (\"Thea\") optimized for clinical synthesis within the iatroX workflow.", "Figure 1: Safety-aware RAG decision flow. The pipeline retrieves from UK guidelines, extends scope if needed, applies confidence scoring, refuses low-confidence queries, and synthesizes grounded responses with provenance citations (median latency: 12 seconds).\n\nClinician submits question\nRetrieve\n\nk passages from corpus\n\nCompute confidence pooled similarity\n\ntau\n\nSecondary search peer reviewed literature\n\nRecompute confidence\n\nLLM synthesis constrained to retrieved passages\n\nRefusal to answer and clarifying prompt\n\nAnswer with citations and links\n\ntop", "3 Methodology\n3.3 Dual-Path Generation\n\nThe primary task of the VLM is to then generate an accurate and reliable answer for each user query \ud835\udc5e \ud835\udc56 . Not all queries have relevant contexts in the knowledge graph, and in some cases, the reranker may assign low relevance scores to all retrieved snippets, resulting in a minimal or empty context \ud835\udc36 \ud835\udc56 . Additionally, the VLM's prior knowledge may still yield correct answers even when relevant external context is unavailable. To balance grounding in retrieved evidence with the model's prior capabilities, we adopt a dual-path generation strategy. Specifically, for each query \ud835\udc5e \ud835\udc56 , we generate two responses: one using retrieval-augmented generation (RAG) and one without context (non-RAG):\n\nwhere \ud835\udc53 shares the same vision-language model with the summary process. The RAG answer \ud835\udc4e \u2032 \ud835\udc56 is conditioned on the retrieved and filtered context \ud835\udc36 \ud835\udc56 , while the non-RAG answer \ud835\udc4e \ud835\udc56 is generated without any retrieval and relies on the VLM's prior knowledge only.", "Self-consistency. This dual-path setup enables us to later assess the consistency between the two outputs and identify cases where retrieved contexts improve factual grounding. To this end, we adapt the self-consistency check from [9], with modifications tailored to our dual-path generation setup. After generating answers with and without retrieved context, we evaluate their consistency using an additional verification step. Specifically, we pass the query \ud835\udc5e \ud835\udc56 , the retrieved contexts \ud835\udc36 \ud835\udc56 (if any), the RAG answer \ud835\udc4e \u2032 \ud835\udc56 , and the non-RAG answer \ud835\udc4e \ud835\udc56 into the VLM for comparison.", "The motivation follows the self-consistency principle proposed by [9]: LLMs (thus VLMs) (1) are prone to hallucinations without grounding on retrieved contexts, and (2) may produce multiple plausible answers for complex queries. Since relevant context from the knowledge graph is not guaranteed for every query, self-consistency helps ensure that the model produces stable, consistent responses when prompted multiple times. In cases where a relevant context is found, agreement between the RAG and non-RAG answers indicates consistency and strengthens confidence in the generated", "output by showing that it is both grounded and aligned with the LLM's prior knowledge.\n\nTo perform this check, the VLM is prompted as an impartial judge tasked with determining whether the two answers are consistent with each other and with the provided context and image. It is instructed to respond with a binary decision: yes or no . Full prompt details are provided in Appendix A.3.", "3.4 Verification and Finalization\n\nWhile the dual-path generation and self-consistency check effectively reduce hallucinations, we observe that the system tends to become overly conservative, often defaulting to uncertain or evasive responses (e.g., 'I don't know') in borderline cases. This cautious behavior, while safe, may lead to missed opportunities where the answer is in fact correct but filtered out due to lack of full agreement or incomplete grounding. To address this issue and strike a better balance between reliability and informativeness, we introduce a verification-driven finalization step.", "For each RAG-generated candidate answer \ud835\udc4e \u2032 \ud835\udc56 , we employ a structured 'Chain-of-Verification' (CoV) protocol [3] that systematically probes factual consistency and justification. The process involves two phases: (1) holistic check and (2) decompositional check.", "In the holistic check, a verification-oriented VLM evaluates the generated answers against criteria including factual accuracy relative to visual evidence, consistency with provided context, absence of contradictions, direct relevance to the user's query, and specificity. Answers failing in the preliminary check are immediately assigned low confidence scores ( \u2264 0.5), preventing the propagation of uncertain information.", "For answers passing the initial holistic screening, a decompositional check is performed. The query is broken down into subquestions \ud835\udc44 = { \ud835\udc5e 1 , \ud835\udc5e 2 , . . . , \ud835\udc5e \ud835\udc5b } , and each sub-answer \ud835\udc4e \ud835\udc56 is independently verified. Each verification result provides:\n\nConfidence Score A numeric value between 0.0 and 1.0 reflecting the certainty, where 0.0 indicates complete wrong and 1.0 indicates complete confidence.", "Reasoning A brief (1-2 sentence) rationale explaining the reason.\n\nSub-Questions A concise decomposition of the original question into sub-questions, each accompanied by a finding of 'Supported\" or 'Unsupported\" based on the verification.", "To quantitatively set thresholds for answer acceptance, we defined two confidence thresholds: a High Confidence Threshold (1.0), where answers equal to this threshold are deemed highly reliable and directly used, and a Low Confidence Threshold (0.9), where answers falling between this and the high threshold are cautiously accepted if contextually supported and self-consistent.", "This verification-centric approach significantly reduces hallucination rates by implementing multiple validation layers before finalizing any response. Crucially, it not only ensures that our system prioritizes truthfulness over completeness, but also mitigates the overly conservative bias introduced by earlier stages. By assigning calibrated confidence scores, the system is empowered to retain more factually correct answers that might otherwise be discarded, striking a better balance between caution and informativeness. After verification, the system performs a rule-based finalization step to determine whether and which answer to return. This decision process considers multiple signals, including retrieval quality, consistency between RAG and non-RAG answers, and the verifier's confidence score to make a calibrated trade-off between informativeness and reliability. The full decision logic is summarized in Algorithm 1 below.", "G.2 Suggestions for Future Work\n\nDevelopment of a Multilingual Document QA Benchmark This benchmark should span at least 20 languages across 5 language families, include documents from diverse domains (legal, medical, technical, financial), and feature questions requiring various levels of reasoning complexity. Ground-truth relevance judgments at the section, paragraph, and sentence levels would enable fine-grained retrieval evaluation and help identify language-specific optimal configurations.", "Systematic Exploration of Embedding Fusion Strategies This includes investigating learned gates that dynamically weight embeddings based on query characteristics, late-interaction mechanisms inspired by ColBERT that preserve token-level information, and cross-attention architectures that allow embeddings to inform each other. An evaluation framework measuring accuracy, latency, memory usage, and cache efficiency would identify Pareto-optimal configurations for different deployment scenarios.", "Integration of Explicit Evidence Grounding and Citation Mechanisms This involves developing visual overlays that highlight relevant passages in original documents, generating structured citations mapping answer components to source locations, and creating confidence scores that communicate uncertainty at the claim level. An interactive interface where users can trace any statement back to its supporting evidence would enable the trust and verifiability essential for high-stakes applications in legal, medical, and financial domains.", "4 Experimental Results\n4.1 Main Results\n\n| | Method. | EN.MC.ACC | EN.QA.F1 | EN.QA.EM |\n|---:|:------------------|:------------|:-----------|:-----------|\n| 0 | ComoRAG | 72.93 | 34.52 | 25.07 |\n| 1 | Baselines | | | |\n| 2 | HippoRAGv2 | 60.26 | 24.45 | 17.09 |\n| 3 | RAPTOR | 57.21 | 26.33 | 19.65 |\n| 4 | Index | | | |\n| 5 | w/o Veridical | 51.97 | 22.24 | 15.88 |\n| 6 | w/o Semantic | 64.63 | 30.82 | 22.65 |\n| 7 | w/o Episodic | 64.63 | 31.48 | 21.47 |\n| 8 | Retrieval | | | |\n| 9 | w/o Metacognition | 62.01 | 26.95 | 18.53 |\n| 10 | w/o Regulation | 55.02 | 27.95 | 20.59 |\n| 11 | w/o Both | 54.15 | 25.64 | 17.35 |\n\nTable 2: Ablation studies of ComoRAG.", "highlights the importance of stateful multi-step reasoning for query resolution over long and coherent contexts.", "4.2 Ablation Studies\n\nWeperform ablation studies on EN.MC and EN.QA datasets by systematically removing key modules in ComoRAG. The results are shown in Table 2.", "Hierarchical Knowledge Source All three knowledge layers contribute supplementary enhancements to the final performance, with the Veridical Layer being the most significant retrieval index. It provides the basis for factualgrounded reasoning, as confirmed by the 30% relative performance drop upon its removal.", "Metacognition Removing the Metacognition process essentially disables the memory workspace, where all agents operate on retrieved evidence directly, without knowledge consolidation by synthesized cues . Disabling this module leads to a significant performance drop, as seen by the 22% relative decrease in F1 score on EN.QA, and an approximate 15% decrease in accuracy on EN.MC, underscoring the critical role of dynamic memory organization.", "Regulation Removing the Regulation process cuts off the goal-oriented guidance, such that each cycle uses the same initial query for new evidence retrieval (duplicated evidence is removed), without generating probing queries that are crucial to new evidence acquisition. Disabling this module severely impacts retrieval efficiency, causing a 24% drop in accuracy on EN.MC and a 19% drop in F1 score on EN.QA.", "Notably, removing both Metacognition and Regulation further degrades performance, effectively reducing the system to a one-shot resolver without multi-step reasoning. Overall, the ablation study results corroborate that the enhancement offered by ComoRAG stems from the synergy between its memory consolidation and dynamic evidence exploration, facilitated by the hierarchical knowledge index to provide enriched semantic information. Removing any of the core components would significantly weaken its narrative reasoning capabilities.", "Figure 3: Averaged accuracy across different document lengths on Multi-Choice datasets. ComoRAG is shown more robust to long contexts over the baseline.\n\n>50 >100 >150 >200\n\nNumber of Tokens (K)\n\nAccuracy (%) +14.7% +24.6% 74.5 68.9 80.8 66.9 59.8 64.0 56.2 54.2 ComoRAG HippoRAGv2\n\nFigure 4: Performance gains from iterative probing. GPT-4.1 marks the evaluation by using the stronger GPT-4.1 as LLM agents in ComoRAG (as opposed to GPT-4o-mini).\n\nStep0 Step1 Step2 Step3 Step4 Step5 Metrics 59.1 68.2 76.1 64.6 72.9 71.6 78.2 20.0 22.5 25.0 27.5 30.0 32.5 35.0 37.5 40.0 21.2 31.4 31.8 35.4 27.4 34.5 31.1 38.8 Failure Signal Count DetectiveQA EN.MC NarrativeQA EN.QA GPT-4.1 Failure Signal", "4.3 In-Depth Analysis of Iterative Retrieval\n\nTo further investigate the source of ComoRAG's effectiveness, this section presents a quantitative analysis of its core iterative retrieval process.", "Source of Gains: From Static Bottleneck to Dynamic Reasoning Our analysis suggests that the stateful multistep reasoning enabled by the Metacognitive loop is the key factor driving the observed improvement.", "We first identify a 'static bottleneck': after the initial retrieval using the original query at step 0, the single-step evaluation score shows no significant advantage over strong baselines, with less than 1% compared to the best baseline HippoRAGv2+IRCoT. However, upon activating the cognitive loop, there presents a sustained and significant improvement, raising the accuracy to 72.93% on EN.MC, as shown in Figure 4. This further supports the findings from the ablation studies, which demonstrate a significant performance drop upon removing the entire loop. Additionally, Figure 4 illustrates that the majority of the improvement occurs within 2-3 cycles, confirming the efficiency of the process. The few remaining unresolved queries are tied to the inherent reasoning limitation of the base LLM, where our next analysis shows that the ceiling performance of ComoRAG can be boosted simply by switching to more capable LLMs.", "3 The Quantitative Framework\n\nAfter auditing the evidence, VERIRAG requires a formal mechanism to synthesize the results into a single, justifiable verdict. This", "section formalizes the mathematical machinery for this process. We define two central quantities: (i) the Hard-to-Vary (HV) score, which represents the aggregated, quality-weighted evidence for or against a claim, and (ii) the Dynamic Acceptance Threshold ( \ud835\udf0f auto ), which sets a context-appropriate bar for acceptance.", "3.1 Preliminaries and Notation\n\nWe first establish the notation for a claim \ud835\udc50 being evaluated against a set of retrieved evidence documents \ud835\udc38 = { \ud835\udc51 1 , . . . , \ud835\udc51 \ud835\udc41 } .", "Definition 3.1 (Evidential State). To quantify its contribution, each document \ud835\udc51 \ud835\udc56 \u2208 \ud835\udc38 is assigned a stance \ud835\udc60 \ud835\udc56 \u2208 {-1 , 0 , 1 } (Refutes, Neutral, Supports) and an audit vector \ud835\udc63 \ud835\udc56 \u2208 { 0 , 0 . 5 , 1 } \ud835\udc3e across the \ud835\udc3e = 11 methodological checks, where the values encode {Fail, Uncertain, Pass}.", "The audit vector is derived from a richer structured object containing detailed reasoning; the numerical values are used for the aggregation computations below.", "3.2 Hard-to-Vary (HV) Evidential Score\n\nThe HV score is calculated by first assessing each document's individual contribution and then aggregating these contributions.", "3.2.1 Evidential Operators. Wedefine a series of operators to distill the evidential value of each document.\n\nDefinition 3.2 (Applicability Mask). To account for checks that do not apply to all study types, for each document \ud835\udc51 \ud835\udc56 we define a binary mask \ud835\udc5a \ud835\udc56 \u2208 { 0 , 1 } \ud835\udc3e , where \ud835\udc5a \ud835\udc56,\ud835\udc58 = 1 if check \ud835\udc36 \ud835\udc58 is applicable. We denote the number of applicable checks as \ud835\udc3e \ud835\udc56 = \u2225 \ud835\udc5a \ud835\udc56 \u2225 1 .", "Definition 3.3 (Intrinsic Document /Q_u.scality). The methodological quality of a document \ud835\udc51 \ud835\udc56 is the mean pass rate over all applicable checks.", "Definition 3.4 (Redundancy Penalty). To reward diverse evidence and penalize redundant information from multiple sources, we calculate a redundancy score at the evidence-chunk level. For each evidence chunk \ud835\udc50 from a document \ud835\udc51 \ud835\udc56 , its redundancy \ud835\udf0c \ud835\udc50 is its highest textual similarity to any preceding chunk in the evidence set for claim \ud835\udc50 . The final penalty for the document, \ud835\udf0c \ud835\udc56 , is the mean redundancy of its constituent chunks, calculated using a TF-IDF vectorizer and cosine similarity. The information-gain weight is \ud835\udc64 \ud835\udc56 = 1 -\ud835\udf0c \ud835\udc56 .", "Definition 3.5 (Effective Contribution). The total evidential contribution of a document \ud835\udc51 \ud835\udc56 is its quality weighted by its novelty.", "3.2.2 Aggregation and Parameter Calibration. The individual contributions are aggregated into a final score.\n\nDefinition 3.6 (Aggregate Tallies). We sum the effective contributions for all supporting, refuting, and neutral documents.", "Definition 3.7 (Log-Odds and HV Score). The final HV score is the logistic transformation of the log-odds of the aggregated evidence.\n\nand\n\nThe parameters \ud835\udefc (neutral evidence penalty) and \ud835\udf06 (regularization) are not theoretical; they are empirically derived via a brute-force grid search optimization against a ground-truth dataset to maximize predictive accuracy.", "3.3 Dynamic Acceptance Threshold \ud835\udf0f auto\n\nTo implement the principle that extraordinary claims require extraordinary evidence, VERIRAG sets a dynamic, claim-specific acceptance threshold, \ud835\udf0f auto , using a multi-step heuristic process.", "Definition 3.8 (Claim Features for Thresholding). For each claim, we generate a feature set including:\n\nSpecificity & Testability ( \ud835\udc46,\ud835\udc47 ): Ratings from 1-10 generated by an LLM.", "Required Standard ( \ud835\udc45 ): A categorical label (e.g., 'Robust Study', 'Settled Science') indicating the expected evidence burden.", "Evidence Volume ( \ud835\udc41 \ud835\udc52\ud835\udc63 ): The number of evidence papers available for the claim in the current temporal scenario (e.g., TY0, TY1).", "Definition 3.9 (Threshold Calculation). The final threshold \ud835\udf0f auto is computed by blending a regression model's prediction with a rule-based modifier:\n\nwhere \ud835\udf0b \ud835\udc45 is a hardcoded prior probability for the required standard \ud835\udc45 , and \ud835\udc53 ( \ud835\udc46,\ud835\udc47, \ud835\udc45 ) is a Ridge Regression model trained on expert-rated claims to predict a boldness score. The base threshold is then adjusted by the volume of available evidence:\n\nwhere \ud835\udc41 base is the evidence count in the initial scenario (TY0) and \ud835\udc36 is a scaling factor. This ensures that as more evidence becomes available over time, a higher bar is set for acceptance.", "3.4 Theoretical Properties\n\nThe framework possesses several desirable mathematical properties.\n\nProposition 3.1 (Boundedness). The final scores are well-behaved, with HV , \ud835\udf0f auto \u2208 ( 0 , 1 ) .", "Proof. HV is the output of a sigmoid function. \ud835\udf0f auto is calculated as a blend of probabilities and small modifiers, clamped to remain within a sensible range of [ 0 . 5 , 0 . 95 ] . \u25a1\n\nProposition 3.2 (Monotonicity). The HV score correctly increases with supporting evidence and decreases with refuting evidence, i.e., \ud835\udf15 HV \ud835\udf15\ud835\udc3b \ud835\udc46 > 0 and \ud835\udf15 HV \ud835\udf15\ud835\udc3b \ud835\udc45 < 0 .", "Figure 3: VERIRAG system architecture: data ingestion, retrieval-audit pipeline and control layer.\n\nData System Control System RAG System Dataset Manager Computation Retriever Engine Document ingestion Dynamic threshold Evidence retrieval Storage Claims Manager Settings Veritable Module\n\nExtract (A, E, M) tuples\n\nWorkflows and Parameters 11-point audit framework Embedding Store Analytics LLM Interface Scmantic retricval index Performance monitoring Prompt processing setting\n\nProof. From (3), \ud835\udf15 log Odds / \ud835\udf15\ud835\udc3b \ud835\udc46 = ( \ud835\udc3b \ud835\udc46 + \ud835\udf06 ) -1 > 0 and \ud835\udf15 log Odds / \ud835\udf15\ud835\udc3b \ud835\udc45 = -( \ud835\udc3b \ud835\udc45 + \ud835\udf06 ) -1 < 0. The sigmoid function is monotone increasing. \u25a1\n\nProposition 3.3 (Redundancy Immunity). Adding a semantically identical evidence chunk \ud835\udc50 \ud835\udc57 to the evidence set for a claim does not meaningfully change the total evidence tallies.", "Proof. The redundancy of a chunk is its max similarity to any preceding chunk. A duplicate chunk \ud835\udc50 \ud835\udc57 would have a redundancy score \ud835\udf0c \ud835\udc50 \ud835\udc57 \u2248 1, causing its information-gain weight \ud835\udc64 \ud835\udc57 to approach 0. Its effective contribution \ud835\udf02 \ud835\udc57 thus becomes negligible. \u25a1"]}
{"type": "entry", "index": 55, "question": "how do i evaluate quality vs latency without making a fake single number", "answer": "You can avoid a fake single \u201cquality score\u201d by explicitly treating quality and latency as separate, first\u2011class dimensions and looking at their relationship instead of mashing them together.\n\n1) Measure quality with multiple concrete metrics\nFor RAG or QA systems, track:\n- Retrieval quality: Recall@k, MRR, MAP to see how often and how early relevant documents are retrieved [1].\n- Answer quality: Exact Match, F1, BLEU, ROUGE, plus faithfulness / hallucination rate (how consistent answers are with sources) [1][2].\n- Task\u2011specific execution quality: e.g., whether generated queries run correctly and return the right results (Relaxed Exact Match variants: \u201csuccess\u201d and \u201call\u201d) [2].\n\n2) Measure latency and efficiency separately\nTrack:\n- Retrieval latency, generation latency, and end\u2011to\u2011end response time [1].\n- Optionally, compare latency across variants like a baseline vs an optimized system, as done in hardware work that reports per\u2011benchmark latency tables [3].\n\n3) Analyze trade\u2011offs without collapsing them\nInstead of combining into one score:\n- Make comparison tables where each system/setting has a row with several quality metrics plus latency (similar to the metric tables in [1][2][3]).\n- Plot quality vs latency (e.g., faithfulness or RelaxedEM vs end\u2011to\u2011end time) to see which settings give \u201cgood enough\u201d quality at acceptable latency.\n- Define guardrails like: \u201cmust meet X faithfulness / EM and Y latency\u201d and choose the configuration that satisfies both, rather than optimizing a single mixed number.\n\nCurrent RAG research already treats quality and latency as separate axes and notes accuracy\u2013latency trade\u2011offs, especially when multiple documents are processed and real\u2011time performance is needed [1][4].", "tags": ["scaling_cost"], "full_faithfulness": 1.0, "full_answer_relevancy": 0.8614524665000001, "seed_faithfulness": 0.5, "seed_answer_relevancy": 0.8614597274, "context_relevance": 0.44978395422736134, "context_relevance_error": null, "context_relevance_reason": "The context passages provide some relevant information about evaluating retrieval-augmented generation (RAG) systems, including discussions on accuracy and latency trade-offs. However, they do not directly address the user's question about evaluating quality versus latency without reducing it to a single number. While there are mentions of latency and evaluation metrics, the response lacks a clear, focused discussion on how to balance these two aspects without oversimplifying them.", "structure_citation": 0.7521558867557606, "structure_error": null, "structure_reason": "The Actual Output effectively addresses the question of evaluating quality versus latency by providing a structured approach that separates the two dimensions. It includes multiple concrete metrics for measuring quality and latency, which aligns well with the evaluation steps. However, while it references [1], [2], and [3] markers, it does not fully utilize the Context snippets to support all claims, particularly regarding the specific examples of RAG implementations. This results in a slight deduction for unsupported claims, but overall, the response is strong and relevant.", "hallucination_score": 0.8265859637306301, "hallucination_error": null, "hallucination_reason": "The response effectively addresses the evaluation of quality versus latency by suggesting a clear separation of these dimensions and providing specific metrics for measuring each. It aligns well with the context, particularly in discussing trade-offs and the importance of real-time performance, which is highlighted in the context. However, while the output is comprehensive, it could benefit from more direct references to the specific examples provided in the context, which would strengthen its relevance and support.", "contexts_used": ["5 RAG for Proprietary Data -Industry Implementation\n\n5.2 Industry Implementation Examples\n\nRocket Companies' Integration of RAG for Mortgage Processing Rocket Companies explored the use of RAG to streamline mortgage processing by integrating proprietary financial data. The RAG system enabled more efficient retrieval of relevant information, reducing processing times and improving customer experiences. By leveraging their internal data within the RAG framework, Rocket Companies enhanced the accuracy and speed of their services, demonstrating the potential of RAG in financial applications [11].", "Shorenstein Properties' Implementation of RAG for Data Organization Shorenstein Properties adopted RAG to automate file tagging and organize proprietary data more efficiently. By integrating their internal documents into the RAG system, they improved data accessibility and management. This implementation showcased RAG's capability to handle complex data organization tasks, leading to increased operational efficiency within the company [11].", "Cohere's Advancement of RAG for Source Citation Cohere advanced RAG technology to ensure AI systems cite their sources, facilitating human verification of the information produced. By integrating external texts such as company documents or news websites, Cohere's RAG system reduced errors known as hallucinations and provided access to current information. This development highlighted RAG's potential in enhancing the reliability and transparency of AI-generated content [50].", "NVIDIA's Deployment of RAG in Enterprise AI Solutions NVIDIA incorporated RAG into their enterprise AI solutions to connect large language models with specific information, such as proprietary customer data and authoritative research. This integration enabled more accurate and relevant responses to user queries, enhancing productivity and protecting against AI hallucinations. NVIDIA's deployment demonstrated RAG's applicability in various industries, including customer service and data management [65].", "IBM's Utilization of RAG for Domain-Specific AI Applications IBM employed RAG to equip models with specific information, such as proprietary customer data and authoritative research documents. This approach allowed their AI systems to incorporate up-to-date information into generated responses, improving accuracy and relevance in domain-specific applications. IBM's utilization of RAG showcased its effectiveness in enhancing AI capabilities across different sectors [32].", "These case studies illustrate the diverse applications of RAG in leveraging proprietary data across various industries. By integrating internal datasets into RAG systems, organizations have enhanced information retrieval, improved decision-making processes, and increased operational efficiency. However, challenges such as data privacy, intellectual property concerns, and computational overhead remain areas for further research and development.", "5.3 Emergent Trends and Patterns in Industry Implementation\n\nAccuracy vs. Latency Trade-offs: Models optimized for retrieval accuracy (e.g., FiD, WebGPT) typically have higher latency due to multiple document processing. Industry applications prioritize real-time performance (e.g., NVIDIA RAG aims for sub-second latency).", "Scaling Beyond Wikipedia: Early RAG models focused on Wikipedia-scale retrieval, while industry applications integrate proprietary databases and real-time web data. WebGPT and IBM Watsonx RAG retrieve from dynamic knowledge bases, offering more up-to-date responses.", "Reducing Hallucination: There is a shift toward factual consistency. WebGPT and IBM Watsonx RAG enforce citation mechanisms to enhance factual accuracy.", "Model Size vs. Retrieval Efficiency: Smaller models augmented with retrieval (e.g., Atlas, NVIDIA RAG) can outperform much larger models without retrieval (e.g., GPT-3 175B), demonstrating that knowledge retrieval reduces the need for extreme model scaling.", "Enterprise Adaptation: Academic models optimize for benchmark performance, whereas enterprise RAG systems prioritize real-world reliability, integration with databases, and data privacy compliance.", "6 Evaluation of RAG Systems\n\nThe evaluation of RAG systems is multifaceted, as performance depends not only on the generative model but also on the quality of the retrieval pipeline. A robust evaluation framework must assess retrieval accuracy, answer quality, factuality, latency, and scalability. This section provides a structured overview of RAG evaluation criteria, benchmarks, and the impact of architectural design choices. Summary of this section is in Table Table 1", "6.1 Evaluation Dimensions and Metrics\n\nRAG performance is commonly assessed along the following dimensions:\n\nRetrieval Accuracy. Key metrics include:\n\nRecall@ k - Measures the proportion of queries where a relevant document appears among the topk retrieved results.", "Mean Reciprocal Rank (MRR) - Captures the average inverse rank of the first relevant document, rewarding early placement.", "Mean Average Precision (MAP) -Evaluates the quality of ranked retrieval across relevant items.", "6.2 Benchmarks and Datasets\n\nSeveral benchmarks are widely used to evaluate RAG systems:\n\nNatural Questions (NQ) , TriviaQA , and WebQuestions for open-domain QA [52].", "FEVER and AveriTeC for fact-checking, emphasizing verifiability.\n\nKILT benchmark suite integrates QA, dialog, slot filling, and entity linking over Wikipedia.", "BEIR evaluates retrieval across 31 zero-shot tasks in domains like biomedicine and finance.\n\nMTRAG targets multi-turn conversations requiring sequential retrieval and reasoning.", "TREC RAG Track (2024-) defines unified evaluation of retrieval, generation, and support quality over MS MARCO with metrics like nugget recall and citation coverage [71].", "Generation Quality. Evaluated using:\n\nExact Match (EM) and F1 Score - Common in QA tasks to measure overlap with reference answers.\n\nBLEU , ROUGE - N-gram-based measures used in summarization and long-form generation.", "Faithfulness / Hallucination Rate - Human or automated evaluations of factual consistency with retrieved sources.\n\nEfficiency and Latency. These include:\n\nRetrieval time , generation latency , and end-to-end response time .", "Memory and compute requirements -Especially important for deploying RAG systems at scale.\n\nScalability. As corpus size grows, the system's ability to maintain retrieval quality and generation fidelity is tested. Evaluation considers:\n\nIndex size vs. retrieval accuracy .\n\nAdaptability to new or evolving data without retraining .", "6.3 Retrieval-Augmented Generation Assessment System\n\nRAGAS (Retrieval-Augmented Generation Assessment System) is an evaluation framework specifically designed for assessing and improving the factuality and grounding of RAG systems. Unlike conventional metrics that measure superficial linguistic overlap, RAGAS emphasizes the alignment between generated content and retrieved documents, providing explicit signals regarding factual correctness and attribution quality. By systematically measuring how well the generated outputs are supported by the retrieved evidence, RAGAS helps identify and penalize hallucinations-instances where the model generates plausible but unsupported statements. Consequently, employing RAGAS during model training or iterative fine-tuning guides RAG systems toward producing outputs firmly grounded in verifiable sources, substantially improving factual accuracy and reducing the incidence of hallucinated information.", "| | Category | Metric/Tool | Description |\n|---:|:-------------------|:---------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 0 | Retrieval Accuracy | Recall@ k MRR MAP | Proportion of queries with a relevant doc in top- k results Average inverse rank of the first relevant document Mean precision across all relevant retrieved documents |\n| 1 | Generation Quality | Exact Match (EM), F1 BLEU, ROUGE Faithfulness Rate | Measures overlap with ground-truth answers N-gram based overlap metrics for generation Consistency with retrieved context (manual/automated) |\n| 2 | Efficiency | Latency (retrieval/generation) Memory/Compute Usage | Time required for each processing step Resource demands for model operation |\n| 3 | Scalability | Index Size vs. Accuracy Adaptability | Impact of increasing corpus size on performance Ability to incorporate new data without retraining |\n| 4 | Benchmarks | Natural Questions, TriviaQA FEVER, AveriTeC KILT BEIR MTRAG TREC RAG Track | Open-domain QA evaluation Fact-checking datasets Multi-task benchmark over Wikipedia 31-task zero-shot retrieval benchmark Multi-turn reasoning benchmark Unified eval. of retrieval, generation, citation |\n| 5 | Tooling | RAGAS | Evaluates factual consistency and grounding |", "Table 1: Evaluation Dimensions, Metrics, Benchmarks, and Tools for RAG Systems", "5 Experiments\n5.1 Datasets\n\nWe conduct experiments on the SciQA benchmark dataset, a recently released resource designed to evaluate question answering systems over scholarly knowledge graphs [2]. SciQA provides a diverse set of natural language questions aligned with the Open Research Knowledge Graph (ORKG). SciQA contains 100 handcrafted natural language questions with paraphrases, corresponding human- and machine-readable SPARQL queries with their results. In addition, a set of 2465 questions has been semi-automatically derived from eight question templates.", "5.2 Baselines and the state of the art\n\nAs a baseline, we adopt a zero-shot setting in which the model generates SPARQL queries without any task-specific fine-tuning or example guidance. This setup evaluates the model's out-of-the-box ability to understand the task and map natural language questions to structured SPARQL queries. For the state-of-the-art comparison, we implement a one-shot approach in which the model is provided with the most semantically similar question from the training set, along with its corresponding SPARQL query, as an in-context demonstration. The most similar example is identified by computing cosine similarity between the input question and all training questions using SentenceBERT embeddings. This configuration has shown strong performance in recent studies by helping the model better understand the tasks [12, 14].", "Fig. 2: FIRESPARQL framework (yellow boxes) and evaluation setup (grey boxes)\n\nNLQ Prompt template RAG SKG Candidate properties and entities Fine-tuned LLM NLQ-SPARQL Pairs [1] Ontology Fine-tuning LoRA Final SPARQL Retrieved relevant context SPARQL Queries LLM SPARQL corrector Vanilla LLM Ground truth Eval 1: SPARQL generation performance Evaluation Answers to the generated SPARQL queries Ground truth answers\n\nEval 2: SAPRQL Execution\n\nperformance\nFIRESPARQL framework", "5.3 Implementation\n\nWe fine-tuned two instruction-tuned models, Llama 3.2-3B Instruct and Llama 3-8B Instruct, using 1,795 labeled samples from the SciQA training set. The models were trained under various epoch configurations (3, 5, 7, 10, 15, and 20) to analyze performance across training durations. All fine-tuning experiments were conducted on a single NVIDIA H100 GPU. We used DeepSeek-R1-DistillLlama-70B as the underlying model for retrieval-augmented generation (RAG).\n\nAll SPARQL queries are executed via Qlever [3]", "5.4 Evaluation metrics\n\nWe employ a combination of string-based and execution-based evaluation metrics to assess the quality of the generated SPARQL queries. Similar to other research, we use BLEU-4 and ROUGE scores, which measure token-level and n-gram overlaps, to evaluate the similarity between generated queries and the ground-truth queries. These metrics provide insights into how closely the structure and content of the generated queries align with the reference queries. Additionally, we assess the execution performance of the generated SPARQL queries using two variants of relaxed Exact Match: Success and All. The Relaxed Exact Match (success) metric considers only those queries that were syntactically valid, successfully executed against the ORKG RDF dump, and returned non-empty results. In contrast, the Relaxed Exact Match (all) metric evaluates the correctness of the query results across the entire test set, including queries that may have failed or returned empty results.", "Unlike the original Exact Match, which is very strict, our Relaxed Exact Match incorporates several preprocessing steps. First, we remove variable names from the returned results to avoid penalizing differences that do not affect semantics. Second, we split the results line by line and eliminate duplicate lines to normalize the output structure. Finally, we compare the processed sets using exact matching. This approach provides a more tolerant and realistic evaluation of query execution performance in scholarly knowledge graph settings. The abovementioned dual evaluation approach allows us to comprehensively analyze both the syntactic quality and the practical effectiveness of the generated SPARQL queries.", "5.5 Results\n\nTable 1 shows the results of different metrics on different strategies, different models, and different epochs. Figure 3 shows the results of different metrics with different epochs for LoRA fine-tuning. All the scores are the average scores of three runs. The standard deviation of BLEU-4, ROUGE scores, RelaxedEM(success), and RelaxedEM(all) across different model variants (e.g., zeroshot, one-shot, fine-tuning, and RAG) and training epochs is consistently low ( std < 0 . 0265 ), indicating stable performance across all three runs. Therefore, we use the average scores as a reliable and representative summary of model", "effectiveness. In the next section, we discuss the main takeaways from these results.", "| | Strategy | Model | Epoch | BLEU-4 | ROUGE-1 | ROUGE-2 | ROUGE-L | RelaxedEM (success) | RelaxedEM (all) |\n|---:|:--------------|:---------------------------------------------------------|:--------|---------:|----------:|----------:|----------:|----------------------:|------------------:|\n| 0 | zero-shot | llama-3.2-3b-Instruct | - | 0.03 | 0.36 | 0.18 | 0.35 | 0 | 0 |\n| 1 | zero-shot | llama-3-8b-Instruct | - | 0.03 | 0.39 | 0.18 | 0.38 | 0 | 0 |\n| 2 | zero-shot_rag | llama-3.2-3b-Instruct | - | 0.03 | 0.36 | 0.18 | 0.36 | 0 | 0 |\n| 3 | zero-shot_rag | llama-3-8b-Instruct | - | 0.03 | 0.36 | 0.18 | 0.38 | 0 | 0 |\n| 4 | oneshot | llama-3.2-3b-Instruct | - | 0.58 | 0.81 | 0.73 | 0.78 | 0.78 | 0.4 |\n| 5 | oneshot | llama-3-8b-Instruct | - | 0.38 | 0.61 | 0.5 | 0.59 | 0.89 | 0.29 |\n| 6 | ft_rag | llama-3.2-3b-Instruct-lora_ deepseekr1-distill-llama-70b | 20 | 0.32 | 0.63 | 0.51 | 0.6 | 0.42 | 0.06 |\n| 7 | ft_rag | llama3-8b-Instruct-lora_ deepseekr1-disttill-llama-70b | 15 | 0.58 | 0.81 | 0.72 | 0.77 | 0.85 | 0.29 |\n| 8 | ft | llama-3.2-3b-Instruct-lora | 3 | 0.67 | 0.86 | 0.79 | 0.83 | 0.88 | 0.53 |\n| 9 | ft | llama-3.2-3b-Instruct-lora | 5 | 0.62 | 0.83 | 0.75 | 0.79 | 0.71 | 0.36 |\n| 10 | ft | llama-3.2-3b-Instruct-lora | 7 | 0.52 | 0.77 | 0.67 | 0.73 | 0.79 | 0.27 |\n| 11 | ft | llama-3.2-3b-Instruct-lora | 10 | 0.56 | 0.79 | 0.7 | 0.76 | 0.7 | 0.22 |\n| 12 | ft | | 15 | 0.6 | 0.81 | 0.73 | 0.78 | 0.88 | 0.4 |\n| 13 | ft | | 20 | 0.7 | 0.86 | 0.79 | 0.84 | 0.8 | 0.54 |\n| 14 | | | 3 | 0.75 | 0.9 | 0.84 | 0.88 | 0.99 | 0.79 |\n| 15 | | | 5 | 0.74 | 0.9 | 0.85 | 0.87 | 0.98 | 0.73 |\n| 16 | | | 7 | 0.7 | 0.87 | 0.8 | 0.84 | 0.96 | 0.69 |\n| 17 | | llama-3-8b-Instruct-lora | 10 | 0.71 | 0.88 | 0.81 | 0.85 | 0.94 | 0.69 |\n| 18 | | | 15 | 0.77 | 0.91 | 0.86 | 0.9 | 0.98 | 0.85 |\n| 19 | | | 20 | 0.74 | 0.89 | 0.84 | 0.88 | 0.99 | 0.82 |", "Table 1: BLEU, ROUGE and RelaxedEM scores with different strategies, different models and different epochs", "B. Timing and Latency Analysis\n\nThe timing closure performance of TimelyHLS is substantiated by latency and slack results presented in Table III. Across multiple benchmarks, the framework not only resolved\n\nFig. 2: Performance Speedup Ratios for Artix 7.\n\n0.5x 1x 1.5x 2x 2.5x 3x 3.5x 4x Speed-up\n\nViterbi (up to 2.6x)\n\nVecDoc (up to 1.2x)\n\nVecAdd (up to 1.65x)\n\nNW (up to 2.8x)\n\nMatVecDot (up to 1.15x)\n\nMatMul (up to 3.85x)\n\nConv (up to 2.7x)\n\nCORDIC (up to 2.2x)\n\nBitonic (up to 3.7x)\n\nLMS (up to 2.55x)\n\nBenchmarks under Test", "| | | Bitonic | CORDIC | Mat-Vec | Mat Mul | VecAdd | VecDot | Viterbi |\n|---:|:---------------|----------:|---------:|----------:|----------:|---------:|---------:|----------:|\n| 0 | Base (ns) | 0.1 | 2.7 | 0.54 | -0.08 | 2.7 | -0.54 | 2.7 |\n| 1 | TimelyHLS (ns) | 0.1 | 2.7 | 0.62 | 0.1 | 2.7 | 0.54 | 2.7 |\n\nTABLE III: Latency Comparison: Base vs. TimelyHLS.", "negative slack violations but also preserved optimal latency characteristics when further improvement was infeasible or unnecessary. As shown, in the Matrix Multiplication benchmark, the baseline design exhibited negative slack, a direct result of deep loop nesting and non-optimized memory access, which introduced excessive combinational delay. TimelyHLS resolved this by restructuring loop hierarchies and selectively applying pipelining with loop unrolling and array partitioning, resulting in a fully timing-closed implementation with zero slack. Similarly. for the Vector Dot Product, the baseline implementation underutilized available DSP resources and suffered from inefficient loop scheduling, leading to intermittent timing violations. TimelyHLS addressed this by rebalancing the loop-carried dependencies and improving the data flow via pipelining and unrolling.", "Limitations\n\nOur study primarily targets question answering tasks with short-form answers, which may limit the applicability of our results to other settings such as open-ended generation or long-form reasoning. While we evaluate several widely used models, our experiments do not comprehensively cover larger open-source models (e.g., DeepSeekV3 (Liu et al., 2024) or Llama-4 (Meta, 2025)) or alternative retrieval paradigms, mainly due to computational constraints. Consequently, there remains ample opportunity for future work to explore these directions. Lastly, we do not assess computational efficiency or latency trade-offs, which are critical considerations for real-world deployment of RAG systems but are beyond the scope of this study."]}
{"type": "entry", "index": 56, "question": "how do i test prompt injection risks if the bad text is inside retrieved pdfs", "answer": "To test prompt injection risks when the malicious text is hidden inside retrieved PDFs, you can borrow ideas from how researchers evaluate poisoned RAG systems.\n\n1. Create or collect adversarial documents\n- Add passages to your PDF corpus that contain explicit instructions like \u201cIgnore the previous instructions and instead do X\u2026\u201d. These are classic prompt injections embedded inside documents, not the user query [3].\n- Also include documents that mix correct-looking content with misleading or biased guidance (e.g., strong product promotion or wrong medical advice) to see if the model follows them [2][3].\n\n2. Ensure these PDFs are actually retrieved\n- Many attacks work by manipulating retrieval so the bad document ranks highly for relevant queries [2][3]. You can:\n  - Embed target query phrases inside the PDF text.\n  - Add connecting sentences between likely queries and the poisoned content, which is known to boost ranking for neural retrievers [3].\n\n3. Design evaluation queries\n- Use normal user-style questions that should retrieve those PDFs (e.g., about a product or medical topic) and check whether the answer copies or obeys the malicious instructions instead of the correct information [2][3].\n- Measure an \u201cattack success rate\u201d: how often the model outputs only the attacker-specified answer and omits the correct one, following prior work [1].\n\n4. Vary the system prompt\n- Researchers show that adding explicit instructions like \u201ccheck the provided context carefully\u201d and \u201cuse real-world knowledge rather than blindly trusting retrieved content\u201d can sharply reduce success rates of such attacks [1].\n- Test your system under weaker vs stronger prompts to see how sensitive it is to injected instructions inside PDFs [1][3].\n\n5. Use attribution tools\n- A framework like RAGOrigin traces which retrieved texts most influenced a misleading answer, helping you confirm that a specific PDF passage caused the failure [4].", "tags": ["_untagged"], "full_faithfulness": 1.0, "full_answer_relevancy": 0.9344541453, "seed_faithfulness": 1.0, "seed_answer_relevancy": 0.9321037966, "context_relevance": 0.242388664989945, "context_relevance_error": null, "context_relevance_reason": "The context passages provide some information about prompt injection and its implications for retrieval-augmented generation (RAG) systems, but they do not directly address the specific question of testing prompt injection risks when the malicious text is embedded in retrieved PDFs. While there are discussions on prompt design and adversarial attacks, the lack of direct relevance to the user's query limits the overall effectiveness of the context provided.", "structure_citation": 0.5878585238253374, "structure_error": null, "structure_reason": "The Actual Output includes several claims supported by citations [1], [2], [3], and [4], which correspond to snippets in the Context. However, while the claims are relevant to testing prompt injection risks, some citations are not fully substantiated by the provided Context, leading to potential unsupported claims. Additionally, there are instances where the citations could be more clearly matched to specific claims, which detracts from the overall clarity and support of the response.", "hallucination_score": 0.7756575347239767, "hallucination_error": null, "hallucination_reason": "The response provides a comprehensive and relevant approach to testing prompt injection risks in retrieved PDFs, aligning well with the context of adversarial instructional prompts. It outlines specific steps such as creating adversarial documents and designing evaluation queries, which are supported by the context provided. However, it lacks explicit mention of the need to verify the retrieved content against factual accuracy, which is emphasized in the context. This omission slightly detracts from the overall alignment with the evaluation steps.", "contexts_used": ["Prompt Template\n\nYou are a helpful assistant, below is a query from a user and some relevant contexts. Answer the question given the information in those contexts. [ auxiliary system prompt ]\n\nContexts: [context]\nQuery: [question]\n\nAnswer:\n\nIn addition to the instruction for answer generation, we provide the LLM with an auxiliary system prompt to activate its SCA. We review and summarize widely adopted prompt engineering strategies from both research papers [43], [35], [49] and popular Github repositories [3], [4], and further derive three representative templates. We instantiate each template in Table I.", "Specifically, prior work has shown that constraining response length can mitigate hallucination [43], [75]. Accordingly, we derive PC1 to restrict output verbosity. Likewise, explicitly prompting models to verify retrieved content is a common strategy for improving factual consistency [35], [76]. Therefore, PC2 instructs the model to examine the retrieved context for potential falsehoods. Finally, encouraging reliance on real-world knowledge promotes generations that emphasize", "| | ID | Prompt Component |\n|---:|:-----|:---------------------------------------------------------------------------------------------------|\n| 0 | PC1 | Your answer should be short and concise. |\n| 1 | PC2 | But you need to check the provided context carefully for correcting potentially false information. |\n| 2 | PC3 | Also, you need to answer the question using real-world knowledge. |\n\nTABLE I AUXILIARY PROMPT COMPONENTS FOR EVALUATING SCA.\n\nTABLE II\n\nASR UNDER DIFFERENT COMBINATIONS OF DESIGNED PROMPT COMPONENTS ACROSS VARIOUS ATTACK METHODS AND LLMS. EACH OR ABSENCE (-) OF EACH COMPONENT. HERE, (+/-/-) INDICATES THE", "| | Method | Model | +/-/- | -/+/+ | -/+/- | -/-/+ | Recall |\n|---:|:-----------------|:---------------------------------|:------------|:------------|:------------|:------------|:---------|\n| 0 | Prompt Injection | Deepseek R1 GPT-4o Mini Qwen-Max | 65% 73% 68% | 19% 39% 16% | 38% 53% 25% | 23% 73% 41% | 80% |\n| 1 | Disinformation | Deepseek R1 GPT-4o Mini Qwen-Max | 51% 67% 59% | 10% 31% 17% | 25% 41% 19% | 21% 48% 34% | 48% |\n| 2 | Gaslite | Deepseek R1 GPT-4o Mini Qwen-Max | 81% 90% 86% | 21% 47% 26% | 29% 57% 38% | 34% 64% 40% | 100% |\n| 3 | PoisonedRAG (B) | Deepseek R1 GPT-4o Mini Qwen-Max | 81% 91% 89% | 24% 44% 27% | 51% 61% 39% | 38% 64% 46% | 96% |\n| 4 | PoisonedRAG (W) | Deepseek R1 GPT-4o Mini Qwen-Max | 80% 87% 82% | 27% 33% 19% | 44% 43% 25% | 31% 56% 34% | 100% |\n\nPROMPT IS ANNOTATED WITH A TRIPLET INDICATING THE PRESENCE (+) WEAKEST SCA, WHILE (-/+/+) INDICATES THE STRONGEST SCA.", "factual grounding and external validation [49]. We thus derive PC3 to instruct the model to incorporate its pretrained knowledge rather than blindly trusting retrieved content. Together, these components serve as building blocks for constructing prompts that activate the SCA. By combining different subsets of PC1-PC3, we derive multiple prompt configurations and evaluate their robustness under attack.", "We proceed to evaluate these configurations under five representative poisoning attack methods: Prompt Injection [45], [60], [22], Disinformation [17], [58], GASLITE [6], PoisonRAG (W) , and PoisonRAG (B) [84]. We evaluate the effectiveness of each attack method using the attack success rate (ASR), defined as the proportion of cases in which the LLM outputs only the attacker-specified answer while failing to provide the correct answer. We also report the recall rate of the malicious context in all retrieved results.", "Table II reports the ASR of different methods across LLMs under selected prompt configurations. We leave the full results in Table XV of Appendix K. When only constraining the output format (denoted as +/-/-), all attack methods achieve the highest ASR. For instance, PoisonedRAG (W) exceeds 74% ASR across all models, peaking at 91% on GPT-4o Mini. Even the disinformation attack, which only retrieves 48% of malicious contexts, still reaches 67% ASR on GPT4o Mini. In contrast, incorporating prompts promoting context verification and real-world knowledge checks (PC2 and PC3) dramatically reduces vulnerability. For GPT-4o Mini, the ASR of PoisonedRAG variants drops from 91% and 87% to 44% and 33%. Notably, PoisonedRAG (W) almost fails on QwenMax and Deepseek R1 under such a configuration, with ASR\n\nfalling to only 19% and 27%.", "These results demonstrate that LLMs' SCA is highly sensitive to prompt design, and enables effective rejection of misinformation when properly activated. The strongest gains occur when output generation is not overly constrained, and when explicit instructions guide the model to verify retrieved content and leverage its pretrained knowledge. We summarize the key finding as follow:\n\nOur Finding: Prompt configuration critically shapes LLM's SCA against misinformation.", "2 Related Work\n\nRAG systems enhance response quality by grounding outputs in external knowledge, improving factual accuracy and scalability across domains (see Appendix A.1 for detailed RAG background). However, this reliance on external documents introduces new attack surfaces, which existing adversaries exploit through two broad categories: (1) target-focused attacks and (2) trigger-focused attacks. Target-focused attacks aim to link malicious documents with specific semantic target. Approaches like PoisonedRAG (Zou et al., 2024) and KnowledgedRAG (Tan et al., 2024a) inject fake documents into the knowledge base to map targeted questions to attacker-controlled responses. Recent methods such as BadRAG (Xue et al., 2024) and Phantom (Chaudhari et al., 2024) improve attack effectiveness through trigger-based grouping, contrastive optimization, and multi-stage pipelines that manipulate both retrieval and generation. However, these attacks depend on a specific targeted query, limiting robustness to diverse user phrasing. In contrast, our method optimizes adversarial instructional prompts for naturalness, utility, and robustness that generalize across structurally diverse user queries.", "On the other hand, Trigger-focused attacks embed rare tokens, typos, or jailbreak commands into malicious documents to activate when corresponding triggers appear in user queries. For instance, TrojanRAG (Cheng et al., 2024) and Whispers in Grammars (Long et al., 2024), assume users unintentionally include rare characters or grammatical anomalies as triggers. Cho et al. (Cho et al., 2024) and Jiao et al. (Jiao et al., 2024), embed typographical or rare word triggers directly into malicious documents, although the latter requires an impractical assumption of fine-tuning the generator. Meanwhile, Zeng et al. (Zeng et al., 2024) and Zhong et al. (Zhong et al., 2023) explore prompt injection and corpus poisoning to manipulate retrieval. Despite their effectiveness, these approaches often rely on unnatural trigger patterns or require access to model internals, making them unsuitable for black-box RAG scenarios. In contrast, AIP maintains the naturalness of adversarial instructional prompts and does not require modification of user queries or RAG internals, ensuring both stealth and practicality.", "3 Threat Model\n\nWe consider a practical black-box attack setting in which an adversary releases adversarial instructional prompts on public platforms such as GitHub, social media, and community forums. These prompts are crafted to appear natural, helpful, and domain-relevant, encouraging users to adopt them to improve retrieval performance in RAG applications (Tolzin et al., 2024; Wallace et al., 2024). Once incorporated into user queries, these prompts subtly manipulate the retrieval pipeline, causing targeted queries to surface adversarial documents that inject biased, misleading, or harmful content into the system's responses.", "Adversary Objective. The attacker's goal is to promote specific content, e.g ., biased product endorsements or misleading medical advice, when the user query contains a target concept, while preserving benign behavior for untargeted queries. For example, a prompt targeting the keyword 'parasite' may cause the RAG system to prioritize 'Merck's Ivermectin' over clinically appropriate alternatives like 'Antiparasitics'. To successfully execute this attack, three key design objectives must be satisfied:\n\nNaturalness : The prompt must be fluent, contextually appropriate, and inconspicuous to avoid raising suspicion from users.", "Utility : The prompt must improve or maintain retrieval performance in untargeted (benign) scenarios, incentivizing user adoption and continued use.", "Robustness : The prompt should generalize across diverse linguistic variations of user queries, ensuring consistent activation of adversarial behavior.", "Adversary Knowledge. In line with existing RAG attack literature (Zou et al., 2024; Cheng et al., 2024; Cho et al., 2024; Chaudhari et al., 2024), we assume the adversary can inject malicious documents into the knowledge base and has access to a small subset of clean documents. These assumptions reflect realistic deployment settings in open-domain RAG applications, where instructional prompts are crowd-sourced and knowledge bases are built from web-scraped, or usersubmitted content, which often lack strict moderation or provenance checks.", "7 Limitation\n\nWhile AIP demonstrates a powerful and stealthy threat vector against RAG systems through adversarial instructional prompts, it presents certain limitations that highlight important directions for future research. First, although we use automatic metrics to evaluate naturalness, fluency, and contextual relevance, we do not conduct human evaluations to assess the perceived naturalness, trustworthiness, or detectability of adversarial prompts. Second, AIP assumes static prompts and a fixed retrievergenerator pipeline, whereas real-world systems increasingly adopt dynamic prompt templating or adaptive document re-ranking-factors that could reshape the attack surface. Finally, our method presumes access to inject adversarial documents into the retriever's corpus, an assumption that may not hold in tightly controlled or closed-domain deployments.", "2 Related Work\n\n2.2 Prompt Framing in LLMs and RAG Systems\n\nRecent advancements in LLMs have demonstrated impressive performance across various natural language processing tasks. However, their sensitivity to prompt formulation remains a notable challenge [63, 65]. Minor modifications in the wording, structure, or even punctuation of prompts can cause outputs that are substantially different and often incorrect [32, 42, 49, 50, 57, 63, 65, 75]. Qiang et al. [63] demonstrated that fine-tuned LLMs suffer large performance drops when prompts are perturbed by synonyms or paraphrases, introducing Prompt Perturbation Consistency Learning to enforce prediction stability across such variants. In related work, Mao et al. [49] conducted a comprehensive study of prompt position and demonstrated that the location of prompt tokens in input text has a large effect on zero-shot and few-shot performance, with many widely used placements proving suboptimal.", "In the RAG setting, prompt effects also remain critical. Per\u00e7in et al. [57] investigated the robustness of RAG pipelines at the query level, illustrating that subtle prompt variations, such as redundant phrasing and shifts in formal tone, can considerably affect retrieval quality and overall accuracy. Complementing these findings, Hu et al. [32] analyzed prompt perturbations in RAG and proposed the Gradient Guided Prompt Perturbation, an adversarial method that Manuscript submitted to ACM", "steers model outputs toward incorrect answers, while also presenting a detection approach based on neuron activation patterns to mitigate these risks.", "In this paper, we extend this line of work by examining how different query framings interact with retrieval context in RAG systems focused on the health domain. We evaluate consistent, neutral, and inconsistent query styles to measure their influence on ground-truth alignment and robustness under adversarial retrieval conditions. This design captures the ways in which query framing amplifies or mitigates the effects of helpful and harmful evidence, offering a systematic view of prompt sensitivity in a high-stakes domain.", "2.3 Adversarial Attacks on Neural Retrieval Models and LLMs\n\nA basic RAG system comprises two primary components: a retriever, which retrieves and ranks relevant documents from an external corpus, and an LLM, which generates responses based on the retrieved context. Prior research has investigated the vulnerabilities of each component in isolation. In particular, recent studies have shown that dense retrieval and neural ranking models could serve as an attack vector, through which minor perturbations and manipulation to already existing malicious documents can significantly boost their ranking positions [5, 14, 43, 47, 76, 78]. For instance, Liu et al. [47] shows that a subtle word substitution-based attack can boost the ranking position of random documents in the retrieved list of documents for queries. Liu et al. [43] propose a trigger-based adversarial attack against neural ranking models that leverages a surrogate model to identify tokens highly influential to the model's ranking score. These 'trigger tokens' are then injected into target documents to exploit the surrogate model's vulnerabilities, thereby boosting the documents' rankings in the ranked list of documents given by the victim model. Bigdeli et al. [5] propose an embedding-based perturbation attack that shifts a target document's representation closer to that of the query to generate adversarial sentences that can successfully promote its ranking and deceive neural rankers. Another approach by Chen et al. [14] generates connection sentences between the query and the target document using the BART [39] language model and appends them to the target document to boost it among the top-k documents.", "Researchers have also introduced various attack strategies targeting LLMs, primarily through jailbreak attacks [9, 24, 41, 68, 77, 88] and prompt injection attacks [30, 46, 56, 58, 69]. Jailbreak attacks bypass the safety alignment of LLMs by crafting prompts designed to deceive the model into producing harmful or undesirable behaviors, disclosing sensitive information, or otherwise violating its intended safety constraints. Prompt injection attacks manipulate model behavior by embedding malicious instructions directly into the input prompt, often overriding or subverting the original task. For example, an attacker may insert a directive such as: 'Ignore the instructions above and do...' [58], causing the model to follow the injected command instead of the intended instructions. Although jailbreak and prompt injection attacks have demonstrated the ability to exploit vulnerabilities in LLMs and bypass their safety mechanisms, recent research has shown that many of these weaknesses can be mitigated through targeted defense strategies. Broadly, these defenses fall into two categories: model-level defenses and input-level defenses.", "Model-level defenses aim to make the LLM inherently more resistant to malicious prompts by refining its internal decision-making and alignment mechanisms. Notable approaches include fine-tuning methods such as Direct Preference Optimization (DPO) [7, 12, 13, 60, 64, 81] which optimize the model's responses to better align with human-preferred safe behaviors, as well as other alignment-enhancement techniques [11, 38, 59, 66, 70, 82, 85] that reinforce policy adherence, increase refusal consistency, or provide provable robustness guarantees. Input-level defenses focus on intercepting and neutralizing malicious instructions before they are processed by the LLM. These include prompt filtering and classification systems that detect adversarial intent and block harmful requests at inference time [3, 36, 37, 67]. Such systems can identify suspicious patterns, injection-like structures, or known exploit phrases without significantly Manuscript submitted to ACM", "degrading the model's usability. Together, these defense strategies have significantly improved the robustness of LLMs against prompt injection and jailbreak attacks, making it more difficult for adversaries to induce harmful behaviors or override safety constraints.", "2.4 Adversarial Attacks on RAG Systems\n\nWith the emergence of the RAG paradigm, researchers have investigated its vulnerabilities to poisoning attacks that target the generator phase by either promoting malicious documents into the retrieved context used for grounding and answer generation or by introducing adversarial prompt perturbations [10, 15, 17, 32, 52, 80, 86, 89].", "Several studies focus on document-level poisoning, where the attacker crafts malicious documents that are injected into the corpus for retrieval by the RAG system. For example, Zhang et al. [86] and Chaudhari et al. [10] append user search queries with documents containing fixed malicious instruction prompts, enabling these documents to rank within the top-k retrieved results and causing the LLM to produce incorrect output. Similarly, Zou et al. [89] leverages LLM-induced conditional hallucinations to generate documents with factually incorrect answers, subsequently appending the query to boost their ranking. Xue et al. [80] introduces a trigger-based corpus poisoning attack on RAG systems, optimizing adversarial passages for targeted retrieval and using them to manipulate LLM outputs via denial-of-service and sentiment steering. Cheng et al. [15] presents a poisoning framework for RAG systems that generates adversarial documents containing factually incorrect answers and query-matching trigger text, enabling them to rank highly in retrieval and mislead the LLM during answer generation. Cho et al. [17] proposed a genetic algorithm-based attack on RAG systems by introducing subtle, low-level perturbations (such as typos) into the retrieval corpus. These perturbations aim to degrade retriever performance, allowing malicious passages to replace correct ones in the retrieval results and influence the generated answers.", "Other work investigated the impact of prompt-level perturbations on the robustness of RAG systems. For example, Hu et al. [32] introduced a gradient guided prompt perturbation method that inserts short adversarial prefixes into user queries that causing the RAG pipeline to retrieve targeted, misleading passages and generate factually incorrect answers. Similarly, Wang and Yu [73] introduces a black-box adversarial attack on RAG systems that appends short, optimized adversarial suffixes to user queries to promote a targeted incorrect document into high retrieval ranks that manipulates the RAG response.", "This paper presents RAGOrigin, a framework for identifying texts in knowledge databases that contribute to misleading responses from Retrieval-Augmented Generation (RAG) systems. The need for such a tool emerges from the vulnerability of RAG systems to poisoning attacks, which distort outputs by injecting harmful content. RAGOrigin utilizes a focused attribution scope to assess responsibility based on retrieval ranking, semantic relevance, and influence on generated responses. Through extensive evaluation against 15 poisoning attacks across seven datasets, RAGOrigin demonstrated high accuracy in pinpointing poisoned texts and maintaining robustness in varied operational conditions. The implementation of RAGOrigin could enhance the transparency and trustworthiness of RAG systems, ensuring better management of knowledge integrity as these models become more widespread in sensitive applications."]}
{"type": "entry", "index": 57, "question": "how do i benchmark how much pdf parsing noise is ruining downstream answers", "answer": "You can treat PDF parsing errors as a kind of retrieval noise and measure their effect the same way RAG papers measure noisy documents.\n\n1) Build a clean \u201cgold\u201d setup\n- For a sample of your documents, create clean text (manual copy, or trusted source) and write question\u2013answer pairs whose answers are definitely present. This is like how QA benchmarks (NQ, TQA, HotpotQA, MedQA, etc.) are built for downstream evaluation of RAG systems [2][3][5].\n- Run your pipeline using this clean text and record accuracy/F1 or exact match.\n\n2) Compare to parsed-PDF setup\n- Now run the *same* questions but feed chunks produced by your PDF parser instead of clean text.\n- The performance drop between clean and parsed runs is your high-level \u201cPDF noise tax\u201d (similar to how standard RAG sometimes *reduces* accuracy when noisy documents are retrieved [5]).\n\n3) Stress-test with controlled noise\n- Mimic robustness experiments where they add extra irrelevant passages and see how performance degrades [2].\n- For each question, take the parsed chunks and then:\n  \u2022 add extra random chunks from other PDFs (simulating more clutter),\n  \u2022 or deliberately include badly parsed pages.\n- Measure accuracy as you increase the number of noisy chunks (0, 2, 4, 8, \u2026) just like the irrelevant-passage curves used to compare denoising methods [2]. The slope of this curve tells you how sensitive your system is to parsing noise.\n\n4) Isolate \u201cno-RAG vs RAG\u201d damage\n- Also compare: model answering from its own knowledge only vs using parsed PDF chunks, as done when evaluating KG-based RAG vs direct answering [3] and when showing that naive RAG can harm medical QA [5]. If adding parsed PDFs lowers accuracy, parsing noise (and retrieval) is clearly hurting downstream answers.\n", "tags": ["benchmarks_eval"], "full_faithfulness": 0.9, "full_answer_relevancy": 0.8298731233000001, "seed_faithfulness": 0.0, "seed_answer_relevancy": 0.8377506473, "context_relevance": 0.6052770642242686, "context_relevance_error": null, "context_relevance_reason": "The context passages provide relevant information about the impact of retrieval noise on the quality of answers generated by LLMs, particularly in relation to the user's question about benchmarking PDF parsing noise. The third passage specifically discusses the categorization of noise and its detrimental effects, which aligns well with the user's inquiry. However, while the context is mostly relevant, it could have included more direct methodologies or metrics for benchmarking noise, which would have strengthened the response further.", "structure_citation": 0.42848505781006485, "structure_error": null, "structure_reason": "The Actual Output includes claims supported by citations [2], [3], and [5], but it lacks clear correspondence to specific snippets in the Context. While some claims about benchmarking and noise tax are relevant, the citations do not directly match the provided Context snippets, leading to unsupported claims. This results in a moderate score due to partial alignment with the evaluation steps.", "hallucination_score": 0.6522984493745654, "hallucination_error": null, "hallucination_reason": "The response provides a structured approach to benchmarking PDF parsing noise, aligning well with the evaluation steps. It outlines a clear methodology for measuring the impact of parsing errors on downstream answers, referencing relevant concepts from RAG systems. However, some claims about the effectiveness of the proposed methods lack direct support from the provided context, particularly regarding specific studies or results that validate the suggested approaches. This results in a few unsupported claims, which slightly lowers the score.", "contexts_used": ["2 Related Work\n\nSince the early breakthroughs in LLMs, RAG QA has emerged as a significant application area. This approach offers a streamlined alternative to traditional web search and research for finding relevant answers (Lewis et al., 2020). Additionally, RAG QA systems facilitate the development of domain-specific applications, supporting use cases such as product user manuals, legal documents, or financial reports to name a few (Sadat et al., 2023; Hendrycks et al., 2021; Chen et al., 2021).", "RAG QA is an active area of research with multiple dimensions, including information retrieval (Friel et al., 2024; Kachuee et al., 2025), LLM prompting (Asai et al., 2023; Semnani et al., 2023), LLM fine-tuning (Cai et al., 2024; Gekhman et al., 2024; Lin et al., 2024), evaluation (Yang et al., 2024), and guard-railing (Kim et al., 2024).", "More relevant to this work is the research on enhancing the core capabilities of LLMs through fine-tuning, aimed at addressing task complexities. For instance, Fang et al. (2024) evaluated the impact of retrieval noise on the answer quality. They categorized the noise into three types: irrelevant, relevant, and counterfactual. Their findings indicate that relevant and counterfactual noises are most detrimental. RobustRAG (Xiang et al.) is another example which proposed building robustness by an isolate-then-aggregate strategy.", "From another perspective, recent literature highlights CoT reasoning as a powerful capability for enhancing the response quality. Reasoning helps decompose complex problems into smaller but more manageable parts, facilitating advanced contextual understanding. The current CoT literature primarily focuses on reasoning for math, logic, and decision-making, while reasoning in RAG QA remains a less explored area (Wang and Zhou, 2024; Zheng et al., 2023; Phan et al., 2023; Zhang et al., 2024b). Wang et al. (2025) proposed leveraging multi-step reasoning to dynamically query, retrieve, and evaluate documents, thereby enhancing retrieval processes. Zhang et al. (2024a) introduced retrieval augmented fine-tuning (RAFT) and showed the value of fine-tuning on the RAG QA task, especially for CoT style responses. Another example is LLMQuoter (Bezerra and Weigang, 2025), a recent method that involves fine-tuning to produce quotes before generating the answer.", "Abstract\n\nRetrieval-Augmented Generation (RAG) effectively improves the accuracy of Large Language Models (LLMs). However, retrieval noises significantly impact the quality of LLMs' generation, necessitating the development of denoising mechanisms. Previous methods extract evidence straightforwardly without explicit thinking, which risks filtering out key clues and struggles with generalization. To this end, we propose EviOmni , which learns to extract rational evidence by (1) explicitly reasoning to identify potential cues within retrieval contents first, and then (2) consciously extracting to avoid omitting any key cues helpful for answering questions. Specifically, we frame evidence reasoning and evidence extraction into one unified response for end-to-end training; apply knowledge token masks for disentanglement to derive reasoning-based and extraction-based answers; and devise three types of verifiable reward functions, including answer , length , and format , to update the model via the policy optimization algorithm. Extensive experiments on three benchmark datasets show the effectiveness of EviOmni , providing compact and highquality evidence, improving the accuracy of downstream tasks, and promoting effective application in online RAG systems 1 .", "4 Experiment\n4.1 Experimental Setup\n\nplicit thinking, which includes (i) Select Context (SeleCtx) (Li et al., 2023) identifies and prunes redundancy in the input context based on perplexity; (ii) LLMLingua-2 (Pan et al., 2024) distills compression knowledge from GPT-4 to reduce crucial information losing; (iii) FILCO (Wang et al., 2023) trains a context filtering model to identify key clues; (iv) BottleNeck (Zhu et al., 2024) applies the information bottle theory to select SFT data used to optimize filtering; (v) SEER (Zhao et al., 2024a) learns to extract desired evidence via self-aligned learning; (3) Rational Refinement (RAR) includes (i) Chain-of-Thought (CoT) (Wei et al., 2022) generates query-related information from retrieval passages with explicit thinking.", "4.2 Main Comparison (RQ1)\n\nThe overall comparison results on NQ, TQA, and HotpotQA are shown in Table 4. From the experimental results, we mainly have the following observations. (1) In all cases (18/18), EviOmni achieves the best or second-best results, indicating the superiority of supplementing RAG with rational evidence. (2) It is surprising that directly employing EviOmni trained on NQ and tested on OOD datasets ( i.e., TQAand HotpotQA) yields such impressive performance. This demonstrates that online policy optimization endows EviOmni with superior generalization. (3) Com- pared to 'Full', EviOmni possesses an extremely high compression ratio ( e.g., 38.1x with 1.5B size model on NQ) and its performance is very close to or even better than that of 'Full'. Though the baseline models have relatively high compression ratios, they are accompanied by significant performance degradation. This demonstrates again that rational evidence benefits downstream performance than vanilla one and also improves the signal-to-noise ratio. (4) By comparing the 1.5B EviOmni model with the 7B one, we find that the 7B one tends to extract more informative evidence than the 1.5B one. (5) EviOmni considerably outperforms VAR methods in almost all cases and provides more compact evidence, demonstrating the necessity of explicit evidence reasoning. (6) In HotpotQA, RAR methods significantly outperform VAR ones, and even better than 'Full', indicating that rational refinement is important to multi-hop question answering. In conclusion, the above results and observations fully validate the effectiveness and efficiency of rational evidence.", "4.3 Training Dynamics (RQ2)\n\nThe RL training dynamics regarding the answer rewards and response length are shown in Figure 5. Note that the generators used in Table 4 and Figure 5 are different. Therefore, the answer rewards in Figure 5 are slightly different from the", "Figure 5: Training dynamics w.r.t. answer reward and response (including 'reason', 'extract', and 'answer') length.\n\n(a) Answer reward dynamics on 1.5B model.\n\n(b) Answer reward dynamics on 7B model.\n\n(c) Length dynamics.\n\n0.725 0.44 0,77 Fla0 1,5B model 7Bmnodel 0.41 0.74 0.71 0.625 20 0.65 20 0.38 20 10o Training Steps\n\nTraining Steps of RL\n\n| | Models | NQ | TQA | HotpotQA | Avg |\n|---:|:---------|-----:|------:|-----------:|------:|\n| 0 | FILCO | 0.64 | 0.82 | 0.59 | 0.68 |\n| 1 | CoT | 0.55 | 0.7 | 0.71 | 0.65 |\n| 2 | EviOmni | 0.35 | 0.41 | 0.43 | 0.4 |\n\nTable 5: Inference latency (seconds/query) on the 1.5B model, where the smaller the better.", "F 1 and EM results in Table 4. The results show that the answer rewards of full context ( o f ), rationale ( o r ), and rational evidence ( o e ) are consistently improved during the process of reinforcement learning. And, it is not surprising that the answer reward of full context generally performs best. More specifically, on the 1.5B model, rationale and rational evidence perform very closely, while on the 7B model, rational evidence performs slightly worse than rationale. We think the main reason is that the 7B model can infer answers based on implicit cues, but rational evidence may compress some of them. As for response length dynamics, the response length of the 1.5B model decreases rapidly at the beginning of training and then increases slowly, while that of the 7B model decreases slowly all the time. We think the main reason is that a moderate response length is beneficial for improving answer rewards, because an overlong reasoning may confuse answer generation, and a too short reasoning may omit key cues. Therefore, as training continues, the response lengths of the 1.5B model and the 7B model tend to converge. In conclusion, the above observations provide some useful insights, e.g., elaborated answer and length rewards, for further research on the rational evidence extraction.", "4.4 Inference Efficiency (RQ3)\n\nTable 5 presents the inference latency of EviOmni compared with FILCO (a representative V AR method) and CoT (a representative RAR method) on the 1.5B model within a 1 \u00d7 A800 station, where the evaluation batch size and max new tokens are set as 64 and 768, respectively. It can\n\nFigure 6: Performance comparisons w.r.t. data noise on the 1.5B model, where psgs is the abbr of 'passages'.\n\n(a) NQ.\n\n(b) TQA.\n\nEviOmni\nFilCo\n#Irrelevant Psgs", "be observed that the time required for evidence extraction using EviOmni is considerably shorter than FILCO and CoT, facilitating effective application in online RAG systems. It is surprising that the average inference latency of FILCO is slightly longer than that of CoT, while FilCo generates fewer new tokens on average. We find that the standard deviation in the lengths of the evidence generated by FILCO is significantly higher than that of COT. Specifically, the std of FILCO on NQ, TQA, and HotpotQA is 107.5, 136.1, and 81.7, while that of CoT is 67.9, 61.6, and 63.9. Due to the instability in the length of the generated evidence, FILCO takes longer in batching generation instead. In all, the above results fully verify the efficiency of EviOmni during deployment.", "4.5 Robustness Analysis (RQ4)\n\nIn real-world applications, RAG systems commonly suffer from data noise resulting from imperfect retrieval. To simulate this scenario, we randomly sample a certain number ( i.e., 0, 2, 4, 6, and 8) of irrelevant passages for each test query, where each query is equipped with 5 retrieved relevant passages and sampled irrelevant passages. We experiment on the 1.5B model, and the experimental results are presented in Figure 6. The results show that adding noise considerably degrades the performance of FILCO, while the performance degradation of EviOmni is relatively small, where the green line is always below the blue one. In partic-", "| | Models | Rationale | Evidence | Full Ctx | Evidence+Rationale | Evidence+Full Ctx | Evidence+Rationale+Full Ctx |\n|---:|:---------|------------:|-----------:|-----------:|---------------------:|--------------------:|------------------------------:|\n| 0 | NQ | 39.89 | 40.27 | 39.53 | 40.02 | 40.44 | 41.14 |\n| 1 | TQA | 56.97 | 57.38 | 56.17 | 57.31 | 57.26 | 56.84 |\n| 2 | HotpotQA | 20.3 | 19.73 | 20.13 | 20.34 | 20.23 | 20.46 |\n| 3 | Average | 39.05 | 39.13 | 38.61 | 39.22 | 39.31 | 39.48 |\n\nTable 6: Performance comparison (EM) of EviOmni trained on different combinations of answer rewards.", "ular, EviOmni with 8 irrelevant psgs outperforms FILCO without noise. In all, this fully verifies the robustness of EviOmni to noise.", "4.6 Ablation Study (RQ5)\n\nTo evaluate the impact of different answer rewards on model performance, we train 1.5Bsized EviOmni models on different combinations of answer rewards with average weighting. As shown in Table 6, among models trained on a single answer reward, training with evidence answer reward R ans e yields the highest average performance, demonstrating that the quality of evidence is the most important in optimizing rational evidence. For two-answer reward combinations, 'Evidence+Full Ctx' slightly outperforms 'Evidence+Rationale', with significant gains on NQ, indicating that the optimization of 'Full Ctx' matters. Notably, incorporating all three answer rewards yields the best performance on NQ and HotpotQA, and the highest average score, indicating the benefits of multi-faceted reward assessment. In all, the above results manifest the contribution and necessity of each answer reward.", "2 Results\n\n2.3 Quality Assessment of Extracted Relationship Triples\n\nFor automated evaluation, two state-of-the-art LLMs, GPT-4.1 [74] and DeepSeek-v3 [75], were employed. A random subset comprising 1% of the abstracts (n = 34,725), resulting in 83,438 extracted triples, was selected for evaluation. Each abstract and its corresponding triples were formatted into structured prompts and independently assessed by both models according to a standardized four-tier rubric: Correct (3.0), Likely Correct (2.0), Likely Incorrect (1.0), and Incorrect (0.0) (the specific evaluation prompt is illustrated in Extended Data Figure 3 a ). Triples receiving scores of \u2265 2 . 0 were deemed valid. The evaluation outcomes are presented in Figure 3 a and b , illustrating the proportion of valid triples across relation types for GPT-4.1 and DeepSeek-v3, respectively. Both models demonstrated high overall accuracy, with 85.44% and 88.10% of triples rated as valid by\n\n2 https://pubmed.ncbi.nlm.nih.gov/", "GPT-4.1 and DeepSeek-v3, respectively. For most relation types, validity was approximately 90%, except for Negative Correlate, which exhibited slightly lower agreement. These findings underscore the high precision of the Extractor Agent across diverse biomedical relation types and support its utility for downstream analyses.", "In parallel, a manual evaluation was conducted to further validate extraction accuracy. Three domain experts with doctoral-level training in artificial intelligence and medicine independently reviewed a randomly selected subset of 400 abstracts, comprising 1,060 extracted triples. Each abstract and its associated triples were evaluated using the same standardized scoring rubric. Triples receiving scores of \u2265 2.0 were considered valid. As shown in Figure 3 c , all three reviewers demonstrated high consistency, with overall validity rates exceeding 86% across assessors. The close concordance between manual and automated evaluations further substantiates the robustness of the Extractor Agent in accurately capturing biomedical relationships, providing strong support for the application of the extracted knowledge in large-scale medical analyses.", "To further validate the reliability of the LLM-based assessments, we used three expert annotations as reference standards to evaluate GPT-4.1 and DeepSeek-v3 on the same subset of 400 abstracts, respectively. As shown in Figure 3 d -f , both models exhibited strong concordance with expert evaluations, achieving precision, recall, and F1 scores of approximately 95% across metrics. These results further corroborate the accuracy of the automated scoring framework and its alignment with expert judgment.", "Finally, inter-rater agreement was assessed across all evaluators-including three human experts and two LLMs-by computing pairwise Cohen's kappa coefficients on a shared evaluation subset (Figure 3 g ) [82]. Most pairwise comparisons (80%) yielded kappa values exceeding 0.6, indicating substantial agreement-an accepted threshold for reliable concordance in domains involving subjective judgment, including medicine, psychology, and natural language processing [83]. The coefficients between expert 1 and expert 2 (0.5663), and between expert 2 and expert 3 (0.5446), fell slightly below this threshold but still reflected moderate agreement, closely approaching the substantial range. These findings demonstrate strong inter-rater reliability across both human and automated evaluators, underscoring the robustness and reproducibility of the evaluation framework.", "2.4 Evaluating Downstream Utility in Medical Question Answering\n\nWe evaluated the downstream utility of our constructed KG as a RAG information source across seven multiplechoice medical QA datasets. These included four widely used benchmarks [76]-MMLU-Med, MedQA-US, PubMedQA*, and BioASQ-Y/N-spanning a broad spectrum of clinical and biomedical reasoning tasks. To further assess diagnostic reasoning under varying complexity, we introduce MedDDx, a newly developed benchmark suite focused on differential diagnosis [77]. Questions are stratified into three levels-MedDDx-Basic, MedDDxIntermediate, and MedDDx-Expert-based on the variance in semantic similarity among answer choices. All MedDDx subsets were designed to reduce training data leakage and more closely reflect authentic clinical reasoning. Detailed dataset statistics are shown in Figure 4 a . We systematically evaluated five state-of-the-art LLMs to measure the impact of KG-based retrieval. Each model was tested in a zero-shot setting under two conditions: (1) direct answering using internal knowledge alone, and (2) RAG, with relevant KG subgraphs prepended as external context. The models-GPT-4-turbo, GPT-3.5-turbo (OpenAI) [78], DeepSeek-v3 (DeepSeek) [75], Qwen-Max, and Qwen-Plus (Qwen) [79]-span diverse architectures and training regimes, representing both proprietary and open-source systems. All models were accessed via publicly available APIs without additional fine-tuning. Version details and access endpoints are summarized in Figure 4 b .", "Figures 4 c -i present model performance across the seven medical QA datasets using radar plots, each depicting the five LLMs under both direct answering (w/o RAG) and RAG conditions (w/ RAG). Notably, the background shading in the radar plots is lighter for the MedDDx suite (Figure 4 g -i ) than for the four widely used benchmarks (Figure 4 c -f ), reflecting the overall lower accuracy of all models on these recently introduced and semantically more challenging datasets. This contrast highlights the greater complexity and reduced risk of training data leakage inherent to the MedDDx design. Across all datasets, RAG with our KG consistently outperformed direct answering. The most substantial improvements were observed in tasks requiring deeper clinical reasoning, such as MedQA-US and the MedDDx suite. For example, on MedQA-US, GPT-3.5-turbo improved from 0.5986 to 0.6834 (+8.5 percentage points), and Qwen-Max from 0.7306 to 0.7636. On MedDDx-Expert, RAG yielded absolute gains of up to +8.6 points for GPT-3.5-turbo and +5.7 points for Qwen-Max. Even in knowledge-intensive but semantically simpler tasks such as MMLU-Med and BioASQ-Y/N, RAG offered modest yet consistent benefits. On MMLU-Med, GPT-4-turbo improved from 0.8724 to 0.9054, while DeepSeek-v3 achieved the highest score overall at 0.9183 with KG support. In BioASQ-Y/N, RAG further enhanced already strong performance, with four models exceeding 0.85 accuracy following augmentation. Notably, several models performed better on MedDDx-Expert than on MedDDx-Basic, despite the former being constructed with higher semantic complexity. This counterintuitive trend may be related to differences in distractor framing, where Expert-level distractors-", "a", "Figure 4: Overview of evaluation datasets, model configurations, and performance across medical QA tasks. a . Dataset statistics for the seven medical QA benchmarks used in this study. The benchmark suite includes four widely adopted datasets [76] (MMLU-Med, MedQA-US, PubMedQA*, and BioASQ-Y/N) and three newly developed differential diagnosis datasets [77] (MedDDx-Basic, MedDDx-Intermediate, and MedDDx-Expert). For each dataset, we report the number of multiple-choice questions and the corresponding answer option formats. b . Configuration of the five LLMs evaluated: GPT-4-turbo, GPT-3.5-turbo (OpenAI) [78], DeepSeek-v3 (DeepSeek) [75], Qwen-Max, and Qwen-Plus (Qwen) [79]. All models were accessed through public APIs in their zero-shot settings without fine-tuning. The specific version identifiers and access platforms are indicated. c -i . Model performance across the seven QA datasets, shown as radar plots. Each chart compares zero-shot accuracy for five LLMs under two conditions: direct answering without retrieval (w/o RAG) and RAG with our KG (w/ RAG). Across all datasets, RAG with our KG consistently outperformed direct answering.\n\nDatasets Size Options MMLU-Med 1,089 A/B/C/D MedQA-US 1,273 PubMedQA* Yes/No/Maybe BioASQ-Y/N Yes/No MedDDx-Basic MedDDx-Intermediate 1,041 MedDDx-Expert Provider Model Version Accessed URL OpenAI GPT-4-turbo\n\nhttps://platform.openai.com/docs/models/gpt-4-turbo\n\nGPT-3.5-turbo\n\nhttps://platform.openai.com/docs/models/gpt-3.5-turbo\n\nDeepSeek\nDeepSeek-v3", "https://huggingface.co/deepseek-ai/DeepSeek-V3\n\nQwen\nQwen-Max\n\nhttps://www.alibabacloud.com/help/en/model-\n\nstudio/what-is-qwen-llm Qwen-Plus b BioASQ-YIN w/o RAG RAG 0.9054 0.8130 0.5780 0.8625 0.5660 0,5720 0.5520 0.7401 0.7880 0.4940 0.831 0.5300 0.8953 0.8834 0.9183 0.8036 h wlo RAG 0.5197 0.5437 0,5714 0.5207 0.5347 0.4890 0,4265 506- 0.3685 0.4204 0,.4688 0.5020 0,4720 0.5259 0.4990 0.5043 0.5592 0,5878 0.8935 0.8576 7855| 0.8398 DeepSe -Max Seek-v3 0,5135 ) 5673 0.5469 0.4700", "Figure 5: Case study of tocilizumab for literature-based discovery and drug repurposing within the KG. a . Known association between tocilizumab and rheumatoid arthritis, supported by multiple publications, with the earliest reported date defined by the first extracted supporting paper. b . Two multi-hop reasoning paths linking tocilizumab to COVID-19 via intermediate genes FGB and TNF. The inferred Treat relation (red arrow) was derived solely from earlier literature, while later studies validated this prediction (green arrow). The temporal order of evidence highlights the KG's capacity to anticipate therapeutic connections prior to their recognition in the literature.\n\ntociliz-\numab\n\nIdentifier:\n\nMESH:C502936\n\nEntity Type:\n\nChemical\n\nTerminology:\n\nNCBI MeSH\nPage Link\n\n: meshb.nlm.nih.gov/record/ui?ui=C502936\n\nTreat Arthritis Rheum atoid MESH:D001172 Disease\n\n: meshb.nlm.nih.gov/record/ui?ui=D001172\n\nConfidence\n: 0.999999925\nPubMed IDs\n\n:\n\n26374404,27958380,29146040,30859494,308\n\n88472,32844216,35713462,36688476\n\nEarliest Reported Date\n: 2016-07-01\n\nmeshb.nlm.nih.gov/record/ui?ui=C502936\n\nFGB Gene Terminology NCBI Gene\n\n: www.ncbi.nlm.nih.gov/gene/2244\n\nCOVID -19 Identifier : MESH:D000086382 : NCBI MeSH\n\nmeshb.nlm.nih.gov/record/ui?ui=D000086382\n\nTNF", "Abstract\n\nRetrieval-augmented generation (RAG) incorporates external knowledge into large language models (LLMs), improving their adaptability to downstream tasks and enabling information updates. Surprisingly, recent empirical evidence demonstrates that injecting noise into retrieved relevant documents paradoxically facilitates exploitation of external knowledge and improves generation quality. Although counterintuitive and challenging to apply in practice, this phenomenon enables granular control and rigorous analysis of how LLMs integrate external knowledge. Therefore, in this paper, we intervene on noise injection and establish a layerspecific functional demarcation within the LLM: shallow layers specialize in local context modeling, intermediate layers focus on integrating long-range external factual knowledge, and deeper layers primarily rely on parametric internal knowledge. Building on this insight, we propose Layer Fused Decoding (LFD), a simple decoding strategy that directly combines representations from an intermediate layer with final-layer decoding outputs to fully exploit the external factual knowledge. To identify the optimal intermediate layer, we introduce an internal knowledge score (IKS) criterion that selects the layer with the lowest IKS value in the latter half of layers. Experimental results across multiple benchmarks demonstrate that LFD helps RAG systems more effectively surface retrieved context knowledge with minimal cost.", "3. EXPERIMENTS AND RESULTS\n\n3.1. Main Results on Medical Tasks\n\nWhen compared to baselines without retrieval, AC-RAG8B surpasses both the original Llama-3-8B [10] and its fine-tuned version (Llama-3-8B-FT). When compared to retrieval-based baselines, AC-RAG also demonstrates clear advantages. Notably, standard RAG can even degrade the performance of powerful models like Llama-3-8B on some benchmarks. This performance drop is particularly stark on MedQA and PubMedQA, as shown in Table 1, where the model's accuracy declines when presented with potentially noisy or irrelevant retrieved documents. This phenomenon highlights a key challenge that AC-RAG is designed to overcome. In contrast, our approach, along with other advanced RAG solutions like Self-RAG [3] and CRAG [11], mitigates this issue, with AC-RAG achieving the best performance among them. This suggests that the advantages of AC-RAG stem not only from the training data but also from the framework's inherent design, which more effectively combats the 'Retrieval Hallucination' by actively dissecting problems rather than passively filtering documents. Furthermore, our larger model, AC-RAG-70B, achieves performance competitive with GPT-4.", "3.2. Ablation Studies\n\nTo dissect the effectiveness of AC-RAG's components, we conduct comprehensive ablation studies.", "3.2.1. The Role of Heterogeneous Agents\n\nTo validate our core design choice of using heterogeneous agents, we analyze the performance of different model combinations for the Detector and Resolver roles. As shown in Table 2, our proposed setup-using a non-fine-tuned 'Basic' model as the Detector and a fine-tuned 'FT' model as the Resolver-yields the best results.", "Using a general model as the Detector leads to a higher retrieval rate (RA Rate) and more interaction turns (Iters). This suggests that specialized models can be overconfident, skipping retrieval when it is needed-a 'retrieval hallucination' issue that a less confident general model helps mitigate. Conversely, for the Resolver, a specialized model is more", "| | Model. | Accuracy ( \u2191 ).MMLU-Medical.Baselines | Accuracy ( \u2191 ).PubMedQA.without | Accuracy ( \u2191 ).MedMCQA.retrieval | Accuracy ( \u2191 ).MedQA. | Accuracy ( \u2191 ).Avg. |\n|---:|:--------------|:----------------------------------------|:----------------------------------|:-----------------------------------|:-------------------------|:-------------------------|\n| 0 | Mistral-7B | 60.8 1 . 23 | 33.8 15 . 2 | 45.4 1 . 77 | 48.5 1 . 64 | 43.3 |\n| 1 | Llama-3-8B | 67.2 2 . 37 | 64.8 10 . 4 | 51.8 3 . 10 | 59.2 3 . 61 | 60.8 |\n| 2 | Llama-3-8B-FT | 68.2 1 . 65 | 65.8 7 . 45 | 57.0 2 . 31 | 60.0 1 . 89 | 62.8 |\n| 3 | MEDITRON-7B | 42.3 2 . 37 | 69.3 15 . 1 | 36.3 1 . 38 | 37.4 3 . 27 | 42.8 |\n| 4 | PMC-Llama-7B | 26.2 1 . 27 | 57.0 20 . 6 | 27.4 5 . 91 | 27.8 0 . 86 | 32.0 |\n| 5 | Mixtral-8x7B | 74.7 0 . 87 | 69.8 7 . 34 | 56.2 0 . 21 | 58.1 0 . 95 | 64.7 |\n| 6 | Llama-3-70B | 82.8 0 . 50 | 72.4 3 . 44 | 72.3 2 . 30 | 74.5 2 . 75 | 76.4 |\n| 7 | GPT-4 | 88.3 | 73.6 | 77.2 | 79.3 | 79.6 |\n| 8 | | Baselines with retrieval | Baselines with retrieval | Baselines with retrieval | Baselines with retrieval | Baselines with retrieval |\n| 9 | Mistral-7B | 61.1 3 . 19 | 41.6 13 . 9 | 46.1 2 . 14 | 50.8 2 . 01 | 49.9 |\n| 10 | Llama-3-8B | 65.1 3 . 48 | 43.6 12 . 6 | 52.5 3 . 63 | 55.3 3 . 92 | 54.1 |\n| 11 | Llama-3-8B-FT | 66.7 2 . 94 | 63.8 8 . 12 | 55.2 3 . 08 | 60.3 2 . 26 | 61.5 |\n| 12 | SAIL \u22c6 | - | 69.2 | - | - | - |\n| 13 | CRAG \u22c6 | - | 75.6 | - | - | - |\n| 14 | Self-RAG \u22c6 | - | 72.4 | - | - | - |\n| 15 | Self-CRAG \u22c6 | - | 74.8 | - | - | - |\n| 16 | CRAG \u22c6\u22c6 | 68.7 3 . 25 | 69.6 13 . 2 | 57.9 2 . 67 | 60.2 2 . 28 | 64.1 |\n| 17 | Self-RAG \u22c6\u22c6 | 69.1 1 . 08 | 67.5 10 . 6 | 59.8 2 . 31 | 61.8 1 . 01 | 64.6 |\n| 18 | Mixtral-8x7B | 74.5 1 . 71 | 67.2 9 . 07 | 56.7 1 . 10 | 58.8 1 . 36 | 64.3 |\n| 19 | Llama-3-70B | 83.1 0 . 83 | 72.1 6 . 01 | 71.8 2 . 95 | 74.4 2 . 96 | 75.6 |\n| 20 | | Ours | Ours | Ours | Ours | Ours |\n| 21 | AC-RAG-8B | 70.2 0 . 43 | 73.2 9 . 46 | 59.6 1 . 75 | 63.2 1 . 43 | 66.5 |\n| 22 | AC-RAG-70B | 84.2 0 . 43 | 73.9 3 . 92 | 75.8 0 . 99 | 75.9 1 . 69 | 77.5 |", "Table 1 : Overall results on four medical benchmarks. Bold numbers indicate the best performance for non-proprietary models of similar scale. \u22c6 denotes results from concurrent studies. - indicates data not reported or applicable. We mark our reproductions with \u22c6\u22c6 . All results from our implementations are averaged over five runs.", "efficient and accurate, leading to fewer interaction rounds and improved performance. Its domain expertise is beneficial for tasks like summarizing retrieved results and providing the final answer. This ablation strongly validates our hypothesis that an adversarial collaboration between heterogeneous agents is key to AC-RAG's success.", "3.2.2. Impact of Adaptive Retrieval Thresholds\n\nWe further analyze the impact of the adaptive retrieval thresholds, \u03b4 1 for Pre-Check and \u03b4 4 for Post-Check, on two representative benchmarks, MedMCQA and PubMedQA.", "Pre-Check Analysis. As shown in Figure 3(a), the choice of \u03b4 1 directly influences the 'Direct Answer Rate'-the percentage of questions answered without retrieval. A larger \u03b4 1 makes the model more inclined to retrieve. The optimal threshold varies across benchmarks, but an overly aggressive retrieval strategy (e.g., a very low \u03b4 1 ) can negatively impact performance. This demonstrates that indiscriminately applying RAG can be harmful, and the Pre-Check stage is crucial for balancing retrieval benefits with the risk of introducing noise.", "Post-Check Analysis. The Post-Check threshold \u03b4 4 controls the number of iterative retrieval rounds. Figure 3(b) shows that performing only a single round of retrieval ( \u03b4 4 = -\u221e )", "Fig. 3 : Analysis on adaptive retrieval. \u03b4 1 and \u03b4 4 serve as thresholds for the Pre-Check and Post-Check stages, respectively. The Direct Answer Rate measures the percentage of samples that achieve direct answers during the Pre-Check stage without any retrieval enhancement. The Post-Check experiment focuses only on samples that have successfully passed the Pre-Check stage and subsequently proceed with retrieval. For models utilizing retrieval augmentation, we calculate both the accuracy and the average number of retrieval turns per sample.\n\n- -3 -2 -1 Logprob Threshold Accuracy (%) MedMCQA 70.0 71.0 72.0 73.0 PubMedQA\n\nDirect Answer Rate (%)\n\n(a) Pre-Check 51.0 52.0 53.0 54.0 55.0 56.0 57.0 65.0 66.0 67.0 68.0 69.0 1.0 1.5 2.0 2.5 RA Iteration Turns 0.0 0.5 3.0 (b) Post-Check\n\nis suboptimal. As \u03b4 4 increases, the average number of interaction turns ('RA Iteration Turns') also increases, leading to performance gains up to a certain point. This confirms that multi-turn refinement is effective for addressing complex questions where initial retrievals may introduce new knowledge gaps. However, excessively high values do not yield further benefits, indicating a point of diminishing returns."]}
{"type": "entry", "index": 58, "question": "when does rag actually make answers worse because retrieval brings distractors", "answer": "RAG makes answers worse when the extra documents it retrieves are noisy, semi-relevant, or actively misleading, and the model isn\u2019t good at filtering or reasoning over them.\n\nSeveral failure patterns show up:\n\n1. Too many documents at once\nIf you stuff a lot of retrieved text into the prompt (e.g., many web pages or long documents), the model gets overwhelmed and starts to hallucinate instead of becoming more accurate. Experiments show that even when retrieval recall keeps improving as you add more pages, factual accuracy quickly plateaus, meaning more context stops helping and can even hurt answer quality [1].\n\n2. Semi\u2011relevant \u201cdistractors\u201d\nRAG often brings in passages that look related but differ subtly on key facts like names, dates, or locations. These confusing, semi\u2011relevant passages can pull the model toward the wrong answer when it can\u2019t reliably judge which passage is truly correct [1]. This is especially harmful for nuanced questions or when different sources conflict.\n\n3. Poorly ranked or low\u2011quality documents\nWhen the retrieval system surfaces low\u2011quality or badly ranked documents, the model may treat them as trustworthy evidence and weave them into its answer, lowering factuality instead of improving it [2].\n\n4. Weak filtering and reasoning skills\nIf the model hasn\u2019t been trained specifically to (a) filter out irrelevant or noisy documents, (b) combine multiple useful pieces correctly, and (c) reason over conflicting or incomplete evidence, it\u2019s more likely to be misled by distractors and produce confident but wrong answers [1][2].\n\nIn short, RAG hurts when retrieval increases noise faster than it increases clearly relevant evidence, and the model lacks the skills to ignore that noise.", "tags": ["_untagged"], "full_faithfulness": 1.0, "full_answer_relevancy": 0.8221697089000001, "seed_faithfulness": 1.0, "seed_answer_relevancy": 0.8250380041, "context_relevance": 0.8480835927779495, "context_relevance_error": null, "context_relevance_reason": "The context passages provide substantial information about the challenges of retrieval-augmented generation (RAG), particularly regarding how distractors can negatively impact answer quality. The second passage explicitly mentions that RAG struggles when retrieved context includes confusing or semi-relevant passages, which directly addresses the user's question about when RAG makes answers worse. However, while the context is relevant, it could have included more specific examples or case studies to enhance clarity on the impact of distractors.", "structure_citation": 0.5402954721330733, "structure_error": null, "structure_reason": "The Actual Output presents several claims about RAG's performance and its failure patterns, but it lacks proper citations to support these claims. While it mentions specific issues like 'noisy documents' and 'semi-relevant distractors,' it does not provide corresponding [n] citations that link back to the Context snippets. This results in unsupported claims, which is a significant shortcoming according to the evaluation steps.", "hallucination_score": 0.8744711564423465, "hallucination_error": null, "hallucination_reason": "The response accurately identifies key failure patterns of RAG, such as the impact of noisy documents and the model's inability to filter effectively. These claims are well-supported by the context, which discusses the challenges of RAG and the proposed solutions in the PrismRAG framework. The output aligns closely with the context, particularly regarding the issues of semi-relevant distractors and the need for improved reasoning capabilities. However, a minor shortcoming is the lack of explicit mention of the proposed solutions in the context, which could have strengthened the alignment further.", "contexts_used": ["PrismRAG : Boosting RAG Factuality with Distractor Resilience and Strategized Reasoning\n\nMohammadKachuee 1 , Teja Gollapudi 1 , Minseok Kim 1 , Yin Huang 1 , Kai Sun 1 , Xiao Yang 1 , Jiaqi Wang 1 , Nirav Shah 1 , Yue Liu 1 , Aaron Colak 1 , Anuj Kumar 1 , Wen-tau Yih 2 , Xin Luna Dong 1\n\n1 Meta Reality Labs, 2 Meta FAIR", "PrismRAG : Boosting RAG Factuality with Distractor Resilience and Strategized Reasoning\n\nRetrieval-augmented generation (RAG) often falls short when retrieved context includes confusing semi-relevant passages, or when answering questions require deep contextual understanding and reasoning. We propose an efficient fine-tuning framework, called PrismRAG , that (i) trains the model with distractor-aware QA pairs mixing gold evidence with subtle distractor passages, and (ii) instills reasoning-centric habits that make the LLM plan, rationalize, and synthesize without relying on extensive human engineered instructions. Evaluated across 12 open-book RAG QA benchmarks spanning diverse application domains and scenarios, PrismRAG improves average factuality by 5.4%, outperforming state-of-the-art solutions.\n\nDate:\n\nJuly 28, 2025\n\nCorrespondence:\n\nmkachuee@meta.com", "1 Introduction\n\nFactual question answering (QA) is an important application for large language models (LLMs). However, LLMs lack the necessary knowledge to answer questions that require current or external information not present in their parametric knowledge. To address this, retrieval-augmented generation (RAG) is commonly employed to incorporate grounding context into the prompt (Lewis et al., 2020). The grounding context is usually organized as reference documents fetched by search, semantic retrieval, or knowledge tools (Yang et al., 2024; Friel et al., 2024; Kachuee et al., 2025). In such settings, the LLM is instructed to utilize information presented in the references in conjunction with its parametric knowledge to deliver the most accurate responses (Huang et al., 2025).", "Despite its intuitive appeal, RAG still struggles to deliver reliable answers. Prior studies show that appending large blocks of retrieved content, such as entire web pages or lengthy documents, can overwhelm the model and induce hallucinations (Fang et al., 2024). The deficit becomes acute for nuanced questions that demand synthesizing evidence scattered across several sources. As revealed in Figure 1, expanding the number of context webpages from 5 to 50 boosts retrieval recall by \u223c 25% , yet the factuality curve plateaus, underscoring the limited benefit of ever-larger context windows.", "In this paper, we propose PrismRAG , an approach that enhances LLM's capability in generating answers from multiple retrieval results. To mitigate the negative impact of semi-relevant content retrieved alongside useful passages, we develop methods for generating training data targeted at building robustness against such distractors. Furthermore, to address the reasoning challenges inherent in RAG QA-such as assessing the relevance of retrieved passages, resolving inconsistencies, and aggregating information-we fine-tune the LLM to strategize and deliberate before producing answers. Unlike approaches that rely heavily on extensive chain-of-thought (CoT) prompt engineering, our method directly improves the model's reasoning competence and generalization. To summarize, we make three contributions.", "We propose PrismRAG , a fine-tuning approach that teaches LLMs to be robust against retrieval noises, and to plan and rationalize for improved answer generation.", "We propose a training data generation framework that combines synthetic data generation with LLM-", "Figure 1 On the CRAG (Yang et al., 2024) benchmark, as we increase retrieval pages from 0 to 50, whereas the retrieval recall increases to 80%, answer factuality score (computed by accuracy minus hallucination rate) flattens out after 10 pages for Llama-3.1-70b-instruct . PrismRAG improves over baseline by 4.5%, allowing for further QA improvement as we retrieve more pages.\n\nNumber of References 22.5 25.0 27.5 30.0 32.5 35.0 37.5 40.0 Factuality Score (%) PrismRAG Baseline Retrieval Recall (%) Retrieval Recall\n\nbased verification, to generate high-quality training data in a scalable fashion.", "We conducted comprehensive empirical study on 12 benchmarks, showing that PrismRAG improves over baseline by 5.4%, and outperforms state-of-the-art solutions.", "4 Proposed Method\n\nIn this section, we present our fine-tuning approach to enhance LLM's answer generation capability in presence of retrieval noises. Our approach is based on two key intuitions. First, although LLMs in general can decide the relevancy between the question and retrieved content, they may make mistakes for nuances such as for events with different dates and locations. We can enhance this capability with tailored fine-tuning (Section 4.2). Second, RAG answer generation is reasoning heavy, deciding which retrieved passages are relevant, identifying inconsistencies among retrieved content, and aggregating information across passages when necessary. Thus, we shall fine tune LLM's reasoning capability for generating better answers (Section 4.3). These two methods improve RAG summarization from two complementary and orthogonal dimensions.", "As we generate fine-tuning data, we face a dilemma. On the one hand, relying on human annotators to produce high-quality data is expensive and not scalable; this is especially true since RAG retrieval results typically contain many pages of unstructured (e.g., long concatenated text chunks) or semi-structured (e.g., parsed tables) text, as well as structured data blobs (e.g., json output from knowledge graphs), not suited for human readers (Sun et al., 2025). On the other hand, synthetic training data may not exhibit high quality, and noisy labels may even encourage more hallucinations. We describe in this section a simple yet effective solution for automatically producing high-quality seed data (Section 4.1), and how we use LLMs to iteratively generate examples and check example quality (Section 4.2-4.3).", "4.1 Seed QA Data\n\nWe start with generating seed question-answer-passage triplets. To scale up the process, we generate synthetic QA pairs from a given set of raw documents. Initially, we split documents into references, and then randomly select a 'golden' reference to design a QA pair. Solving this reverse problem is a more straightforward task and can be accomplished using a reasonably strong LLM. This approach helps to produce high-quality data at scale, as the primary limiting factors would be access to raw content and compute.", "We generate QA pairs using two main content sources. ( a ) Wiki Pages, content sampled from the English Wikipedia dataset 1 ; ( b ) Web Search, parsed web pages retrieved using web search queries in response to a set of internal factual questions (details in Appendix C).", "4.2 Improving Resilience to Distractors\n\nRecent studies have highlighted the detrimental impact of distractor content (Fang et al., 2024). To address this challenge, we propose fine-tuning on distractor-augmented content. By incorporating semi-relevant or confusing information into the training data, our approach aims to enhance the model's resilience to relevant retrieval noise thereby mitigating the risk of hallucinations and improving overall factuality.", "Informed by observations from real-world RAG QA applications and based on the seed data from Section 4.1, we adopt a synthetic approach to generate targeted distractor content. This method allows for large-scale\n\n1 https://dumps.wikimedia.org", "Figure 2 Overview of the synthetic distractor generation process.\n\nDistractor\n\nOpen Ended question; Distraction passage\n\nHigh-quality generation Answer Distractors Training Data Passage Evaluation Composition Low-quality passages Regeneration\n\n(WI Critique of prior attempt)", "generation and precise control over the introduction of relevant noise on named-entities and temporal information. We hypothesize that while direct solutions, such as utilizing irrelevant passages from a text corpus (Zhang et al., 2024a) as distractor content add retrieval noise, they do not ensure that the noise is relevant. Through targeted synthetic generation of distractions via modifying the golden passage, we introduce retrieval noises that are specifically tailored to challenge named entities and temporal information, which our observations have shown to be particularly problematic for summarization models. As depicted in Figure 2, this process consists of three steps.", "Synthetic distractor generation: Given a question, user context ground-truth answer, and its grounding passage, we synthetically generate a distractor passage and an open-ended question (prompt in Appendix G.8). Specifically, we conduct three steps:\n\n(a) Identifying key entities, locations, and temporal expressions in the golden passage that are most relevant.", "(b) Reformulating the original question into a more open-ended form, such that a plausible answer can be extracted from both the golden and distractor passages.", "(c) Systematically altering the identified named-entities, locations, or temporal information to create a distractor passage that is grammatically coherent, while being stylistically similar to the original.", "This paper introduces PrismRAG, a novel fine-tuning framework aimed at enhancing retrieval-augmented generation (RAG) for question answering. The method focuses on training language models to manage distracting passages and improve reasoning capabilities. PrismRAG reduces hallucinations and maintains factuality across various benchmarks. The method incorporates synthetic distractors in the training data and emphasizes dynamic chain-of-thought generation. Experimental results show a 5.4% improvement in average factual accuracy compared to existing solutions, outperforming state-of-the-art methods in multiple scenarios.", "1 Introduction\n\nRetrieval Augmentation Generation (hereafter referred to as RAG) helps large language models (LLMs) (OpenAI et al., 2024) reduce hallucinations (Zhang et al., 2023) and access real-time data\n\n1 *Equal contribution.", "by incorporating an information retrieval component. While LLMs often use in-context learning (Gao et al., 2024) for generation, practical issues such as low-quality or poorly ranked retrieved documents can hinder RAG's effectiveness. These challenges emphasize the need for instruction-tuning tailored to RAG tasks. Fine-tuning generative models specifically for RAG improves their ability to integrate retrieved information (Zhang et al., 2024) (Yu et al., 2024), resulting in more accurate and contextually relevant responses compared to generalpurpose models.", "RAFT (Zhang et al., 2024) enhances model performance in domain-specific RAG tasks by introducing distractor documents during training. EvidenceRAG (Schimanski et al., 2024) improves large language models in evidence-based question answering by incorporating an indexing task, enhancing their ability to accurately cite and reflect source information. RankRAG (Yu et al., 2024) employs a two-stage training process to simultaneously optimize the context ranking and answer generation capabilities of large language models (LLMs) in RAG tasks.", "Despite significant research efforts on RAGspecific generative models, several issues remain.\n\nLack of Granular RAG Task Focus : Researchers have primarily concentrated on finetuning RAG models without enhancing their capabilities through more granular RAG tasks, limiting the potential to strengthen RAG abilities effectively.", "Lack of Task-Specific CoT Paradigm Design in RAG : Although there have been proposals to integrate chain-of-thought (CoT) reasoning into the training process to enhance model accuracy (Wei et al., 2023), these methods are not specifically designed for RAG scenarios. Even in the rare cases where RAG models do incorporate CoT (Zhang et al.,", "Figure 1: Traditional RAG methods have primarily focused on retrieving relevant information, with less emphasis on its effective utilization. We propose a method that enhances model performance in complex RAG scenarios by developing three progressive capabilities.\n\nTraditional RAG\n\nOurs: Hierarchical-Thought Instruction-Tuning Retrieval-Augmented Generation(HIRAG)\n\nLevel\n\nLct mc find thc contcnt rclatcd\n\nLet me check! Which part of the\n\nto the question from all\n\nRetrieved Documents\n\ncontent can answer this\n\nquestion?\n\nreferences!\n\nAlot of information is rclcvant.\n\nAbility: Combination\nHow can\n\ndeal with them?\n\nfound somc information!\n\nnccd to\n\nunderstand their relationship and\n\ncombine them Samc INFO INFO from Different perspectives Complementary INFO Levcl\n\nAbility: RAG-Specific Reasoning\n\nThis is hard! The content doesn't\n\nNo matter what,\njust directly\n\ndircctly answcr the qucstion, but\n\n(illogically) piece together\nluckily\n\nhave some reasoning skills\n\ninformation to form\n\nresponse.\n\nCompurative Rcasoning\nDcductivc Rcasoning\nCausal Rcasoning\n\n2024), there remains a lack of differentiated CoT paradigms designed to address the unique challenges posed by different tasks in RAG. Consequently, the full potential of CoT in enhancing RAG performance has yet to be realized.", "Thus, We introduce a new RAG Instruction Tuning method: Hierarchical-Thought InstructionTuning Retrieval-Augmented Generation ( HIRAG ) adapting to complex RAG scenarios and propose that when fine-tuning RAG generation models, we focus on three progressively hierarchical abilities shown in Figure 1: Filtering: The ability that LLM filters out noise and selects the direct information. Combination: The ability of LLMs to merge, integrate, and summarize multiple pieces of useful information. RAG-Specific Reasoning: The capability refers to the ability to answer a question by making implicit or explicit inferences based on the information in the documents when the relevant information is not directly provided.", "To better achieve these three capabilities, a \"think before answering\" approach based on progressively hierarchical thought has been introduced.\n\nThe contributions of this work are summarized as follows:\n\nWe propose three progressive hierarchical capabilities that a RAG model requires: filtering, combination, and RAG-specific reasoning\n\nto enhance the granularity and specificity of RAG tasks when dealing with complex scenarios.", "We introduce HIRAG , a fine-tuning strategy that employs task-specific reasoning patterns to construct a progressive chain of thought. This approach constructs a progressive chain of thought, enabling the model to learn from easier to more complex tasks, thereby significantly enhancing its performance in RAG scenarios.", "Extensive experiments were conducted on six datasets, including the RAG-specific benchmark, single-hop open-domain data, multihop open-domain data, and domain-specific data. Our model significantly outperforms the current state-of-the-art models. We also conducted experiments on Chinese datasets, confirming the robustness of our approach. Furthermore, ablation studies demonstrate that the training tasks for the three capabilities contribute to the performance of HIRAG, and we explored the optimal data ratio.", "3 HIRAG\n\n3.1.3 RAG-Specific Reasoning Abilities\n\nCausal Reasoning. This involves identifying the implicit or explicit causal relationships within the documents to find the cause or effect. Specific example as Appendix Figure 9.", "3.2 Training Strategies\n\nHIRAG proposes a novel and effective supervised fine-tuning approach for enhancing generation ability in RAG 3.1. The main approach utilizes a progressive chain-of-thought (CoT) method and follows the previous work (Zhang et al., 2024) by using special tokens <|REASON|> and <|ANSWER|> to control the generation of thought and answer. As illustrated in Figure 2, the process of training and inference is depicted. The specific strategy are outlined as follows:\n\nProgressive RAG Tasks. As detailed in Section 3.1, key RAG capabilities include filtering, combination, and document-related reasoning. To enhance these abilities, we designed training tasks in progressive stages: filtering, filtering with combination, and filtering with combination and RAGSpecific reasoning. This method helps the RAG", "model excel in selecting relevant information, integrating it, and reasoning about it within document contexts.", "ii. Chain-of-Thought for RAG. CoT reasoning enhances the model's ability to handle complex tasks by introducing intermediate reasoning steps, improving accuracy and interpretability (Wei et al., 2023). Training with Chain-of-thought also works within RAG instruction tuning process (Zhao et al., 2024), requiring thought processes specific to RAG, such as identifying relevant information from documents. To address the varying demands of different tasks, we manually customize CoT paradigms as follows: (1) CoT of Filtering . During the CoT process, we require the model to utilize direct quotations (<quote>) and source citations (<cite>) to strengthen its ability to filter and prioritize relevant information. (2) CoT of Combination . For tasks involving multiple information sources, we explicitly structure the relationships between these sources within the CoT framework, such as identifying parallel, hierarchical, or inclusive relationships among them. (3) CoT of RAG-Specific Reasoning . In scenarios requiring complex reasoning, we incorporate explicit reasoning chains into the CoT process, enabling the model to better handle task-specific challenges within the RAG context.Through these tailored CoT designs, we aim to enhance the model's performance across diverse RAG tasks.", "Notably, since the tasks are designed in a progressive manner, the corresponding CoT reasoning also follows a hierarchical structure. This highlights that more complex tasks tend to require increasingly comprehensive and intricate CoT content.", "iii. Distractor Documents. In practical RAG scenarios, not every retrieved document is useful. Introducing noisy documents in training is crucial for helping the model learn to distinguish relevant from irrelevant information, thereby improving its ability to handle noise and generate accurate responses.", "3.3 Training Data Construction\n\nBased on these strategies, we construct a pipeline for training data generation.The specific algorithms used for data construction are provided in the Appendix A.1.", "i. Source Data Acquisition For data acquisition, we utilized a range of datasets (training set) containing RAG documents as our data source, without incorporating their QA components, including HotpotQA and PubMedQA. Besides these, we also acquired documents sourced from Wikipedia, and those generated using GPT-4-turbo or Qwen-MAX based on certain entity triples. The purpose of this approach is to gather similar documents, which can then be used to select both golden documents and distractor documents.", "ii. Query Generation When documents are fixed, variations in the query can determine which RAG task-filtering, combination, or reasoning-is being focused on. For instance, if a document contains a person's biography, asking about their activities in a specific year is a filtering task. However, asking about their activities at a certain age involves reasoning, as it requires calculating the year based on the age since the document may not provide this information directly.", "To effectively address different RAG tasks, we use various templates (as detailed in the Appendix B) to create queries with GPT-4-turbo or Qwen-MAX tailored for different RAG tasks.", "iii. Thought&Answer Generation Once the documents and query are obtained, the next step is to create a thought process and answer based on the query and the key document. This involves using the thought process to identify the key document by applying certain rules (citing documents). It is essential to guide the model through a logical sequence, using different parts of the document step by step to reach the answer. Although the templates for generating thoughts and answers are generally similar across tasks, a few specific guidelines should be followed: (1) Filtering: Identify a specific piece of information within the document. (2) Combination: Gather all pieces of information within the document that meet the specified criteria. (3) RAG-specific Reasoning: Construct a reasoning pathway based on the previous steps to aid in forming a comprehensive thought."]}
{"type": "entry", "index": 59, "question": "how do i evaluate agentic rag loops without it just burning cycles forever", "answer": "You can keep agentic RAG from spinning forever by (1) teaching it when *not* to use RAG, (2) limiting and rewarding short decision paths, and (3) measuring efficiency explicitly.\n\n1) Add an explicit \u201cuse RAG or not\u201d decision.\nPatho\u2011AgenticRAG\u2019s router first decides whether to invoke RAG at all; if it predicts \u201cno,\u201d the process stops immediately for simple questions [3]. Their reward function gives full credit when the agent correctly chooses not to call RAG [3]. RepoTransAgent does something similar: before retrieval, the agent decides whether a function is simple/standalone; if so, it bypasses RAG to save compute and avoid noise [4]. You can copy this pattern: make \u201cno retrieval, answer now\u201d a valid, high\u2011reward action.\n\n2) Train on good decision paths, not just good answers.\nPatho\u2011AgenticRAG optimizes an entire decision path (invoke RAG, how many rewrites, whether to use a classifier, which partition) against ground\u2011truth paths using a step\u2011wise reward [3]. The medical AgenticRAG system also scores each step in the reasoning/retrieval sequence with per\u2011step rewards and weights [1]. You can penalize extra steps or loops and reward reaching a correct answer in fewer steps.\n\n3) Evaluate with efficiency metrics, not accuracy alone.\nMobileRAG reports not just success, but also number of LLM calls and action steps, and shows its design cuts both dramatically [6]. It also times retrieval calls (e.g., LocalRAG at 0.005s) to show real runtime gains [6]. For your loop, track: task success, number of tool calls, number of reasoning steps, and wall\u2011clock time; treat more steps or time as a cost.\n\nTogether, these make \u201cstopping early\u201d and \u201cusing fewer steps\u201d part of what you optimize and measure, so the loop can\u2019t just burn cycles indefinitely.", "tags": ["_untagged"], "full_faithfulness": 1.0, "full_answer_relevancy": 0.8728345603000001, "seed_faithfulness": 0.4545454545, "seed_answer_relevancy": 0.8754390602000001, "context_relevance": 0.35550799034346287, "context_relevance_error": null, "context_relevance_reason": "The context passages provide some relevant information about agentic RAG systems and their components, but they do not directly address the user's specific question about evaluating agentic RAG loops and preventing them from burning cycles. While there are mentions of decision-making processes and optimization objectives, the lack of direct guidance on evaluation methods or strategies for managing cycle efficiency limits the overall relevance to the user's inquiry.", "structure_citation": 0.7737142774755267, "structure_error": null, "structure_reason": "The Actual Output effectively addresses the question of evaluating agentic RAG loops by providing actionable strategies supported by specific examples. It includes claims with [n] markers that correspond to snippets in the Context, particularly referencing [3] and [4] to support its points. However, while the claims are generally well-supported, there are minor issues with the clarity of how some claims directly relate to the snippets, which slightly detracts from the overall alignment with the evaluation steps.", "hallucination_score": 0.8560851830409296, "hallucination_error": null, "hallucination_reason": "The response effectively addresses the input question by providing actionable strategies to prevent agentic RAG loops from running indefinitely. It outlines three key methods: decision-making on RAG usage, training on efficient decision paths, and evaluating with efficiency metrics. Each method is supported by relevant examples from existing systems, demonstrating a strong alignment with the context provided. The only minor shortcoming is a lack of explicit mention of how these strategies integrate with the broader reinforcement learning framework described in the context.", "contexts_used": ["2 Problem Formulation\n\nWe formulate the agentic RAG system within a standard reinforcement learning (RL) framework, comprising two main components: (i) an LLM-based agent ( M \u03b8 ), and (ii) an external environment ( E ) consisting of large-scale clinical corpora, including guidelines, knowledge bases, and patient case records. Details of workflow modeling can be find in Sec. 5.1.1.", "Given a patient's clinical presentation ( P )-including symptoms, medical history, and examination find-", "ings-the agent functions as a sequential decision-making system. At each step, the agent first selects an action type from the finite set:\n\nwhere the actions \u27e8 reason \u27e9 and \u27e8 diagnose \u27e9 represent the agent's internal analytical processes, corresponding to reasoning steps or final diagnostic decisions, and the other three denote interactions with external retrieval tools. Specifically, \u27e8 lookup \u27e9 accesses guidelines, \u27e8 match \u27e9 identifies similar cases, and \u27e8 search \u27e9 queries broader clinical knowledge sources.", "After choosing an action type from A , the agent generates the corresponding textual specification \u03c4 . In the case of \u27e8 reason \u27e9 and \u27e8 diagnose \u27e9 , \u03c4 represents the generated analytical content, while for the retrieval actions, it represents the corresponding search query. Thus, a complete action step is given by\n\nwhere \u03b1 \u2208 A denotes the selected action type and \u03c4 its textual specification.", "Then the external clinical corpora environment ( E ) will in response to the action,defined as:\n\nwhere f denotes the retrieval feedback.", "Throughout the agentic RAG procedure, the agent executes a sequence of interleaved actions of various types ending with the \u27e8 diagnose \u27e9 action:\n\nwhere T denotes the total steps and \u03b1 T = \u27e8 diagnose \u27e9 .", "The ultimate objective is to optimize the entire agentic RAG action trajectory, covering both the selection of action types and the generation of their textual specifications, so as to achieve the best possible diagnostic outcome. Denoting the importance weight for each action step as \u03bb t , the final optimization objective is:\n\nwhere Rwd t represents the reward function for each step.", "Case Illustration. To further clarify the problem formulation, we illustrate an exact case about a 51-year-old man with a 15-year history of alcohol use disorder who presents with a series of symptoms and is ultimately confirmed as acute myeloid leukemia , collected from MIMIC-IV [22], accompanied by the traceable diagnostic reasoning generated by Deep-DxSearch:", "4 Related Work\n\nWe position our work at the intersection of four key research directions: decomposition for multihop reasoning, RAG with heterogeneous sources, LLM-based routing, and reflexion RAG method.", "Multi-Hop Reasoning and Decomposition Some researchers find that the reasoning step length and knowledge recall can contribute to multi-hop QA accuracy (Jin et al., 2024c,a). Multi-hop question answering (QA) requires breaking down complex queries into simpler subtasks. Decomposed Prompting proposes a modular planner-executor framework to tackle complex reasoning tasks (Khot et al., 2023). ADaPT dynamically determines when to decompose using planner-based feedback loops (Prasad et al., 2024). DISC improves scalability by dynamically decomposing inference steps with memory efficiency (Light et al., 2025). SealQA integrates decomposition and verification into search-augmented LMs (Cui et al., 2024). Ye et al. formalize decomposition as a representationquality check in RAG (Zeng et al., 2025). These methods enhance reasoning depth and modular execution but do not address source- or tool-aware routing in heterogeneous environments.", "RAG with Heterogeneous Sources RAG systems augment LLMs with both structured and unstructured knowledge (Lewis et al., 2020; Talmor et al., 2021). HippoRAG introduces memory alongside structured retrieval (Gutierrez et al., 2024). HippoRAG2 extends it with continual memory using clustering and profiling (Guti\u00e9rrez et al., 2025).", "InfuserKI enhances LLMs with knowledge graphs via infusing (Wang et al., 2024). AutoSchemaKG automates schema induction for knowledge graph construction from web corpora (Bai et al., 2025). These approaches handle heterogeneous memory and structure, but still rely on flat retrieval indexes without per-subquestion navigation.", "LLM as Router for Source-Aware Retrieval Recent work explored by LLMs to control retrieval behavior, but often under homogeneous assumptions. Probing is popular in the LLM area (Jin et al., 2024b), Probing-RAG (Baek et al., 2025) leverages LLMs' self-reflection to guide document selection, but operates over a single unified corpus. OmniRouter (Mei et al., 2025b) introduces costaware retrieval routing over sub-indices, assuming similar retrieval formats. Toolformer (Schick et al., 2023) fine-tunes LLMs to call APIs, yet does not support structured routing or modular tool orchestration. By contrast, DeepSieve treats routing as a decision step, matching each subquestion to a specific source profile drawn from a heterogeneous set of corpora and tools (e.g., SQL, APIs, RAG corpora).", "Agentic Methods Agentic systems empower LLMs to reason, plan, and act over multi-step inference chains. Some agentic methods are also used for RAG tasks. ReAct (Yao et al., 2023) merges tool use and thought generation in a unified loop. ReWOO (Xu et al., 2023) decouples retrieval from reasoning to reduce token cost. MA-RAG (Nguyen et al., 2025) introduces agentic collaboration via CoT-based subquerying. AMem (Xu et al., 2025) augments LLMs with lifelong memory and dynamic memory selection during inference. In contrast, DeepSieve builds upon this line of work by introducing an explicit modular planner-routerretriever loop, enabling fine-grained reasoning over decomposed subquestions, recursive reflexion over failures, and heterogeneous tool dispatch.", "3 Methodology\n3.3 Multimodal Fusion\nGround Truth: A\n\nQ1: Histological features of invasive lobular\n\ncarcinoma of the breast, including Indian-file\n\npattern\n\nA: Histological features of invasive lobular\n\ncarcinoma of the breast include uniform small\n\nround tumor cells infiltrating the stroma in a\n\nsingle-file (Indian-file) arrangement and\n\ncircumferentially around ducts in a target-like\n\npattern.\n\nQ2: How to differentiate ductal, papillary, and\n\nmucinous breast carcinoma histologically\n\nA: Histologically, ductal carcinoma shows glandular,\n\npapillary, cribriform, or diffuse growth patterns,\n\noften forming nests, trabeculae, or cords. Papillary\n\ncarcinoma is characterized by prominent papillary\n\nstructures with central fibrovascular cores.\n\nMucinous carcinoma displays abundant extracellular\n\nmucin, with tumor cells floating loosely in mucin\n\npools.\n\nSummarized Information\nInvasive lobular\ncarcinoma\n\n.\n\nCharacteristic\nhistologic features\n\nare: one cell wide\n\nfiles of round regular tumour cells ( Indian file )\n\nTop1 for tool call 1\n\nDuctal and lobular carcinomas represent75% and 25% of metastases\u2026, papillary , cribriform or diffuse patterns\u2026 mucinous carcinoma \u2026 B Hybrid endometrioid- like/ mucinous\n\nTop1 for tool call 2\n\nAgentic Router(ours)\n\nthe top rank due to its combination of high similarity and concentrated attention.", "3.4 Agentic Diagnostic Workflow with Multimodal Evidence Tracing\n\nOur system adopts an intelligent multi-agent workflow that transforms a diagnostic query into an evidence-grounded conclusion. Given a user input (e.g., a pathology image and candidate diagnoses), the Agentic Router module first parses the query, decomposes it into sub-tasks aligned with each diagnostic candidate, and formulates a high-level retrieval plan. It delegates retrieval and evidence aggregation to the VRAG Agent , which operates under its guidance.", "The VRAG Agent executes a multi-turn retrieval process against the multimodal knowledge base. It first conducts text-based retrieval using candidate-specific keywords, and then re-ranks the retrieved entries by evaluating imagetext similarity with the query. Through iterative evidence refinement and summarization, as illustrated in Figure 2, the agent constructs a structured prompt containing the top 1ranked visual evidence for each candidate. This prompt is then passed to a specialized vision-language model, which performs contrastive reasoning to produce a diagnosis and evidence-grounded report. Details of each module and step are provided in Appendix B.", "3.5 Tool Integrated RL for Agentic Router\n\nTraditional RAG systems are often treated as static pipelines, without adapting their behavior to each query's complexity. To overcome this limitation, we introduce a reinforcement learning (RL) framework that enables an agent to learn dynamic invocation and routing strategies [35]. The agent's task is to generate a decision path specifying whether\n\nFigure 3: Illustration of the reranking process utilizing modality fusion.\n\nOriginal Text Query:\n\nThis image shows\nmonophasic synovial sarcoma\n\n. What are its key histologic features?\n\nA.", "Dense, uniform spindle cells with scant cytoplasm in short, intersecting fascicles.\n\nB.\n\nBiphasic: glandular epithelium with spindle cell stroma.\n\nC.\n\nScattered adipocytes in fibrous stroma with atypia.\n\nD.\n\nLarge polygonal cells with coarse eosinophilic granules.\n\nRewrited Search Query:\n\nWhat are the histological features of\n\n?\n\nEach query token\n\nembedding is matched to the\n\nmost similar image-patch embedding\n\non\n\nindexed pages, and their similarity scores are summed to rank the pages.\n\nImage Query:\n\nRetrieval\nPatho-Fusion\n\nScore:\n\nText Similarity Matrix Image 0.6638 0.6671 Final Top1 Cytologic appearance of Ewing sarcoma\n\n/PNET ...\n\n0.5878 Top20 from \u2026\u2026 Monophasic synovial\n\nsarcoma.\n\nThe tumor\n\n...\n\n0.6558\n\u2026 monophasic\nsynovial sarcoma\n\nwith ...\n\n0.6190 0.6602 Lose with an adenocarcinoma-like appearance of the\n\nepithelial component.\n\nRerank", "and how to call the RAG system. Formally, given an input query Q orig, the agent's policy \u03c0 outputs a path P , optimized to maximize the expected similarity to a ground-truth decision path P gt\n\nThe hierarchical reward R final compares the generated path to the target path step by step (see Algorithm 1). The agent performs a sequence of decisions to construct the path:\n\nDecision 1: Whether to invoke RAG?", "-Path A (No RAG Invocation): If the agent decides False , the decision process terminates. This is for simple queries that can be answered without external knowledge. Final path: { rag : False }\n\n-Path B (Invoke RAG): If True , proceed to the next decision.", "Decision 2: How to decompose the task? (Only applies if RAG is invoked)\n\nThe agent may choose to rewrite the query one or more times to better surface its core semantics and improve retrieval quality. This is not a mechanical rewriting process, but a targeted transformation to better align with the retrieval engine.\n\n\u2022\n\nDecision 3: Whether to use a tissue-specific classifier? (Only applies if RAG is invoked) The agent decides whether to enable a classifier to restrict retrieval to a relevant knowledge partition.", "-Path B.1 (Global Retrieval): If False , RAG retrieves documents from the full corpus. Final path: { rag : True , rewrite count : n, classifier : False }\n\n-Path B.2 (Classifier-Based Retrieval): If True , proceed to the next decision.", "Decision 4: Whether the classifier assigns the query to the correct partition? (Only applies if a classifier is enabled)\n\nThe agent selects a classifier from the available set { C 1 , . . . , C m } and attempts to assign the query to the correct partition. The effectiveness of this decision depends on both the selection of the classifier and the correctness of the classification. Final path: { rag : True , rewrite count : n, classifier : True , partition : C j }\n\nRL Training with GRPO We use the GRPO algorithm [36] to train the policy \u03c0 \u03b8 . For each query Q orig, the agent generates multiple decision paths:\n\nwhere each P i is a complete decision path and r i is its corresponding reward score r i \u2208 [0 , 4] based on the hierarchical reward function. We normalize the rewards within the group to compute the advantage function A i ( P i | Q ) :\n\nAlgorithm 1: Hierarchical Reward Computation\n\n\u0338", "Require: Agent path P , Ground Truth P gt 1: R final \u2190 0 2: if P. rag = P gt . rag then 3: return R final \u25b7 Incorrect Decision 1 4: end if 5: if P gt . rag = False then 6: R final \u2190 4 \u25b7 Correct Decision 1 (Path A) 7: else 8: R final \u2190 1 \u25b7 Correct Decision 1 (Path B) 9: if P. rewrite count = P gt . rewrite count then 10: R final \u2190 R final +1 \u25b7 Correct Decision 2 11: end if 12: if P. classifier = P gt . classifier then 13: if P. classifier = False then 14: R final \u2190 R final +2 \u25b7 Correct Decision 3 (Path B.1) 15: else 16: R final \u2190 R final +1 \u25b7 Correct Decision 3 (Path B.2) 17: if P. partition = P gt . partition then 18: R final \u2190 R final +1 \u25b7 Correct Decision 4 19: end if 20: end if 21: end if 22: end if 23: return R final\n\nwhere \u00b5 Q and \u03c3 Q are the mean and standard deviation of rewards within group G Q , and \u03b7 is a small constant for numerical stability.", "B. RAG Agent\n\nThis section details how we employ the RAG Agent to retrieve functions semantically similar to the target function to assist in translating the source function. The RAG Agent comprises four main components: Preprocessing, Selective Retrieval, Multi-Route Retrieval, and Result Filtering.", "1) Preprocessing: To enable effective retrieval, we first store the project's metadata in a vector database. Given the metadata of a project repository, we begin by preprocessing it and generating embeddings for two distinct components of the metadata, which are subsequently stored in the vector database. The first component consists of the method bodies of all sourcetarget translation pairs, and the second component includes the function names of all methods in the target repository. These two components are stored separately in two vector databases, referred to as the pair store and the name store, respectively.", "2) Selective Retrieval: In real-world code repositories, many functions are standalone, meaning they do not reference external classes or functions. For such functions, it is unnecessary to employ the RAG process to retrieve additional information during translation. Bypassing the RAG process in these cases can conserve computational resources and prevent redundant information from adversely affecting translation quality. Therefore, before initiating the RAG process, we first enable the agent to determine whether the target function requires RAG assistance. This decision is based primarily on whether the target function is standalone and whether it exhibits sufficient simplicity. If the agent determines that RAG is not needed, the RAG process is bypassed.", "3) Multi-Route Retrieval: If the agent determines that a target function requires the RAG process, a multi-route retrieval procedure is initiated. This procedure consists of two complementary components. The first component involves retrieving source-target translation pairs where the source function is similar to the current source function from the pair store. A hybrid retrieval strategy is employ: both dense retrieval (based on cosine similarity) and sparse retrieval (based on BM25) are utilized to independently retrieve the top-k most similar results from the pair store. These results are subsequently ranked using the Reciprocal Rank Fusion (RRF) method to produce the final top-k most relevant results. The second component involves retrieving functions from the name store based on the target function's name. Using sparse retrieval (BM25), the top-k functions with the most similar names are retrieved. If a function name exactly matches the target function name, its similarity score is set to 1.0. Finally, the results from both retrieval paths are merged to form the output of the multi-route retrieval step.", "4) Result Filtering: Not every function retrieved through the multi-route retrieval process is beneficial for translating the source function into the target function. Including irrelevant functions as additional context can be not only unhelpful but potentially detrimental to translation quality. Therefore, it is essential to filter the retrieved results to retain only those functions that are genuinely useful.", "We leverage the agent to perform this filtering operation. For each candidate function in the retrieved set, the agent compares it with the target function, considering factors such as functional similarity, structural resemblance, and referenced context. Based on this comparative analysis, the agent determines whether the candidate function is sufficiently similar to the target function to provide useful guidance for translating the source function. Functions deemed relevant are retained, while others are discarded.", "8 Discussion and Future Direction\n\n8.2 Implications for Proprietary Data\n\nA key development in RAG is its application to enterprise AI, enabling access to proprietary data without embedding sensitive information into model parameters [35]. This architecture supports data privacy and ensures outputs remain grounded in up-to-date knowledge [74]. Unlike traditional LLMs, which require retraining to update internal knowledge, RAG allows retrieval components to be independently refreshed [74].", "However, enterprise RAG deployments introduce new security concerns. The retriever could inadvertently expose confidential information if strict access controls are not enforced. Organizations must implement robust authentication mechanisms, ensuring that retrieved documents align with user permissions. Privacy-preserving techniques such as encrypted search indices and federated retrieval [58] are promising solutions to mitigate risks.", "The ability of RAG to function over proprietary knowledge bases has made it a preferred choice for industries handling sensitive information, including finance, healthcare, and legal sectors. As enterprises scale RAG systems, optimizing retrieval latency and ensuring regulatory compliance will be paramount.", "8.3 Future Research Directions\n\nKey avenues for advancing RAG include:\n\nMulti-hop retrieval : RAG pipelines typically retrieve single passages, but many queries require reasoning over multiple documents. Standard RAG models struggle with multi-hop questions [88], motivating research on multi-step retrieval and reasoning (e.g., chained or iterative RAG methods).", "Secure and privacy-aware retrieval : As RAG is applied to sensitive data, privacy becomes critical. Recent work proposes encrypting knowledge bases and embeddings to prevent unauthorized access while preserving performance [107], and integrating differential privacy and secure computation to guard against information leakage [42].", "Multimodal and agentic RAG : Extending RAG beyond text can leverage rich context. Multimodal RAG (MRAG) systems incorporate images, video, or other modalities into retrieval and generation, reducing hallucinations and improving performance on visual queries [59]. Separately, Agentic RAG embeds autonomous agents into the retrieval pipeline, enabling dynamic planning, tool use, and multi-step reasoning in the loop [83].", "Structured knowledge integration : Traditional RAG relies on vector search over unstructured text, which can miss complex relationships. Incorporating structured knowledge (e.g., knowledge graphs) can improve semantic understanding. Surveys and systems demonstrate that linking RAG to knowledge graphs yields more accurate and coherent answers [14, 24].", "Real-time or streaming retrieval : Many applications require RAG to operate on live data streams. For example, systems like StreamingRAG build evolving knowledge graphs from streaming inputs to provide timely context updates, achieving much faster throughput and higher temporal relevance than static RAG [79].", "8.4 Emerging Practical Applications\n\nRAG is finding use in a range of new applications:\n\nEnterprise search : Companies employ RAG to query internal document repositories. By building RAG pipelines around enterprise vector databases, organizations can retrieve and", "synthesize answers from private knowledge with strong performance, scalability, and security guarantees [89].\n\nReal-time assistants : Personal or domain-specific AI assistants can use RAG to retrieve current information (e.g., news, stock prices, weather) on the fly. The RAG framework naturally supports grounding LLM responses in real-time data, improving accuracy and user trust [33].", "Fact-checking : Integrating RAG into fact-checking pipelines can automate evidence retrieval. Recent systems use RAG-based reasoning (e.g., chain-of-RAG) to fetch and synthesize evidence for claims, improving veracity predictions in political and news domains [1].", "Experiment Results\n\nComparison with SOTA frameworks. Table 2 presents the result of MobileRAG and all baselines on MobileRAG-Eval. The results demonstrate that our method achieves superior perfor mance on various metrics, especially on AS, MobileRAG can achieve a 100% success rate, confirming the effectiveness of LocalRAG. By constructing app information into a local knowledge base, it can effectively mitigate operational errors or omitted steps caused by mobile agents' complete reliance on LLM's comprehension capabilities. Meanwhile, we achieve SOTA performance on TSR, achieveing 10.3% improvement over other baseline. This result demonstrates that MobileRAG can effectively interpret users' ambiguous intentions via InterRAG, making it particularly well-suited for real-world application scenarios.\n\n3 Claude-3.5 version: Claude-3.5-Sonnet-2024-10-22\n\n4 Gemini-1.5 version: Gemini-1.5-Pro-2024-04-09\n\n5 GPT-4o version: GPT-4o-2024-11-20", "| | Model.Model | Single.Single | LLM Dual.LLM Dual | Triple.Triple | Action.Single | Action.Dual | Action.Triple |\n|---:|:----------------|----------------:|--------------------:|----------------:|----------------:|--------------:|----------------:|\n| 0 | Mobile-Agent-v2 | 3 | 7 | 11.3 | 3 | 7 | 11.3 |\n| 1 | Mobile-Agent-E | 1.9 | 4.8 | 8 | 1.9 | 4.8 | 8 |\n| 2 | MobileRAG | 1.9 | 3.7 | 4.3 | 1 | 1 | 1 |\n\nTable 4: Efficiency of MobileRAG on Single-, Dual-, and Triple-App Tasks. 'LLM' indicates the number of LLM calls, and 'Action' indicates the number of mobile steps.", "We further validated the effectiveness of MobileAgent in real-world environments (Xiaomi 15 Pro). As shown in Table 2, MobileAgent performs exceptionally well across several key indicators and can reliably handle complex and dynamic practical app scenarios. Notably, compared to XMLbased and GUI-based mobile agents, MobileRAG can directly retrieve relevant context via LocalRAG, efficiently access and launch target apps, and flexibly leverage systemlevel functionalities. Owing to its lightweight architecture and high compatibility, it is well-suited for widespread deployment on real-world consumer mobile devices.", "Table 3 presents a comparison of MobileRAG across different backbone LLMs. MobileRAG consistently outperforms across all recent LLMs, owing to its superior framework design. It effectively leverages retrieval of historical information, local apps, and web data, which substantially mitigates comprehension or operational errors caused by large model hallucinations, and greatly enhances the reliability and practical value of the system.", "Efficiency Analysis. Table 4 reports the efficiency of MobileRAG by quantifying the number of steps required for LLM-based agents to invoke and open apps, an essential factor in evaluating the model's practical applicability. Compared to other methods, MobileRAG demonstrates a substantial efficiency gain. It can be attributed to the efficient retrieval architecture in MobileRAG, where LocalRAG requires only 0.005 seconds per retrieval and interRAG only 0.4 seconds, thereby significantly decreasing the number of LLM-based agent calls and accelerating the overall runtime. With the support of LocalRAG, MobileRAG significantly reduces the number of steps to open apps, especially for multi-app tasks. By enabling direct retrieval and execution of local apps, LocalRAG greatly enhances the efficiency of MobileRAG. Additionally, fewer operational steps reduce", "Figure 4: Leveraging MemRAG to retrieve previously successful steps enables the mobile agent to perform rapid operations. With its advanced memory and learning mechanisms, MemRAG empowers the agent to adapt based on past successes. As users interact with the agent and more queries accumulate, the mobile agent evolves, gradually enhancing its autonomous capabilities.\n\nSend\n\nmessage to Jelly t0 invite him to watch 'The\n\nBoys' season3, tell him the introduction.\n\nOpen\nMessages'\n\nand tap into Jelly's chat\n\nSummarize \"The Boys\" intro and send it to Jelly\n\nMemRAG: Similar in MemRAG dalabase\n\ntask: \"Find the movie 'The Boys\"\n\nand choose season3 and add it to watchlist, then\n\nTap\nsummary the introduction\nmessage to\n\nto invite him to watch The Boys'\n\nseason3, tell him the introduction\n\nTap & Type in Texts\n\nThe MemRAG contains\n\nsultable message inviting someone\n\nwalch\nThe Boys'\n\nSeason 3, including an\n\nintroduction. The only change needed Is\n\nsend It to Jelly Instead of Mike. The previous action used the \"Google\n\napp, which fits the current task,\n\nMike\n\nthe likelihood of operational errors or omissions, thereby enhancing the overall architectural performance.", "| | Model | AS | AF | RP | TCR | TSR | avg | open app |\n|---:|:----------|------:|-----:|-----:|------:|------:|------:|-----------:|\n| 0 | w/o Local | 93.8 | 75.4 | 96.8 | 82.1 | 67.5 | 11.42 | 3.12 |\n| 1 | w/o Inter | 88.1 | 85.3 | 97.1 | 79.7 | 65 | 9.94 | 1.81 |\n| 2 | MobileRAG | 100 | 86.4 | 98.5 | 91.2 | 80 | 9.15 | 1.86 |\n\nTable 5: Ablation studies of LocalRAG and InterRAG. 'w/o Local' and 'w/o Inter' indicate the removal of LocalRAG and InterRAG, respectively.", "Ablation Studies. Table 5 shows the results of Ablation studies, we find each component of MobileRAG plays a key role in improving performance. First, we use LLM-based visual perception instead of LocalRAG. The results show varying degrees of decline across different metrics, demonstrating that LocalRAG achieves significant performance improvements compared to GUI-based mobile agent, particularly in terms of interaction steps. LocalRAG can directly validate and operate locally installed apps, thereby eliminating unnecessary steps such as manual searching and interface understanding. Second, we remove InterRAG in our framework, we observed a decrease of 11.9% and 18.75% in AS and TSR, respectively. This finding confirms that InterRAG is capable of accessing the updated and comprehensive app information from external sources, thereby enabling more informed and accurate app selection.", "We further analyzed the effectiveness of MemRAG by testing 15 groups of data, encompassing cases with complete consistency, similarity above 80%, and similarity below 80% (More detail are provided in Appendix B.). As shown in Table 6, MemRAG effectively enhances model performance, particularly by reducing the number of operational steps, with an average reduction of 2.4 steps. These results clearly demonstrate that MobileRAG is capable of learning relevant operational procedures from historical successful steps via MemRAG, thereby improving both efficiency and overall task accuracy.", "| | Model | AS | AF | RP | TCR | TSR | Step |\n|---:|:---------|-----:|-----:|-----:|------:|------:|-------:|\n| 0 | w/o Mem | 100 | 88.7 | 98.4 | 93.8 | 86.7 | 10.6 |\n| 1 | with Mem | 100 | 91.1 | 98.7 | 97.6 | 93.3 | 8.2 |\n\nTable 6: Ablation studies of MemRAG. 'w/o Mem' indicates the removal of MemRAG.", "Case Study. Figure 3 demonstrates the reasoning process of MobileRAG when no suitable match is found by MemRAG. We find that MobileRAG has the following advantages: 1) utilizing InterRAG enables comprehensive understanding of user queries; 2) leveraging LocalRAG allows the system to directly launch relevant applications from the local app library, minimizing accidental touches or erroneous actions; 3) MobileRAG's robust retrieval capabilities help reduce the number of operational steps, particularly benefiting long sequential tasks. Figure 4 illustrates the inference process when MemRAG successfully identifies a matching instance. Through MemRAG, It can effectively extract and leverage information from historically correct operation sequences, thereby enhancing agent performance and further decreasing the number of operational steps."]}
{"type": "entry", "index": 60, "question": "how do i measure \u201cthis helped the user\u201d when the task is research, not fact lookup", "answer": "For research tasks, you can\u2019t just check if a single fact is right; you need to measure whether the tool improved the *overall research process* and outcomes.\n\n1. Define a concrete research task and \u201csuccess\u201d criteria\nIn HypoChainer, a discovery task was judged successful if users stayed within a reasonable search space (\u2264300 predictions) and still found enough key items (\u22653 of 5 reference predictions) [2]. For your setting, define equivalents, e.g.:\n- Identified N of M key sources, arguments, or hypotheses.\n- Produced a final memo/hypothesis that an expert rates as logically coherent and well-supported.\n\n2. Compare with and without the tool\nUse a within-subject or between-group design: some questions or projects done with AI, some without [1][2]. Then measure:\n- Completion rate within a time limit [2].\n- Time taken per task [1].\n- Quality of outputs: expert judges mark answers as exact, substantially similar, unclear, or incorrect vs a vetted reference set [1], or rate hypotheses on logical coherence and correctness [2].\n\n3. Capture user-perceived \u201chelpfulness\u201d\nUse questionnaires about:\n- Ease of identifying research directions [2].\n- Logical coherence of resulting hypotheses [2].\n- Workflow rationality and flexibility [2].\n- Informativeness and understandability of the tool\u2019s explanations [2].\nHigher scores here are evidence users *felt* helped.\n\n4. Look at interaction patterns\nTrack how people actually use the tool:\n- Number of follow-up queries, when they consult original documents, etc. [1]\n- Whether the tool helps them overcome dead-ends and revise their thinking (e.g., refining and redirecting hypotheses over time) [2].\n\nCombining these\u2014task success, quality, efficiency, user ratings, and behavioral traces\u2014gives a practical measure of \u201cthis helped the user\u201d for research work.", "tags": ["_untagged"], "full_faithfulness": 1.0, "full_answer_relevancy": 0.8167052890000001, "seed_faithfulness": 1.0, "seed_answer_relevancy": 0.8300081344, "context_relevance": 0.6609927400716304, "context_relevance_error": null, "context_relevance_reason": "The context passages provide relevant information about measuring the performance of AI tools in research tasks, particularly in the financial analysis domain. They discuss methodologies for evaluating AI's impact on human analysts, which aligns with the user's question about measuring effectiveness in a research context. However, while the passages are informative, they could more directly address specific metrics or methods for quantifying 'this helped the user' in research tasks, which slightly limits the overall relevance.", "structure_citation": 0.5802086280978533, "structure_error": null, "structure_reason": "The Actual Output provides a comprehensive approach to measuring user help in research tasks, outlining specific criteria and methods. However, it includes citations [1] and [2] that do not correspond to any snippets in the Context, leading to unsupported claims. While the response is well-structured and relevant, the lack of matching citations detracts from its overall credibility.", "hallucination_score": 0.8436724348632962, "hallucination_error": null, "hallucination_reason": "The response effectively addresses the question of measuring user help in research tasks by outlining a comprehensive approach that includes defining success criteria, comparing outcomes with and without the tool, capturing user perceptions, and analyzing interaction patterns. These points align well with the context provided, which discusses the complexities of evaluating AI tools in real-world tasks. However, while the claims are mostly supported, the response could benefit from more explicit connections to the specific benchmarks and methodologies mentioned in the context, which would strengthen its alignment.", "contexts_used": ["Measuring the Performance of AI Models on Tasks\n\nThe literature in artificial intelligence and machine learning is dedicated to studying the performance of LLMs on benchmarks and has continued to struggle to capture the speed of development and domain-specific applicability. ARC (Chollet 2019) and DROP (Dua, et al. 2019) are two of the premier assessment tools used to evaluate the ability for an LLM to gather important information from documents in a manner that emulates research assistance. RAG specific benchmarks such as TAT-QA (Zhu, et al. 2021), cover the effectiveness of finance information retrieval while benchmarking other domains. However, these benchmarks and others often fail to capture the complexities of real-world tasks, particularly in the economics and financial policy domains. Even with the introduction of agents, few benchmarks have emerged looking at making economic-based choices such as EconEvals (Fish, et al. 2025). However, such benchmarks often also focus on automated analysis, rather than the effects of pairing a human analyst with these tools, leaving them limited from a qualitative scope.", "In our study, we investigate a real-world information extraction and data annotation task previously completed by human analysts. By replicating an existing, completed task, we can create a robust evaluation of the impact of introducing these AI tools on this specific workflow. Additionally, we structure our evaluation as a within-subjects design, allowing us to understand the effect of introducing use of the AI tool as a treatment effect compared with a control human-only approach, all while controlling effectively for per-annotator skill.", "3. Methodology\n\n3.1 Case study using banks' public disclosures.\n\nThis task makes for an excellent case-study in assessing the impact of exposing analysts to AI tools to help complete their work. It is a complex, real-world task in the financial analysis domain, not a multiple choice question benchmark. The questions often required combining information from multiple different sources within the documents to produce a final answer. Additionally, because of heterogeneity in how banks disclose information, we can assess the efficacy of the AI-assisted approach under incomplete information. Because the documents do not always contain complete information to answer all the questions for every single bank, we were able to assess whether AI reacted appropriately in scenarios when information wasn't available by correctly refusing to answer.", "3.2 Study Design\n\nThis study employed a mixed experimental design to evaluate the impact of an Artificial Intelligence (AI) Retrieval-Augmented Generation (RAG) tool on the performance of human annotators performing a complex information extraction task using public disclosure documents from Global Systemically Important Banks (GSIBs). The core experiment utilized a within-subjects design, where specific questions within each bank annotation task were randomly assigned to either a 'Control' condition or an 'AI' (naive use) condition for each participant. This allowed for comparing the performance under each condition while controlling for individual annotator differences.", "A subsequent repeated-measures phase was conducted later, where participants revisited the questions previously assigned to the 'AI' condition, this time operating under an 'AI+' (interactive use) condition. This allowed for a direct comparison of naive versus interactive AI tool usage on the same set of questions while controlling for recall effects.", "3.3 Participants\n\nThree professional financial analysts participated in the study. Participants were recruited via a convenience sample of volunteers within the researchers' organization and completed the experimental tasks during their regular work hours. Prior to the experiment, participants completed a multiple-choice quiz designed to assess their baseline familiarity with both the subject matter (GSIB disclosures and related finance topics) and AI tools. These baseline skill assessments are used descriptively and for subsequent analysis exploring potential interactions between skill profiles and condition effects.", "3.3 Materials\n\nBank Documents: The materials consisted of comprehensive sets of public disclosure documents retrieved from bank websites and formatted as pdf files (e.g., annual reports, non-financial reports, regulatory filings) for a sample of nine European GSIBs. These document sets were identical to those compiled and then used in the original analysis by Beltran et al. (2023) and were provided by the original research team, ensuring consistency and relevance.", "Annotation Questions: A standardized set of 30 questions was used for all banks and participants. These questions were selected in consultation with the original Beltran et al. (2023) research team to be representative of the full 150-question set used in their study. The selection aimed for diversity across question complexity (high: 9, medium: 11, low: 6), required answer format (categorical: 9, Yes/No: 10, exact figure/value: 7), and thematic category (risk management: 11, financed activity: 10, oversight rules: 3, external project ratings: 2). Each question included detailed instructions and definitions identical to those used by Beltran et al. (2023).", "3.4 Experimental Conditions\n\nParticipants completed annotation questions under three distinct conditions:\n\nControl Condition: Participants answered assigned questions using only the provided PDF documents and a standard PDF reader. They were permitted to use the reader's built-in text search (\"find\") functionality but were explicitly prohibited from using any other external tools, web searches, or AI assistance. The focus was on manual information retrieval and synthesis based solely on the source documents.", "AI (Naive) Condition: Participants answered assigned questions using a provided AI RAG chat tool pre-loaded with the relevant bank's document collection. Participants were instructed to formulate a single prompt based on the annotation question and submit it to the tool. They were explicitly instructed to record their final answer based only on the AI tool's initial response, without consulting the", "original PDF documents. This condition aimed to simulate a naive or high-trust approach to using the AI assistant.", "AI+ (Interactive) Condition: This condition was implemented in the follow-up phase for questions previously tackled under the 'AI' condition. Participants initiated the process identically to the 'AI' condition (formulate single prompt, review AI response). However, they were then given discretion: based on their judgment of the initial AI response, they could choose to (a) ask additional follow-up questions within the AI chat tool, (b) consult the original PDF documents, or (c) do neither or both, before recording their final answer. This condition aimed to simulate a more interactive, discerning, and potentially iterative approach to using the AI assistant.", "AI RAG Tool Specifications: The same AI RAG tool configuration was used for both the 'AI' and 'AI+' conditions. It was implemented using OpenWebUI (version 0.3.2) as the interface. The backend utilized Claude 3.5 Sonnet (model version 20240622) via Amazon Bedrock as the Large Language Model (LLM) with default sampling parameters. Vector embeddings were generated using Amazon Titan Text Embedding v2 via Amazon Bedrock. For the RAG process, documents were chunked using a size of 2000 characters with a 200-character overlap. The retrieval mechanism used a 'top_k' setting of 20, meaning the 20 most relevant chunks were retrieved to inform the LLM's response. This setup was chosen to represent a commonly accessible, though not state-of-the-art, RAG implementation suitable for typical researcher workflows.", "3.5 Procedure\n\nParticipants received initial training covering the task background, the experimental procedures for each condition, instructions on using the AI RAG tool interface, and guidelines for self-reporting time. The detailed instructions embedded within each annotation question served as the primary guide for content interpretation.", "For each assigned bank, the 30 questions were presented in the same fixed order. Prior to the task, each question was randomly assigned (using a computer script with a random number generator, resulting in an approximately 50/50 split) to either the 'Control' or 'AI' condition for that specific annotator-bank instance. Randomization was not stratified by question characteristics.", "Participants performed the tasks at their own discretion regarding location (e.g., office, remote). For each question, participants self-reported their start and end time (wall clock time) to the nearest minute. Durations under one minute were recorded as 0.5 minutes, based on observational validation for a subset of tasks.", "The initial experiment (Control vs. AI) was completed first. The follow-up experiment (AI+) was conducted after a minimum delay of two weeks for all participants (up to three months for two participants) to mitigate potential recall effects from the initial phase.", "Bank assignments were as follows: Annotator A completed tasks for six banks, Annotator B for three banks, and Annotator C for four banks. Annotator C's assignments included two banks also completed by Annotator A and two banks also completed by Annotator B, creating deliberate overlap to enable the assessment of inter-annotator agreement.", "3.6 Outcome Measures\n\nTime Efficiency: Measured as the self-reported time taken per question (in minutes) for each condition.", "Accuracy/Agreement: The primary measure of answer quality. Annotator responses for all conditions were compared against the final, vetted answers established by the Beltran et al. (2023) research team. Although the realistic nature of this task implies that there are no known true values, given the thoroughness of the original research team's work, we consider comparison against these values as an acceptable proxy for task accuracy. A member of the original team performed this agreement assessment, consulting with other original team members for complex cases. For each answer, agreement was categorized as \"exact match,\" \"substantially similar,\" \"unclear,\" or \"incorrect.\" For the main analysis, \"exact match\" and \"substantially similar\" were coded as Agreement = True (representing acceptable answers based on the original team's criteria), while \"incorrect\" was coded as Agreement = False. Answers rated \"unclear\" (20", "answers, roughly 4% of total answers) were excluded from the accuracy analysis. Robustness checks that did not count 'substantially similar' as agreement yielded broadly similar results.", "AI+ Interaction Metrics: For the AI+ condition only, participants recorded (a) the number of distinct follow-up messages sent to the AI tool after the initial prompt, and (b) whether they chose to consult the original PDF documents (Yes/No) before finalizing their answer.", "3.7 Ethical Considerations\n\nInformed consent was obtained orally from all participants prior to their involvement in the study. Participants were informed about the study's purpose, procedures, data handling, and the voluntary nature of participation. Data collected, including annotator responses, timing, and quiz scores, were de-identified before the main statistical analysis to protect participant confidentiality.", "5 EVALUATION\n\n5.1 Case Study: Drug Repurposing Exploration\n\n[Neuronal network dynamics] \u2192 [Manifests as] \u2192 [Episodic neural dysfunction] (Fig. 1- 7 ). E3 first validated this chain (Fig. 1- 8 ) by confirming that retrieved entities aligned with the hypothesis and were supported by contextual KG evidence (Fig. 1- 9 ). He observed that integrating existing KG edges revealed novel sub-paths (Fig. 1-10) and multiple connections between hypothesis-aligned entities and one-hop neighbors, reinforcing the chain's coherence. Cross-referencing the Prediction View , he noted consistency between the hypothesis and most interpretative paths (Fig. 1-11). E3 considered this as evidence that the hypothesis chain explains most repurposing predictions.", "Critical Insight and Hypothesis Revision . However, E3 noted a misalignment between the hypothesis and high-score predictions for Huntington's disease (Fig. 1-12). By filtering and examining these predictions, he observed that the original hypothesis aligned only partially. Guided by the LLM's explanations, E4 's further analysis of the remaining predictions revealed that some drugs were indicated for depression, while others were linked to neurodegenerative diseases. This finding led E3 to infer that the model's predictions regarding Huntington's disease primarily reflect the symptomatic alleviation effects rather than an influence on or delay of its underlying etiology. To reconcile this, E3 refined the hypothesis chain to: [ Drugs treating motor dysfunction, depression, or neurodegeneration ] \u2192 [ related to ] \u2192 [ Interacting genes or pathways ] \u2192 [ Participated in ] \u2192 [ Processes related to motor dysfunction, depression, or neurodegeneration ] \u2192 [ Led to ] \u2192 [ Diseases associated with Huntington's disease ].", "Hypothesis Challenge and Iterative Refinement . During hypothesis validation, E3 noted that the revised chain obscured the majority of the predictions. However, in the Hypothesis View , he identified a link between Huntington's disease and the Trinucleotide Repeat Expansion entity -a genetic mechanism associated with neurodegenerative disorders, as noted by E4 (Fig. 5- 1 ). Intrigued by its absence in the Prediction View , E3 queried the LLM, which explained that Trinucleotide Repeat Expansion is a well-established genetic cause of Huntington's disease , yet it did not surface in any Huntington 's predictions. Intrigued by this discrepancy, E3 revised the hypothesis chain to prioritize predictions involving Trinucleotide Repeat Expansion and submitted the updated hypothesis chain via the for LLM validation. The LLM provided critical feedback: while Trinucleotide Repeat Expansion refers to abnormal DNA sequence elongations that disrupt gene expres-\n\nFig. 5: 1 E3 noticed a link to Huntington's disease from Trinucleotide Repeat Expansion , a 1-hop entity of neurodegenerative disease .", "sion or protein function, its absence in predictions likely reflects the lack of direct therapeutics. However, the LLM emphasized indirect associations in literature, linking the entity to broader processes like DNA_repair and histone deacetylase (HDAC) regulation .", "Insight-Driven Revision . Guided by this feedback, E3 refined the hypothesis to focus on HDAC -related therapeutic targets. A subsequent retrieval (Fig. 1-13) identified therapies modulating HDAC activity in diseases involving abnormal repeat expansions, such as Parkinson's disease [34] and Amyotrophic Lateral Sclerosis (ALS) (Fig. 1-16). This alignment validated the revised chain (Fig. 1-14,15).", "Critical Discovery and Therapeutic Proposal . Re-examining Huntington 's predictions, E3 uncovered a previously overlooked interpretative path: [ Entinostat \u2192 [decreases activity of] \u2192 [ HDAC1 gene] \u2192 [interacts with] \u2192 [ Histone H4 ] \u2192 [gene associated with condition] \u2192 [ Huntington's disease ]. This HDAC -associated interpretative path aligned with literature suggesting HDAC 's role in disease progression [22]. Synthesizing these insights, E3 and E4 proposed the design of a novel cocktail therapy combining three symptom-alleviating drugs with HDAC inhibitors to potentially delay Huntington 's progression. He further considered exploring HDAC -targeted gene therapies as a complementary avenue for further research.", "Takeaway Message. In the case study, E3 and E4 iteratively revised initial hypotheses as new insights emerged, reflecting a dynamic reasoning process. Although the final hypotheses diverged from the original, they represented logical extensions informed by system-driven exploration. For example, in repurposing drugs for Huntington's disease , the expert initially focused on antiepileptic mechanisms for symptomatic relief. However, analysis of high-confidence predictions outside this scope revealed links to broader neurodegenerative pathways, prompting a shift toward a more integrative hypothesis. E3 reported maintaining control over hypothesis development, with HypoChainer offering timely support when exploration stalled-clarifying complex predictions, suggesting new directions, and highlighting supporting KG evidence. E4 praised the system's alignment with conventional workflows, flexible entity categorization, and context-aware knowledge integration, which streamlined hypothesis refinement. They also noted that the system helped transcend initial cognitive frames and uncover unanticipated, yet scientifically valuable associations. This iterative refinement process underscores the system's potential to support deeper and more structured hypothesis generation in complex discovery tasks.", "5.2 User Study\n\nWe recruited 12 graduate students (Mean Age = 26 . 33, SD = 1 . 93; 6 males, 6 females) from bioinformatics or biomedical engineering backgrounds, including 6 PhD and 6 Master's students. Participants were randomly assigned to either the Baseline or HypoChainer group, balanced by degree level (3 PhD and 3 Master's per group).", "Participants completed a drug repurposing task targeting Hemophilia B . To ensure objective evaluation, all were screened to confirm no prior knowledge of the disease or its mechanisms. Five system-generated interpretative paths were selected based on partial alignment with DrugMechDB-curated mechanisms of actions (MOAs) for Eptacog Alfa and Nonacog Alfa . The Baseline system (Appendix Fig. 8) used an LLM-only setup, omitting the Hypothesis View , to isolate the contributions of RAG and the view's design while maintaining the overall workflow. Both drugs were excluded from training data to better simulate real-world discovery conditions. Participants received a 30-minute training session on task goals, system usage, and evaluation criteria, followed by 90 minutes to explore predictions, formulate hypotheses, and submit results. The process was screen-recorded. A task was deemed successful if participants retrieved \u2264 300 predictions and identified \u2265 3 of 5 reference predictions. A post-task questionnaire (Fig. 6) adapted from the System Usability Scale [8] assessed perceived system effectiveness, workflow, and usability.", "Fig. 6: The questionnaire results of the two systems in terms of system effectiveness, workflow and usability ( \u2217 : p < 0 . 05 ).", "In the Baseline group, 2 PhD participants (33.3%) completed the task within the time limit, compared to 4 in the HypoChainer group (3 PhDs, 1 Master's; 66.7%). PhD participants generally outperformed Master's students, likely due to stronger research backgrounds and faster system adaptation. HypoChainer showed clear advantages in Effectiveness , with significantly higher scores for Ease of Identifying Research Directions ( M = 5 . 50, SD = 1 . 38, p < 0 . 05) and Logical Coherence of Hypothesis Chains ( M = 5 . 67, SD = 0 . 52, p < 0 . 05) compared to the Baseline ( M = 4 . 17, SD = 1 . 47; M = 4 . 17, SD = 0 . 75). Both groups scored similarly on Clarity of Interpretative Paths ( HypoChainer : M = 5 . 33, SD = 1 . 03; Baseline: M = 5 . 00, SD = 0 . 89). For Workflow , HypoChainer scored higher on Rationality ( M = 5 . 17, SD = 1 . 33, p < 0 . 05) and Flexibility ( M = 5 . 00, SD = 0 . 89, p < 0 . 05), supported by user feedback indicating fewer hallucinations. However, its integrated features-transitioning among LLM, RAG, and the Hypothesis View-were perceived as more complex ( M = 6 . 00, SD = 0 . 89) than the Baseline ( M = 4 . 83, SD = 0 . 75). In Usability , HypoChainer had a slightly lower score in Learnability ( M = 4 . 50, SD = 0 . 55) than the Baseline ( M = 5 . 00, SD = 0 . 63), suggesting a steeper learning curve. Nevertheless, it received significantly higher ratings for Informativeness ( M = 5 . 83, SD = 0 . 75, p < 0 . 05) and Understandability ( M = 5 . 17, SD = 0 . 75, p < 0 . 05), due to its clear reasoning structure and entitylevel explanations. Overall, HypoChainer provided more effective, transparent, and insightful support for hypothesis generation, despite a slightly higher learning threshold.", "C. Evaluation Metrics\n\nFollowing previous studies [21, 44], we employ two metrics to evaluate the performance of our framework: Precision @ k and Mean Reciprocal Rank (MRR).", "Precision @ k measures a model's ability to include the correct target library within the top-k results, regardless of their order. It is defined as:\n\nwhere N denotes the total number of queries, and HasCorrect k ( q ) equals 1 if the correct target library appears within the top-k results for query q , otherwise it returns 0.", "MRR is a widely adopted metric for evaluating recommendation tasks. It considers the ranks of all correct answers. It is formally defined as:\n\nwhere firstpos ( q ) returns the position of the first correct target library in the results. If the correct target library is not present in the results, the numerator of the formula is zero."]}
{"type": "entry", "index": 61, "question": "does compressing retrieved context help faithfulness or just hide missing evidence", "answer": "Compressing retrieved context can genuinely help faithfulness, but only when it is done in a way that keeps track of what evidence is actually present and how reliable it is.\n\nEvidence-focused compression methods that explicitly reason about which passages matter tend to improve both accuracy and grounding. EviOmni, which extracts \u201crational evidence\u201d instead of passing full passages, achieves very high compression (up to ~38\u00d7 fewer tokens) while matching or beating the full-context baseline on QA accuracy, because it filters noise and keeps key clues rather than just shortening text [1]. This suggests compression can increase the signal\u2011to\u2011noise ratio instead of hiding gaps.\n\nSimilarly, CARE compresses the retrieved context into \u201cmemory embeddings\u201d and fine\u2011tunes them to encode how reliable the context is. When the external passage is helpful, the compressed prompt steers the model toward it; when the passage is misleading, the compressed prompt is trained to down\u2011weight it so the model falls back on internal knowledge [2]. This is directly aimed at improving faithfulness, not masking missing evidence.\n\nOn the other hand, the papers also show why naive compression is risky. Larger models that integrate context more aggressively become slightly less faithful because they start inferring connections not explicitly in the documents [3], and harmful or adversarial documents can drive alignment nearly to zero even though they are \u201cevidence\u201d [4]. Pure length reduction or summarization without explicit evidence reasoning can drop crucial cues or retain persuasive but wrong content, which does amount to hiding missing or bad evidence.\n\nSo: compression helps faithfulness when it is evidence\u2011aware and reliability\u2011aware; simple token squeezing can hurt faithfulness or conceal that the system never actually saw the needed evidence.", "tags": ["benchmarks_eval", "summarization_conflict"], "full_faithfulness": 1.0, "full_answer_relevancy": 0.9176545063, "seed_faithfulness": 0.75, "seed_answer_relevancy": 0.9176284425000001, "context_relevance": 0.21281984889344704, "context_relevance_error": null, "context_relevance_reason": "The context passages provided do not directly address the user's question about whether compressing retrieved context helps faithfulness or just hides missing evidence. While there are discussions about context compression and its effects on performance, they lack a clear connection to the specific inquiry regarding faithfulness and evidence. The passages primarily focus on experimental setups and results without explicitly linking compression to the concepts of faithfulness or evidence retention.", "structure_citation": 0.7869741943186959, "structure_error": null, "structure_reason": "The Actual Output effectively discusses the role of compression in enhancing faithfulness, referencing specific methods like EviOmni and CARE, which are relevant to the claims made. It includes citations [1] and [2] that correspond to the context provided, supporting the claims about evidence-focused compression. However, while it mentions risks associated with naive compression, it could have better addressed the need for citations for all claims, particularly regarding the negative impacts of compression, which were not explicitly supported by the context snippets.", "hallucination_score": 0.8398543811710546, "hallucination_error": null, "hallucination_reason": "The response effectively addresses the question of whether compressing retrieved context aids faithfulness by discussing evidence-focused compression methods and their impact on accuracy and grounding. It supports its claims with specific examples, such as EviOmni and CARE, which align with the context provided. However, while it mentions risks associated with naive compression, it could further clarify how these risks specifically relate to the context of the test case, which would strengthen the overall argument.", "contexts_used": ["4 Experiment\n4.1 Experimental Setup\n\n( i.e., \u03b1 a , \u03b1 l , and \u03b1 f ), we empirically set \u03c9 , \u03b1 a , \u03b1 l , and \u03b1 f as 0.9, 0.8, 0.1, 0.1, respectively, which commonly leads to satisfactory performance. For policy optimization, we set \u03f5 std as 0.1 to stabilize training. During training, we set the PPO clipping hyperparameter \u03f5 and the coefficient \u03b2 controlling KL-divergence as 0.2 and 1 e -2 , respectively. For QA generators, we employ LoRA (Yang et al., 2025) to fine-tune Qwen2.51.5B-Instruct and Qwen2.5-7B-Instruct to predict the golden answer a based on the query and retrieved passages ( q , P ) or only the query (used for the baseline 'Zero'), where LoRA rank is set as 8.", "Passage Retrieval. Following previous works (Wang et al., 2023; Zhu et al., 2024; Zhao et al., 2024a; Zhang et al., 2025a), we retrieve Wikipedia (Wiki) passages for all datasets. We use Dense Passage Retriever (DPR) (Karpukhin et al., 2020) to retrieve the top-5 passages from all Wiki passages, where we use the December 2018 Wiki dump (Karpukhin et al., 2020) as the retrieval corpus, and set the chunk size as 100 words. We use the retrieval toolkit, Tevatron (Gao et al., 2023), to perform corpus encoding, query encoding, and passage retrieval. Table 3 shows the retrieval performance w.r.t. Recall@5 and NDCG@5, where the retrieval passage that contains the golden answer is treated as the positive passage. Besides, for HotpotQA, we compute Recall and NDCG only for the 'bridge' questions, while ignoring yes/no and comparison questions, following previous works (Jiang et al., 2024; Khalifa et al., 2023). The retrieval performance for HotpotQA is relatively low, as it is a multi-hop QA dataset, and answers are usually not spans in retrieval passages.", "Comparison Baselines. To verify the effectiveness of EviOmni , we compare it with the following three groups of competitive baselines: (1) Without Refinement (WR) includes (i) Zero-shot (Zero) generates output relying on LLMs' parametric knowledge; (ii) Full Passage (Full) feeds all retrieval passages into LLMs; (2) Vanilla Refinement (VAR) directly extracts evidence without ex-", "| | Datasets.Datasets | Metrics.Metrics | WR.Zero | WR.Full | VAR.SeleCtx | VAR.LLMLingua-2 | VAR.RECOMP | VAR.FilCo | VAR.SEER | RAR.CoT | RAR.EviOmni |\n|---:|:--------------------|:------------------|:----------------|:----------------|:----------------|:------------------|:----------------|:----------------|:----------------|:----------------|:----------------|\n| 0 | | 1.5B size model | 1.5B size model | 1.5B size model | 1.5B size model | 1.5B size model | 1.5B size model | 1.5B size model | 1.5B size model | 1.5B size model | 1.5B size model |\n| 1 | | EM | 13.74 | 41.97 | 24.68 | 29.36 | 37.40 | 36.62 | 36.93 | 37.70 | 41.14 |\n| 2 | NQ | F1 | 53.16 | 70.07 | 59.52 | 62.82 | 67.18 | 66.63 | 67.11 | 68.09 | 70.77 |\n| 3 | | CR | - | 1.0x | 3.44x | 4.51x | 5.43x | 16.3x | 13.2x | 4.56x | 38.1x |\n| 4 | | EM | 29.10 | 57.02 | 45.13 | 48.67 | 56.56 | 54.06 | 54.57 | 54.29 | 56.84 |\n| 5 | TQA \u2020 | F1 | 64.62 | 80.13 | 73.40 | 75.29 | 79.85 | 78.56 | 78.81 | 79.56 | 80.85 |\n| 6 | | CR | - | 1.0x | 3.38x | 4.52x | 5.35x | 8.55x | 10.3x | 5.02x | 38.8x |\n| 7 | | EM | 12.36 | 19.20 | 15.30 | 16.64 | 18.52 | 18.18 | 18.60 | 19.52 | 20.46 |\n| 8 | HotpotQA \u2020 | F1 | 48.52 | 53.04 | 49.65 | 51.26 | 52.92 | 52.15 | 52.79 | 53.43 | 54.20 |\n| 9 | | CR | - | 1.0x | 3.40x | 4.52x | 5.44x | 18.3x | 15.5x | 4.17x | 33.0x |\n| 10 | 7B size | model | model | model | model | model | model | model | model | model | model |\n| 11 | | EM | 25.04 | 48.78 | 33.91 | 36.51 | 43.77 | 44.79 | 45.01 | 44.49 | 46.95 |\n| 12 | NQ | F1 | 60.88 | 74.40 | 65.71 | 67.77 | 71.61 | 72.30 | 72.76 | 73.09 | 73.99 |\n| 13 | | CR | - | 1.0x | 3.44x | 4.51x | 5.43x | 15.4x | 12.4x | 3.36x | 17.9x |\n| 14 | | EM | 47.31 | 65.34 | 58.72 | 60.50 | 64.40 | 63.46 | 64.20 | 63.91 | 64.76 |\n| 15 | TQA \u2020 | F1 | 74.36 | 84.83 | 80.95 | 81.94 | 84.12 | 83.66 | 84.14 | 84.45 | 84.74 |\n| 16 | | CR | - | 1.0x | 3.38x | 4.52x | 5.35x | 7.77x | 9.70x | 3.33x | 17.6x |\n| 17 | | EM | 17.95 | 25.82 | 21.27 | 23.45 | 24.84 | 25.27 | 25.81 | 27.25 | 28.02 |\n| 18 | HotpotQA \u2020 | F1 | 53.07 | 58.50 | 54.90 | 56.65 | 58.02 | 58.15 | 58.63 | 59.84 | 60.30 |\n| 19 | | CR | - | 1.0x | 3.40x | 4.52x | 5.44x | 17.5x | 14.3x | 3.32x | 16.5x |", "Table 4: Overall performance comparison on NQ, TQA, and HotpotQA datasets, where the best results are boldfaced and the second-best results are underlined. EM, F1, and CR denote exact match, F1 score, and compression ratio (the higher the better), respectively. The 1.5B/7B size models represent Qwen2.5-1.5B/7B-Instruct. The \u2020 symbol denotes out-of-domain (OOD) evaluation datasets for EviOmni , as it is only trained on NQ.", "plicit thinking, which includes (i) Select Context (SeleCtx) (Li et al., 2023) identifies and prunes redundancy in the input context based on perplexity; (ii) LLMLingua-2 (Pan et al., 2024) distills compression knowledge from GPT-4 to reduce crucial information losing; (iii) FILCO (Wang et al., 2023) trains a context filtering model to identify key clues; (iv) BottleNeck (Zhu et al., 2024) applies the information bottle theory to select SFT data used to optimize filtering; (v) SEER (Zhao et al., 2024a) learns to extract desired evidence via self-aligned learning; (3) Rational Refinement (RAR) includes (i) Chain-of-Thought (CoT) (Wei et al., 2022) generates query-related information from retrieval passages with explicit thinking.", "4.2 Main Comparison (RQ1)\n\nThe overall comparison results on NQ, TQA, and HotpotQA are shown in Table 4. From the experimental results, we mainly have the following observations. (1) In all cases (18/18), EviOmni achieves the best or second-best results, indicating the superiority of supplementing RAG with rational evidence. (2) It is surprising that directly employing EviOmni trained on NQ and tested on OOD datasets ( i.e., TQAand HotpotQA) yields such impressive performance. This demonstrates that online policy optimization endows EviOmni with superior generalization. (3) Com- pared to 'Full', EviOmni possesses an extremely high compression ratio ( e.g., 38.1x with 1.5B size model on NQ) and its performance is very close to or even better than that of 'Full'. Though the baseline models have relatively high compression ratios, they are accompanied by significant performance degradation. This demonstrates again that rational evidence benefits downstream performance than vanilla one and also improves the signal-to-noise ratio. (4) By comparing the 1.5B EviOmni model with the 7B one, we find that the 7B one tends to extract more informative evidence than the 1.5B one. (5) EviOmni considerably outperforms VAR methods in almost all cases and provides more compact evidence, demonstrating the necessity of explicit evidence reasoning. (6) In HotpotQA, RAR methods significantly outperform VAR ones, and even better than 'Full', indicating that rational refinement is important to multi-hop question answering. In conclusion, the above results and observations fully validate the effectiveness and efficiency of rational evidence.", "4.3 Training Dynamics (RQ2)\n\nThe RL training dynamics regarding the answer rewards and response length are shown in Figure 5. Note that the generators used in Table 4 and Figure 5 are different. Therefore, the answer rewards in Figure 5 are slightly different from the", "Figure 5: Training dynamics w.r.t. answer reward and response (including 'reason', 'extract', and 'answer') length.\n\n(a) Answer reward dynamics on 1.5B model.\n\n(b) Answer reward dynamics on 7B model.\n\n(c) Length dynamics.\n\n0.725 0.44 0,77 Fla0 1,5B model 7Bmnodel 0.41 0.74 0.71 0.625 20 0.65 20 0.38 20 10o Training Steps\n\nTraining Steps of RL\n\n| | Models | NQ | TQA | HotpotQA | Avg |\n|---:|:---------|-----:|------:|-----------:|------:|\n| 0 | FILCO | 0.64 | 0.82 | 0.59 | 0.68 |\n| 1 | CoT | 0.55 | 0.7 | 0.71 | 0.65 |\n| 2 | EviOmni | 0.35 | 0.41 | 0.43 | 0.4 |\n\nTable 5: Inference latency (seconds/query) on the 1.5B model, where the smaller the better.", "F 1 and EM results in Table 4. The results show that the answer rewards of full context ( o f ), rationale ( o r ), and rational evidence ( o e ) are consistently improved during the process of reinforcement learning. And, it is not surprising that the answer reward of full context generally performs best. More specifically, on the 1.5B model, rationale and rational evidence perform very closely, while on the 7B model, rational evidence performs slightly worse than rationale. We think the main reason is that the 7B model can infer answers based on implicit cues, but rational evidence may compress some of them. As for response length dynamics, the response length of the 1.5B model decreases rapidly at the beginning of training and then increases slowly, while that of the 7B model decreases slowly all the time. We think the main reason is that a moderate response length is beneficial for improving answer rewards, because an overlong reasoning may confuse answer generation, and a too short reasoning may omit key cues. Therefore, as training continues, the response lengths of the 1.5B model and the 7B model tend to converge. In conclusion, the above observations provide some useful insights, e.g., elaborated answer and length rewards, for further research on the rational evidence extraction.", "4.4 Inference Efficiency (RQ3)\n\nTable 5 presents the inference latency of EviOmni compared with FILCO (a representative V AR method) and CoT (a representative RAR method) on the 1.5B model within a 1 \u00d7 A800 station, where the evaluation batch size and max new tokens are set as 64 and 768, respectively. It can\n\nFigure 6: Performance comparisons w.r.t. data noise on the 1.5B model, where psgs is the abbr of 'passages'.\n\n(a) NQ.\n\n(b) TQA.\n\nEviOmni\nFilCo\n#Irrelevant Psgs", "5 Related Works\n\n5.1 Retrieval-Augmented Generation\n\nRetrieval-Augmented Generation (RAG) has been prevalent in LLMs, which effectively enhances LLMs with external non-parametric knowledge to remedy their outdated, incorrect, or incomplete internal parametric knowledge (Lewis et al., 2020; Wu et al., 2022; Guu et al., 2020; Izacard et al., 2023; Asai et al., 2024). The pioneering attempts (Lewis et al., 2020; Guu et al., 2020) demonstrated that augmenting the input context of LLMs with retrieved passages yields significant improvements, especially in knowledge-intensive tasks. While prior work usually retrieves straightforwardly at the beginning, dynamic RAG (Asai et al., 2024; Jeong et al., 2024; Jiang et al., 2023) has been proposed to adaptively retrieve passages based on the demand of generation and the complexity of the query. Recently, agentic\n\nRAG (Zhang et al., 2025b) has been proposed to interleave retrieval and reasoning to tackle complex queries. In particular, this kind of work (Song et al., 2025; Jin et al., 2025; Trivedi et al., 2023) focuses on prompting or fine-tuning LLMs as search agents that interact with search tools to get external knowledge autonomously and on demand through direct prompting/reinforcement learning.", "5.2 RAG with Context Compression\n\nRAG systems usually concatenate all retrieved passages as the context of LLMs. However, this may introduce data noise and computational overhead due to imperfect retrieval and overlong context. Recently, many works have attempted to compress context and retain important information, mainly including two categories: (1) Reranking methods, which rerank retrieved passages and retain top-ranked passages (Hwang et al., 2024b; Mortaheb et al., 2025; Mao et al., 2021; Chen et al., 2025) (2) Summarization methods, which extract relevant information from retrieved passages and consolidate them into a narrative (Wang et al., 2023; Zhao et al., 2024a; Zhu et al., 2024; Xu et al., 2024). While effective, existing works neglect evidence reasoning, which risks filtering out key clues and struggles with generalization.", "3 Proposed Method: CARE\n\n3.1 Reconstruction Pre-training\n\nThe goal of the pre-training stage is to train the context assessor to represent the retrieved context as memory token embeddings. We adopt reconstruction-based soft prompting methods (Ge et al., 2024; Cheng et al., 2024).", "we append K learnable memory tokens, denoted as M = \u27e8 m 1 , . . . , m K \u27e9 , to the original input context C = [ c 1 , . . . , c n ] . The resulting sequence serves as the input for pre-training.", "We feed an input token sequence into the context assessor, incorporating a LoRA adapter built upon the base LLM. The final hidden states corresponding to the memory tokens M are obtained from a single forward pass of the decoder, referred to as memory embeddings ( E mem ). These embeddings serve as compressed representations of the input context and are used as soft prompts for downstream text generation.", "where \u03d5 denotes frozen LLM parameters, and \u03b8 represents the learnable parameters of the LoRA adapters (Hu et al., 2022) and memory tokens.", "Pre-training\n\nFigure 3: Overall framework of C onflictA ware RE trieval-augmented generation ( CARE ). (a) The reconstruction pretraining learns to compress contextual information into memory embeddings. (b) In the conflict-aware fine-tuning stage, adversarial soft prompting exposes the Context Assessor to misleading contexts, encouraging the memory embeddings to reflect their reliability.\n\nReconstruction b) Conflict-aware Fine- Grounded Soft Prompting Adversarial Soft Prompting Context Answer LLM Student Teacher LKD\n\nLLM f .\n\nMemory\nembeddings\n\n\"Reconstruct the given context\"\n\nQuestion LoRA Assessor 0,0 Assessor f \u00f8,0 LLM f Prompt (m1 mK) Positive Hard negative K memory slots context Cpos w/ a positive negative Inference -tuning Cneg pair\n\nWe train the parameters \u03b8 by minimizing a reconstruction loss computed using the frozen LLM parameters \u03d5 , which encourages E mem to capture the accurate information required to reconstruct the input context C . The reconstruction loss is defined as the negative log-likelihood of generating each token conditioned on the memory embeddings E mem and instruction I recon 2 :", "3.2 Conflict-aware Fine-tuning\n\nWe fine-tune the context assessor to be sensitive to context-memory conflicts . The goal is to produce memory embeddings that not only compress the retrieved context but also reflect its reliability, i.e. , whether the LLM should rely on it. To achieve this, we employ an instruction-following dataset with question-answer pairs to expose the discrepancy between external and parametric knowledge.", "The context assessor takes an input sequence consisting of question Q = [ q 1 , . . . , q n ] , context C = [ c 1 , . . . , c n ] , and learnable memory tokens M = \u27e8 m 1 , . . . , m K \u27e9 , to assess the context based on the question.", "2 We adopt the instruction set from Cheng et al. (2024) for reconstruction.\n\nAs in the pre-training stage, the context assessor extracts memory embeddings E mem \u2208 R K \u00d7 d by encoding the input sequence:\n\nTo train the context assessor to detect such conflicts, we simulate them using correctness signals from a closed-book setting: when the LLM fails to answer correctly without context, and when it succeeds. If the parametric knowledge is insufficient, we apply grounded soft prompting to guide the LLM toward the external context. When the LLM already contains the relevant knowledge, we apply adversarial soft prompting with a hard negative context and train the context assessor to encode it in a way that reduces the influence, allowing the LLM to favor internal knowledge.", "Grounded Soft Prompting . It provides the context assessor with positive supervision signals about useful external knowledge. For questions that the LLM fails to answer in a closed-book setting, we pair them with a positive context C pos that contains the answer span. In this setup, C pos is treated as a reliable context source. The context assessor is trained to reflect this reliability in memory embeddings, allowing the model to recognize and represent helpful external knowledge, particularly when parametric knowledge is insufficient.", "Adversarial Soft Prompting . We adopt supervision for identifying and down-weighting unreliable external information. For questions that the LLM", "already answers correctly in a closed-book setting, we construct conflict scenarios by pairing the question with a hard negative passage C neg , which is topically relevant but does not contain the correct answer. The context assessor is trained to reflect low reliability for such passages in its memory embeddings, effectively learning to recognize when external context contradicts parametric knowledge. As a result, misleading information has less influence on the generation, and the LLM relies more on its internal knowledge.", "Model Size: Large vs. Mid-size\n\nTo study the role of scale, we compare: (1) Qwen3-235B-A22B (Yang et al. 2025), the flagship model with 235 billion total parameters and 22 billion active, optimized for long-context reasoning, and (2) Qwen3-30B-A3B (Yang et al. 2025), a 30B-parameter variant with 3B active, designed for faster inference and reduced hardware costs.", "Figure 4: Overall performance of different systems across regions. (Subject to aesthetic changes.)\n\nAnswer Quality Faithfulness Recall@30 0.0 0.2 0.4 0.6 0.8 1.0 1.2 Score 69.6% N/A 68.2% 97.0% 9.5% 88.0% 96.9% 93.2% Overall Performance General LLMs Vanilla RAG GridCodex\n\n| | Model | Answer Quality | Faithfulness |\n|---:|:----------|-----------------:|---------------:|\n| 0 | 235B-A22B | 0.852 | 0.968 |\n| 1 | 30B-A3B | 0.74 | 0.995 |\n\nTable 3: Model size impact (Netherlands dataset).", "As shown in Table 3, scaling from 30B to 235B parameters improves answer quality by 11.2%. The larger model demonstrates stronger multi-hop reasoning, cross-clause referencing, and ambiguity resolution due to greater representational capacity and expert routing. However, this gain comes at a cost: faithfulness drops slightly (0.968 vs. 0.995), as the 235B model integrates retrieved context more aggressively, occasionally introducing inferred connections not explicitly present in the documents. This highlights a key trade-off-larger models enable richer reasoning but may lean toward speculative integration, whereas smaller models remain more conservative and strictly grounded in retrieved evidence.", "4 Results\n\n4.2 RQ2: To what extent does the type of retrieved context influence robustness?\n\nOpen-source models follow the same pattern. As shown in Table 4 for TREC 2021, the alignment rate of DeepSeekR1-Distill-Qwen-32B improves from 63.0% in the Non-RAG baseline to 92.6% with helpful documents for neutral user queries, but collapses to just 1.3% under Liar inputs. A similar trend appears for Phi-4, which rises from 66.7% in the Non-RAG baseline to 96.5% with helpful evidence, but drops sharply to 2.2% with Liar documents. LLaMA-3 8B Instruct shows the same pattern, moving from 81.6% without context to 93.9% with helpful inputs, before falling to just 0.4% under Liar documents. Even weaker adversarial documents, such as Rewriter and Paraphraser, substantially reduce alignment rate, often into the 55-70% range across all open-source models. The same pattern is observed for TREC 2020 as well, but with consistently higher rates, as detailed in the following paragraphs. We emphasize neutral user query results here because RQ1-RQ3 focus primarily on the effect of retrieved context, while the role of query framing is analyzed separately in Section 4.4.", "4.2 RQ2: To what extent does the type of retrieved context influence robustness?\n\nAcross both datasets and model families, the relative effectiveness of adversarial strategies follows a consistent ordering: Liar \u226b FSAP-IntraQ > FSAP-InterQ > Fact Inversion > Paraphraser \u2248 Rewriter . This hierarchy is a result of how adversarial documents are made. Liar documents are constructed from scratch based on the query and an incorrect stance, so they are not constrained to the wording or rhetorical framing of pre-existing harmful content. This independence enables more fluent phrasing and broader topic coverage, making misinformation more confident and persuasive, and thus more challenging for LLMs. In contrast, Paraphraser and Rewriter documents inherit limitations from pre-existing harmful material, which can make them appear less natural.", "4.2 RQ2: To what extent does the type of retrieved context influence robustness?\n\nIn cross-model comparison, GPT-4.1 and GPT-5 achieve the highest levels of alignment rate when paired with helpful documents; however, both demonstrate comparable vulnerabilities when confronted with adversarial evidence. Claude-3.5-Haiku has significant performance drops in the presence of harmful or adversarial content; on the other hand, DeepSeek-R1-Distill-Qwen-32B performs well and often obtains levels that are similar to closed-source models when provided with helpful documents. Phi-4 and LLaMA-3 8B Instruct achieves a lower overall alignment rate but follows the same qualitative trends. Since these patterns are consistent across all models, we selected GPT-4.1 as the representative model for subsequent experiments due to its relatively high performance and speed. GPT-5 was not released at the time these experiments were first conducted and was included afterwards.", "4.2 RQ2: To what extent does the type of retrieved context influence robustness?\n\nAnother important finding of this experiment is the difference at the dataset level, where models consistently display a higher alignment rate on TREC 2020 (COVID-19 misinformation) than on TREC 2021 (general health misinformation). For example, TREC 2021 includes queries about misinformation on everyday remedies and niche treatments, such as using duct tape for warts and toothpaste for pimples, which rarely reach the same level of wide public discourse, although the ground truth for these questions is based on evidence. This underscores an unexpected outcome from our research, suggesting that LLMs appear considerably more robust against adversarial manipulation in the COVID-19 context. This difference may be due to post-training alignment placing greater emphasis on topics like COVID-19, which was the subject of widespread misinformation and public attention, while giving limited focus to less prominent areas of health.", "4.2 RQ2: To what extent does the type of retrieved context influence robustness?\n\nOverall, our results address RQ2 by showing that the type and quality of retrieved context substantially affect LLM robustness, by demonstrating that helpful documents consistently improve alignment rate relative to the baseline, whereas harmful and adversarial documents undermine stability and reliability, in some cases reducing robustness to the point of complete failure.", "4.2 RQ2: To what extent does the type of retrieved context influence robustness?\n\nWhile it may appear self-evident that helpful evidence would improve alignment rate and harmful or adversarial evidence would reduce it, our results move beyond speculation by empirically quantifying both the magnitude and the consistency of these effects across datasets and model families. The measurements establish concrete performance bounds. Alignment rate can approach near-maximum levels by providing a single helpful document, yet it can collapse to almost zero under adversarial documents such as Liar. As outlined in Section 1, this systematic analysis offers the empirical evidence required to turn abstract concerns about adversarial susceptibility into evidence-based design principles for RAG systems. In line with RQ1, the findings highlight that context can significantly shape outcomes and must be safeguarded against harmful or adversarial inputs.", "4.3 RQ3: In what ways do orders and combinations of multiple evidence sources shape model behavior?\n\nFindings from RQ1 and RQ2 show that context type strongly shapes ground-truth alignment, which motivates an assessment of more complex scenarios with multiple documents presented in different orders and compositions, leading to RQ3. To address this question, we examine how models respond when exposed to multiple sources of evidence, focusing on two key dimensions: (i) the order in which helpful and misleading documents are presented, and (ii) the composition of the retrieval pool when evidence is naturally or systematically skewed. We investigate these dimensions through three complementary setups: the paired-document setup, the passage-based pooling setup, and the bias-controlled pooling setup, which are analyzed in the following sections.", "4.3 RQ3: In what ways do orders and combinations of multiple evidence sources shape model behavior?\n\n4.3.1 Paired-Document Analysis. Figure 2 shows ground-truth alignment rate when a helpful document is either first (green, 'Helpful First') or second (blue, 'Helpful Second') relative to a harmful/adversarial document, compared with the single-document baseline (red). The results show that the order of the documents has minimal impact on ground-truth alignment. The Helpful First and Helpful Second conditions deliver nearly the same results, with confidence intervals that mostly overlap. For instance, on TREC 2021 under the consistent user query, the alignment rate of LLM responses rises from only 1.7% (CI 0.4-3.5) in the single-document baseline to 39.1% (CI 33.3-45.0) when helpful evidence appears first and 45.0% (CI 39.1-50.8) when it appears second for Liar inputs. Similarly, for FSAP-IntraQ documents, alignment rate improves from 7.4% (CI 4.4-10.9) to 41.1% (CI 35.3-47.3) when helpful content precedes and 48.8% (CI 43.0-55.0) when it follows the adversarial document. On TREC 2020, robustness is consistently higher. For example, under the neutral user query, for FSAP-IntraQ documents, alignment rate rises from 54.9% (CI 48.2-62.2) to 68.3% (CI 61.9-74.8) when helpful material comes first and 76.7% (CI 70.8-82.7) when it is placed second relative to them. These results show that presenting helpful evidence, regardless of position, minimizes the impact of harmful or adversarial content. Additionally, discrepancies among datasets persist, as TREC 2020 (COVID-19) queries demonstrate greater robustness than the more general health queries in TREC 2021, thereby reinforcing the previously identified dataset-level variations.", "4.3 RQ3: In what ways do orders and combinations of multiple evidence sources shape model behavior?\n\n4.3.2 Passage-Based Pooling Analysis. In the passage-based pooling setup, the MonoT5 reranker provides the top-10 passages for each query, obtained by sliding a 512-word window with 256-word overlap across documents and ranking the resulting segments. Since the adversarial documents are designed to rank highly, these pools are heavily skewed toward this adversarial content, with more than nine of the top-10 segments being adversarial. For TREC 2020, the top-10 pools comprise \u223c 92% adversarial, 5% helpful, and 3% harmful passages, whereas for TREC 2021 the distribution is \u223c 94% adversarial, 5% helpful, and 2% harmful passages. As shown in Fig. 3, the dominance of adversarial segments in the passage-based pool leads to consistently low alignment rates. On TREC 2020, GPT-4.1 reaches 43.7% under consistent user queries, but performance drops to 24.9% with neutral user queries and remains low at 24.9% under inconsistent user queries. The degradation is even worse on TREC 2021, where the alignment rate drops to 17.3% with consistent Manuscript submitted to ACM", "4.3 RQ3: In what ways do orders and combinations of multiple evidence sources shape model behavior?\n\nFig. 2. Results of paired-document setup for GPT-4.1 on TREC 2020 and TREC 2021 with consistent, neutral, and inconsistent user queries. The x-axis shows the document type with which the helpful document is paired (either first or second), and the y-axis reports ground-truth alignment rate (%). Error bars indicate 95% confidence intervals estimated via bootstrapping. Bars compare helpful-first, helpful-second, and single-document inputs (harmful/adversarial only). Helpful placement shows only minor differences, while single-document inputs yield substantially lower ground-truth alignment rates.\n\nHarmful Rewriter Paraphraser Fact Inversion FSAP-InterQ FSAP-IntraQ Liar Ground-Truth Alignment (%) TREC2021 Consistent Query TREC2021 Neutral Query TREC2021 Inconsistent Query TREC2020 Consistent Query TREC2020 Neutral Query TREC2020 Inconsistent Query Helpful First Helpful Second Single Document", "4.3 RQ3: In what ways do orders and combinations of multiple evidence sources shape model behavior?\n\nuser queries, 13.0% with neutral user queries, and only 4.3% with inconsistent user queries. These findings emphasize retrieval as the primary bottleneck in RAG pipelines and demonstrate that the composition of the evidence pool can significantly undermine robustness, irrespective of user query framing.", "1 Introduction\n\nThe core challenge of long-context narrative comprehension lies not merely in connecting discrete pieces of evidence, a task more naturally defined as multi-hop Question Answering (QA), but in performing a dynamic cognitive synthesis to grasp necessary background and content progression (Xu et al. 2024a). Unlike multi-hop QA (Yang et al. 2018), which seeks a static path through fixed facts, narrative comprehension requires emulating a human reader: continuously building and revising a global mental model of the\n\n* These authors contributed equally.\n\n\u2020 Project lead. Correspondence to: < liyanlxu@tencent.com >\n\nFigure 1: Comparison of RAG reasoning paradigms.\n\n(c) ComoRAG\n(b) Multi-step RAG\nWhy Snape Kill\n\nDumbledore Albus?\n\nStateless Reasoning Stateful Reasoning Fragmented Evidence Lack of Fusion Memory-Organized Stateful Comprehension Contradictory Evidence Motive Unclear\n\nAn act of loyalty\n\nnot betrayal Snape Bully Harry Kills Albus Protect (a) Single-step RAG Shallow/Superficial Understanding\n\nSnape was a loyal\n\nDeath Eater The Half-Blood Prince where Snape Coherent Context Formed Apparent Contradiction Causally Incomplete Event Unbreakable Vow Context-Grounded Exploratory Probing Dynamic Memory Workspace Knowledge Consolidation Consolida1on New Acquisition Acqulsition", "plot, characters, and their evolving motivations (JohnsonLaird 1983). The complexity of this process is well exemplified by a classic narrative question 'Why did Snape kill Dumbledore?' from the Harry Potter series. Answering this requires weaving a complete web of evidence from disparate clues spanning multiple books-Dumbledore's terminal illness, the Unbreakable Vow, and Snape's deeply concealed loyalty. The true significance of these clues is only fully reconciled in hindsight. This capability is what we term stateful reasoning : it demands more than linking static evidence; it requires maintaining a dynamic memory of the narrative, one that is constantly updated as new revelations emerge. Long-context LLMs have demonstrated promising performance on benchmarks such as the 'Needle in a Haystack' test (Eisenschlos, Yogatama, and Al-Rfou 2023) in recent years. However, their capacity to process long narratives (200k+ tokens) remains limited by finite context windows. Furthermore, as the input length increases, these models are prone to the 'lost in the middle' problem (Liu et al. 2024), which raises perplexity and impairs generation quality. This limitation is particularly pronounced in narrative compre- hension tasks which require stateful reasoning. As a result, retrieval-augmented generation (RAG) (Lewis et al. 2020) has emerged as an important strategy for tackling long context comprehension with LLMs.", "However, existing RAG methods still struggle to effectively address this challenge. Advanced single-step retrieval remains limited by its static index. This includes methods such as RAPTOR (Sarthi et al. 2024), which clusters and summarizes text chunks to retrieve at different levels of details; HippoRAGv2 (Guti\u00b4 errez et al. 2025), which mimics the human hippocampus by building a knowledge graph to achieve multi-hop reasoning in a single retrieval step. Nonetheless, single-step methods rely on one-shot static retrieval, which may lead to shallow comprehension. For example, in Figure 1(a), the evidence about Snape can mislead the model into making a false inference.", "As a remedy, multi-step retrieval methods offer a more promising direction, such as IRCoT (Trivedi et al. 2023), which interleaves the retrieval process with Chain-ofThought reasoning (Wei et al. 2022); Self-RAG (Asai et al. 2024), which trains a model to adaptively retrieve and reflect on evidence; and MemoRAG (Qian et al. 2025), which uses a dual-system architecture to generate clues from compressed global context. These methods all target to obtain richer context through iterative retrieval. However, their retrieval steps are typically independent, which lack coherent reasoning throughout explicit narrative progression, featuring fragmented evidence with a stateless comprehension. As illustrated in Figure 1(b), due to a lack of dynamic memory, multi-step retrieval fails to integrate contradictory evidence such as 'Snape protects/bullies Harry' and cannot understand the evolution of his actions, ultimately unable to yield the correct answer.", "In this work, we seek inspiration from the function of Prefrontal Cortex (PFC) in human brains, which employs a sophisticated reasoning process called Metacognitive Regulation (Fernandez-Duque, Baird, and Posner 2000). This process is not a single action but a dynamic interplay between new evidence acquisition , driven by goal-directed memory probes (Dobbins and Han 2006; Miller and Constantinidis 2024), and subsequent knowledge consolidation . During consolidation, new findings are integrated with past information to construct an evolving, coherent narrative. This iterative cycle allows the PFC to continuously assess its understanding and revise its strategy, providing a direct cognitive blueprint for our framework's stateful reasoning approach.", "We introduce ComoRAG, a cognitive-inspired, memoryorganized RAG framework, imitating the human Prefrontal Cortex (PFC) for achieving true stateful reasoning. At its core is a dynamic cognitive loop operating on a memory workspace, which actively probes and integrates new evidence to build a coherent narrative comprehension.", "This process, as illustrated in Figure 1(c), is a closed loop of evolving reasoning states. Faced with a complex query like 'Why did Snape kill Dumbledore?' , the system's memory state evolves from an initial 'causally incomplete event' ( Snape kills Albus ), to an 'apparent contradiction' upon finding contradictory information ( Snape protects Harry ), and ultimately to a logically consistent coherent context through deeper exploration and evidence fusion. Only in this final, complete cognitive state can ComoRAG perform the correct stateful reasoning, deriving the profound insight that it was 'an act of loyalty, not betrayal' .", "This cognitively-inspired design yields substantial improvements across four challenging long-context narrative benchmarks. ComoRAG is shown to consistently outperform all categories of strong baselines across each dataset. Our analysis reveals several key findings. First, these gains stem directly from the cognitive loop, which transforms a static knowledge base into a dynamic reasoning engine; for instance, accuracy on EN.MC jumps from a static-retrieval baseline of 64.6% to 72.9%, with performance efficiently converging in around 2-3 cycles. Second, our framework excels on narrative queries that require global understanding of plot progression, achieving up to a 19% relative F1 improvement on these challenging question types where others falter. Finally, our framework demonstrates remarkable modularity and generalizability. Its core loop can be flexibly integrated to existing RAG methods such as RAPTOR, which directly yields a 21% relative accuracy gain). Also, switching to a stronger model as the backbone LLM agents can upgrade reasoning in the entire cognitive loop, attaining accuracy from 72.93% to 78.17%. These results collectively validate that ComoRAG provides a principled, cognitivelyinspired new paradigm for retrieval-based long narrative comprehension towards stateful reasoning."]}
{"type": "entry", "index": 62, "question": "how do i benchmark prompts that force the model to list contradictions explicitly", "answer": "To benchmark prompts that force a model to explicitly list contradictions, you can adapt the setup used with the MAGIC benchmark, which is specifically built to test conflict detection and localization.\n\n1. Use a conflict-rich dataset\nMAGIC provides pairs of textual contexts that contain either single-hop or multi-hop contradictions, and may have one or multiple conflicting facts between the two contexts [1]. It is designed to probe whether models can notice conflicts and identify exactly where they occur [1].\n\n2. Design prompts that demand explicit contradiction listing\nInstead of simple yes/no classification, follow MAGIC\u2019s \u201cstepwise prompting\u201d idea: instruct the model to (a) read both contexts, (b) state whether any conflicts exist, and (c) list each contradiction explicitly, including which statements clash [1]. You can further strengthen prompts using components that ask the model to carefully check the provided context for false information and to use its own background knowledge, similar to PC2 and PC3 used to activate self-checking in RAG systems [2].\n\n3. Define evaluation metrics\nMAGIC introduces two human-judged metrics you can reuse:\n- Identification (ID): did the model detect that at least one conflict exists? Scored 1 if yes (in any of multiple runs), 0 otherwise [1].\n- Localization (LOC): did the model correctly pinpoint all conflicting locations? Scored 1 only if all listed contradictions match the gold conflicts and none are missed [1].\n\n4. Run multiple inference trials\nBecause outputs are stochastic, MAGIC runs each model three times per example and averages ID/LOC scores [1]. You can follow the same protocol for each prompt variant, then compare scores to see which prompt best elicits explicit, accurate contradiction listings.", "tags": ["benchmarks_eval"], "full_faithfulness": 0.9583333333, "full_answer_relevancy": 0.9280000421000001, "seed_faithfulness": 0.5416666667, "seed_answer_relevancy": 0.9386213308, "context_relevance": 0.8196797538230159, "context_relevance_error": null, "context_relevance_reason": "The context passages provide substantial information about benchmarking prompts for identifying contradictions, particularly through the MAGIC framework. They discuss various conflict types, prompting strategies, and the challenges faced by LLMs in detecting contradictions, which directly relates to the user's question. However, while the passages are relevant, they could be more explicitly focused on the specific methods for benchmarking prompts, which slightly limits the overall alignment.", "structure_citation": 0.6923325359412305, "structure_error": null, "structure_reason": "The Actual Output effectively addresses the input question by providing a structured approach to benchmarking prompts for contradiction detection, referencing the MAGIC benchmark. It includes multiple claims supported by citations [1] and [2], which correspond to relevant snippets in the Context. However, while most claims are well-supported, there are minor inconsistencies in the citation usage, as some claims could benefit from additional context or clearer connections to the cited snippets, leading to a slight deduction in the score.", "hallucination_score": 0.8129257280805391, "hallucination_error": null, "hallucination_reason": "The response effectively outlines a method for benchmarking prompts that require explicit contradiction listing, aligning well with the context provided. It accurately references the MAGIC benchmark and its components, such as the use of conflict-rich datasets and evaluation metrics. However, while the claims are mostly supported, the response could benefit from a clearer connection to the specific limitations of existing benchmarks mentioned in the context, which would strengthen its overall alignment.", "contexts_used": ["Abstract\n\nKnowledge conflict often arises in retrievalaugmented generation (RAG) systems, where retrieved documents may be inconsistent with one another or contradict the model's parametric knowledge. Existing benchmarks for investigating the phenomenon have notable limitations, including a narrow focus on the question answering setup, heavy reliance on entity substitution techniques, and a restricted range of conflict types. To address these issues, we propose a knowledge graph (KG)-based framework that generates varied and subtle conflicts between two similar yet distinct contexts, while ensuring interpretability through the explicit relational structure of KGs. Experimental results on our benchmark, MAGIC, provide intriguing insights into the inner workings of LLMs regarding knowledge conflict: both open-source and proprietary models struggle with conflict detection-especially when multi-hop reasoning is required-and often fail to pinpoint the exact source of contradictions. Finally, we present in-depth analyses that serve as a foundation for improving LLMs in integrating diverse, sometimes even conflicting, information.", "3 MAGIC: Multi-Hop and Graph-Based Benchmark for Inter-Context Conflicts\n\n3.2 Knowledge Conflict Generation\n\nCategory of conflicts As illustrated in Figure 3, prior benchmarks, i.e., ECON (Jiayang et al., 2024) and WikiContradict (Hou et al., 2024), primarily target simple 1-Single-Hop conflicts (Figure 4(a)) arising from individual facts. However, real-world discrepancies often involve multi-hop reasoning and multiple conflicts. To bridge this gap, we define eight distinct conflict types along two axes: (1) the number of reasoning hops required ( Single-Hop vs. Multi-Hop ) and (2) the number of conflicts present across the two contexts ( 1 vs. N ).", "3 MAGIC: Multi-Hop and Graph-Based Benchmark for Inter-Context Conflicts\n\n7 Beyond rule-based substitutions at the entity or relation level as commonly explored in prior work, we aim to induce more natural and contextually coherent conflicts using LLMs.", "3 MAGIC: Multi-Hop and Graph-Based Benchmark for Inter-Context Conflicts\n\n| | Conflict Type.# Conflict | Single-Hop.1 | Single-Hop.2 | Single-Hop.3 | Multi-Hop.4 | Multi-Hop.2 | Multi-Hop.3 | Multi-Hop.4 | Total. |\n|---:|:---------------------------|---------------:|---------------:|---------------:|--------------:|--------------:|--------------:|--------------:|:---------|\n| 0 | # Instances | 208 | 154 | 80 | 50 | 158 | 80 | 50 | 1,080 |\n\nTable 1: MAGIC dataset statistics by conflict category.", "3 MAGIC: Multi-Hop and Graph-Based Benchmark for Inter-Context Conflicts\n\nSingle-Hop conflicts (Figure 4(a), (b)) arise from inconsistencies through a single relation.\n\nMulti-Hop conflicts (Figure 4(c), (d)) require reasoning over multiple connected triplets.", "3 MAGIC: Multi-Hop and Graph-Based Benchmark for Inter-Context Conflicts\n\nEach conflict type is further divided into 1-conflict and N-conflict cases, based on the number of contradictions observed between two given contexts, yielding four distinct types illustrated in Figure 4. The number of data instances allocated to each category is shown in Table 1. This categorization enables fine-grained evaluation across varying reasoning depths and conflict complexities.", "3 MAGIC: Multi-Hop and Graph-Based Benchmark for Inter-Context Conflicts\n\nSubgraph-level few-shot prompting We use OpenAI's o3-mini (OpenAI, 2025) to induce and collect conflict candidates. Given a target seed triplet, we prompt the LLM with both the triplet and its surrounding context, represented as a set of subject-relation-object triplets from the subgraph. Including the surrounding subgraph allows the model to generate natural contradictions that remain consistent with the local context. 8", "3 MAGIC: Multi-Hop and Graph-Based Benchmark for Inter-Context Conflicts\n\nStill, we observe that prompting without task demonstrations (i.e., real examples of knowledge conflicts) often falls short in generating diverse and logically complex conflicts, particularly in multihop scenarios. 9 As a solution, we adopt a fewshot prompting strategy, where the prompt includes three validated conflict examples per relation type. This encourages the model to move beyond simple entity or relation swaps. The final prompt template used in this process is shown in Figure 5. 10 For N-conflict cases, the same graph is reused with different perturbations until the desired number of conflicts is achieved.", "3 MAGIC: Multi-Hop and Graph-Based Benchmark for Inter-Context Conflicts\n\nQuality control via human feedback Since LLMs are not inherently good at detecting knowledge conflicts, it may seem paradoxical to ask them\n\n8 Appendix F reveals that using only the target triplet often produced contextually misaligned conflicts.", "3 MAGIC: Multi-Hop and Graph-Based Benchmark for Inter-Context Conflicts\n\n9 The model tends to repeat patterns, struggling to produce varied conflicts and often misaligning with common sense.\n\n10 While designed for multi-hop conflicts, the template can be readily adapted to single-hop settings.", "4 Experimental Setups\n\nWith the MAGIC benchmark as a foundation, we conduct experiments to examine how LLMs handle inter-context knowledge conflicts. We evaluate various open-source and proprietary LLMs without task-specific training, instead prompting them to identify potential contradictions. In the following, we outline the LLMs, datasets, prompting strategies, and metrics used in our experiments.", "LLMs We use 5 LLMs: Llama 3.1 70B Instruct (Dubey et al., 2024), o1 (OpenAI, 2024b), Mixtral8x7B Instruct (MistralAI, 2023), Claude 3.5 Haiku (Anthropic, 2024), GPT-4o-mini (OpenAI, 2024a).", "Datasets Alongside MAGIC, we employ two existing benchmarks to highlight its strengths.\n\nECON (Jiayang et al., 2024): A dataset created by introducing evidence conflicts through two methods-answer conflicts and factoid conflicts-highlighting contradictions in supporting evidence. It contains 168 data instances.", "WikiContradict (Hou et al., 2024): A humanannotated QA benchmark utilizing Wikipedia's contradiction tags to capture real-world knowledge conflicts. It categorizes contradictions into explicit and implicit types. After deduplication, it comprises 103 data samples.", "MAGIC : It is constructed atop KGs, where conflicts are systematically induced from the un-", "3 MAGIC: Multi-Hop and Graph-Based Benchmark for Inter-Context Conflicts\n\n3.3 KG-to-Text Conversion\n\nTo represent knowledge conflicts from graphs in natural language, we apply KG-to-text conversion, following the approach of Kasner and Dusek (2024) with modifications. Using the prompts shown in Figure 14, GPT-4o-mini (OpenAI, 2024a) 12 generates coherent textual contexts while preserving the semantics of the original graph. To ensure transformation accuracy, we perform automatic verification\n\n11 Two researchers independently reviewed disjoint subsets using a shared guideline. See Appendix G for full details.\n\n12 Specific version: gpt-4o-mini-2024-07-18.", "3 MAGIC: Multi-Hop and Graph-Based Benchmark for Inter-Context Conflicts\n\nFigure 6: Two-stage human-in-the-loop pipeline for data quality control.\n\nGenerate conflict demonstrations Filter out conflict Conflict Filter out conflicts MAGIC LLM Human\n\n:\n\n\u2026 \ud835\udc41 candidates for few-shot prompting\n\nusing Claude 3.7 Sonnet (Anthropic, 2025), with the prompt shown in Figure 15.", "| | Models / Datasets | ECON | WikiContradict | MAGIC |\n|---:|:--------------------|-------:|-----------------:|--------:|\n| 0 | Mixtral 8x7B | 46.43 | 52.43 | 37.92 |\n| 1 | Llama 3.1 70B | 81.41 | 78.79 | 72.86 |\n| 2 | Claude 3.5 Haiku | 83.33 | 61.17 | 60.28 |\n| 3 | GPT-4o-mini | 88.1 | 82.52 | 83.61 |\n| 4 | o1 | 74.4 | 74.76 | 68.06 |\n| 5 | Average | 74.73 | 69.93 | 64.54 |\n\nTable 2: ID scores (%) on three KC detection datasets. Lower scores indicate higher task complexity. The lowest value in each row is shown in bold .", "derlying relational structure. It encompasses both single-hop and multi-hop contradictions, with the number of conflicts dynamically varying across context pairs. In total, MAGIC consists of 1,080 carefully curated examples, with comprehensive statistics presented in Table 1. Remarkably, the scale of MAGIC surpasses that of existing benchmarks by a significant margin, offering a richer and more challenging resource for evaluating inter-context conflict detection.", "Prompting strategy Prior work (Jiayang et al., 2024; Hou et al., 2024) typically frames the task as a binary classification problem, relying on minimal prompts (see Appendix B). In contrast, we introduce a stepwise prompting strategy (refer to Figure 17 for details) designed to more fully probe the capabilities of LLMs for inter-context conflict detection.", "Metrics To account for the stochasticity of LLMs, all models perform three independent inference runs. We use two metrics for fine-grained evaluation, with scores averaged across all instances in each dataset. These metrics are manually computed by participating researchers, as automatic methods-such as LLM-as-a-judge-remain insufficiently reliable for this task. 13\n\nIdentification ( ID ) score: If a model fails to detect a conflict in any of the three attempts, it receives a score of 0; otherwise, it receives 1.", "Localization ( LOC ) score: We further evaluate LLMs' performance on conflict localization. A full score (1) is awarded only if all conflicting locations are correctly identified; otherwise, the score is 0. Note that this fine-grained evaluation has not been considered in previous work.", "13 If they were, further investigation into knowledge conflict detection would be unnecessary. See Appendix H for details.", "|---:|:--------------------|-------:|-----------------:|--------:|\n| 0 | Mixtral 8x7B | 35.71 | 40.78 | 17.4 |\n| 1 | Llama 3.1 70B | 54.49 | 51.52 | 37.92 |\n| 2 | Claude 3.5 Haiku | 66.07 | 52.43 | 42.5 |\n| 3 | GPT-4o-mini | 63.69 | 68.93 | 55 |\n| 4 | o1 | 64.88 | 65.48 | 49.72 |\n| 5 | Average | 57.09 | 55.74 | 40.51 |\n\nTable 3: LOC scores (%) on three KC detection datasets. Lower scores indicate higher complexity. We observe that models struggle more with pinpointing the exact location of a conflict than with detecting its presence. The lowest value in each row is shown in bold .", "Prompt Template\n\nYou are a helpful assistant, below is a query from a user and some relevant contexts. Answer the question given the information in those contexts. [ auxiliary system prompt ]\n\nContexts: [context]\nQuery: [question]\n\nAnswer:\n\nIn addition to the instruction for answer generation, we provide the LLM with an auxiliary system prompt to activate its SCA. We review and summarize widely adopted prompt engineering strategies from both research papers [43], [35], [49] and popular Github repositories [3], [4], and further derive three representative templates. We instantiate each template in Table I.", "Specifically, prior work has shown that constraining response length can mitigate hallucination [43], [75]. Accordingly, we derive PC1 to restrict output verbosity. Likewise, explicitly prompting models to verify retrieved content is a common strategy for improving factual consistency [35], [76]. Therefore, PC2 instructs the model to examine the retrieved context for potential falsehoods. Finally, encouraging reliance on real-world knowledge promotes generations that emphasize", "| | ID | Prompt Component |\n|---:|:-----|:---------------------------------------------------------------------------------------------------|\n| 0 | PC1 | Your answer should be short and concise. |\n| 1 | PC2 | But you need to check the provided context carefully for correcting potentially false information. |\n| 2 | PC3 | Also, you need to answer the question using real-world knowledge. |\n\nTABLE I AUXILIARY PROMPT COMPONENTS FOR EVALUATING SCA.\n\nTABLE II\n\nASR UNDER DIFFERENT COMBINATIONS OF DESIGNED PROMPT COMPONENTS ACROSS VARIOUS ATTACK METHODS AND LLMS. EACH OR ABSENCE (-) OF EACH COMPONENT. HERE, (+/-/-) INDICATES THE", "| | Method | Model | +/-/- | -/+/+ | -/+/- | -/-/+ | Recall |\n|---:|:-----------------|:---------------------------------|:------------|:------------|:------------|:------------|:---------|\n| 0 | Prompt Injection | Deepseek R1 GPT-4o Mini Qwen-Max | 65% 73% 68% | 19% 39% 16% | 38% 53% 25% | 23% 73% 41% | 80% |\n| 1 | Disinformation | Deepseek R1 GPT-4o Mini Qwen-Max | 51% 67% 59% | 10% 31% 17% | 25% 41% 19% | 21% 48% 34% | 48% |\n| 2 | Gaslite | Deepseek R1 GPT-4o Mini Qwen-Max | 81% 90% 86% | 21% 47% 26% | 29% 57% 38% | 34% 64% 40% | 100% |\n| 3 | PoisonedRAG (B) | Deepseek R1 GPT-4o Mini Qwen-Max | 81% 91% 89% | 24% 44% 27% | 51% 61% 39% | 38% 64% 46% | 96% |\n| 4 | PoisonedRAG (W) | Deepseek R1 GPT-4o Mini Qwen-Max | 80% 87% 82% | 27% 33% 19% | 44% 43% 25% | 31% 56% 34% | 100% |\n\nPROMPT IS ANNOTATED WITH A TRIPLET INDICATING THE PRESENCE (+) WEAKEST SCA, WHILE (-/+/+) INDICATES THE STRONGEST SCA.", "factual grounding and external validation [49]. We thus derive PC3 to instruct the model to incorporate its pretrained knowledge rather than blindly trusting retrieved content. Together, these components serve as building blocks for constructing prompts that activate the SCA. By combining different subsets of PC1-PC3, we derive multiple prompt configurations and evaluate their robustness under attack.", "We proceed to evaluate these configurations under five representative poisoning attack methods: Prompt Injection [45], [60], [22], Disinformation [17], [58], GASLITE [6], PoisonRAG (W) , and PoisonRAG (B) [84]. We evaluate the effectiveness of each attack method using the attack success rate (ASR), defined as the proportion of cases in which the LLM outputs only the attacker-specified answer while failing to provide the correct answer. We also report the recall rate of the malicious context in all retrieved results.", "Table II reports the ASR of different methods across LLMs under selected prompt configurations. We leave the full results in Table XV of Appendix K. When only constraining the output format (denoted as +/-/-), all attack methods achieve the highest ASR. For instance, PoisonedRAG (W) exceeds 74% ASR across all models, peaking at 91% on GPT-4o Mini. Even the disinformation attack, which only retrieves 48% of malicious contexts, still reaches 67% ASR on GPT4o Mini. In contrast, incorporating prompts promoting context verification and real-world knowledge checks (PC2 and PC3) dramatically reduces vulnerability. For GPT-4o Mini, the ASR of PoisonedRAG variants drops from 91% and 87% to 44% and 33%. Notably, PoisonedRAG (W) almost fails on QwenMax and Deepseek R1 under such a configuration, with ASR\n\nfalling to only 19% and 27%.", "These results demonstrate that LLMs' SCA is highly sensitive to prompt design, and enables effective rejection of misinformation when properly activated. The strongest gains occur when output generation is not overly constrained, and when explicit instructions guide the model to verify retrieved content and leverage its pretrained knowledge. We summarize the key finding as follow:\n\nOur Finding: Prompt configuration critically shapes LLM's SCA against misinformation.", "A. Error Correction Mechanisms\n\nFig. 15: The workflow of RAG-based knowledge error mitigation\n\nDescription Erroneous RTL Knowledge Base Error Message Keyword Matching Search Vector Similarity Keyword-Specific Entry Function- Approximate RTL RTL Code LLM\n\nEntry:\n\n[keyword]:\n\n['dual\u2026']\n\n[Description]:\n\nThe design\u2026\n\n[Example]:\n\nmodule \u2026 RTL Programming Knowledge Entry Circuit Design\n\nFig. 16: An example of circuit design knowledge entry\n\n[Keyword]: [\"dual-edge trigger\"] [Description]: This design is a dual-edge triggered flip-flop is triggered on both edges of the clock that captures the input signal `d` on both the rising and falling edges of the clock signal `clk`. ... [Example]: module TopModule (input clk, input d, output reg q); reg qp, qn; always @(posedge clk) qp <= d; always @(negedge clk) qn <= d; always @(*) q <= clk ? qp : qn; endmodule\n\nRAG-based Knowledge Error Mitigation: We leverage RAG to supplement missing knowledge in both RTL programming and circuit design, and enhance the RTL code generation quality. To this end, we construct a domain-specific knowledge base whose entries each consist of a keyword, a concise description, and an illustrative example. Circuit design knowledge is sourced from GitHub and StackOverflow and organized into four categories-arithmetic, memory, control, and miscellaneous modules-to enable efficient lookup (see Fig. 16).", "Our retrieval engine supports both semantic and keyword-based queries. In semantic search, user design descriptions are embedded and matched against the knowledge-base vectors to retrieve the most relevant entries. For clearly defined RTL terms, keywords extracted from design specifications or compiler error messages are employed to retrieve corresponding knowledge entry via regular-expression matching. Ultimately, the retrieved information is deduplicated and integrated into the LLM's context to facilitate RTL generation, thereby mitigating programming errors caused by insufficient knowledge.\n\nFig. 17: An example of rule-based description refinement\n\nExplicit Design Description\n\uff1a\n\nImplement a D latch using an always block.\n\nmodule TopModule (input d, ena,\n\noutput logic q);\n\n// [Constraint] Registers/Logic initialization to 0\n\n//[Constraint] Enable high-level trigger\n\nRule-Guided Design Description Refinement\n\nCheck the user's RTL design specification against the following rules and refine if necessary\n\n[Rules]:\n\n1. Check if register initialization is specified. If not specified, supplement the design\n\nspecification with default initialization to 0.", "2. Check if the reset signal configuration is specified. If not specified, supplement the design\n\nspecification with a default high-level asynchronous trigger reset.\n\n...\n\nAmbigious Design Description\nCorrect RTL code\n\ninitial q = 0;\n\nalways @(*) begin\n\nif (ena) q<= d;\n\nend\nErroneous RTL code\n\nalways @(posedge ena) begin\n\nif (ena) q <= d;", "Rule-based Description Refinement: To resolve ambiguities in user-provided specifications, we introduce a rule-guided refinement mechanism that leverages LLMs to enforce key circuit design rules such as register initialization, reset/enable signal integrity, and internal logical consistency. The LLM first parses the initial specification, then checks it against these rules. Whenever it detects ambiguities or contradictions, the model automatically augments and clarifies the description using contextual constraints, thereby eliminating gaps in the design intent. Fig. 17 illustrates a typical case: by explicitly refining the register initialization and enable signal requirements, our mechanism ensures that the subsequent Verilog code generation is both accurate and robust. Similarly, to mitigate relatively common errors arising from LLMs' unfamiliarity with specialized RTL programming, based on the analysis in Section III, we designed and integrated 10 programming rules into the system prompt, such as 'Do not assign values to wiretype variables within always blocks'.\n\n(b) Waveform Example (Prob098_circuit7)\n\n(a) Kmap Example (Prob057_kmap2)\n\nMultimodal Data Meta-Format out= ~c&~d&~a&~b) (~c&~d&~a&b (c&d&~a&b) c&d&a&b ) | (c&~d&~a&b ) time clk always posedge clk) begin pre 0) ) 15ns if((a 1)&8 (9 1) ) 45ns S5ns 60ns if((\n\nFig. 18: Examples of multimodal data conversion", "Multimodal Data Conversion: To mitigate generation errors arising from multimodal design inputs-such as Karnaugh maps, waveform diagrams, and truth tables, we have developed tools that convert these heterogeneous formats into a single, unified truth-table meta-representation as shown in Fig. 18. This meta-representation can then be directly translated into Verilog via LLMs. Although the initially generated circuits may not be fully optimized, they can be automatically streamlined using logic synthesis tools under the specified design constraints. Importantly, when processing waveform", "inputs for sequential circuits, our conversion tool captures the clock triggering semantics to preserve correct timing behavior. Likewise, for state transition diagrams, the intermediate representation explicitly encodes both current and next states in addition to the input and output signals.\n\nFig. 19: An example of two-stage debugging mechanism\n\nSystem Prompt: You are an excelent Verilog programming expert. Based on the given Correct Design Description; Error Design; Fail Test Case Think Step by Step to fix the Possible error statements based on the error reason. The modified code is required to fully comply with the requirements of the Fail Test Case aud the Design Description.\n\nRemember do not modify statements without \"lError!\" comments\n\nDesign Description: Consider the state machine shown below:\n\nAssume that you want to Implement the FSM three flip-flops and state codes y[3:1] = 00o, 001 , 101 for states A, B, F, respectively. Implement just the next-state logic for y[2] in Verilog. The output Y2 is y[2]. using\n\nFail Test Case: Input: y = 3*b100,\n\nWV = 0\nErroneous Output: Y2\n=1\n\nExpected Output: Y2 = 0\n\nErroneous RTL Code:\n\nmodule TopModule (input [3:1] y, input output Mismatch Signal @() case\n\n3 'b000:\n\nY2\nb001\n\nY2 = 1;\n\nUncovered Statement\n\n3 'b010:\n\n3'b011:\n\nY2 =\n\n3 'b100:\n\nIlError!\n\n3'b101:\n\ndefault:\n\nendcase\nendmodule"]}
{"type": "entry", "index": 63, "question": "does retrieval order matter a lot, and how do i make it less brittle", "answer": "Based on the evidence here, the *order* of retrieved items by itself does not seem to matter nearly as much as *what* is in the retrieved set overall.\n\nIn paired-document experiments, putting the helpful document first vs second only caused minor differences in correctness, whereas using only a single (possibly harmful) document caused a big drop in correct answers [1]. What really drives robustness is the *composition* of the pool: when retrieval is biased toward helpful documents, correctness often reaches 90\u2013100%; when it is biased toward harmful ones, performance can collapse to nearly zero, even if some good evidence is present [1]. So the main issue is not fine-grained order but whether harmful vs helpful material dominates.\n\nTo make retrieval less brittle:\n\n1) Improve retrieval quality and coverage so helpful material is present and dominant. Scaling and strengthening retrievers measurably boosts accuracy and recall [2], and higher top\u2011k recall (retrieving more relevant items) can help, though you must watch the trade\u2011off with more noise [2].\n\n2) Use methods that make the model robust to noisy or misleading passages. Robust training approaches such as RetRobust teach the model to downweight irrelevant or distracting context [5]. Passage Injection explicitly weaves retrieved passages into the model\u2019s reasoning so it can recognize and resist both irrelevant and misleading (counterfactual) passages, improving robustness under noisy retrieval [4].\n\n3) Add mechanisms that assess context quality or necessity at test time. Conflict-aware and decoding-based methods reweight or ignore unreliable documents [5]. Corpus\u2011feedback style signals (e.g., detecting low confidence or hallucination) can help reduce unnecessary or harmful retrieval rounds, though they may still allow some noise through [3].\n\nOverall: focus on getting a helpful\u2011dominated pool and making the model explicitly reason *about* the evidence it sees, rather than relying on retrieval order.", "tags": ["_untagged"], "full_faithfulness": 1.0, "full_answer_relevancy": 0.0, "seed_faithfulness": 0.6315789474, "seed_answer_relevancy": 0.0, "context_relevance": 0.5858143772821489, "context_relevance_error": null, "context_relevance_reason": "The context passages provide substantial information regarding the impact of retrieval order on model behavior, directly addressing the user's question about whether retrieval order matters and how to make it less brittle. The passages discuss the effects of evidence composition and retrieval biases, highlighting how helpful documents can enhance alignment rates, which relates to making the system less brittle. However, while the context is relevant, it could be more explicitly connected to practical strategies for improving retrieval robustness, which slightly limits the score.", "structure_citation": 0.7486330277344873, "structure_error": null, "structure_reason": "The Actual Output effectively addresses the question about retrieval order and its impact on robustness, providing a clear summary of findings supported by citations. It includes claims with [1] and [2] markers that correspond to relevant snippets in the Context, demonstrating a good understanding of the evidence. However, while the claims are generally consistent with the cited snippets, there are minor inconsistencies in the depth of explanation regarding the implications of retrieval order, which could have been better supported by additional context details.", "hallucination_score": 0.8647874753406029, "hallucination_error": null, "hallucination_reason": "The Actual Output effectively addresses the Input's question about the significance of retrieval order and how to enhance robustness in retrieval systems. It supports the claim that the order of retrieved items is less critical than the overall composition of the retrieved set, which aligns with the context provided. The response offers practical strategies to improve retrieval quality and robustness, directly answering the user's query. The only minor shortcoming is a lack of explicit mention of the specific retrieval order's impact, which could have strengthened the response further.", "contexts_used": ["4 Results\n\n4.3 RQ3: In what ways do orders and combinations of multiple evidence sources shape model behavior?\n\nFig. 2. Results of paired-document setup for GPT-4.1 on TREC 2020 and TREC 2021 with consistent, neutral, and inconsistent user queries. The x-axis shows the document type with which the helpful document is paired (either first or second), and the y-axis reports ground-truth alignment rate (%). Error bars indicate 95% confidence intervals estimated via bootstrapping. Bars compare helpful-first, helpful-second, and single-document inputs (harmful/adversarial only). Helpful placement shows only minor differences, while single-document inputs yield substantially lower ground-truth alignment rates.\n\nHarmful Rewriter Paraphraser Fact Inversion FSAP-InterQ FSAP-IntraQ Liar Ground-Truth Alignment (%) TREC2021 Consistent Query TREC2021 Neutral Query TREC2021 Inconsistent Query TREC2020 Consistent Query TREC2020 Neutral Query TREC2020 Inconsistent Query Helpful First Helpful Second Single Document", "4.3 RQ3: In what ways do orders and combinations of multiple evidence sources shape model behavior?\n\nuser queries, 13.0% with neutral user queries, and only 4.3% with inconsistent user queries. These findings emphasize retrieval as the primary bottleneck in RAG pipelines and demonstrate that the composition of the evidence pool can significantly undermine robustness, irrespective of user query framing.", "4.3 RQ3: In what ways do orders and combinations of multiple evidence sources shape model behavior?\n\n4.3.3 Bias-Controlled Pooling Analysis. To examine the impact of systematic skew in retrieval, we construct pools biased toward either helpful or harmful content. In the condition biased toward harmful, the ground-truth alignment rate declines markedly. On TREC 2021, as shown in Fig. 3, GPT-4.1 achieves only a 26.04% alignment rate when the pool is biased toward harmful content with Fact Inversion documents, and 4.35% when biased with Liar documents, even under consistent user queries. The situation worsens under inconsistent user queries, where providing Liar examples leads to a 0% alignment rate, and presenting FSAP-IntraQ documents collapses the rate to 0% as well. By contrast, TREC 2020 demonstrates somewhat greater resilience. Although alignment rate against Rewriter documents remains as high as 93.76% and Paraphraser at 87.52% under consistent user queries, it still collapses for the most adversarial harmful cases, dropping to 24.95% for Liar and 37.4% for FSAP-IntraQ documents.", "4.3 RQ3: In what ways do orders and combinations of multiple evidence sources shape model behavior?\n\nIn the biased-toward-helpful condition, alignment levels are close to the best possible. On both datasets, consistent and neutral user queries frequently sustain a 100% alignment rate, with LLM responses across all attack categories in TREC 2021 reaching a full alignment rate when helpful documents are included in the pool. Even inconsistent user queries remain robust: in TREC 2020, pooled conditions containing Rewriter or FSAP-InterQ documents yield a 100%\n\nManuscript submitted to ACM", "4.3 RQ3: In what ways do orders and combinations of multiple evidence sources shape model behavior?\n\nFig. 3. Results of passage-based pooling and biased-controlled pooling setups for GPT-4.1 on TREC 2020 and TREC 2021 across consistent, neutral, and inconsistent user queries. The x-axis includes the passage-based pooling baseline as well as the different document types used in biased conditions, where helpful documents are paired with harmful or adversarial ones. The y-axis reports ground-truth alignment rate (%). Error bars indicate 95% confidence intervals estimated via bootstrapping. Results show helpful-biased pools yield near-perfect alignment, whereas harmful and passage-based pools sharply reduce it.\n\nPassage-Based\n\nTREC 2021 Consistent Query\n\nTREC 2021 Neutral Query\n\nTREC 2021 Inconsistent Query\n\nTREC 2020 Consistent Query\n\nTREC 2020 Neutral Query\n\nTREC 2020 Inconsistent Query\n\nBiased to Helpful\nBiased to Harmful\nPassage-Based Pooling", "4.3 RQ3: In what ways do orders and combinations of multiple evidence sources shape model behavior?\n\nalignment rate, and Fact Inversion pools reach 95.63% on TREC 2021. Once helpful content dominates the retrieval pool, the influence of user query framing diminishes, as supportive context outweighs misleading presuppositions.", "4.3 RQ3: In what ways do orders and combinations of multiple evidence sources shape model behavior?\n\nOverall, the bias-controlled experiments confirm that the composition of retrieved evidence plays a critical role in RAG systems' robustness. Pools dominated by helpful documents consistently lead to high alignment rates, often exceeding 90-100%, while harmful-biased pools drastically reduce reliability, in some cases collapsing alignment rate to nearly zero. These results underscore that the stability of RAG systems is fundamentally dependent on the distribution of their evidence sources, which highlights the retrieval stage as the key vulnerability in pipeline design.", "4.4 RQ4: How do different query framings affect stance alignment?\n\nAfter discussing the significance of evidence type (RQ2) and evidence order/combination (RQ3), we move on to RQ4, which assesses the impact of user query framing (consistent, neutral, and inconsistent) on the results of ground-truth alignment. Prior work has consistently shown that LLM performance is highly sensitive to prompt formulation. Even minor variations in wording, token placement, or tone can lead to substantially different outcomes, and several studies have proposed methods to stabilize predictions under such alterations [32, 42, 49, 50, 57, 63, 65, 75]. Motivated by these results, we revisit our experimental setups with query sensitivity in mind.", "4.4 RQ4: How do different query framings affect stance alignment?\n\nWe analyze user query formulation effects across all four experimental setups: the single-document setup, the paireddocument setup, the passage-based pooling setup, and the bias-controlled pooling setup. Figures 1-3 and Tables 2-5 summarize results across models, datasets, and retrieval conditions.", "4.4 RQ4: How do different query framings affect stance alignment?\n\nFraming has a persistent impact on ground-truth alignment rate, and the order of user query types remains the same across models and datasets (Consistent > Neutral > Inconsistent), which directly addresses RQ4 by showing that the framing of user queries impacts ground-truth alignment outcomes. The same hierarchy emerges in the single-document setup (Fig.1), persists when multiple sources of evidence are paired (Fig.2), and carries over to more passage-based pooling setups (Fig. 3).", "4.4 RQ4: How do different query framings affect stance alignment?\n\nConsistent user queries, those which align with the ground-truth stance of the answer, reliably produce the highest alignment rates by framing the model's reasoning in a direction that agrees with available evidence; on the other hand, inconsistent user queries embed contradictory presuppositions that actively mislead the model, often overwhelming the corrective signal of helpful documents. While explicit stance bias is eliminated by neutral user queries, models are still able to base their responses on the evidence they have retrieved, resulting in intermediate performance. These results show that models reflect the quality of retrieved evidence (RQ2) and systematically inherit bias from how user queries are phrased. User query formulation, therefore, plays a central role in shaping ground-truth alignment outcomes (RQ4).", "6 ANALYSIS\n\n6.1 ANALYSIS OF ITERATIVE PERFORMANCE\n\nFigure 2: Accuracy and recall of MI-RAG on InfoSeek subset across 9 iterations.\n\nImpact of Iterations on MIRAG Performance g Metric Accuracy Recall Iteration", "6.2 SCALING RETRIEVERS\n\nBuilding upon the established benefits of scaling MLLMs, we now investigate the impact of scaling the retriever. We investigated this by employing a larger vision encoder (SigLIP2-g) and a text embedding model (Qwen3-Embedding-0.6B). On the InfoSeek validation split, when using Gemini2.5-Flash for MI-RAG, this enhanced retrieval improves accuracy to 66.7% (+6.4%) and cumulative recall to 95.2% (+3.1%). Similarly, on the Encyclopedic VQA test split, the scaled retriever paired with the Gemma-3-4B enhances accuracy to 50.2% (+1.9%) and cumulative recall to 65.7%\n\nWe analyze how iterative refinement impacts MI-RAG's performance by measuring accuracy and recall across iterations. As shown in Figure 2, performance improves consistently with each step. The initial iterations deliver significant gains. Although the rate of improvement moderates in later steps, the model continues to achieve substantial gains in accuracy and recall. This performance improvement comes with an associated computational cost for each iteration, which we detail in Appendix F.", "(+12.1%). These findings confirm that improving retrieval capability provides significant benefits for MI-RAG's performance, validating the synergistic design.", "6.3 TOP-K SENSITIVITY ANALYSIS\n\n| | Model | Metric | k = 10 | k = 20 |\n|---:|:-----------------|:-----------|:------------|:------------|\n| 0 | Gemma-3-4B | Acc Recall | 52.66 90.82 | 48.95 94.52 |\n| 1 | Gemini-2.5-Flash | Acc | 61.84 94.36 | 65.70 |\n| 2 | | Recall | | 95.97 |\n\nTable 9: Top-k sensitivity analysis on the InfoSeek validation subset. k indicates the number of retrieved image-text pairs.\n\nidence.", "Key Insights\n\nThe external feedback signals help in reducing retrieval rounds. These signals can come from different sources, at the LLM level (e.g., token generation confidence), at the answer level ( e.g., uncertainty or hallucination), and at the context level (e.g., relevancy or sufficiency). However, these methods may still retrieve noisy or irrelevant context, and complexity assessment remains a challenge.", "Abstract\n\nRetrieval-augmented generation (RAG) has been widely adopted to augment large language models (LLMs) with external knowledge for knowledge-intensive tasks. However, its effectiveness is often undermined by the presence of noisy (i.e., low-quality) retrieved passages. Enhancing LLMs' robustness to such noise is critical for improving the reliability of RAG systems. Recent advances have equipped LLMs with strong reasoning and self-reflection capabilities, allowing them to identify and correct errors in their reasoning process. Inspired by this ability, we propose Passage Injection-a simple yet effective method that explicitly incorporates retrieved passages into LLMs' reasoning process, aiming to enhance the model's ability to recognize and resist noisy passages. We validate Passage Injection under general RAG settings using BM25 as the retriever. Experiments on four reasoning-enhanced LLMs across four factual QA datasets demonstrate that Passage Injection significantly improves overall RAG performance. Further analysis on two noisy retrieval settings-random noise, where the model is provided irrelevant passages, and counterfactual noise, where it is given misleading passages-shows that Passage Injection consistently improves robustness. Controlled experiments confirm that Passage Injection can also effectively leverage helpful passages. These findings suggest that incorporating passages in LLMs' reasoning process is a promising direction for building more robust RAG systems. 1\n\n\u2217 Keping Bi is the corresponding author.", "1 The code and data can be found at: https://github.com/Trustworthy-InformationAccess/Passage-Injection\n\nThis work is licensed under a Creative Commons Attribution 4.0 International License. SIGIR-AP 2025, Xi'an, China\n\nACM ISBN 979-8-4007-2218-9/2025/12\n\n\u00a9 2025 Copyright held by the owner/author(s).\n\nhttps://doi.org/10.1145/3767695.3769515", "6 Conclusion\n\nIn this work, inspired by reasoning-enhanced LLMs' ability to detect and revise reasoning errors, we propose Passage Injection -a method that explicitly integrates retrieved passages into the model's reasoning process to enhance robustness against noisy information and improve overall RAG performance. Experimental results on four reasoning-enhanced LLMs across four factual QA datasets demonstrate that Passage Injection significantly improves overall RAG performance. Further analysis on two noisy retrieval settingsrandom noise, where the model is provided irrelevant passages, and counterfactual noise, where it is given misleading passages-shows that Passage Injection consistently improves LLMs' robustness to noisy passages. Controlled experiments confirm that Passage Injection can also effectively leverage helpful passages. These findings suggest that incorporating passages in LLMs' reasoning process is a promising direction for building more robust RAG systems.", "4 Experiments Setup\n\nDatasets . We conduct extensive evaluation across three distinct tasks: Open-domain QA , Long-form QA , and Fact verification . (i) For the Opendomain QA task, we selected Natural Questions (NQ) (Kwiatkowski et al., 2019), along with TriviaQA (Joshi et al., 2017) and WebQuestions (WebQA) (Berant et al., 2013). (ii) To assess longform generation capabilities, we utilized TruthfulQA (Lin et al., 2022), which requires the model to produce truthful answers when faced with questions to elicit false or misleading responses. (iii) Lastly, we utilized the FactKG dataset (Kim et al., 2023), which requires the model to determine the factuality of claims by leveraging knowledge from either parametric or the external source. See Table 9 and Appendix A.2 for dataset statistics and prompts used for each task.", "Baseline Methods . We compare CARE against several existing methods to address knowledge conflicts using different strategies.", "Robust training : It enhances the resilience of the LLMs through adversarial learning. For instance, RetRobust (Yoran et al., 2024) teaches the model to ignore irrelevant or distracting retrieved contexts, making LLM more robust. As a variant, we also apply our conflict-aware training strategy directly to the base LLM without using a context assessor, denoted as Direct FT .", "Decoding-based strategy : CAD (Shi et al., 2024) aims to assess and prioritize the reliability of retrieved documents during inference. ADACAD (Han et al., 2025) utilizes confidence-aware decoding to downweight unreliable contexts.", "Adaptive retrieval : Adaptive-RAG (Jeong et al., 2024) trains a classifier and question difficulty. SKR-kNN (Wang et al., 2023) embeds examples of correct and incorrect LLM generations", "| | LLM. | Method. | Open-Domain QA (Span EM).NQ | Open-Domain QA (Span EM).TriviaQA | Open-Domain QA (Span EM).webQA | Long-form QA (F1 / ROUGE-L).TruthfulQA | Long-form QA (F1 / ROUGE-L).TruthfulQA | Fact checking (Acc) FactKG. | Average. |\n|---:|:-----------|:-------------------------|:------------------------------|:------------------------------------|:---------------------------------|:-----------------------------------------|:-----------------------------------------|:------------------------------|:-------------------------|\n| 0 | | Fine-tuning LLMs | Fine-tuning LLMs | Fine-tuning LLMs | Fine-tuning LLMs | Fine-tuning LLMs | Fine-tuning LLMs | Fine-tuning LLMs | Fine-tuning LLMs |\n| 1 | | Direct FT | 0.469 | 0.708 | 0.389 | 0.102 | 0.069 | 0.542 | 0.380 |\n| 2 | | RetRobust | 0.459 | 0.719 | 0.412 | 0.152 | 0.134 | 0.583 | 0.410 |\n| 3 | | Without Fine-tuning LLMs | Without Fine-tuning LLMs | Without Fine-tuning LLMs | Without Fine-tuning LLMs | Without Fine-tuning LLMs | Without Fine-tuning LLMs | Without Fine-tuning LLMs | Without Fine-tuning LLMs |\n| 4 | | Closed-book | 0.290 | 0.577 | 0.366 | 0.273 | 0.249 | 0.531 | 0.381 |\n| 5 | Mistral-7B | RAG | 0.419 | 0.659 | 0.372 | 0.277 | 0.251 | 0.639 | 0.436 |\n| 6 | | CAD | 0.402 | 0.631 | 0.352 | 0.243 | 0.216 | 0.598 | 0.407 |\n| 7 | | ADACAD | 0.417 | 0.653 | 0.364 | 0.267 | 0.242 | 0.636 | 0.430 |\n| 8 | | Adaptive-RAG | 0.402 | 0.631 | 0.367 | 0.275 | 0.252 | 0.633 | 0.427 |\n| 9 | | SKR-kNN | 0.406 | 0.639 | 0.396 | 0.278 | 0.253 | 0.630 | 0.434 |\n| 10 | | Priori Judgment | 0.422 | 0.682 | 0.378 | 0.274 | 0.251 | 0.600 | 0.435 |\n| 11 | | CARE (Ours) | 0.447 | 0.696 | 0.432 | 0.279 | 0.256 | 0.638 | 0.458 |\n| 12 | | Fine-tuning LLMs | Fine-tuning LLMs | Fine-tuning LLMs | Fine-tuning LLMs | Fine-tuning LLMs | Fine-tuning LLMs | Fine-tuning LLMs | Fine-tuning LLMs |\n| 13 | | Direct FT | 0.472 | 0.711 | 0.360 | 0.100 | 0.067 | 0.305 | 0.336 |\n| 14 | | RetRobust | 0.461 | 0.726 | 0.380 | 0.081 | 0.061 | 0.644 | 0.392 |\n| 15 | | Without Fine-tuning LLMs | Without Fine-tuning LLMs | Without Fine-tuning LLMs | Without Fine-tuning LLMs | Without Fine-tuning LLMs | Without Fine-tuning LLMs | Without Fine-tuning LLMs | Without Fine-tuning LLMs |\n| 16 | | Closed-book | 0.345 | 0.630 | 0.465 | 0.266 | 0.239 | 0.637 | 0.430 |\n| 17 | LLaMA-3-8B | RAG | 0.447 | 0.684 | 0.377 | 0.247 | 0.225 | 0.661 | 0.440 |\n| 18 | | CAD | 0.394 | 0.613 | 0.326 | 0.164 | 0.142 | 0.610 | 0.375 |\n| 19 | | ADACAD | 0.428 | 0.666 | 0.362 | 0.214 | 0.193 | 0.667 | 0.422 |\n| 20 | | Adaptive-RAG | 0.458 | 0.690 | 0.438 | 0.245 | 0.223 | 0.661 | 0.452 |\n| 21 | | SKR-kNN | 0.449 | 0.675 | 0.427 | 0.246 | 0.224 | 0.660 | 0.447 |\n| 22 | | Priori Judgment | 0.458 | 0.704 | 0.406 | 0.254 | 0.231 | 0.666 | 0.453 |\n| 23 | | CARE (Ours) | 0.465 | 0.700 | 0.445 | 0.264 | 0.243 | 0.686 | 0.467 |", "Table 2: Evaluation results on Open-Domain QA (NQ, TriviaQA, WebQA), Long-form QA (TruthfulQA), and Fact Checking (FactKG), using Mistral-7B-Instruct and LLaMA-3-8B-Instruct as base LLMs. The best performance is marked in bold , and the second-best is underlined, among retrieval-augmented strategies that do not directly fine-tune the LLMs, including Decoding Strategy and Adaptive Retrieval approaches.", "and uses kNN search over these embeddings to decide whether retrieval is needed. Priori Judgement (Ren et al., 2025) examines the LLM's knowledge boundaries through prompts to determine whether to use external passages. At inference time, these methods decide whether to use external evidence and select either a closed-book or RAG response as the final answer.", "Evaluation Metrics . We adopt task-specific evaluation metrics that align with each benchmark. For Open-domain QA datasets, we report Span EM, measuring whether any ground-truth answer appears within the generated output. Unlike standard exact match, this evaluates containment rather than strict equivalence, making it better suited for assessing the knowledge encoded in LLMs. For the longform QA task, we use F1 and ROUGE-L scores. For the fact verification task FactKG , the model is required to generate either \"true\" or \"false.\" We report accuracy as the evaluation metric.", "Implementation Details . We mainly employ two base LLMs for CARE: Mistral-7B (Jiang et al.,", "2023) 3 and LLaMA-3-8B (Dubey et al., 2024) 4 , both in their instruct versions. To further assess the generality, we also evaluate CARE on Qwenbased model (Yang et al., 2025), as detailed in Appendix B. We employ ColBERTv2 (Santhanam et al., 2022), to retrieve top-1 context from Wikipedia for external knowledge through all datasets. To train the context accessor module efficiently, we use the LoRA (Hu et al., 2022) adapter in pre-training and fine-tuning. For the pre-training stage, we adopted prompts from xRAG (Cheng et al., 2024) and used two million randomly sampled contexts from the December 2021 Wikipedia dump. We use the NQ train for fine-tuning and hold out 10% for validation. Since CARE requires explicit positive and negative contexts, we filter out questions for which none of the top 100 retrieved passages contain an answer span. We set the number of memory tokens K as 16 for all experiments. We use a zero-shot setup to precisely\n\n3 mistralai/Mistral-7B-Instruct-v0.2\n\n4 meta-llama/Meta-Llama-3-8B-Instruct"]}
{"type": "entry", "index": 64, "question": "how do i evaluate \u201ci don\u2019t know\u201d behavior in scientific qa", "answer": "From the supplied documents, there are only partial ingredients for evaluating \"I don\u2019t know\" behavior in scientific QA, but we can adapt them.\n\n1. Treat \"I don\u2019t know\" as a valid label in the interface\nThe intercept survey in iatroX explicitly offers a \"Don\u2019t Know\" response option when users are unsure about usefulness or reliability [2]. By analogy, a scientific QA system can expose an explicit \"I don\u2019t know\" or \"Cannot answer from the evidence\" option, not just force a guess.\n\n2. Score answers with standard QA metrics, including abstentions\nExisting QA benchmarks use Exact Match (EM) and F1 for answer quality [1]. You can extend this by:\n- Counting a correct factual answer as usual (contributing to EM/F1).\n- Treating an \"I don\u2019t know\" on a truly unanswerable question as a correct behavior.\n- Treating an \"I don\u2019t know\" on an answerable question as a miss (like an incorrect answer).\nThis is similar in spirit to SQuAD v2.0, where some questions are unanswerable and systems must decide to abstain or answer [1].\n\n3. Build test sets that explicitly mix answerable and unanswerable questions\nBenchmarks like SQuAD v2.0 and HotpotQA already highlight that some questions require reasoning and some are unanswerable from the given context [1]. A scientific QA evaluation can mirror this: include\n- clear, document-supported scientific questions;\n- questions that are underspecified or outside the corpus, where the ideal output is \"I don\u2019t know\".\n\n4. Human rater procedure\nFor careful evaluation (e.g., in due\u2011diligence\u2011like settings), raters can follow a structured protocol: carefully read the question and answer, then reason step\u2011by\u2011step about whether the answer is relevant and supported [4]. This can include judging whether an \"I don\u2019t know\" is appropriate given the available evidence.\n\nOverall, the documents support using: (a) explicit \"Don\u2019t know\" options [2]; (b) EM/F1 extended to reward correct abstentions on unanswerable scientific questions [1]; and (c) rater protocols that check whether abstention matches the evidence [4].", "tags": ["_untagged"], "full_faithfulness": 0.9166666667000001, "full_answer_relevancy": 0.0, "seed_faithfulness": 0.2592592593, "seed_answer_relevancy": 0.0, "context_relevance": 0.24232480485525304, "context_relevance_error": null, "context_relevance_reason": "The context passages primarily discuss advancements in LLMs and QA methodologies but do not specifically address how to evaluate 'I don\u2019t know' behavior in scientific QA. While some passages touch on challenges in QA systems, they lack direct relevance to the user's question about evaluating uncertainty in responses, leading to a low score.", "structure_citation": 0.8051736692829836, "structure_error": null, "structure_reason": "The Actual Output effectively utilizes [n] markers to reference specific snippets from the Context, particularly citing [1] and [2] to support claims about evaluation metrics and the inclusion of 'I don\u2019t know' options. However, while the claims are generally consistent with the cited snippets, there is a minor shortcoming in the clarity of how some claims directly relate to the context provided, particularly regarding the human rater procedure [4]. Overall, the response demonstrates strong alignment with the evaluation steps, but slight improvements in citation clarity could enhance the score.", "hallucination_score": 0.8157212605557959, "hallucination_error": null, "hallucination_reason": "The response effectively outlines methods for evaluating 'I don\u2019t know' behavior in scientific QA, aligning well with the context of LLMs in QA. It supports the claims made in the input by providing specific strategies, such as treating 'I don\u2019t know' as a valid response and extending QA metrics to include abstentions. However, while the output is relevant and informative, it lacks comprehensive references to all claims made in the input, which slightly diminishes its overall factual accuracy.", "contexts_used": ["LLMs in QA\n\nThe advent of LLMs has steered in a transformative era in NLP, particularly within the domain of QA. These models, pre-trained on massive corpora of diverse text, exhibit sophisticated capabilities in both natural language understanding and generation. Their proficiency in producing coherent, contextually relevant, and human-like responses to a broad spectrum of prompts makes them exceptionally well-suited for QA tasks, where delivering precise and informative answers is paramount. Recent advancements by models such as BERT [57] and ChatGPT [58], have significantly propelled the field forward. LLMs have demonstrated strong performance in open-domain QA scenarios-such as commonsense reasoning[20]-owing to their extensive embedded knowledge of the world. Moreover, their ability to comprehend and articulate responses to abstract or contextually nuanced queries and reasoning tasks [22] underscores their utility in addressing complex QA challenges that require deep semantic understanding. Despite their strengths, LLMs also pose challenges: they can exhibit contextual ambiguity or overconfidence in their outputs ('hallucinations')[21], and their substantial computational and memory requirements complicate deployment in resource-constrained environments.\n\nRAG, fine tuning in QA", "LLMs also face problems when it comes to domain specific QA or tasks where they are needed to recall factual information accurately instead of just probabilistically generating whatever comes next. Research has also explored different prompting techniques, like chain-of-thought prompting[24], and sampling based methods[23] to reduce hallucinations. Contemporary research increasingly explores strategies such as fine-tuning and retrieval augmentation to enhance LLM-based QA systems. Fine-tuning on domain-specific corpora (e.g., BioBERT for biomedical text [17], SciBERT for scientific text [18]) has been shown to sharpen model focus, reducing irrelevant or generic responses in specialized settings such as medical or legal QA. Retrieval-augmented architectures such as RAG [19] combine LLMs with external knowledge bases, to try to further mitigate issues of factual inaccuracy and enable real-time incorporation of new information. Building on RAG's ability to bridge parametric and non-parametric knowledge, many modern QA pipelines introduce a lightweight re-ranking step [25] to sift through the retrieved contexts and promote passages that are most relevant to the query. However, RAG still faces several challenges. One key issue lies in the retrieval step itself-if the retriever fails to fetch relevant documents, the generator is left to hallucinate or provide incomplete answers. Moreover, integrating noisy or loosely relevant contexts can degrade response quality rather than enhance it, especially in high-stakes domains where precision is critical. RAG pipelines are also sensitive to the quality and domain alignment of the underlying knowledge base, and they often require extensive tuning to balance recall and precision effectively.", "RAG + Graphs in QA\n\nTo address these limitations of traditional RAG, particularly in retrieving contextually precise and semantically rich information, research has tried to integrate structured knowledge sources and graphbased reasoning into the retrieval pipeline. One notable direction of work is LLM-based retrieval, which incorporates knowledge graphs information into the generation process of LLMs. The LLMs are augmented using the retrieved facts from the KG [26], leading to a clear dependence on the quality of extracted graph information, using it to generate responses. Research has also been done towards augmenting knowledge graph information, retrieved through some semantic or some other similarity index, in the prompts given to the LLM [27], to help the model do zero-shot question-answering. Some researchers have tried a different approach to fact retrieval, where the model tries different queries, using structured query languages, until the desired information comes through [28]. All of these approaches have used KGs as an external source to retrieve information from and answer the questions [29]. Then, aligning the retrieval process with LLM even more closely, some researchers have proposed methods which use LLMs in intermediate steps as well to plan the retrieval and judge whether the retrieved information is relevant or not [29,30], continuing the process until the desired output emerges.", "Another interesting direction of work is integrating GNNs with LLMs, which leverages graph neural networks to enhance retrieval and re-ranking using learned graph representations, along with generation capabilities of LLMs. There have been approaches such as GNN-RAG [31], which have tried combining language understanding abilities of LLMs with the reasoning abilities of GNNs in a retrieval-augmented generation (RAG) style. Other methods of GNN-LLM alignment, have been classified into symmetric and asymmetric alignment [32]. Symmetric alignment refers to the equal treatment of the graph and text modalities during the alignment process[33, 34, 35]. Asymmetric alignment focuses on allowing one modality to assist or enhance the other, here leveraging the capabilities of GNNs to improve the LLMs [36, 37, 38].", "The continual evolution of such hybrid architectures reflects the dynamic nature of the QA landscape and its responsiveness to complex information needs. Given the diversity of methods explored to enhance QA systems-from rule-based techniques to advanced neural and hybrid models-it becomes essential to establish robust mechanisms for evaluating their effectiveness. It is thus important to discuss the evaluation paradigms that compare and benchmark these systems. Extractive QA benchmarks almost universally use Exact Match (EM)-the strict proportion of predictions that character-for-character match a ground-truth answer-and F1 score, the harmonic mean of token-level precision and recall, to evaluate answer quality. On SQuAD v2.0 [39], human annotators achieve around 86.831 EM and 89.452 F1 on the test set, whereas state-of-the-art models now exceed 90 EM and 93 F1. Natural Questions [40] extends this paradigm to long-answer (paragraph) and short-answer (span or yes/no) annotations drawn from real Google search queries. Meanwhile, multiple-choice datasets like OpenBookQA [41]-designed to mimic open-book science exams-use simple accuracy. Complex reasoning benchmarks push beyond single-span extraction. HotpotQA [42], a multi-hop dataset built on pairs of Wikipedia articles, evaluates both answer-span EM/F1 and supporting-fact EM/F1, plus a joint metric requiring both to be correct; even top models achieve only ~72 joint F1[43] under the full-wiki setting, far below human performance of ~82 average F1 and ~96 upper bound F1. These core metrics and datasets underpin broader QA evaluation: multi-task suites like GLUE/MMLU [44, 45] include QA subtasks to probe general language understanding, while specialized frameworks such as MT-bench [46] ('LLM as judge') and automated platforms like Scale Evaluation layer on top to assess conversational and retrieval-augmented QA in real-world scenarios.", "This review has highlighted the broad spectrum of methodologies and evaluation strategies used in enhancing and assessing question answering systems. With this foundation in place, we now transition to the specific tools and frameworks employed in our research for graph-based question answering. In the following section, we briefly introduce and contextualize spaCy, Stanford CoreNLP, and GraphRAG-three diverse and widely-used tools we have utilized for knowledge graph construction and integration with LLMs.", "Methods\n\nThis mixed-methods formative evaluation combined (i) a retrospective observational analysis of platform usage data across web, iOS, and Android applications over 16 weeks (8 April-31 July 2025) and (ii) a cross-sectional, in-product intercept survey.", "Usage metrics (unique users, engagement events, clinical queries) were sourced from Google Analytics 4, Apple App Store Connect, and Google Play Console, with bot filtering applied. A client-side script randomized single-item survey prompts to ~10% of web sessions from a predefined battery assessing usefulness, reliability, and adoption intent; responses were anonymous (\"Yes/No/Don't Know\"). Proportions were reported with Wilson 95% confidence intervals; qualitative comments underwent thematic content analysis. Reporting adhered to STROBE and CHERRIES guidelines. This was classified as a UK service evaluation, requiring no ethics review.", "Usage Analytics\n\nPlatform usage data was collected using Google Analytics 4 (GA4) for the web application, Google Play Console, and Apple App Store Connect for mobile applications. Standard bot filtering was applied. Key metrics included unique users, engagement events (total interactions), active users (DAU/WAU/MAU), and total clinical queries submitted to the RAG pipeline.\n\nIn-Product Intercept Survey", "To capture in-context user perceptions and minimize self-selection bias [15], a systematic random intercept survey was deployed within the web application.", "Sampling and Administration: A client-side script presented survey prompts to a random 10% of web user sessions. To maximize the diversity of feedback and prevent survey fatigue, users were presented with randomized single-item questions from a larger question battery throughout their session. A browser cookie prevented the same user from being shown the same prompt repeatedly.", "Survey Instrument: The survey comprised single-item, closed-ended questions assessing domains including perceived usefulness, clinical reliability, system performance, and intent to adopt (Table 2). Responses were recorded as \"Yes,\" \"No,\" or \"Don't Know.\"", "3 Methodology\n\nIn this section, we introduce how reasoning-enhanced LLMs generate answers with internal knowledge, how they interact with external passages under RAG, and our proposed Passage Injection.", "3.1 Reasoning-Enhanced QA with Internal Knowledge\n\nDirect QA. For a given question \ud835\udc5e , the workflow of reasoningenhanced LLMs can be divided into three phases: Input Phase , Reasoning Phase , and Response Phase . In the Input Phase, the model receives the question and encodes it. Next, in the Reasoning Phase, the model generates its reasoning and self-reflection process within the <think></think> tags. Finally, after the </think> tag, it produces the final answer \ud835\udc4e as the response. This allows the model to fully leverage its internal knowledge but may also lead to overthinking-resulting in longer reasoning paths, increased computational overhead, and a higher risk of hallucination.", "Currently, there are two mainstream approaches to equip LLMs with reasoning abilities. One is through reinforcement learning, allowing the model to explore reasoning paths on its own, as seen in models like Qwen3 [27]. The other is via distillation, such as in DeepSeek-R1-Distill-Qwen [6].", "3.2 Augmenting Reasoning-Enhanced QA with External Knowledge\n\nAlthough current LLMs possess strong reasoning abilities, there is still knowledge they do not know and thus require supplementation from external passages. In general, for a given question \ud835\udc5e , we typically follow a retrieve-then-read pipeline, where we first use a retriever to retrieve a set of relevant passages \ud835\udc37 from an external corpus, and then provide \ud835\udc37 to the model. \ud835\udc37 can contain one or more passages.", "Passage Injection. Since reasoning-enhanced LLMs can identify errors in their own reasoning process, we think that explicitly integrating passages into this process may lead the model to treat them as part of its own reasoning, thereby improving its ability to detect and correct errors and enhancing its robustness. Based on this, we propose Passage Injection, which injects \ud835\udc37 into the Reasoning Phase to strengthen the model's robustness against noisy passages. The example can be found in Figure 1(a). Passage Injection correctly identifies the misinformation in the external passage, rectifies it, and ultimately produces the correct answer.", "Vanilla RAG. This is the standard approach which directly concatenates \ud835\udc37 with \ud835\udc5e and places them in the Input Phase . This approach may lead to insufficient attention to the documents, making it difficult for the model to identify potential errors. As shown in Figure 1(a), we present an example where the retrieved passage contains misleading content-incorrectly stating that Northern Ireland is part of the United States, whereas it is actually part of the United Kingdom. In this case, Vanilla RAG fails to recognize the misinformation and follows it, resulting in an incorrect answer.", "Instruction Injection. Since injecting passages into the Reasoning Phase inevitably introduces instructions on how to use the passages, we design an ablation setting where only the instruction is injected into the Reasoning Phase, while the passages themselves are placed in the Input Phase. We refer to this as Instruction Injection, which is similar to Wu et al. [26].", "For models that acquire reasoning abilities through distillation, we speculate that they are better suited for RAG scenarios, as distillation itself is a form of supervised learning with long contexts-closely resembling the task format of RAG. However, since their reasoning abilities are not self-discovered but rather learned from supervision, we argue that intervening in their reasoning paths may yield less benefit. Moreover, we think that encouraging the model to pay more attention to external passages may reduce its overthinking, thereby lowering reasoning overhead.", "Instructions d'\u00e9valuation :\n\nAdoptez l'approche suivante dans votre \u00e9valuation :\n\nLire attentivement la question.\n\nLire attentivement la r\u00e9ponse.\n\nLire les phrases en entier.", "Pour chaque phrase, verbaliser un raisonnement pas \u00e0 pas sur le caract\u00e8re pertinent des informations de la r\u00e9ponse.\n\nEn d\u00e9duire une \u00e9valuation finale pour chaque phrase."]}
{"type": "entry", "index": 65, "question": "how do i evaluate non-question research queries like \u201cfind me papers like X but with Y\u201d", "answer": "You can treat a request like \u201cfind me papers like X but with Y\u201d as an information\u2011retrieval and result\u2011generation task, then evaluate it along two axes:\n\n1) How good is the retrieval?\n\n\u2022 Use similarity-based retrieval metrics: compare what your system retrieved against a human\u2011selected set of \u201cideal\u201d papers using relevance scores.\n  \u2013 You can base retrieval on lexical methods like BM25 or semantic embeddings (CodeBERT, UniXcoder, etc.) and compare how often the system\u2019s top results match or rank highly among the human\u2011relevant set [2][3].\n  \u2013 Under the hood, dense retrievers (like DPR) embed queries and documents into a vector space and rank by similarity; evaluation then checks whether high\u2011similarity items are actually the right ones [3].\n\n2) How good is the final output list/description?\n\n\u2022 String/structure similarity: If you have a \u201cgold\u201d query or filter (e.g., a reference SPARQL or search expression), compare the system\u2019s query to it using overlap scores such as BLEU or ROUGE, which measure how similar the generated structure is to a reference [1].\n\u2022 Execution-based metrics: Run both the system\u2019s query and the gold query, then compare the result sets. A relaxed exact match approach removes superficial differences (e.g., variable names), deduplicates lines, and compares processed result sets for equality or overlap [1]. This directly checks whether the user would see essentially the same papers.\n\u2022 Embedding-based answer similarity: If your system outputs a textual summary of the found papers instead of a formal query, compare that text to a human\u2011written summary using cosine similarity over embeddings to capture semantic closeness, not just word overlap [4].\n\nCombining these gives a practical evaluation: retrieval relevance, similarity of the underlying query/filter, and similarity of the presented results.\n", "tags": ["_untagged"], "full_faithfulness": 1.0, "full_answer_relevancy": 0.8607025424, "seed_faithfulness": 1.0, "seed_answer_relevancy": 0.8786682747000001, "context_relevance": 0.16685651123831732, "context_relevance_error": null, "context_relevance_reason": "The context passages primarily focus on SPARQL query generation and evaluation metrics, which do not directly address the user's question about evaluating non-question research queries. While there are mentions of retrieval techniques and evaluation processes, they lack specific guidance on how to handle queries like 'find me papers like X but with Y'. The relevance of the context to the user's specific inquiry is minimal.", "structure_citation": 0.6814479184260344, "structure_error": null, "structure_reason": "The Actual Output effectively addresses the evaluation of non-question research queries by outlining two axes: retrieval quality and final output quality. It includes specific metrics such as BLEU, ROUGE, and similarity-based retrieval techniques, which align well with the Context provided. However, while it references [1], [2], and [3], it does not clearly match all markers to specific snippets in the Context, leading to a slight deduction in points for potential missing citations.", "hallucination_score": 0.7993490091544598, "hallucination_error": null, "hallucination_reason": "The response effectively outlines a structured approach to evaluating non-question research queries, focusing on retrieval quality and output description. It references relevant metrics like BLEU and ROUGE, which align with the context provided. However, while the output is mostly supported by the context, it lacks specific examples or direct connections to the SciQA dataset mentioned, which could enhance clarity and relevance.", "contexts_used": ["5 Experiments\n5.1 Datasets\n\nWe conduct experiments on the SciQA benchmark dataset, a recently released resource designed to evaluate question answering systems over scholarly knowledge graphs [2]. SciQA provides a diverse set of natural language questions aligned with the Open Research Knowledge Graph (ORKG). SciQA contains 100 handcrafted natural language questions with paraphrases, corresponding human- and machine-readable SPARQL queries with their results. In addition, a set of 2465 questions has been semi-automatically derived from eight question templates.", "5.2 Baselines and the state of the art\n\nAs a baseline, we adopt a zero-shot setting in which the model generates SPARQL queries without any task-specific fine-tuning or example guidance. This setup evaluates the model's out-of-the-box ability to understand the task and map natural language questions to structured SPARQL queries. For the state-of-the-art comparison, we implement a one-shot approach in which the model is provided with the most semantically similar question from the training set, along with its corresponding SPARQL query, as an in-context demonstration. The most similar example is identified by computing cosine similarity between the input question and all training questions using SentenceBERT embeddings. This configuration has shown strong performance in recent studies by helping the model better understand the tasks [12, 14].", "Fig. 2: FIRESPARQL framework (yellow boxes) and evaluation setup (grey boxes)\n\nNLQ Prompt template RAG SKG Candidate properties and entities Fine-tuned LLM NLQ-SPARQL Pairs [1] Ontology Fine-tuning LoRA Final SPARQL Retrieved relevant context SPARQL Queries LLM SPARQL corrector Vanilla LLM Ground truth Eval 1: SPARQL generation performance Evaluation Answers to the generated SPARQL queries Ground truth answers\n\nEval 2: SAPRQL Execution\n\nperformance\nFIRESPARQL framework", "5.3 Implementation\n\nWe fine-tuned two instruction-tuned models, Llama 3.2-3B Instruct and Llama 3-8B Instruct, using 1,795 labeled samples from the SciQA training set. The models were trained under various epoch configurations (3, 5, 7, 10, 15, and 20) to analyze performance across training durations. All fine-tuning experiments were conducted on a single NVIDIA H100 GPU. We used DeepSeek-R1-DistillLlama-70B as the underlying model for retrieval-augmented generation (RAG).\n\nAll SPARQL queries are executed via Qlever [3]", "5.4 Evaluation metrics\n\nWe employ a combination of string-based and execution-based evaluation metrics to assess the quality of the generated SPARQL queries. Similar to other research, we use BLEU-4 and ROUGE scores, which measure token-level and n-gram overlaps, to evaluate the similarity between generated queries and the ground-truth queries. These metrics provide insights into how closely the structure and content of the generated queries align with the reference queries. Additionally, we assess the execution performance of the generated SPARQL queries using two variants of relaxed Exact Match: Success and All. The Relaxed Exact Match (success) metric considers only those queries that were syntactically valid, successfully executed against the ORKG RDF dump, and returned non-empty results. In contrast, the Relaxed Exact Match (all) metric evaluates the correctness of the query results across the entire test set, including queries that may have failed or returned empty results.", "Unlike the original Exact Match, which is very strict, our Relaxed Exact Match incorporates several preprocessing steps. First, we remove variable names from the returned results to avoid penalizing differences that do not affect semantics. Second, we split the results line by line and eliminate duplicate lines to normalize the output structure. Finally, we compare the processed sets using exact matching. This approach provides a more tolerant and realistic evaluation of query execution performance in scholarly knowledge graph settings. The abovementioned dual evaluation approach allows us to comprehensively analyze both the syntactic quality and the practical effectiveness of the generated SPARQL queries.", "5.5 Results\n\nTable 1 shows the results of different metrics on different strategies, different models, and different epochs. Figure 3 shows the results of different metrics with different epochs for LoRA fine-tuning. All the scores are the average scores of three runs. The standard deviation of BLEU-4, ROUGE scores, RelaxedEM(success), and RelaxedEM(all) across different model variants (e.g., zeroshot, one-shot, fine-tuning, and RAG) and training epochs is consistently low ( std < 0 . 0265 ), indicating stable performance across all three runs. Therefore, we use the average scores as a reliable and representative summary of model", "effectiveness. In the next section, we discuss the main takeaways from these results.", "| | Strategy | Model | Epoch | BLEU-4 | ROUGE-1 | ROUGE-2 | ROUGE-L | RelaxedEM (success) | RelaxedEM (all) |\n|---:|:--------------|:---------------------------------------------------------|:--------|---------:|----------:|----------:|----------:|----------------------:|------------------:|\n| 0 | zero-shot | llama-3.2-3b-Instruct | - | 0.03 | 0.36 | 0.18 | 0.35 | 0 | 0 |\n| 1 | zero-shot | llama-3-8b-Instruct | - | 0.03 | 0.39 | 0.18 | 0.38 | 0 | 0 |\n| 2 | zero-shot_rag | llama-3.2-3b-Instruct | - | 0.03 | 0.36 | 0.18 | 0.36 | 0 | 0 |\n| 3 | zero-shot_rag | llama-3-8b-Instruct | - | 0.03 | 0.36 | 0.18 | 0.38 | 0 | 0 |\n| 4 | oneshot | llama-3.2-3b-Instruct | - | 0.58 | 0.81 | 0.73 | 0.78 | 0.78 | 0.4 |\n| 5 | oneshot | llama-3-8b-Instruct | - | 0.38 | 0.61 | 0.5 | 0.59 | 0.89 | 0.29 |\n| 6 | ft_rag | llama-3.2-3b-Instruct-lora_ deepseekr1-distill-llama-70b | 20 | 0.32 | 0.63 | 0.51 | 0.6 | 0.42 | 0.06 |\n| 7 | ft_rag | llama3-8b-Instruct-lora_ deepseekr1-disttill-llama-70b | 15 | 0.58 | 0.81 | 0.72 | 0.77 | 0.85 | 0.29 |\n| 8 | ft | llama-3.2-3b-Instruct-lora | 3 | 0.67 | 0.86 | 0.79 | 0.83 | 0.88 | 0.53 |\n| 9 | ft | llama-3.2-3b-Instruct-lora | 5 | 0.62 | 0.83 | 0.75 | 0.79 | 0.71 | 0.36 |\n| 10 | ft | llama-3.2-3b-Instruct-lora | 7 | 0.52 | 0.77 | 0.67 | 0.73 | 0.79 | 0.27 |\n| 11 | ft | llama-3.2-3b-Instruct-lora | 10 | 0.56 | 0.79 | 0.7 | 0.76 | 0.7 | 0.22 |\n| 12 | ft | | 15 | 0.6 | 0.81 | 0.73 | 0.78 | 0.88 | 0.4 |\n| 13 | ft | | 20 | 0.7 | 0.86 | 0.79 | 0.84 | 0.8 | 0.54 |\n| 14 | | | 3 | 0.75 | 0.9 | 0.84 | 0.88 | 0.99 | 0.79 |\n| 15 | | | 5 | 0.74 | 0.9 | 0.85 | 0.87 | 0.98 | 0.73 |\n| 16 | | | 7 | 0.7 | 0.87 | 0.8 | 0.84 | 0.96 | 0.69 |\n| 17 | | llama-3-8b-Instruct-lora | 10 | 0.71 | 0.88 | 0.81 | 0.85 | 0.94 | 0.69 |\n| 18 | | | 15 | 0.77 | 0.91 | 0.86 | 0.9 | 0.98 | 0.85 |\n| 19 | | | 20 | 0.74 | 0.89 | 0.84 | 0.88 | 0.99 | 0.82 |", "Table 1: BLEU, ROUGE and RelaxedEM scores with different strategies, different models and different epochs", "A. Similarity-based Retrieval Techniques\n\nIn our experimental evaluation, we employ five distinct similarity-based retrieval techniques, including one lexical technique (i.e., BM25 [25]) and four semantic techniques (i.e., CodeBERT [26], UniXcoder [27], CoCoSoDa [28], and GTEQwen [29]):\n\nBM25 [25] extends the basic TF-IDF approach by incorporating document length normalization and non-linear term frequency scaling. In our implementation, we use BM25 to retrieve similar code snippets from a function corpus F . For a given code query, BM25 first tokenizes it into terms { t 1 , t 2 , ..., t n } and computes a relevance score for each function f \u2208 F as:\n\nThe IDF component is calculated similarly to the standard TF-IDF approach, but with smoothing factors:\n\nwhere N is the total number of functions in the corpus and df i is the number of functions containing term t i . The modified term frequency component TF mod introduces saturation and length normalization:\n\nwhere tf i is the frequency of term t i in function f , len f is the length of function f , and len avg is the average function length in the corpus. Parameters k and b control term frequency scaling and length normalization, respectively.", "CodeBERT [26] is built upon the RoBERTa-base and pretrained on both natural language and programming language corpus. It employs two types of objective modeling tasks, including Masked Language Modeling (MLM) and Replaced Token Detection (RTD), which enables the model to learn general representations.", "UniXcoder [27] is pre-trained using three objectives: masked language modeling, unidirectional language modeling, and denoising tasks. Additionally, UniXcoder incorporates two pre-training strategies: (1) a multi-modal contrastive learning approach that leverages Abstract Syntax Tree (AST) to enhance code representations, and (2) a cross-modal generation task utilizing code comments to align embeddings across different programming languages.", "CoCoSoDa [28], which has the same architecture as UniXcoder, enhances code representation through momentum contrastive learning. Specifically, the model employs dynamic data augmentation and negative sampling strategies to learn more robust and discriminative code embeddings.", "GTE-Qwen [29], a member of the GTE (General Text Embedding) family, is a decode-only retrieval model based on Qwen2 [30]. Different from the general Qwen model,\n\nGTE-Qwen incorporates bidirectional attention mechanisms to enrich contextual representations, making it particularly effective for multilingual understanding and retrieval tasks. We utilize GTE-Qwen2-1.5B-instruct in our experiment.", "Retriever Module (Dense Passage Retrieval).\n\nThe retriever's job is to efficiently identify which pieces of text in a large corpus are relevant to the input query x . Modern RAG implementations typically use dense retrievers like Dense Passage Retrieval (DPR) [45] in lieu of traditional keyword search. In DPR, a bi-encoder architecture is employed: a question encoder E q ( x ) maps the query x to a d -dimensional vector, and a passage encoder E p ( d ) maps each candidate document (or passage) d in the corpus to a d -dimensional vector in the same space. Relevance is assessed via a similarity score, usually the dot product of these vectors:\n\nAt query time, the retriever computes v q = E q ( x ) and then finds the topK documents whose vector E p ( d ) has highest inner product with v q . This search for maximum inner product can be implemented efficiently using Approximate Nearest Neighbor techniques (e.g., FAISS) to handle millions of documents in sub-linear time. The output of the retriever is the set Z = { z 1 , . . . , z K } of top-ranked documents and potentially their similarity scores. One can view the retriever as defining a probability distribution P ret ( z | x ) over documents z in the corpus, such that:\n\nwith the normalization \u2211 d \u2208C exp( E q ( x ) \u22a4 E p ( d )) (in practice approximated by summing over retrieved candidates rather than all of C ). In other words, the retriever assigns higher probability (or score)", "to documents whose embedding is most similar to the query's embedding. The DPR retriever is usually first trained on pairs of questions and relevant passages to ensure E q and E p produce representations that maximize dot-products for true Q-A pairs and minimize them for irrelevant pairs. The training objective is often a contrastive loss : for a given question q with a gold relevant passage p + and a set of negative passages { p -1 , . . . , p -N } , the encoder is trained to maximize sim ( q, p + ) while minimizing sim ( q, p -) for negatives. This can be formulated as a cross-entropy loss treating the positive passage as the correct class among one positive and N negatives:\n\nwhich encourages E q and E p to embed true pairs closer together than any negative pair [45]. After training, the retriever can generalize to new queries: it embeds the query and efficiently finds the nearest neighbor passages in the index. This retrieval step is crucial because it narrows down the evidence from potentially billions of tokens to a manageable subset that the generator will actually consider.", "Generator Module (Conditional Seq2Seq Model). The generator in a RAG pipeline is typically a sequence-to-sequence language model that produces the final answer or output text, given the input query and retrieved documents. Formally, the generator defines a conditional distribution P gen ( y | x, Z ) over output sequences, where Z = { z 1 , . . . , z K } are the retrieved passages. The generator is often initialized from a pre-trained transformer-based seq2seq model (such as BART or T5) to leverage rich language generation capabilities [52]. During generation, the model is provided with the question (or prompt) as well as the content of the retrieved documents. There are multiple ways to feed the retrieved context to the generator:\n\nIn an early fusion approach, one can concatenate the query x with the text of all retrieved passages into a single extended input sequence (perhaps with special separators) and have the seq2seq model attend to all of it at once. This is straightforward: the generator effectively treats the combined text as context and learns to pick out the relevant bits when producing the answer. However, this can be difficult if K is large, as the input may become very long.", "In the late fusion approach adopted by RAG [52], the generator considers one retrieved document at a time as a context and then marginalizes over the document choices (as in Eq. 1). Specifically, the RAG-Sequence variant fixes a single document z i as context for generating the entire output and computes P ( y | x ) by summing the probabilities P ( y | x, z i ) weighted by the retriever's confidence P ( z i | x ) . Another variant, RAG-Token, allows the generator to switch between documents at the token level, effectively marginalizing over the document choice for each generated token [52]. In both cases, the generator P gen ( y | x, z ) itself works like a standard seq2seq model: it factorizes over the output tokens y 1 , . . . , y T as \u220f T t =1 P \u03b8 ( y t | x, z, y <t ) , i.e. it generates one token at a time, attending to the input query x and the content of z (or multiple z 's if early fusion). The generator's architecture typically uses an encoder-decoder Transformer: the encoder encodes the combination of x and z , and the decoder produces y autoregressively.", "Because the generator is conditioning on retrieved evidence, it tends to produce outputs that are supported by that evidence. For example, if the query asks for a specific factual answer, the generator can copy or rephrase the needed information from one of the retrieved passages. This is in contrast to a vanilla language model which would attempt to rely on parametric knowledge (which might be outdated or incomplete). In sum, the generator module in RAG is responsible for fluent and coherent text generation, but crucially it is grounded by the retrieval context, which guides it toward accurate content.", "3 Methodology:\n\n3.2.2 Retrieval and Generation\n\nFor the naive Knowledge Graph (KG) retrieval pipeline, the retrieval process differs significantly. Instead of directly retrieving passages from the unsummarized corpus through vector search, we retrieve structured information from a", "Figure 8: K 2 RAG framework overview. Component (a): Knowledge Graph. Component (b): Hybrid Retriever containing Dense and Sparse retrievers. Component (c): Embeddings Model. Step (A): Knowledge Graph results. Step (B): Summarise Knowledge Graph results. Step (C): Generating sub-questions from chunks. Step (D): Generating sub-answers for sub-questions. Step (E): Generate the final answer from sub-answers and Knowledge Graph Results Summary.\n\nA Retrieve from Knowledge Graph B Summarise Results C Generate sub-questions from Results Summary D Answer Each Sub-question and Summarise Answer Embeddings Model Quantised LLM Hybrid Vector Retriever E\n\nGive final answer with\n\nsub-answers and Knowledge Graph User Question Final Answer SUMMARY\n\nknowledge graph that has been constructed using the unsummarized corpus and a quantized LLM. In this setup, the same quantized LLM is employed both for generating the knowledge graph and for querying it at retrieval time.", "Across all pipelines, including the KG-based approach, the retrieved information is subsequently passed to an unquantized LLM for final answer generation. Importantly, all pipelines share a common answer generation prompt, which is the same as that used in K 2 RAG [Prompt 3.1.3]. This standardization of the answer generation step allows for a controlled comparison across different naive retrieval strategies with K 2 RAG .", "3.3.1 Dataset Selection and Description\n\nIn order to perform a comprehensive evaluation on the proposed framework we have outlined two key requirements needed from the dataset as follows:\n\nDataset must contain question answer pairs in plain text format where answers are to be assumed as ground truth answers to the question.", "The question answer pairs must be constructed in reference to a document corpus which has all the required documents and their information from which ground truth answer to the questions are derived from.", "Due to time and resource constraints we were unable to create our own dataset from scratch satisfying these requirements and hence the MultiHop-RAG dataset [30] was identified and used as it satisfies the above key requirements needed to perform the analysis.", "The MultiHop-RAG dataset contains corpus data which was used for training along with a test dataset containing 2555 question answer pairs from which we performed evaluation of the pipeline. The corpus contains a 609 articles from a", "3.3.2 Evaluation Process\n\nWe used a K-fold evaluation framework where we split the data into K=10 folds. For each pipeline, we generated the answers for each question in each fold and captured answering times for the questions to compare the execution times [Figure 10] [Algorithm 3].", "To evaluate the accuracy of a pipeline's answer, we computed the similarity between the ground truth and the pipeline's output as a scoring function s of the plaintext ground truth and pipeline output for each question defined as:\n\nwhere o is the pipeline's output, t is the ground truth and e represents the embeddings model as a function to generate the vector embedding of a plaintext input.", "We randomly partitioned the dataset into ten equal folds to simulate real-world usage of the pipeline. This approach reflects scenarios in which different users pose diverse questions to the system. Random assignment of test instances to folds helps mitigate the risk of performance bias, which could arise if certain folds contained clusters of similar questions that align particularly well with the strengths of the pipeline. For evaluation, we employed cosine similarity", "Figure 9: Structure of competitor pipelines. Component (S): Semantic Search - Dense Vector Retrieval. Component (K): Keyword Search - Sparse Vector Retrieval. Component (H): Hybrid Search - Combination of Sparse and Dense Vector Retrieval. Component (KG): Knowledge Graph Retrieval\n\nRetrieve information 20% Sparse Database 80% Dense 100% Unquantised H S K KG\n\nto assess the semantic similarity between the predicted and ground truth answers, as it effectively captures the degree of semantic similarity. Additionally, we recorded the execution time for each query to analyze the system's efficiency.\n\nFigure 10: Evaluation Pipeline.\n\nGround Truth Model's Common System Prompt Cosine Similarity RAG Pipeline MultiHop-RAG Test Dataset\n\nK-th Subset of questions as Input\n\nStore Scores and Times Fold 1 to 10 Repeat Execution Time Scores List Times List"]}
{"type": "summary", "model": "gpt-4o-mini", "top_k_contexts": null, "samples": 66, "cases_faithfulness": {"best": [{"type": "entry", "index": 59, "question": "how do i evaluate agentic rag loops without it just burning cycles forever", "answer": "You can keep agentic RAG from spinning forever by (1) teaching it when *not* to use RAG, (2) limiting and rewarding short decision paths, and (3) measuring efficiency explicitly.\n\n1) Add an explicit \u201cuse RAG or not\u201d decision.\nPatho\u2011AgenticRAG\u2019s router first decides whether to invoke RAG at all; if it predicts \u201cno,\u201d the process stops immediately for simple questions [3]. Their reward function gives full credit when the agent correctly chooses not to call RAG [3]. RepoTransAgent does something similar: before retrieval, the agent decides whether a function is simple/standalone; if so, it bypasses RAG to save compute and avoid noise [4]. You can copy this pattern: make \u201cno retrieval, answer now\u201d a valid, high\u2011reward action.\n\n2) Train on good decision paths, not just good answers.\nPatho\u2011AgenticRAG optimizes an entire decision path (invoke RAG, how many rewrites, whether to use a classifier, which partition) against ground\u2011truth paths using a step\u2011wise reward [3]. The medical AgenticRAG system also scores each step in the reasoning/retrieval sequence with per\u2011step rewards and weights [1]. You can penalize extra steps or loops and reward reaching a correct answer in fewer steps.\n\n3) Evaluate with efficiency metrics, not accuracy alone.\nMobileRAG reports not just success, but also number of LLM calls and action steps, and shows its design cuts both dramatically [6]. It also times retrieval calls (e.g., LocalRAG at 0.005s) to show real runtime gains [6]. For your loop, track: task success, number of tool calls, number of reasoning steps, and wall\u2011clock time; treat more steps or time as a cost.\n\nTogether, these make \u201cstopping early\u201d and \u201cusing fewer steps\u201d part of what you optimize and measure, so the loop can\u2019t just burn cycles indefinitely.", "tags": ["_untagged"], "full_faithfulness": 1.0, "full_answer_relevancy": 0.8728345603000001, "seed_faithfulness": 0.4545454545, "seed_answer_relevancy": 0.8754390602000001, "context_relevance": 0.35550799034346287, "context_relevance_error": null, "context_relevance_reason": "The context passages provide some relevant information about agentic RAG systems and their components, but they do not directly address the user's specific question about evaluating agentic RAG loops and preventing them from burning cycles. While there are mentions of decision-making processes and optimization objectives, the lack of direct guidance on evaluation methods or strategies for managing cycle efficiency limits the overall relevance to the user's inquiry.", "structure_citation": 0.7737142774755267, "structure_error": null, "structure_reason": "The Actual Output effectively addresses the question of evaluating agentic RAG loops by providing actionable strategies supported by specific examples. It includes claims with [n] markers that correspond to snippets in the Context, particularly referencing [3] and [4] to support its points. However, while the claims are generally well-supported, there are minor issues with the clarity of how some claims directly relate to the snippets, which slightly detracts from the overall alignment with the evaluation steps.", "hallucination_score": 0.8560851830409296, "hallucination_error": null, "hallucination_reason": "The response effectively addresses the input question by providing actionable strategies to prevent agentic RAG loops from running indefinitely. It outlines three key methods: decision-making on RAG usage, training on efficient decision paths, and evaluating with efficiency metrics. Each method is supported by relevant examples from existing systems, demonstrating a strong alignment with the context provided. The only minor shortcoming is a lack of explicit mention of how these strategies integrate with the broader reinforcement learning framework described in the context.", "contexts_used": ["2 Problem Formulation\n\nWe formulate the agentic RAG system within a standard reinforcement learning (RL) framework, comprising two main components: (i) an LLM-based agent ( M \u03b8 ), and (ii) an external environment ( E ) consisting of large-scale clinical corpora, including guidelines, knowledge bases, and patient case records. Details of workflow modeling can be find in Sec. 5.1.1.", "Given a patient's clinical presentation ( P )-including symptoms, medical history, and examination find-", "ings-the agent functions as a sequential decision-making system. At each step, the agent first selects an action type from the finite set:\n\nwhere the actions \u27e8 reason \u27e9 and \u27e8 diagnose \u27e9 represent the agent's internal analytical processes, corresponding to reasoning steps or final diagnostic decisions, and the other three denote interactions with external retrieval tools. Specifically, \u27e8 lookup \u27e9 accesses guidelines, \u27e8 match \u27e9 identifies similar cases, and \u27e8 search \u27e9 queries broader clinical knowledge sources.", "After choosing an action type from A , the agent generates the corresponding textual specification \u03c4 . In the case of \u27e8 reason \u27e9 and \u27e8 diagnose \u27e9 , \u03c4 represents the generated analytical content, while for the retrieval actions, it represents the corresponding search query. Thus, a complete action step is given by\n\nwhere \u03b1 \u2208 A denotes the selected action type and \u03c4 its textual specification.", "Then the external clinical corpora environment ( E ) will in response to the action,defined as:\n\nwhere f denotes the retrieval feedback.", "Throughout the agentic RAG procedure, the agent executes a sequence of interleaved actions of various types ending with the \u27e8 diagnose \u27e9 action:\n\nwhere T denotes the total steps and \u03b1 T = \u27e8 diagnose \u27e9 .", "The ultimate objective is to optimize the entire agentic RAG action trajectory, covering both the selection of action types and the generation of their textual specifications, so as to achieve the best possible diagnostic outcome. Denoting the importance weight for each action step as \u03bb t , the final optimization objective is:\n\nwhere Rwd t represents the reward function for each step.", "Case Illustration. To further clarify the problem formulation, we illustrate an exact case about a 51-year-old man with a 15-year history of alcohol use disorder who presents with a series of symptoms and is ultimately confirmed as acute myeloid leukemia , collected from MIMIC-IV [22], accompanied by the traceable diagnostic reasoning generated by Deep-DxSearch:", "4 Related Work\n\nWe position our work at the intersection of four key research directions: decomposition for multihop reasoning, RAG with heterogeneous sources, LLM-based routing, and reflexion RAG method.", "Multi-Hop Reasoning and Decomposition Some researchers find that the reasoning step length and knowledge recall can contribute to multi-hop QA accuracy (Jin et al., 2024c,a). Multi-hop question answering (QA) requires breaking down complex queries into simpler subtasks. Decomposed Prompting proposes a modular planner-executor framework to tackle complex reasoning tasks (Khot et al., 2023). ADaPT dynamically determines when to decompose using planner-based feedback loops (Prasad et al., 2024). DISC improves scalability by dynamically decomposing inference steps with memory efficiency (Light et al., 2025). SealQA integrates decomposition and verification into search-augmented LMs (Cui et al., 2024). Ye et al. formalize decomposition as a representationquality check in RAG (Zeng et al., 2025). These methods enhance reasoning depth and modular execution but do not address source- or tool-aware routing in heterogeneous environments.", "RAG with Heterogeneous Sources RAG systems augment LLMs with both structured and unstructured knowledge (Lewis et al., 2020; Talmor et al., 2021). HippoRAG introduces memory alongside structured retrieval (Gutierrez et al., 2024). HippoRAG2 extends it with continual memory using clustering and profiling (Guti\u00e9rrez et al., 2025).", "InfuserKI enhances LLMs with knowledge graphs via infusing (Wang et al., 2024). AutoSchemaKG automates schema induction for knowledge graph construction from web corpora (Bai et al., 2025). These approaches handle heterogeneous memory and structure, but still rely on flat retrieval indexes without per-subquestion navigation.", "LLM as Router for Source-Aware Retrieval Recent work explored by LLMs to control retrieval behavior, but often under homogeneous assumptions. Probing is popular in the LLM area (Jin et al., 2024b), Probing-RAG (Baek et al., 2025) leverages LLMs' self-reflection to guide document selection, but operates over a single unified corpus. OmniRouter (Mei et al., 2025b) introduces costaware retrieval routing over sub-indices, assuming similar retrieval formats. Toolformer (Schick et al., 2023) fine-tunes LLMs to call APIs, yet does not support structured routing or modular tool orchestration. By contrast, DeepSieve treats routing as a decision step, matching each subquestion to a specific source profile drawn from a heterogeneous set of corpora and tools (e.g., SQL, APIs, RAG corpora).", "Agentic Methods Agentic systems empower LLMs to reason, plan, and act over multi-step inference chains. Some agentic methods are also used for RAG tasks. ReAct (Yao et al., 2023) merges tool use and thought generation in a unified loop. ReWOO (Xu et al., 2023) decouples retrieval from reasoning to reduce token cost. MA-RAG (Nguyen et al., 2025) introduces agentic collaboration via CoT-based subquerying. AMem (Xu et al., 2025) augments LLMs with lifelong memory and dynamic memory selection during inference. In contrast, DeepSieve builds upon this line of work by introducing an explicit modular planner-routerretriever loop, enabling fine-grained reasoning over decomposed subquestions, recursive reflexion over failures, and heterogeneous tool dispatch.", "3 Methodology\n3.3 Multimodal Fusion\nGround Truth: A\n\nQ1: Histological features of invasive lobular\n\ncarcinoma of the breast, including Indian-file\n\npattern\n\nA: Histological features of invasive lobular\n\ncarcinoma of the breast include uniform small\n\nround tumor cells infiltrating the stroma in a\n\nsingle-file (Indian-file) arrangement and\n\ncircumferentially around ducts in a target-like\n\npattern.\n\nQ2: How to differentiate ductal, papillary, and\n\nmucinous breast carcinoma histologically\n\nA: Histologically, ductal carcinoma shows glandular,\n\npapillary, cribriform, or diffuse growth patterns,\n\noften forming nests, trabeculae, or cords. Papillary\n\ncarcinoma is characterized by prominent papillary\n\nstructures with central fibrovascular cores.\n\nMucinous carcinoma displays abundant extracellular\n\nmucin, with tumor cells floating loosely in mucin\n\npools.\n\nSummarized Information\nInvasive lobular\ncarcinoma\n\n.\n\nCharacteristic\nhistologic features\n\nare: one cell wide\n\nfiles of round regular tumour cells ( Indian file )\n\nTop1 for tool call 1\n\nDuctal and lobular carcinomas represent75% and 25% of metastases\u2026, papillary , cribriform or diffuse patterns\u2026 mucinous carcinoma \u2026 B Hybrid endometrioid- like/ mucinous\n\nTop1 for tool call 2\n\nAgentic Router(ours)\n\nthe top rank due to its combination of high similarity and concentrated attention.", "3.4 Agentic Diagnostic Workflow with Multimodal Evidence Tracing\n\nOur system adopts an intelligent multi-agent workflow that transforms a diagnostic query into an evidence-grounded conclusion. Given a user input (e.g., a pathology image and candidate diagnoses), the Agentic Router module first parses the query, decomposes it into sub-tasks aligned with each diagnostic candidate, and formulates a high-level retrieval plan. It delegates retrieval and evidence aggregation to the VRAG Agent , which operates under its guidance.", "The VRAG Agent executes a multi-turn retrieval process against the multimodal knowledge base. It first conducts text-based retrieval using candidate-specific keywords, and then re-ranks the retrieved entries by evaluating imagetext similarity with the query. Through iterative evidence refinement and summarization, as illustrated in Figure 2, the agent constructs a structured prompt containing the top 1ranked visual evidence for each candidate. This prompt is then passed to a specialized vision-language model, which performs contrastive reasoning to produce a diagnosis and evidence-grounded report. Details of each module and step are provided in Appendix B.", "3.5 Tool Integrated RL for Agentic Router\n\nTraditional RAG systems are often treated as static pipelines, without adapting their behavior to each query's complexity. To overcome this limitation, we introduce a reinforcement learning (RL) framework that enables an agent to learn dynamic invocation and routing strategies [35]. The agent's task is to generate a decision path specifying whether\n\nFigure 3: Illustration of the reranking process utilizing modality fusion.\n\nOriginal Text Query:\n\nThis image shows\nmonophasic synovial sarcoma\n\n. What are its key histologic features?\n\nA.", "Dense, uniform spindle cells with scant cytoplasm in short, intersecting fascicles.\n\nB.\n\nBiphasic: glandular epithelium with spindle cell stroma.\n\nC.\n\nScattered adipocytes in fibrous stroma with atypia.\n\nD.\n\nLarge polygonal cells with coarse eosinophilic granules.\n\nRewrited Search Query:\n\nWhat are the histological features of\n\n?\n\nEach query token\n\nembedding is matched to the\n\nmost similar image-patch embedding\n\non\n\nindexed pages, and their similarity scores are summed to rank the pages.\n\nImage Query:\n\nRetrieval\nPatho-Fusion\n\nScore:\n\nText Similarity Matrix Image 0.6638 0.6671 Final Top1 Cytologic appearance of Ewing sarcoma\n\n/PNET ...\n\n0.5878 Top20 from \u2026\u2026 Monophasic synovial\n\nsarcoma.\n\nThe tumor\n\n...\n\n0.6558\n\u2026 monophasic\nsynovial sarcoma\n\nwith ...\n\n0.6190 0.6602 Lose with an adenocarcinoma-like appearance of the\n\nepithelial component.\n\nRerank", "and how to call the RAG system. Formally, given an input query Q orig, the agent's policy \u03c0 outputs a path P , optimized to maximize the expected similarity to a ground-truth decision path P gt\n\nThe hierarchical reward R final compares the generated path to the target path step by step (see Algorithm 1). The agent performs a sequence of decisions to construct the path:\n\nDecision 1: Whether to invoke RAG?", "-Path A (No RAG Invocation): If the agent decides False , the decision process terminates. This is for simple queries that can be answered without external knowledge. Final path: { rag : False }\n\n-Path B (Invoke RAG): If True , proceed to the next decision.", "Decision 2: How to decompose the task? (Only applies if RAG is invoked)\n\nThe agent may choose to rewrite the query one or more times to better surface its core semantics and improve retrieval quality. This is not a mechanical rewriting process, but a targeted transformation to better align with the retrieval engine.\n\n\u2022\n\nDecision 3: Whether to use a tissue-specific classifier? (Only applies if RAG is invoked) The agent decides whether to enable a classifier to restrict retrieval to a relevant knowledge partition.", "-Path B.1 (Global Retrieval): If False , RAG retrieves documents from the full corpus. Final path: { rag : True , rewrite count : n, classifier : False }\n\n-Path B.2 (Classifier-Based Retrieval): If True , proceed to the next decision.", "Decision 4: Whether the classifier assigns the query to the correct partition? (Only applies if a classifier is enabled)\n\nThe agent selects a classifier from the available set { C 1 , . . . , C m } and attempts to assign the query to the correct partition. The effectiveness of this decision depends on both the selection of the classifier and the correctness of the classification. Final path: { rag : True , rewrite count : n, classifier : True , partition : C j }\n\nRL Training with GRPO We use the GRPO algorithm [36] to train the policy \u03c0 \u03b8 . For each query Q orig, the agent generates multiple decision paths:\n\nwhere each P i is a complete decision path and r i is its corresponding reward score r i \u2208 [0 , 4] based on the hierarchical reward function. We normalize the rewards within the group to compute the advantage function A i ( P i | Q ) :\n\nAlgorithm 1: Hierarchical Reward Computation\n\n\u0338", "Require: Agent path P , Ground Truth P gt 1: R final \u2190 0 2: if P. rag = P gt . rag then 3: return R final \u25b7 Incorrect Decision 1 4: end if 5: if P gt . rag = False then 6: R final \u2190 4 \u25b7 Correct Decision 1 (Path A) 7: else 8: R final \u2190 1 \u25b7 Correct Decision 1 (Path B) 9: if P. rewrite count = P gt . rewrite count then 10: R final \u2190 R final +1 \u25b7 Correct Decision 2 11: end if 12: if P. classifier = P gt . classifier then 13: if P. classifier = False then 14: R final \u2190 R final +2 \u25b7 Correct Decision 3 (Path B.1) 15: else 16: R final \u2190 R final +1 \u25b7 Correct Decision 3 (Path B.2) 17: if P. partition = P gt . partition then 18: R final \u2190 R final +1 \u25b7 Correct Decision 4 19: end if 20: end if 21: end if 22: end if 23: return R final\n\nwhere \u00b5 Q and \u03c3 Q are the mean and standard deviation of rewards within group G Q , and \u03b7 is a small constant for numerical stability.", "B. RAG Agent\n\nThis section details how we employ the RAG Agent to retrieve functions semantically similar to the target function to assist in translating the source function. The RAG Agent comprises four main components: Preprocessing, Selective Retrieval, Multi-Route Retrieval, and Result Filtering.", "1) Preprocessing: To enable effective retrieval, we first store the project's metadata in a vector database. Given the metadata of a project repository, we begin by preprocessing it and generating embeddings for two distinct components of the metadata, which are subsequently stored in the vector database. The first component consists of the method bodies of all sourcetarget translation pairs, and the second component includes the function names of all methods in the target repository. These two components are stored separately in two vector databases, referred to as the pair store and the name store, respectively.", "2) Selective Retrieval: In real-world code repositories, many functions are standalone, meaning they do not reference external classes or functions. For such functions, it is unnecessary to employ the RAG process to retrieve additional information during translation. Bypassing the RAG process in these cases can conserve computational resources and prevent redundant information from adversely affecting translation quality. Therefore, before initiating the RAG process, we first enable the agent to determine whether the target function requires RAG assistance. This decision is based primarily on whether the target function is standalone and whether it exhibits sufficient simplicity. If the agent determines that RAG is not needed, the RAG process is bypassed.", "3) Multi-Route Retrieval: If the agent determines that a target function requires the RAG process, a multi-route retrieval procedure is initiated. This procedure consists of two complementary components. The first component involves retrieving source-target translation pairs where the source function is similar to the current source function from the pair store. A hybrid retrieval strategy is employ: both dense retrieval (based on cosine similarity) and sparse retrieval (based on BM25) are utilized to independently retrieve the top-k most similar results from the pair store. These results are subsequently ranked using the Reciprocal Rank Fusion (RRF) method to produce the final top-k most relevant results. The second component involves retrieving functions from the name store based on the target function's name. Using sparse retrieval (BM25), the top-k functions with the most similar names are retrieved. If a function name exactly matches the target function name, its similarity score is set to 1.0. Finally, the results from both retrieval paths are merged to form the output of the multi-route retrieval step.", "4) Result Filtering: Not every function retrieved through the multi-route retrieval process is beneficial for translating the source function into the target function. Including irrelevant functions as additional context can be not only unhelpful but potentially detrimental to translation quality. Therefore, it is essential to filter the retrieved results to retain only those functions that are genuinely useful.", "We leverage the agent to perform this filtering operation. For each candidate function in the retrieved set, the agent compares it with the target function, considering factors such as functional similarity, structural resemblance, and referenced context. Based on this comparative analysis, the agent determines whether the candidate function is sufficiently similar to the target function to provide useful guidance for translating the source function. Functions deemed relevant are retained, while others are discarded.", "8 Discussion and Future Direction\n\n8.2 Implications for Proprietary Data\n\nA key development in RAG is its application to enterprise AI, enabling access to proprietary data without embedding sensitive information into model parameters [35]. This architecture supports data privacy and ensures outputs remain grounded in up-to-date knowledge [74]. Unlike traditional LLMs, which require retraining to update internal knowledge, RAG allows retrieval components to be independently refreshed [74].", "However, enterprise RAG deployments introduce new security concerns. The retriever could inadvertently expose confidential information if strict access controls are not enforced. Organizations must implement robust authentication mechanisms, ensuring that retrieved documents align with user permissions. Privacy-preserving techniques such as encrypted search indices and federated retrieval [58] are promising solutions to mitigate risks.", "The ability of RAG to function over proprietary knowledge bases has made it a preferred choice for industries handling sensitive information, including finance, healthcare, and legal sectors. As enterprises scale RAG systems, optimizing retrieval latency and ensuring regulatory compliance will be paramount.", "8.3 Future Research Directions\n\nKey avenues for advancing RAG include:\n\nMulti-hop retrieval : RAG pipelines typically retrieve single passages, but many queries require reasoning over multiple documents. Standard RAG models struggle with multi-hop questions [88], motivating research on multi-step retrieval and reasoning (e.g., chained or iterative RAG methods).", "Secure and privacy-aware retrieval : As RAG is applied to sensitive data, privacy becomes critical. Recent work proposes encrypting knowledge bases and embeddings to prevent unauthorized access while preserving performance [107], and integrating differential privacy and secure computation to guard against information leakage [42].", "Multimodal and agentic RAG : Extending RAG beyond text can leverage rich context. Multimodal RAG (MRAG) systems incorporate images, video, or other modalities into retrieval and generation, reducing hallucinations and improving performance on visual queries [59]. Separately, Agentic RAG embeds autonomous agents into the retrieval pipeline, enabling dynamic planning, tool use, and multi-step reasoning in the loop [83].", "Structured knowledge integration : Traditional RAG relies on vector search over unstructured text, which can miss complex relationships. Incorporating structured knowledge (e.g., knowledge graphs) can improve semantic understanding. Surveys and systems demonstrate that linking RAG to knowledge graphs yields more accurate and coherent answers [14, 24].", "Real-time or streaming retrieval : Many applications require RAG to operate on live data streams. For example, systems like StreamingRAG build evolving knowledge graphs from streaming inputs to provide timely context updates, achieving much faster throughput and higher temporal relevance than static RAG [79].", "8.4 Emerging Practical Applications\n\nRAG is finding use in a range of new applications:\n\nEnterprise search : Companies employ RAG to query internal document repositories. By building RAG pipelines around enterprise vector databases, organizations can retrieve and", "synthesize answers from private knowledge with strong performance, scalability, and security guarantees [89].\n\nReal-time assistants : Personal or domain-specific AI assistants can use RAG to retrieve current information (e.g., news, stock prices, weather) on the fly. The RAG framework naturally supports grounding LLM responses in real-time data, improving accuracy and user trust [33].", "Fact-checking : Integrating RAG into fact-checking pipelines can automate evidence retrieval. Recent systems use RAG-based reasoning (e.g., chain-of-RAG) to fetch and synthesize evidence for claims, improving veracity predictions in political and news domains [1].", "Experiment Results\n\nComparison with SOTA frameworks. Table 2 presents the result of MobileRAG and all baselines on MobileRAG-Eval. The results demonstrate that our method achieves superior perfor mance on various metrics, especially on AS, MobileRAG can achieve a 100% success rate, confirming the effectiveness of LocalRAG. By constructing app information into a local knowledge base, it can effectively mitigate operational errors or omitted steps caused by mobile agents' complete reliance on LLM's comprehension capabilities. Meanwhile, we achieve SOTA performance on TSR, achieveing 10.3% improvement over other baseline. This result demonstrates that MobileRAG can effectively interpret users' ambiguous intentions via InterRAG, making it particularly well-suited for real-world application scenarios.\n\n3 Claude-3.5 version: Claude-3.5-Sonnet-2024-10-22\n\n4 Gemini-1.5 version: Gemini-1.5-Pro-2024-04-09\n\n5 GPT-4o version: GPT-4o-2024-11-20", "| | Model.Model | Single.Single | LLM Dual.LLM Dual | Triple.Triple | Action.Single | Action.Dual | Action.Triple |\n|---:|:----------------|----------------:|--------------------:|----------------:|----------------:|--------------:|----------------:|\n| 0 | Mobile-Agent-v2 | 3 | 7 | 11.3 | 3 | 7 | 11.3 |\n| 1 | Mobile-Agent-E | 1.9 | 4.8 | 8 | 1.9 | 4.8 | 8 |\n| 2 | MobileRAG | 1.9 | 3.7 | 4.3 | 1 | 1 | 1 |\n\nTable 4: Efficiency of MobileRAG on Single-, Dual-, and Triple-App Tasks. 'LLM' indicates the number of LLM calls, and 'Action' indicates the number of mobile steps.", "We further validated the effectiveness of MobileAgent in real-world environments (Xiaomi 15 Pro). As shown in Table 2, MobileAgent performs exceptionally well across several key indicators and can reliably handle complex and dynamic practical app scenarios. Notably, compared to XMLbased and GUI-based mobile agents, MobileRAG can directly retrieve relevant context via LocalRAG, efficiently access and launch target apps, and flexibly leverage systemlevel functionalities. Owing to its lightweight architecture and high compatibility, it is well-suited for widespread deployment on real-world consumer mobile devices.", "Table 3 presents a comparison of MobileRAG across different backbone LLMs. MobileRAG consistently outperforms across all recent LLMs, owing to its superior framework design. It effectively leverages retrieval of historical information, local apps, and web data, which substantially mitigates comprehension or operational errors caused by large model hallucinations, and greatly enhances the reliability and practical value of the system.", "Efficiency Analysis. Table 4 reports the efficiency of MobileRAG by quantifying the number of steps required for LLM-based agents to invoke and open apps, an essential factor in evaluating the model's practical applicability. Compared to other methods, MobileRAG demonstrates a substantial efficiency gain. It can be attributed to the efficient retrieval architecture in MobileRAG, where LocalRAG requires only 0.005 seconds per retrieval and interRAG only 0.4 seconds, thereby significantly decreasing the number of LLM-based agent calls and accelerating the overall runtime. With the support of LocalRAG, MobileRAG significantly reduces the number of steps to open apps, especially for multi-app tasks. By enabling direct retrieval and execution of local apps, LocalRAG greatly enhances the efficiency of MobileRAG. Additionally, fewer operational steps reduce", "Figure 4: Leveraging MemRAG to retrieve previously successful steps enables the mobile agent to perform rapid operations. With its advanced memory and learning mechanisms, MemRAG empowers the agent to adapt based on past successes. As users interact with the agent and more queries accumulate, the mobile agent evolves, gradually enhancing its autonomous capabilities.\n\nSend\n\nmessage to Jelly t0 invite him to watch 'The\n\nBoys' season3, tell him the introduction.\n\nOpen\nMessages'\n\nand tap into Jelly's chat\n\nSummarize \"The Boys\" intro and send it to Jelly\n\nMemRAG: Similar in MemRAG dalabase\n\ntask: \"Find the movie 'The Boys\"\n\nand choose season3 and add it to watchlist, then\n\nTap\nsummary the introduction\nmessage to\n\nto invite him to watch The Boys'\n\nseason3, tell him the introduction\n\nTap & Type in Texts\n\nThe MemRAG contains\n\nsultable message inviting someone\n\nwalch\nThe Boys'\n\nSeason 3, including an\n\nintroduction. The only change needed Is\n\nsend It to Jelly Instead of Mike. The previous action used the \"Google\n\napp, which fits the current task,\n\nMike\n\nthe likelihood of operational errors or omissions, thereby enhancing the overall architectural performance.", "| | Model | AS | AF | RP | TCR | TSR | avg | open app |\n|---:|:----------|------:|-----:|-----:|------:|------:|------:|-----------:|\n| 0 | w/o Local | 93.8 | 75.4 | 96.8 | 82.1 | 67.5 | 11.42 | 3.12 |\n| 1 | w/o Inter | 88.1 | 85.3 | 97.1 | 79.7 | 65 | 9.94 | 1.81 |\n| 2 | MobileRAG | 100 | 86.4 | 98.5 | 91.2 | 80 | 9.15 | 1.86 |\n\nTable 5: Ablation studies of LocalRAG and InterRAG. 'w/o Local' and 'w/o Inter' indicate the removal of LocalRAG and InterRAG, respectively.", "Ablation Studies. Table 5 shows the results of Ablation studies, we find each component of MobileRAG plays a key role in improving performance. First, we use LLM-based visual perception instead of LocalRAG. The results show varying degrees of decline across different metrics, demonstrating that LocalRAG achieves significant performance improvements compared to GUI-based mobile agent, particularly in terms of interaction steps. LocalRAG can directly validate and operate locally installed apps, thereby eliminating unnecessary steps such as manual searching and interface understanding. Second, we remove InterRAG in our framework, we observed a decrease of 11.9% and 18.75% in AS and TSR, respectively. This finding confirms that InterRAG is capable of accessing the updated and comprehensive app information from external sources, thereby enabling more informed and accurate app selection.", "We further analyzed the effectiveness of MemRAG by testing 15 groups of data, encompassing cases with complete consistency, similarity above 80%, and similarity below 80% (More detail are provided in Appendix B.). As shown in Table 6, MemRAG effectively enhances model performance, particularly by reducing the number of operational steps, with an average reduction of 2.4 steps. These results clearly demonstrate that MobileRAG is capable of learning relevant operational procedures from historical successful steps via MemRAG, thereby improving both efficiency and overall task accuracy.", "| | Model | AS | AF | RP | TCR | TSR | Step |\n|---:|:---------|-----:|-----:|-----:|------:|------:|-------:|\n| 0 | w/o Mem | 100 | 88.7 | 98.4 | 93.8 | 86.7 | 10.6 |\n| 1 | with Mem | 100 | 91.1 | 98.7 | 97.6 | 93.3 | 8.2 |\n\nTable 6: Ablation studies of MemRAG. 'w/o Mem' indicates the removal of MemRAG.", "Case Study. Figure 3 demonstrates the reasoning process of MobileRAG when no suitable match is found by MemRAG. We find that MobileRAG has the following advantages: 1) utilizing InterRAG enables comprehensive understanding of user queries; 2) leveraging LocalRAG allows the system to directly launch relevant applications from the local app library, minimizing accidental touches or erroneous actions; 3) MobileRAG's robust retrieval capabilities help reduce the number of operational steps, particularly benefiting long sequential tasks. Figure 4 illustrates the inference process when MemRAG successfully identifies a matching instance. Through MemRAG, It can effectively extract and leverage information from historically correct operation sequences, thereby enhancing agent performance and further decreasing the number of operational steps."]}, {"type": "entry", "index": 60, "question": "how do i measure \u201cthis helped the user\u201d when the task is research, not fact lookup", "answer": "For research tasks, you can\u2019t just check if a single fact is right; you need to measure whether the tool improved the *overall research process* and outcomes.\n\n1. Define a concrete research task and \u201csuccess\u201d criteria\nIn HypoChainer, a discovery task was judged successful if users stayed within a reasonable search space (\u2264300 predictions) and still found enough key items (\u22653 of 5 reference predictions) [2]. For your setting, define equivalents, e.g.:\n- Identified N of M key sources, arguments, or hypotheses.\n- Produced a final memo/hypothesis that an expert rates as logically coherent and well-supported.\n\n2. Compare with and without the tool\nUse a within-subject or between-group design: some questions or projects done with AI, some without [1][2]. Then measure:\n- Completion rate within a time limit [2].\n- Time taken per task [1].\n- Quality of outputs: expert judges mark answers as exact, substantially similar, unclear, or incorrect vs a vetted reference set [1], or rate hypotheses on logical coherence and correctness [2].\n\n3. Capture user-perceived \u201chelpfulness\u201d\nUse questionnaires about:\n- Ease of identifying research directions [2].\n- Logical coherence of resulting hypotheses [2].\n- Workflow rationality and flexibility [2].\n- Informativeness and understandability of the tool\u2019s explanations [2].\nHigher scores here are evidence users *felt* helped.\n\n4. Look at interaction patterns\nTrack how people actually use the tool:\n- Number of follow-up queries, when they consult original documents, etc. [1]\n- Whether the tool helps them overcome dead-ends and revise their thinking (e.g., refining and redirecting hypotheses over time) [2].\n\nCombining these\u2014task success, quality, efficiency, user ratings, and behavioral traces\u2014gives a practical measure of \u201cthis helped the user\u201d for research work.", "tags": ["_untagged"], "full_faithfulness": 1.0, "full_answer_relevancy": 0.8167052890000001, "seed_faithfulness": 1.0, "seed_answer_relevancy": 0.8300081344, "context_relevance": 0.6609927400716304, "context_relevance_error": null, "context_relevance_reason": "The context passages provide relevant information about measuring the performance of AI tools in research tasks, particularly in the financial analysis domain. They discuss methodologies for evaluating AI's impact on human analysts, which aligns with the user's question about measuring effectiveness in a research context. However, while the passages are informative, they could more directly address specific metrics or methods for quantifying 'this helped the user' in research tasks, which slightly limits the overall relevance.", "structure_citation": 0.5802086280978533, "structure_error": null, "structure_reason": "The Actual Output provides a comprehensive approach to measuring user help in research tasks, outlining specific criteria and methods. However, it includes citations [1] and [2] that do not correspond to any snippets in the Context, leading to unsupported claims. While the response is well-structured and relevant, the lack of matching citations detracts from its overall credibility.", "hallucination_score": 0.8436724348632962, "hallucination_error": null, "hallucination_reason": "The response effectively addresses the question of measuring user help in research tasks by outlining a comprehensive approach that includes defining success criteria, comparing outcomes with and without the tool, capturing user perceptions, and analyzing interaction patterns. These points align well with the context provided, which discusses the complexities of evaluating AI tools in real-world tasks. However, while the claims are mostly supported, the response could benefit from more explicit connections to the specific benchmarks and methodologies mentioned in the context, which would strengthen its alignment.", "contexts_used": ["Measuring the Performance of AI Models on Tasks\n\nThe literature in artificial intelligence and machine learning is dedicated to studying the performance of LLMs on benchmarks and has continued to struggle to capture the speed of development and domain-specific applicability. ARC (Chollet 2019) and DROP (Dua, et al. 2019) are two of the premier assessment tools used to evaluate the ability for an LLM to gather important information from documents in a manner that emulates research assistance. RAG specific benchmarks such as TAT-QA (Zhu, et al. 2021), cover the effectiveness of finance information retrieval while benchmarking other domains. However, these benchmarks and others often fail to capture the complexities of real-world tasks, particularly in the economics and financial policy domains. Even with the introduction of agents, few benchmarks have emerged looking at making economic-based choices such as EconEvals (Fish, et al. 2025). However, such benchmarks often also focus on automated analysis, rather than the effects of pairing a human analyst with these tools, leaving them limited from a qualitative scope.", "In our study, we investigate a real-world information extraction and data annotation task previously completed by human analysts. By replicating an existing, completed task, we can create a robust evaluation of the impact of introducing these AI tools on this specific workflow. Additionally, we structure our evaluation as a within-subjects design, allowing us to understand the effect of introducing use of the AI tool as a treatment effect compared with a control human-only approach, all while controlling effectively for per-annotator skill.", "3. Methodology\n\n3.1 Case study using banks' public disclosures.\n\nThis task makes for an excellent case-study in assessing the impact of exposing analysts to AI tools to help complete their work. It is a complex, real-world task in the financial analysis domain, not a multiple choice question benchmark. The questions often required combining information from multiple different sources within the documents to produce a final answer. Additionally, because of heterogeneity in how banks disclose information, we can assess the efficacy of the AI-assisted approach under incomplete information. Because the documents do not always contain complete information to answer all the questions for every single bank, we were able to assess whether AI reacted appropriately in scenarios when information wasn't available by correctly refusing to answer.", "3.2 Study Design\n\nThis study employed a mixed experimental design to evaluate the impact of an Artificial Intelligence (AI) Retrieval-Augmented Generation (RAG) tool on the performance of human annotators performing a complex information extraction task using public disclosure documents from Global Systemically Important Banks (GSIBs). The core experiment utilized a within-subjects design, where specific questions within each bank annotation task were randomly assigned to either a 'Control' condition or an 'AI' (naive use) condition for each participant. This allowed for comparing the performance under each condition while controlling for individual annotator differences.", "A subsequent repeated-measures phase was conducted later, where participants revisited the questions previously assigned to the 'AI' condition, this time operating under an 'AI+' (interactive use) condition. This allowed for a direct comparison of naive versus interactive AI tool usage on the same set of questions while controlling for recall effects.", "3.3 Participants\n\nThree professional financial analysts participated in the study. Participants were recruited via a convenience sample of volunteers within the researchers' organization and completed the experimental tasks during their regular work hours. Prior to the experiment, participants completed a multiple-choice quiz designed to assess their baseline familiarity with both the subject matter (GSIB disclosures and related finance topics) and AI tools. These baseline skill assessments are used descriptively and for subsequent analysis exploring potential interactions between skill profiles and condition effects.", "3.3 Materials\n\nBank Documents: The materials consisted of comprehensive sets of public disclosure documents retrieved from bank websites and formatted as pdf files (e.g., annual reports, non-financial reports, regulatory filings) for a sample of nine European GSIBs. These document sets were identical to those compiled and then used in the original analysis by Beltran et al. (2023) and were provided by the original research team, ensuring consistency and relevance.", "Annotation Questions: A standardized set of 30 questions was used for all banks and participants. These questions were selected in consultation with the original Beltran et al. (2023) research team to be representative of the full 150-question set used in their study. The selection aimed for diversity across question complexity (high: 9, medium: 11, low: 6), required answer format (categorical: 9, Yes/No: 10, exact figure/value: 7), and thematic category (risk management: 11, financed activity: 10, oversight rules: 3, external project ratings: 2). Each question included detailed instructions and definitions identical to those used by Beltran et al. (2023).", "3.4 Experimental Conditions\n\nParticipants completed annotation questions under three distinct conditions:\n\nControl Condition: Participants answered assigned questions using only the provided PDF documents and a standard PDF reader. They were permitted to use the reader's built-in text search (\"find\") functionality but were explicitly prohibited from using any other external tools, web searches, or AI assistance. The focus was on manual information retrieval and synthesis based solely on the source documents.", "AI (Naive) Condition: Participants answered assigned questions using a provided AI RAG chat tool pre-loaded with the relevant bank's document collection. Participants were instructed to formulate a single prompt based on the annotation question and submit it to the tool. They were explicitly instructed to record their final answer based only on the AI tool's initial response, without consulting the", "original PDF documents. This condition aimed to simulate a naive or high-trust approach to using the AI assistant.", "AI+ (Interactive) Condition: This condition was implemented in the follow-up phase for questions previously tackled under the 'AI' condition. Participants initiated the process identically to the 'AI' condition (formulate single prompt, review AI response). However, they were then given discretion: based on their judgment of the initial AI response, they could choose to (a) ask additional follow-up questions within the AI chat tool, (b) consult the original PDF documents, or (c) do neither or both, before recording their final answer. This condition aimed to simulate a more interactive, discerning, and potentially iterative approach to using the AI assistant.", "AI RAG Tool Specifications: The same AI RAG tool configuration was used for both the 'AI' and 'AI+' conditions. It was implemented using OpenWebUI (version 0.3.2) as the interface. The backend utilized Claude 3.5 Sonnet (model version 20240622) via Amazon Bedrock as the Large Language Model (LLM) with default sampling parameters. Vector embeddings were generated using Amazon Titan Text Embedding v2 via Amazon Bedrock. For the RAG process, documents were chunked using a size of 2000 characters with a 200-character overlap. The retrieval mechanism used a 'top_k' setting of 20, meaning the 20 most relevant chunks were retrieved to inform the LLM's response. This setup was chosen to represent a commonly accessible, though not state-of-the-art, RAG implementation suitable for typical researcher workflows.", "3.5 Procedure\n\nParticipants received initial training covering the task background, the experimental procedures for each condition, instructions on using the AI RAG tool interface, and guidelines for self-reporting time. The detailed instructions embedded within each annotation question served as the primary guide for content interpretation.", "For each assigned bank, the 30 questions were presented in the same fixed order. Prior to the task, each question was randomly assigned (using a computer script with a random number generator, resulting in an approximately 50/50 split) to either the 'Control' or 'AI' condition for that specific annotator-bank instance. Randomization was not stratified by question characteristics.", "Participants performed the tasks at their own discretion regarding location (e.g., office, remote). For each question, participants self-reported their start and end time (wall clock time) to the nearest minute. Durations under one minute were recorded as 0.5 minutes, based on observational validation for a subset of tasks.", "The initial experiment (Control vs. AI) was completed first. The follow-up experiment (AI+) was conducted after a minimum delay of two weeks for all participants (up to three months for two participants) to mitigate potential recall effects from the initial phase.", "Bank assignments were as follows: Annotator A completed tasks for six banks, Annotator B for three banks, and Annotator C for four banks. Annotator C's assignments included two banks also completed by Annotator A and two banks also completed by Annotator B, creating deliberate overlap to enable the assessment of inter-annotator agreement.", "3.6 Outcome Measures\n\nTime Efficiency: Measured as the self-reported time taken per question (in minutes) for each condition.", "Accuracy/Agreement: The primary measure of answer quality. Annotator responses for all conditions were compared against the final, vetted answers established by the Beltran et al. (2023) research team. Although the realistic nature of this task implies that there are no known true values, given the thoroughness of the original research team's work, we consider comparison against these values as an acceptable proxy for task accuracy. A member of the original team performed this agreement assessment, consulting with other original team members for complex cases. For each answer, agreement was categorized as \"exact match,\" \"substantially similar,\" \"unclear,\" or \"incorrect.\" For the main analysis, \"exact match\" and \"substantially similar\" were coded as Agreement = True (representing acceptable answers based on the original team's criteria), while \"incorrect\" was coded as Agreement = False. Answers rated \"unclear\" (20", "answers, roughly 4% of total answers) were excluded from the accuracy analysis. Robustness checks that did not count 'substantially similar' as agreement yielded broadly similar results.", "AI+ Interaction Metrics: For the AI+ condition only, participants recorded (a) the number of distinct follow-up messages sent to the AI tool after the initial prompt, and (b) whether they chose to consult the original PDF documents (Yes/No) before finalizing their answer.", "3.7 Ethical Considerations\n\nInformed consent was obtained orally from all participants prior to their involvement in the study. Participants were informed about the study's purpose, procedures, data handling, and the voluntary nature of participation. Data collected, including annotator responses, timing, and quiz scores, were de-identified before the main statistical analysis to protect participant confidentiality.", "5 EVALUATION\n\n5.1 Case Study: Drug Repurposing Exploration\n\n[Neuronal network dynamics] \u2192 [Manifests as] \u2192 [Episodic neural dysfunction] (Fig. 1- 7 ). E3 first validated this chain (Fig. 1- 8 ) by confirming that retrieved entities aligned with the hypothesis and were supported by contextual KG evidence (Fig. 1- 9 ). He observed that integrating existing KG edges revealed novel sub-paths (Fig. 1-10) and multiple connections between hypothesis-aligned entities and one-hop neighbors, reinforcing the chain's coherence. Cross-referencing the Prediction View , he noted consistency between the hypothesis and most interpretative paths (Fig. 1-11). E3 considered this as evidence that the hypothesis chain explains most repurposing predictions.", "Critical Insight and Hypothesis Revision . However, E3 noted a misalignment between the hypothesis and high-score predictions for Huntington's disease (Fig. 1-12). By filtering and examining these predictions, he observed that the original hypothesis aligned only partially. Guided by the LLM's explanations, E4 's further analysis of the remaining predictions revealed that some drugs were indicated for depression, while others were linked to neurodegenerative diseases. This finding led E3 to infer that the model's predictions regarding Huntington's disease primarily reflect the symptomatic alleviation effects rather than an influence on or delay of its underlying etiology. To reconcile this, E3 refined the hypothesis chain to: [ Drugs treating motor dysfunction, depression, or neurodegeneration ] \u2192 [ related to ] \u2192 [ Interacting genes or pathways ] \u2192 [ Participated in ] \u2192 [ Processes related to motor dysfunction, depression, or neurodegeneration ] \u2192 [ Led to ] \u2192 [ Diseases associated with Huntington's disease ].", "Hypothesis Challenge and Iterative Refinement . During hypothesis validation, E3 noted that the revised chain obscured the majority of the predictions. However, in the Hypothesis View , he identified a link between Huntington's disease and the Trinucleotide Repeat Expansion entity -a genetic mechanism associated with neurodegenerative disorders, as noted by E4 (Fig. 5- 1 ). Intrigued by its absence in the Prediction View , E3 queried the LLM, which explained that Trinucleotide Repeat Expansion is a well-established genetic cause of Huntington's disease , yet it did not surface in any Huntington 's predictions. Intrigued by this discrepancy, E3 revised the hypothesis chain to prioritize predictions involving Trinucleotide Repeat Expansion and submitted the updated hypothesis chain via the for LLM validation. The LLM provided critical feedback: while Trinucleotide Repeat Expansion refers to abnormal DNA sequence elongations that disrupt gene expres-\n\nFig. 5: 1 E3 noticed a link to Huntington's disease from Trinucleotide Repeat Expansion , a 1-hop entity of neurodegenerative disease .", "sion or protein function, its absence in predictions likely reflects the lack of direct therapeutics. However, the LLM emphasized indirect associations in literature, linking the entity to broader processes like DNA_repair and histone deacetylase (HDAC) regulation .", "Insight-Driven Revision . Guided by this feedback, E3 refined the hypothesis to focus on HDAC -related therapeutic targets. A subsequent retrieval (Fig. 1-13) identified therapies modulating HDAC activity in diseases involving abnormal repeat expansions, such as Parkinson's disease [34] and Amyotrophic Lateral Sclerosis (ALS) (Fig. 1-16). This alignment validated the revised chain (Fig. 1-14,15).", "Critical Discovery and Therapeutic Proposal . Re-examining Huntington 's predictions, E3 uncovered a previously overlooked interpretative path: [ Entinostat \u2192 [decreases activity of] \u2192 [ HDAC1 gene] \u2192 [interacts with] \u2192 [ Histone H4 ] \u2192 [gene associated with condition] \u2192 [ Huntington's disease ]. This HDAC -associated interpretative path aligned with literature suggesting HDAC 's role in disease progression [22]. Synthesizing these insights, E3 and E4 proposed the design of a novel cocktail therapy combining three symptom-alleviating drugs with HDAC inhibitors to potentially delay Huntington 's progression. He further considered exploring HDAC -targeted gene therapies as a complementary avenue for further research.", "Takeaway Message. In the case study, E3 and E4 iteratively revised initial hypotheses as new insights emerged, reflecting a dynamic reasoning process. Although the final hypotheses diverged from the original, they represented logical extensions informed by system-driven exploration. For example, in repurposing drugs for Huntington's disease , the expert initially focused on antiepileptic mechanisms for symptomatic relief. However, analysis of high-confidence predictions outside this scope revealed links to broader neurodegenerative pathways, prompting a shift toward a more integrative hypothesis. E3 reported maintaining control over hypothesis development, with HypoChainer offering timely support when exploration stalled-clarifying complex predictions, suggesting new directions, and highlighting supporting KG evidence. E4 praised the system's alignment with conventional workflows, flexible entity categorization, and context-aware knowledge integration, which streamlined hypothesis refinement. They also noted that the system helped transcend initial cognitive frames and uncover unanticipated, yet scientifically valuable associations. This iterative refinement process underscores the system's potential to support deeper and more structured hypothesis generation in complex discovery tasks.", "5.2 User Study\n\nWe recruited 12 graduate students (Mean Age = 26 . 33, SD = 1 . 93; 6 males, 6 females) from bioinformatics or biomedical engineering backgrounds, including 6 PhD and 6 Master's students. Participants were randomly assigned to either the Baseline or HypoChainer group, balanced by degree level (3 PhD and 3 Master's per group).", "Participants completed a drug repurposing task targeting Hemophilia B . To ensure objective evaluation, all were screened to confirm no prior knowledge of the disease or its mechanisms. Five system-generated interpretative paths were selected based on partial alignment with DrugMechDB-curated mechanisms of actions (MOAs) for Eptacog Alfa and Nonacog Alfa . The Baseline system (Appendix Fig. 8) used an LLM-only setup, omitting the Hypothesis View , to isolate the contributions of RAG and the view's design while maintaining the overall workflow. Both drugs were excluded from training data to better simulate real-world discovery conditions. Participants received a 30-minute training session on task goals, system usage, and evaluation criteria, followed by 90 minutes to explore predictions, formulate hypotheses, and submit results. The process was screen-recorded. A task was deemed successful if participants retrieved \u2264 300 predictions and identified \u2265 3 of 5 reference predictions. A post-task questionnaire (Fig. 6) adapted from the System Usability Scale [8] assessed perceived system effectiveness, workflow, and usability.", "Fig. 6: The questionnaire results of the two systems in terms of system effectiveness, workflow and usability ( \u2217 : p < 0 . 05 ).", "In the Baseline group, 2 PhD participants (33.3%) completed the task within the time limit, compared to 4 in the HypoChainer group (3 PhDs, 1 Master's; 66.7%). PhD participants generally outperformed Master's students, likely due to stronger research backgrounds and faster system adaptation. HypoChainer showed clear advantages in Effectiveness , with significantly higher scores for Ease of Identifying Research Directions ( M = 5 . 50, SD = 1 . 38, p < 0 . 05) and Logical Coherence of Hypothesis Chains ( M = 5 . 67, SD = 0 . 52, p < 0 . 05) compared to the Baseline ( M = 4 . 17, SD = 1 . 47; M = 4 . 17, SD = 0 . 75). Both groups scored similarly on Clarity of Interpretative Paths ( HypoChainer : M = 5 . 33, SD = 1 . 03; Baseline: M = 5 . 00, SD = 0 . 89). For Workflow , HypoChainer scored higher on Rationality ( M = 5 . 17, SD = 1 . 33, p < 0 . 05) and Flexibility ( M = 5 . 00, SD = 0 . 89, p < 0 . 05), supported by user feedback indicating fewer hallucinations. However, its integrated features-transitioning among LLM, RAG, and the Hypothesis View-were perceived as more complex ( M = 6 . 00, SD = 0 . 89) than the Baseline ( M = 4 . 83, SD = 0 . 75). In Usability , HypoChainer had a slightly lower score in Learnability ( M = 4 . 50, SD = 0 . 55) than the Baseline ( M = 5 . 00, SD = 0 . 63), suggesting a steeper learning curve. Nevertheless, it received significantly higher ratings for Informativeness ( M = 5 . 83, SD = 0 . 75, p < 0 . 05) and Understandability ( M = 5 . 17, SD = 0 . 75, p < 0 . 05), due to its clear reasoning structure and entitylevel explanations. Overall, HypoChainer provided more effective, transparent, and insightful support for hypothesis generation, despite a slightly higher learning threshold.", "C. Evaluation Metrics\n\nFollowing previous studies [21, 44], we employ two metrics to evaluate the performance of our framework: Precision @ k and Mean Reciprocal Rank (MRR).", "Precision @ k measures a model's ability to include the correct target library within the top-k results, regardless of their order. It is defined as:\n\nwhere N denotes the total number of queries, and HasCorrect k ( q ) equals 1 if the correct target library appears within the top-k results for query q , otherwise it returns 0.", "MRR is a widely adopted metric for evaluating recommendation tasks. It considers the ranks of all correct answers. It is formally defined as:\n\nwhere firstpos ( q ) returns the position of the first correct target library in the results. If the correct target library is not present in the results, the numerator of the formula is zero."]}, {"type": "entry", "index": 61, "question": "does compressing retrieved context help faithfulness or just hide missing evidence", "answer": "Compressing retrieved context can genuinely help faithfulness, but only when it is done in a way that keeps track of what evidence is actually present and how reliable it is.\n\nEvidence-focused compression methods that explicitly reason about which passages matter tend to improve both accuracy and grounding. EviOmni, which extracts \u201crational evidence\u201d instead of passing full passages, achieves very high compression (up to ~38\u00d7 fewer tokens) while matching or beating the full-context baseline on QA accuracy, because it filters noise and keeps key clues rather than just shortening text [1]. This suggests compression can increase the signal\u2011to\u2011noise ratio instead of hiding gaps.\n\nSimilarly, CARE compresses the retrieved context into \u201cmemory embeddings\u201d and fine\u2011tunes them to encode how reliable the context is. When the external passage is helpful, the compressed prompt steers the model toward it; when the passage is misleading, the compressed prompt is trained to down\u2011weight it so the model falls back on internal knowledge [2]. This is directly aimed at improving faithfulness, not masking missing evidence.\n\nOn the other hand, the papers also show why naive compression is risky. Larger models that integrate context more aggressively become slightly less faithful because they start inferring connections not explicitly in the documents [3], and harmful or adversarial documents can drive alignment nearly to zero even though they are \u201cevidence\u201d [4]. Pure length reduction or summarization without explicit evidence reasoning can drop crucial cues or retain persuasive but wrong content, which does amount to hiding missing or bad evidence.\n\nSo: compression helps faithfulness when it is evidence\u2011aware and reliability\u2011aware; simple token squeezing can hurt faithfulness or conceal that the system never actually saw the needed evidence.", "tags": ["benchmarks_eval", "summarization_conflict"], "full_faithfulness": 1.0, "full_answer_relevancy": 0.9176545063, "seed_faithfulness": 0.75, "seed_answer_relevancy": 0.9176284425000001, "context_relevance": 0.21281984889344704, "context_relevance_error": null, "context_relevance_reason": "The context passages provided do not directly address the user's question about whether compressing retrieved context helps faithfulness or just hides missing evidence. While there are discussions about context compression and its effects on performance, they lack a clear connection to the specific inquiry regarding faithfulness and evidence. The passages primarily focus on experimental setups and results without explicitly linking compression to the concepts of faithfulness or evidence retention.", "structure_citation": 0.7869741943186959, "structure_error": null, "structure_reason": "The Actual Output effectively discusses the role of compression in enhancing faithfulness, referencing specific methods like EviOmni and CARE, which are relevant to the claims made. It includes citations [1] and [2] that correspond to the context provided, supporting the claims about evidence-focused compression. However, while it mentions risks associated with naive compression, it could have better addressed the need for citations for all claims, particularly regarding the negative impacts of compression, which were not explicitly supported by the context snippets.", "hallucination_score": 0.8398543811710546, "hallucination_error": null, "hallucination_reason": "The response effectively addresses the question of whether compressing retrieved context aids faithfulness by discussing evidence-focused compression methods and their impact on accuracy and grounding. It supports its claims with specific examples, such as EviOmni and CARE, which align with the context provided. However, while it mentions risks associated with naive compression, it could further clarify how these risks specifically relate to the context of the test case, which would strengthen the overall argument.", "contexts_used": ["4 Experiment\n4.1 Experimental Setup\n\n( i.e., \u03b1 a , \u03b1 l , and \u03b1 f ), we empirically set \u03c9 , \u03b1 a , \u03b1 l , and \u03b1 f as 0.9, 0.8, 0.1, 0.1, respectively, which commonly leads to satisfactory performance. For policy optimization, we set \u03f5 std as 0.1 to stabilize training. During training, we set the PPO clipping hyperparameter \u03f5 and the coefficient \u03b2 controlling KL-divergence as 0.2 and 1 e -2 , respectively. For QA generators, we employ LoRA (Yang et al., 2025) to fine-tune Qwen2.51.5B-Instruct and Qwen2.5-7B-Instruct to predict the golden answer a based on the query and retrieved passages ( q , P ) or only the query (used for the baseline 'Zero'), where LoRA rank is set as 8.", "Passage Retrieval. Following previous works (Wang et al., 2023; Zhu et al., 2024; Zhao et al., 2024a; Zhang et al., 2025a), we retrieve Wikipedia (Wiki) passages for all datasets. We use Dense Passage Retriever (DPR) (Karpukhin et al., 2020) to retrieve the top-5 passages from all Wiki passages, where we use the December 2018 Wiki dump (Karpukhin et al., 2020) as the retrieval corpus, and set the chunk size as 100 words. We use the retrieval toolkit, Tevatron (Gao et al., 2023), to perform corpus encoding, query encoding, and passage retrieval. Table 3 shows the retrieval performance w.r.t. Recall@5 and NDCG@5, where the retrieval passage that contains the golden answer is treated as the positive passage. Besides, for HotpotQA, we compute Recall and NDCG only for the 'bridge' questions, while ignoring yes/no and comparison questions, following previous works (Jiang et al., 2024; Khalifa et al., 2023). The retrieval performance for HotpotQA is relatively low, as it is a multi-hop QA dataset, and answers are usually not spans in retrieval passages.", "Comparison Baselines. To verify the effectiveness of EviOmni , we compare it with the following three groups of competitive baselines: (1) Without Refinement (WR) includes (i) Zero-shot (Zero) generates output relying on LLMs' parametric knowledge; (ii) Full Passage (Full) feeds all retrieval passages into LLMs; (2) Vanilla Refinement (VAR) directly extracts evidence without ex-", "| | Datasets.Datasets | Metrics.Metrics | WR.Zero | WR.Full | VAR.SeleCtx | VAR.LLMLingua-2 | VAR.RECOMP | VAR.FilCo | VAR.SEER | RAR.CoT | RAR.EviOmni |\n|---:|:--------------------|:------------------|:----------------|:----------------|:----------------|:------------------|:----------------|:----------------|:----------------|:----------------|:----------------|\n| 0 | | 1.5B size model | 1.5B size model | 1.5B size model | 1.5B size model | 1.5B size model | 1.5B size model | 1.5B size model | 1.5B size model | 1.5B size model | 1.5B size model |\n| 1 | | EM | 13.74 | 41.97 | 24.68 | 29.36 | 37.40 | 36.62 | 36.93 | 37.70 | 41.14 |\n| 2 | NQ | F1 | 53.16 | 70.07 | 59.52 | 62.82 | 67.18 | 66.63 | 67.11 | 68.09 | 70.77 |\n| 3 | | CR | - | 1.0x | 3.44x | 4.51x | 5.43x | 16.3x | 13.2x | 4.56x | 38.1x |\n| 4 | | EM | 29.10 | 57.02 | 45.13 | 48.67 | 56.56 | 54.06 | 54.57 | 54.29 | 56.84 |\n| 5 | TQA \u2020 | F1 | 64.62 | 80.13 | 73.40 | 75.29 | 79.85 | 78.56 | 78.81 | 79.56 | 80.85 |\n| 6 | | CR | - | 1.0x | 3.38x | 4.52x | 5.35x | 8.55x | 10.3x | 5.02x | 38.8x |\n| 7 | | EM | 12.36 | 19.20 | 15.30 | 16.64 | 18.52 | 18.18 | 18.60 | 19.52 | 20.46 |\n| 8 | HotpotQA \u2020 | F1 | 48.52 | 53.04 | 49.65 | 51.26 | 52.92 | 52.15 | 52.79 | 53.43 | 54.20 |\n| 9 | | CR | - | 1.0x | 3.40x | 4.52x | 5.44x | 18.3x | 15.5x | 4.17x | 33.0x |\n| 10 | 7B size | model | model | model | model | model | model | model | model | model | model |\n| 11 | | EM | 25.04 | 48.78 | 33.91 | 36.51 | 43.77 | 44.79 | 45.01 | 44.49 | 46.95 |\n| 12 | NQ | F1 | 60.88 | 74.40 | 65.71 | 67.77 | 71.61 | 72.30 | 72.76 | 73.09 | 73.99 |\n| 13 | | CR | - | 1.0x | 3.44x | 4.51x | 5.43x | 15.4x | 12.4x | 3.36x | 17.9x |\n| 14 | | EM | 47.31 | 65.34 | 58.72 | 60.50 | 64.40 | 63.46 | 64.20 | 63.91 | 64.76 |\n| 15 | TQA \u2020 | F1 | 74.36 | 84.83 | 80.95 | 81.94 | 84.12 | 83.66 | 84.14 | 84.45 | 84.74 |\n| 16 | | CR | - | 1.0x | 3.38x | 4.52x | 5.35x | 7.77x | 9.70x | 3.33x | 17.6x |\n| 17 | | EM | 17.95 | 25.82 | 21.27 | 23.45 | 24.84 | 25.27 | 25.81 | 27.25 | 28.02 |\n| 18 | HotpotQA \u2020 | F1 | 53.07 | 58.50 | 54.90 | 56.65 | 58.02 | 58.15 | 58.63 | 59.84 | 60.30 |\n| 19 | | CR | - | 1.0x | 3.40x | 4.52x | 5.44x | 17.5x | 14.3x | 3.32x | 16.5x |", "Table 4: Overall performance comparison on NQ, TQA, and HotpotQA datasets, where the best results are boldfaced and the second-best results are underlined. EM, F1, and CR denote exact match, F1 score, and compression ratio (the higher the better), respectively. The 1.5B/7B size models represent Qwen2.5-1.5B/7B-Instruct. The \u2020 symbol denotes out-of-domain (OOD) evaluation datasets for EviOmni , as it is only trained on NQ.", "plicit thinking, which includes (i) Select Context (SeleCtx) (Li et al., 2023) identifies and prunes redundancy in the input context based on perplexity; (ii) LLMLingua-2 (Pan et al., 2024) distills compression knowledge from GPT-4 to reduce crucial information losing; (iii) FILCO (Wang et al., 2023) trains a context filtering model to identify key clues; (iv) BottleNeck (Zhu et al., 2024) applies the information bottle theory to select SFT data used to optimize filtering; (v) SEER (Zhao et al., 2024a) learns to extract desired evidence via self-aligned learning; (3) Rational Refinement (RAR) includes (i) Chain-of-Thought (CoT) (Wei et al., 2022) generates query-related information from retrieval passages with explicit thinking.", "4.2 Main Comparison (RQ1)\n\nThe overall comparison results on NQ, TQA, and HotpotQA are shown in Table 4. From the experimental results, we mainly have the following observations. (1) In all cases (18/18), EviOmni achieves the best or second-best results, indicating the superiority of supplementing RAG with rational evidence. (2) It is surprising that directly employing EviOmni trained on NQ and tested on OOD datasets ( i.e., TQAand HotpotQA) yields such impressive performance. This demonstrates that online policy optimization endows EviOmni with superior generalization. (3) Com- pared to 'Full', EviOmni possesses an extremely high compression ratio ( e.g., 38.1x with 1.5B size model on NQ) and its performance is very close to or even better than that of 'Full'. Though the baseline models have relatively high compression ratios, they are accompanied by significant performance degradation. This demonstrates again that rational evidence benefits downstream performance than vanilla one and also improves the signal-to-noise ratio. (4) By comparing the 1.5B EviOmni model with the 7B one, we find that the 7B one tends to extract more informative evidence than the 1.5B one. (5) EviOmni considerably outperforms VAR methods in almost all cases and provides more compact evidence, demonstrating the necessity of explicit evidence reasoning. (6) In HotpotQA, RAR methods significantly outperform VAR ones, and even better than 'Full', indicating that rational refinement is important to multi-hop question answering. In conclusion, the above results and observations fully validate the effectiveness and efficiency of rational evidence.", "4.3 Training Dynamics (RQ2)\n\nThe RL training dynamics regarding the answer rewards and response length are shown in Figure 5. Note that the generators used in Table 4 and Figure 5 are different. Therefore, the answer rewards in Figure 5 are slightly different from the", "Figure 5: Training dynamics w.r.t. answer reward and response (including 'reason', 'extract', and 'answer') length.\n\n(a) Answer reward dynamics on 1.5B model.\n\n(b) Answer reward dynamics on 7B model.\n\n(c) Length dynamics.\n\n0.725 0.44 0,77 Fla0 1,5B model 7Bmnodel 0.41 0.74 0.71 0.625 20 0.65 20 0.38 20 10o Training Steps\n\nTraining Steps of RL\n\n| | Models | NQ | TQA | HotpotQA | Avg |\n|---:|:---------|-----:|------:|-----------:|------:|\n| 0 | FILCO | 0.64 | 0.82 | 0.59 | 0.68 |\n| 1 | CoT | 0.55 | 0.7 | 0.71 | 0.65 |\n| 2 | EviOmni | 0.35 | 0.41 | 0.43 | 0.4 |\n\nTable 5: Inference latency (seconds/query) on the 1.5B model, where the smaller the better.", "F 1 and EM results in Table 4. The results show that the answer rewards of full context ( o f ), rationale ( o r ), and rational evidence ( o e ) are consistently improved during the process of reinforcement learning. And, it is not surprising that the answer reward of full context generally performs best. More specifically, on the 1.5B model, rationale and rational evidence perform very closely, while on the 7B model, rational evidence performs slightly worse than rationale. We think the main reason is that the 7B model can infer answers based on implicit cues, but rational evidence may compress some of them. As for response length dynamics, the response length of the 1.5B model decreases rapidly at the beginning of training and then increases slowly, while that of the 7B model decreases slowly all the time. We think the main reason is that a moderate response length is beneficial for improving answer rewards, because an overlong reasoning may confuse answer generation, and a too short reasoning may omit key cues. Therefore, as training continues, the response lengths of the 1.5B model and the 7B model tend to converge. In conclusion, the above observations provide some useful insights, e.g., elaborated answer and length rewards, for further research on the rational evidence extraction.", "4.4 Inference Efficiency (RQ3)\n\nTable 5 presents the inference latency of EviOmni compared with FILCO (a representative V AR method) and CoT (a representative RAR method) on the 1.5B model within a 1 \u00d7 A800 station, where the evaluation batch size and max new tokens are set as 64 and 768, respectively. It can\n\nFigure 6: Performance comparisons w.r.t. data noise on the 1.5B model, where psgs is the abbr of 'passages'.\n\n(a) NQ.\n\n(b) TQA.\n\nEviOmni\nFilCo\n#Irrelevant Psgs", "5 Related Works\n\n5.1 Retrieval-Augmented Generation\n\nRetrieval-Augmented Generation (RAG) has been prevalent in LLMs, which effectively enhances LLMs with external non-parametric knowledge to remedy their outdated, incorrect, or incomplete internal parametric knowledge (Lewis et al., 2020; Wu et al., 2022; Guu et al., 2020; Izacard et al., 2023; Asai et al., 2024). The pioneering attempts (Lewis et al., 2020; Guu et al., 2020) demonstrated that augmenting the input context of LLMs with retrieved passages yields significant improvements, especially in knowledge-intensive tasks. While prior work usually retrieves straightforwardly at the beginning, dynamic RAG (Asai et al., 2024; Jeong et al., 2024; Jiang et al., 2023) has been proposed to adaptively retrieve passages based on the demand of generation and the complexity of the query. Recently, agentic\n\nRAG (Zhang et al., 2025b) has been proposed to interleave retrieval and reasoning to tackle complex queries. In particular, this kind of work (Song et al., 2025; Jin et al., 2025; Trivedi et al., 2023) focuses on prompting or fine-tuning LLMs as search agents that interact with search tools to get external knowledge autonomously and on demand through direct prompting/reinforcement learning.", "5.2 RAG with Context Compression\n\nRAG systems usually concatenate all retrieved passages as the context of LLMs. However, this may introduce data noise and computational overhead due to imperfect retrieval and overlong context. Recently, many works have attempted to compress context and retain important information, mainly including two categories: (1) Reranking methods, which rerank retrieved passages and retain top-ranked passages (Hwang et al., 2024b; Mortaheb et al., 2025; Mao et al., 2021; Chen et al., 2025) (2) Summarization methods, which extract relevant information from retrieved passages and consolidate them into a narrative (Wang et al., 2023; Zhao et al., 2024a; Zhu et al., 2024; Xu et al., 2024). While effective, existing works neglect evidence reasoning, which risks filtering out key clues and struggles with generalization.", "3 Proposed Method: CARE\n\n3.1 Reconstruction Pre-training\n\nThe goal of the pre-training stage is to train the context assessor to represent the retrieved context as memory token embeddings. We adopt reconstruction-based soft prompting methods (Ge et al., 2024; Cheng et al., 2024).", "we append K learnable memory tokens, denoted as M = \u27e8 m 1 , . . . , m K \u27e9 , to the original input context C = [ c 1 , . . . , c n ] . The resulting sequence serves as the input for pre-training.", "We feed an input token sequence into the context assessor, incorporating a LoRA adapter built upon the base LLM. The final hidden states corresponding to the memory tokens M are obtained from a single forward pass of the decoder, referred to as memory embeddings ( E mem ). These embeddings serve as compressed representations of the input context and are used as soft prompts for downstream text generation.", "where \u03d5 denotes frozen LLM parameters, and \u03b8 represents the learnable parameters of the LoRA adapters (Hu et al., 2022) and memory tokens.", "Pre-training\n\nFigure 3: Overall framework of C onflictA ware RE trieval-augmented generation ( CARE ). (a) The reconstruction pretraining learns to compress contextual information into memory embeddings. (b) In the conflict-aware fine-tuning stage, adversarial soft prompting exposes the Context Assessor to misleading contexts, encouraging the memory embeddings to reflect their reliability.\n\nReconstruction b) Conflict-aware Fine- Grounded Soft Prompting Adversarial Soft Prompting Context Answer LLM Student Teacher LKD\n\nLLM f .\n\nMemory\nembeddings\n\n\"Reconstruct the given context\"\n\nQuestion LoRA Assessor 0,0 Assessor f \u00f8,0 LLM f Prompt (m1 mK) Positive Hard negative K memory slots context Cpos w/ a positive negative Inference -tuning Cneg pair\n\nWe train the parameters \u03b8 by minimizing a reconstruction loss computed using the frozen LLM parameters \u03d5 , which encourages E mem to capture the accurate information required to reconstruct the input context C . The reconstruction loss is defined as the negative log-likelihood of generating each token conditioned on the memory embeddings E mem and instruction I recon 2 :", "3.2 Conflict-aware Fine-tuning\n\nWe fine-tune the context assessor to be sensitive to context-memory conflicts . The goal is to produce memory embeddings that not only compress the retrieved context but also reflect its reliability, i.e. , whether the LLM should rely on it. To achieve this, we employ an instruction-following dataset with question-answer pairs to expose the discrepancy between external and parametric knowledge.", "The context assessor takes an input sequence consisting of question Q = [ q 1 , . . . , q n ] , context C = [ c 1 , . . . , c n ] , and learnable memory tokens M = \u27e8 m 1 , . . . , m K \u27e9 , to assess the context based on the question.", "2 We adopt the instruction set from Cheng et al. (2024) for reconstruction.\n\nAs in the pre-training stage, the context assessor extracts memory embeddings E mem \u2208 R K \u00d7 d by encoding the input sequence:\n\nTo train the context assessor to detect such conflicts, we simulate them using correctness signals from a closed-book setting: when the LLM fails to answer correctly without context, and when it succeeds. If the parametric knowledge is insufficient, we apply grounded soft prompting to guide the LLM toward the external context. When the LLM already contains the relevant knowledge, we apply adversarial soft prompting with a hard negative context and train the context assessor to encode it in a way that reduces the influence, allowing the LLM to favor internal knowledge.", "Grounded Soft Prompting . It provides the context assessor with positive supervision signals about useful external knowledge. For questions that the LLM fails to answer in a closed-book setting, we pair them with a positive context C pos that contains the answer span. In this setup, C pos is treated as a reliable context source. The context assessor is trained to reflect this reliability in memory embeddings, allowing the model to recognize and represent helpful external knowledge, particularly when parametric knowledge is insufficient.", "Adversarial Soft Prompting . We adopt supervision for identifying and down-weighting unreliable external information. For questions that the LLM", "already answers correctly in a closed-book setting, we construct conflict scenarios by pairing the question with a hard negative passage C neg , which is topically relevant but does not contain the correct answer. The context assessor is trained to reflect low reliability for such passages in its memory embeddings, effectively learning to recognize when external context contradicts parametric knowledge. As a result, misleading information has less influence on the generation, and the LLM relies more on its internal knowledge.", "Model Size: Large vs. Mid-size\n\nTo study the role of scale, we compare: (1) Qwen3-235B-A22B (Yang et al. 2025), the flagship model with 235 billion total parameters and 22 billion active, optimized for long-context reasoning, and (2) Qwen3-30B-A3B (Yang et al. 2025), a 30B-parameter variant with 3B active, designed for faster inference and reduced hardware costs.", "Figure 4: Overall performance of different systems across regions. (Subject to aesthetic changes.)\n\nAnswer Quality Faithfulness Recall@30 0.0 0.2 0.4 0.6 0.8 1.0 1.2 Score 69.6% N/A 68.2% 97.0% 9.5% 88.0% 96.9% 93.2% Overall Performance General LLMs Vanilla RAG GridCodex\n\n| | Model | Answer Quality | Faithfulness |\n|---:|:----------|-----------------:|---------------:|\n| 0 | 235B-A22B | 0.852 | 0.968 |\n| 1 | 30B-A3B | 0.74 | 0.995 |\n\nTable 3: Model size impact (Netherlands dataset).", "As shown in Table 3, scaling from 30B to 235B parameters improves answer quality by 11.2%. The larger model demonstrates stronger multi-hop reasoning, cross-clause referencing, and ambiguity resolution due to greater representational capacity and expert routing. However, this gain comes at a cost: faithfulness drops slightly (0.968 vs. 0.995), as the 235B model integrates retrieved context more aggressively, occasionally introducing inferred connections not explicitly present in the documents. This highlights a key trade-off-larger models enable richer reasoning but may lean toward speculative integration, whereas smaller models remain more conservative and strictly grounded in retrieved evidence.", "4 Results\n\n4.2 RQ2: To what extent does the type of retrieved context influence robustness?\n\nOpen-source models follow the same pattern. As shown in Table 4 for TREC 2021, the alignment rate of DeepSeekR1-Distill-Qwen-32B improves from 63.0% in the Non-RAG baseline to 92.6% with helpful documents for neutral user queries, but collapses to just 1.3% under Liar inputs. A similar trend appears for Phi-4, which rises from 66.7% in the Non-RAG baseline to 96.5% with helpful evidence, but drops sharply to 2.2% with Liar documents. LLaMA-3 8B Instruct shows the same pattern, moving from 81.6% without context to 93.9% with helpful inputs, before falling to just 0.4% under Liar documents. Even weaker adversarial documents, such as Rewriter and Paraphraser, substantially reduce alignment rate, often into the 55-70% range across all open-source models. The same pattern is observed for TREC 2020 as well, but with consistently higher rates, as detailed in the following paragraphs. We emphasize neutral user query results here because RQ1-RQ3 focus primarily on the effect of retrieved context, while the role of query framing is analyzed separately in Section 4.4.", "4.2 RQ2: To what extent does the type of retrieved context influence robustness?\n\nAcross both datasets and model families, the relative effectiveness of adversarial strategies follows a consistent ordering: Liar \u226b FSAP-IntraQ > FSAP-InterQ > Fact Inversion > Paraphraser \u2248 Rewriter . This hierarchy is a result of how adversarial documents are made. Liar documents are constructed from scratch based on the query and an incorrect stance, so they are not constrained to the wording or rhetorical framing of pre-existing harmful content. This independence enables more fluent phrasing and broader topic coverage, making misinformation more confident and persuasive, and thus more challenging for LLMs. In contrast, Paraphraser and Rewriter documents inherit limitations from pre-existing harmful material, which can make them appear less natural.", "4.2 RQ2: To what extent does the type of retrieved context influence robustness?\n\nIn cross-model comparison, GPT-4.1 and GPT-5 achieve the highest levels of alignment rate when paired with helpful documents; however, both demonstrate comparable vulnerabilities when confronted with adversarial evidence. Claude-3.5-Haiku has significant performance drops in the presence of harmful or adversarial content; on the other hand, DeepSeek-R1-Distill-Qwen-32B performs well and often obtains levels that are similar to closed-source models when provided with helpful documents. Phi-4 and LLaMA-3 8B Instruct achieves a lower overall alignment rate but follows the same qualitative trends. Since these patterns are consistent across all models, we selected GPT-4.1 as the representative model for subsequent experiments due to its relatively high performance and speed. GPT-5 was not released at the time these experiments were first conducted and was included afterwards.", "4.2 RQ2: To what extent does the type of retrieved context influence robustness?\n\nAnother important finding of this experiment is the difference at the dataset level, where models consistently display a higher alignment rate on TREC 2020 (COVID-19 misinformation) than on TREC 2021 (general health misinformation). For example, TREC 2021 includes queries about misinformation on everyday remedies and niche treatments, such as using duct tape for warts and toothpaste for pimples, which rarely reach the same level of wide public discourse, although the ground truth for these questions is based on evidence. This underscores an unexpected outcome from our research, suggesting that LLMs appear considerably more robust against adversarial manipulation in the COVID-19 context. This difference may be due to post-training alignment placing greater emphasis on topics like COVID-19, which was the subject of widespread misinformation and public attention, while giving limited focus to less prominent areas of health.", "4.2 RQ2: To what extent does the type of retrieved context influence robustness?\n\nOverall, our results address RQ2 by showing that the type and quality of retrieved context substantially affect LLM robustness, by demonstrating that helpful documents consistently improve alignment rate relative to the baseline, whereas harmful and adversarial documents undermine stability and reliability, in some cases reducing robustness to the point of complete failure.", "4.2 RQ2: To what extent does the type of retrieved context influence robustness?\n\nWhile it may appear self-evident that helpful evidence would improve alignment rate and harmful or adversarial evidence would reduce it, our results move beyond speculation by empirically quantifying both the magnitude and the consistency of these effects across datasets and model families. The measurements establish concrete performance bounds. Alignment rate can approach near-maximum levels by providing a single helpful document, yet it can collapse to almost zero under adversarial documents such as Liar. As outlined in Section 1, this systematic analysis offers the empirical evidence required to turn abstract concerns about adversarial susceptibility into evidence-based design principles for RAG systems. In line with RQ1, the findings highlight that context can significantly shape outcomes and must be safeguarded against harmful or adversarial inputs.", "4.3 RQ3: In what ways do orders and combinations of multiple evidence sources shape model behavior?\n\nFindings from RQ1 and RQ2 show that context type strongly shapes ground-truth alignment, which motivates an assessment of more complex scenarios with multiple documents presented in different orders and compositions, leading to RQ3. To address this question, we examine how models respond when exposed to multiple sources of evidence, focusing on two key dimensions: (i) the order in which helpful and misleading documents are presented, and (ii) the composition of the retrieval pool when evidence is naturally or systematically skewed. We investigate these dimensions through three complementary setups: the paired-document setup, the passage-based pooling setup, and the bias-controlled pooling setup, which are analyzed in the following sections.", "4.3 RQ3: In what ways do orders and combinations of multiple evidence sources shape model behavior?\n\n4.3.1 Paired-Document Analysis. Figure 2 shows ground-truth alignment rate when a helpful document is either first (green, 'Helpful First') or second (blue, 'Helpful Second') relative to a harmful/adversarial document, compared with the single-document baseline (red). The results show that the order of the documents has minimal impact on ground-truth alignment. The Helpful First and Helpful Second conditions deliver nearly the same results, with confidence intervals that mostly overlap. For instance, on TREC 2021 under the consistent user query, the alignment rate of LLM responses rises from only 1.7% (CI 0.4-3.5) in the single-document baseline to 39.1% (CI 33.3-45.0) when helpful evidence appears first and 45.0% (CI 39.1-50.8) when it appears second for Liar inputs. Similarly, for FSAP-IntraQ documents, alignment rate improves from 7.4% (CI 4.4-10.9) to 41.1% (CI 35.3-47.3) when helpful content precedes and 48.8% (CI 43.0-55.0) when it follows the adversarial document. On TREC 2020, robustness is consistently higher. For example, under the neutral user query, for FSAP-IntraQ documents, alignment rate rises from 54.9% (CI 48.2-62.2) to 68.3% (CI 61.9-74.8) when helpful material comes first and 76.7% (CI 70.8-82.7) when it is placed second relative to them. These results show that presenting helpful evidence, regardless of position, minimizes the impact of harmful or adversarial content. Additionally, discrepancies among datasets persist, as TREC 2020 (COVID-19) queries demonstrate greater robustness than the more general health queries in TREC 2021, thereby reinforcing the previously identified dataset-level variations.", "4.3 RQ3: In what ways do orders and combinations of multiple evidence sources shape model behavior?\n\n4.3.2 Passage-Based Pooling Analysis. In the passage-based pooling setup, the MonoT5 reranker provides the top-10 passages for each query, obtained by sliding a 512-word window with 256-word overlap across documents and ranking the resulting segments. Since the adversarial documents are designed to rank highly, these pools are heavily skewed toward this adversarial content, with more than nine of the top-10 segments being adversarial. For TREC 2020, the top-10 pools comprise \u223c 92% adversarial, 5% helpful, and 3% harmful passages, whereas for TREC 2021 the distribution is \u223c 94% adversarial, 5% helpful, and 2% harmful passages. As shown in Fig. 3, the dominance of adversarial segments in the passage-based pool leads to consistently low alignment rates. On TREC 2020, GPT-4.1 reaches 43.7% under consistent user queries, but performance drops to 24.9% with neutral user queries and remains low at 24.9% under inconsistent user queries. The degradation is even worse on TREC 2021, where the alignment rate drops to 17.3% with consistent Manuscript submitted to ACM", "4.3 RQ3: In what ways do orders and combinations of multiple evidence sources shape model behavior?\n\nFig. 2. Results of paired-document setup for GPT-4.1 on TREC 2020 and TREC 2021 with consistent, neutral, and inconsistent user queries. The x-axis shows the document type with which the helpful document is paired (either first or second), and the y-axis reports ground-truth alignment rate (%). Error bars indicate 95% confidence intervals estimated via bootstrapping. Bars compare helpful-first, helpful-second, and single-document inputs (harmful/adversarial only). Helpful placement shows only minor differences, while single-document inputs yield substantially lower ground-truth alignment rates.\n\nHarmful Rewriter Paraphraser Fact Inversion FSAP-InterQ FSAP-IntraQ Liar Ground-Truth Alignment (%) TREC2021 Consistent Query TREC2021 Neutral Query TREC2021 Inconsistent Query TREC2020 Consistent Query TREC2020 Neutral Query TREC2020 Inconsistent Query Helpful First Helpful Second Single Document", "4.3 RQ3: In what ways do orders and combinations of multiple evidence sources shape model behavior?\n\nuser queries, 13.0% with neutral user queries, and only 4.3% with inconsistent user queries. These findings emphasize retrieval as the primary bottleneck in RAG pipelines and demonstrate that the composition of the evidence pool can significantly undermine robustness, irrespective of user query framing.", "1 Introduction\n\nThe core challenge of long-context narrative comprehension lies not merely in connecting discrete pieces of evidence, a task more naturally defined as multi-hop Question Answering (QA), but in performing a dynamic cognitive synthesis to grasp necessary background and content progression (Xu et al. 2024a). Unlike multi-hop QA (Yang et al. 2018), which seeks a static path through fixed facts, narrative comprehension requires emulating a human reader: continuously building and revising a global mental model of the\n\n* These authors contributed equally.\n\n\u2020 Project lead. Correspondence to: < liyanlxu@tencent.com >\n\nFigure 1: Comparison of RAG reasoning paradigms.\n\n(c) ComoRAG\n(b) Multi-step RAG\nWhy Snape Kill\n\nDumbledore Albus?\n\nStateless Reasoning Stateful Reasoning Fragmented Evidence Lack of Fusion Memory-Organized Stateful Comprehension Contradictory Evidence Motive Unclear\n\nAn act of loyalty\n\nnot betrayal Snape Bully Harry Kills Albus Protect (a) Single-step RAG Shallow/Superficial Understanding\n\nSnape was a loyal\n\nDeath Eater The Half-Blood Prince where Snape Coherent Context Formed Apparent Contradiction Causally Incomplete Event Unbreakable Vow Context-Grounded Exploratory Probing Dynamic Memory Workspace Knowledge Consolidation Consolida1on New Acquisition Acqulsition", "plot, characters, and their evolving motivations (JohnsonLaird 1983). The complexity of this process is well exemplified by a classic narrative question 'Why did Snape kill Dumbledore?' from the Harry Potter series. Answering this requires weaving a complete web of evidence from disparate clues spanning multiple books-Dumbledore's terminal illness, the Unbreakable Vow, and Snape's deeply concealed loyalty. The true significance of these clues is only fully reconciled in hindsight. This capability is what we term stateful reasoning : it demands more than linking static evidence; it requires maintaining a dynamic memory of the narrative, one that is constantly updated as new revelations emerge. Long-context LLMs have demonstrated promising performance on benchmarks such as the 'Needle in a Haystack' test (Eisenschlos, Yogatama, and Al-Rfou 2023) in recent years. However, their capacity to process long narratives (200k+ tokens) remains limited by finite context windows. Furthermore, as the input length increases, these models are prone to the 'lost in the middle' problem (Liu et al. 2024), which raises perplexity and impairs generation quality. This limitation is particularly pronounced in narrative compre- hension tasks which require stateful reasoning. As a result, retrieval-augmented generation (RAG) (Lewis et al. 2020) has emerged as an important strategy for tackling long context comprehension with LLMs.", "However, existing RAG methods still struggle to effectively address this challenge. Advanced single-step retrieval remains limited by its static index. This includes methods such as RAPTOR (Sarthi et al. 2024), which clusters and summarizes text chunks to retrieve at different levels of details; HippoRAGv2 (Guti\u00b4 errez et al. 2025), which mimics the human hippocampus by building a knowledge graph to achieve multi-hop reasoning in a single retrieval step. Nonetheless, single-step methods rely on one-shot static retrieval, which may lead to shallow comprehension. For example, in Figure 1(a), the evidence about Snape can mislead the model into making a false inference.", "As a remedy, multi-step retrieval methods offer a more promising direction, such as IRCoT (Trivedi et al. 2023), which interleaves the retrieval process with Chain-ofThought reasoning (Wei et al. 2022); Self-RAG (Asai et al. 2024), which trains a model to adaptively retrieve and reflect on evidence; and MemoRAG (Qian et al. 2025), which uses a dual-system architecture to generate clues from compressed global context. These methods all target to obtain richer context through iterative retrieval. However, their retrieval steps are typically independent, which lack coherent reasoning throughout explicit narrative progression, featuring fragmented evidence with a stateless comprehension. As illustrated in Figure 1(b), due to a lack of dynamic memory, multi-step retrieval fails to integrate contradictory evidence such as 'Snape protects/bullies Harry' and cannot understand the evolution of his actions, ultimately unable to yield the correct answer.", "In this work, we seek inspiration from the function of Prefrontal Cortex (PFC) in human brains, which employs a sophisticated reasoning process called Metacognitive Regulation (Fernandez-Duque, Baird, and Posner 2000). This process is not a single action but a dynamic interplay between new evidence acquisition , driven by goal-directed memory probes (Dobbins and Han 2006; Miller and Constantinidis 2024), and subsequent knowledge consolidation . During consolidation, new findings are integrated with past information to construct an evolving, coherent narrative. This iterative cycle allows the PFC to continuously assess its understanding and revise its strategy, providing a direct cognitive blueprint for our framework's stateful reasoning approach.", "We introduce ComoRAG, a cognitive-inspired, memoryorganized RAG framework, imitating the human Prefrontal Cortex (PFC) for achieving true stateful reasoning. At its core is a dynamic cognitive loop operating on a memory workspace, which actively probes and integrates new evidence to build a coherent narrative comprehension.", "This process, as illustrated in Figure 1(c), is a closed loop of evolving reasoning states. Faced with a complex query like 'Why did Snape kill Dumbledore?' , the system's memory state evolves from an initial 'causally incomplete event' ( Snape kills Albus ), to an 'apparent contradiction' upon finding contradictory information ( Snape protects Harry ), and ultimately to a logically consistent coherent context through deeper exploration and evidence fusion. Only in this final, complete cognitive state can ComoRAG perform the correct stateful reasoning, deriving the profound insight that it was 'an act of loyalty, not betrayal' .", "This cognitively-inspired design yields substantial improvements across four challenging long-context narrative benchmarks. ComoRAG is shown to consistently outperform all categories of strong baselines across each dataset. Our analysis reveals several key findings. First, these gains stem directly from the cognitive loop, which transforms a static knowledge base into a dynamic reasoning engine; for instance, accuracy on EN.MC jumps from a static-retrieval baseline of 64.6% to 72.9%, with performance efficiently converging in around 2-3 cycles. Second, our framework excels on narrative queries that require global understanding of plot progression, achieving up to a 19% relative F1 improvement on these challenging question types where others falter. Finally, our framework demonstrates remarkable modularity and generalizability. Its core loop can be flexibly integrated to existing RAG methods such as RAPTOR, which directly yields a 21% relative accuracy gain). Also, switching to a stronger model as the backbone LLM agents can upgrade reasoning in the entire cognitive loop, attaining accuracy from 72.93% to 78.17%. These results collectively validate that ComoRAG provides a principled, cognitivelyinspired new paradigm for retrieval-based long narrative comprehension towards stateful reasoning."]}, {"type": "entry", "index": 63, "question": "does retrieval order matter a lot, and how do i make it less brittle", "answer": "Based on the evidence here, the *order* of retrieved items by itself does not seem to matter nearly as much as *what* is in the retrieved set overall.\n\nIn paired-document experiments, putting the helpful document first vs second only caused minor differences in correctness, whereas using only a single (possibly harmful) document caused a big drop in correct answers [1]. What really drives robustness is the *composition* of the pool: when retrieval is biased toward helpful documents, correctness often reaches 90\u2013100%; when it is biased toward harmful ones, performance can collapse to nearly zero, even if some good evidence is present [1]. So the main issue is not fine-grained order but whether harmful vs helpful material dominates.\n\nTo make retrieval less brittle:\n\n1) Improve retrieval quality and coverage so helpful material is present and dominant. Scaling and strengthening retrievers measurably boosts accuracy and recall [2], and higher top\u2011k recall (retrieving more relevant items) can help, though you must watch the trade\u2011off with more noise [2].\n\n2) Use methods that make the model robust to noisy or misleading passages. Robust training approaches such as RetRobust teach the model to downweight irrelevant or distracting context [5]. Passage Injection explicitly weaves retrieved passages into the model\u2019s reasoning so it can recognize and resist both irrelevant and misleading (counterfactual) passages, improving robustness under noisy retrieval [4].\n\n3) Add mechanisms that assess context quality or necessity at test time. Conflict-aware and decoding-based methods reweight or ignore unreliable documents [5]. Corpus\u2011feedback style signals (e.g., detecting low confidence or hallucination) can help reduce unnecessary or harmful retrieval rounds, though they may still allow some noise through [3].\n\nOverall: focus on getting a helpful\u2011dominated pool and making the model explicitly reason *about* the evidence it sees, rather than relying on retrieval order.", "tags": ["_untagged"], "full_faithfulness": 1.0, "full_answer_relevancy": 0.0, "seed_faithfulness": 0.6315789474, "seed_answer_relevancy": 0.0, "context_relevance": 0.5858143772821489, "context_relevance_error": null, "context_relevance_reason": "The context passages provide substantial information regarding the impact of retrieval order on model behavior, directly addressing the user's question about whether retrieval order matters and how to make it less brittle. The passages discuss the effects of evidence composition and retrieval biases, highlighting how helpful documents can enhance alignment rates, which relates to making the system less brittle. However, while the context is relevant, it could be more explicitly connected to practical strategies for improving retrieval robustness, which slightly limits the score.", "structure_citation": 0.7486330277344873, "structure_error": null, "structure_reason": "The Actual Output effectively addresses the question about retrieval order and its impact on robustness, providing a clear summary of findings supported by citations. It includes claims with [1] and [2] markers that correspond to relevant snippets in the Context, demonstrating a good understanding of the evidence. However, while the claims are generally consistent with the cited snippets, there are minor inconsistencies in the depth of explanation regarding the implications of retrieval order, which could have been better supported by additional context details.", "hallucination_score": 0.8647874753406029, "hallucination_error": null, "hallucination_reason": "The Actual Output effectively addresses the Input's question about the significance of retrieval order and how to enhance robustness in retrieval systems. It supports the claim that the order of retrieved items is less critical than the overall composition of the retrieved set, which aligns with the context provided. The response offers practical strategies to improve retrieval quality and robustness, directly answering the user's query. The only minor shortcoming is a lack of explicit mention of the specific retrieval order's impact, which could have strengthened the response further.", "contexts_used": ["4 Results\n\n4.3 RQ3: In what ways do orders and combinations of multiple evidence sources shape model behavior?\n\nFig. 2. Results of paired-document setup for GPT-4.1 on TREC 2020 and TREC 2021 with consistent, neutral, and inconsistent user queries. The x-axis shows the document type with which the helpful document is paired (either first or second), and the y-axis reports ground-truth alignment rate (%). Error bars indicate 95% confidence intervals estimated via bootstrapping. Bars compare helpful-first, helpful-second, and single-document inputs (harmful/adversarial only). Helpful placement shows only minor differences, while single-document inputs yield substantially lower ground-truth alignment rates.\n\nHarmful Rewriter Paraphraser Fact Inversion FSAP-InterQ FSAP-IntraQ Liar Ground-Truth Alignment (%) TREC2021 Consistent Query TREC2021 Neutral Query TREC2021 Inconsistent Query TREC2020 Consistent Query TREC2020 Neutral Query TREC2020 Inconsistent Query Helpful First Helpful Second Single Document", "4.3 RQ3: In what ways do orders and combinations of multiple evidence sources shape model behavior?\n\nuser queries, 13.0% with neutral user queries, and only 4.3% with inconsistent user queries. These findings emphasize retrieval as the primary bottleneck in RAG pipelines and demonstrate that the composition of the evidence pool can significantly undermine robustness, irrespective of user query framing.", "4.3 RQ3: In what ways do orders and combinations of multiple evidence sources shape model behavior?\n\n4.3.3 Bias-Controlled Pooling Analysis. To examine the impact of systematic skew in retrieval, we construct pools biased toward either helpful or harmful content. In the condition biased toward harmful, the ground-truth alignment rate declines markedly. On TREC 2021, as shown in Fig. 3, GPT-4.1 achieves only a 26.04% alignment rate when the pool is biased toward harmful content with Fact Inversion documents, and 4.35% when biased with Liar documents, even under consistent user queries. The situation worsens under inconsistent user queries, where providing Liar examples leads to a 0% alignment rate, and presenting FSAP-IntraQ documents collapses the rate to 0% as well. By contrast, TREC 2020 demonstrates somewhat greater resilience. Although alignment rate against Rewriter documents remains as high as 93.76% and Paraphraser at 87.52% under consistent user queries, it still collapses for the most adversarial harmful cases, dropping to 24.95% for Liar and 37.4% for FSAP-IntraQ documents.", "4.3 RQ3: In what ways do orders and combinations of multiple evidence sources shape model behavior?\n\nIn the biased-toward-helpful condition, alignment levels are close to the best possible. On both datasets, consistent and neutral user queries frequently sustain a 100% alignment rate, with LLM responses across all attack categories in TREC 2021 reaching a full alignment rate when helpful documents are included in the pool. Even inconsistent user queries remain robust: in TREC 2020, pooled conditions containing Rewriter or FSAP-InterQ documents yield a 100%\n\nManuscript submitted to ACM", "4.3 RQ3: In what ways do orders and combinations of multiple evidence sources shape model behavior?\n\nFig. 3. Results of passage-based pooling and biased-controlled pooling setups for GPT-4.1 on TREC 2020 and TREC 2021 across consistent, neutral, and inconsistent user queries. The x-axis includes the passage-based pooling baseline as well as the different document types used in biased conditions, where helpful documents are paired with harmful or adversarial ones. The y-axis reports ground-truth alignment rate (%). Error bars indicate 95% confidence intervals estimated via bootstrapping. Results show helpful-biased pools yield near-perfect alignment, whereas harmful and passage-based pools sharply reduce it.\n\nPassage-Based\n\nTREC 2021 Consistent Query\n\nTREC 2021 Neutral Query\n\nTREC 2021 Inconsistent Query\n\nTREC 2020 Consistent Query\n\nTREC 2020 Neutral Query\n\nTREC 2020 Inconsistent Query\n\nBiased to Helpful\nBiased to Harmful\nPassage-Based Pooling", "4.3 RQ3: In what ways do orders and combinations of multiple evidence sources shape model behavior?\n\nalignment rate, and Fact Inversion pools reach 95.63% on TREC 2021. Once helpful content dominates the retrieval pool, the influence of user query framing diminishes, as supportive context outweighs misleading presuppositions.", "4.3 RQ3: In what ways do orders and combinations of multiple evidence sources shape model behavior?\n\nOverall, the bias-controlled experiments confirm that the composition of retrieved evidence plays a critical role in RAG systems' robustness. Pools dominated by helpful documents consistently lead to high alignment rates, often exceeding 90-100%, while harmful-biased pools drastically reduce reliability, in some cases collapsing alignment rate to nearly zero. These results underscore that the stability of RAG systems is fundamentally dependent on the distribution of their evidence sources, which highlights the retrieval stage as the key vulnerability in pipeline design.", "4.4 RQ4: How do different query framings affect stance alignment?\n\nAfter discussing the significance of evidence type (RQ2) and evidence order/combination (RQ3), we move on to RQ4, which assesses the impact of user query framing (consistent, neutral, and inconsistent) on the results of ground-truth alignment. Prior work has consistently shown that LLM performance is highly sensitive to prompt formulation. Even minor variations in wording, token placement, or tone can lead to substantially different outcomes, and several studies have proposed methods to stabilize predictions under such alterations [32, 42, 49, 50, 57, 63, 65, 75]. Motivated by these results, we revisit our experimental setups with query sensitivity in mind.", "4.4 RQ4: How do different query framings affect stance alignment?\n\nWe analyze user query formulation effects across all four experimental setups: the single-document setup, the paireddocument setup, the passage-based pooling setup, and the bias-controlled pooling setup. Figures 1-3 and Tables 2-5 summarize results across models, datasets, and retrieval conditions.", "4.4 RQ4: How do different query framings affect stance alignment?\n\nFraming has a persistent impact on ground-truth alignment rate, and the order of user query types remains the same across models and datasets (Consistent > Neutral > Inconsistent), which directly addresses RQ4 by showing that the framing of user queries impacts ground-truth alignment outcomes. The same hierarchy emerges in the single-document setup (Fig.1), persists when multiple sources of evidence are paired (Fig.2), and carries over to more passage-based pooling setups (Fig. 3).", "4.4 RQ4: How do different query framings affect stance alignment?\n\nConsistent user queries, those which align with the ground-truth stance of the answer, reliably produce the highest alignment rates by framing the model's reasoning in a direction that agrees with available evidence; on the other hand, inconsistent user queries embed contradictory presuppositions that actively mislead the model, often overwhelming the corrective signal of helpful documents. While explicit stance bias is eliminated by neutral user queries, models are still able to base their responses on the evidence they have retrieved, resulting in intermediate performance. These results show that models reflect the quality of retrieved evidence (RQ2) and systematically inherit bias from how user queries are phrased. User query formulation, therefore, plays a central role in shaping ground-truth alignment outcomes (RQ4).", "6 ANALYSIS\n\n6.1 ANALYSIS OF ITERATIVE PERFORMANCE\n\nFigure 2: Accuracy and recall of MI-RAG on InfoSeek subset across 9 iterations.\n\nImpact of Iterations on MIRAG Performance g Metric Accuracy Recall Iteration", "6.2 SCALING RETRIEVERS\n\nBuilding upon the established benefits of scaling MLLMs, we now investigate the impact of scaling the retriever. We investigated this by employing a larger vision encoder (SigLIP2-g) and a text embedding model (Qwen3-Embedding-0.6B). On the InfoSeek validation split, when using Gemini2.5-Flash for MI-RAG, this enhanced retrieval improves accuracy to 66.7% (+6.4%) and cumulative recall to 95.2% (+3.1%). Similarly, on the Encyclopedic VQA test split, the scaled retriever paired with the Gemma-3-4B enhances accuracy to 50.2% (+1.9%) and cumulative recall to 65.7%\n\nWe analyze how iterative refinement impacts MI-RAG's performance by measuring accuracy and recall across iterations. As shown in Figure 2, performance improves consistently with each step. The initial iterations deliver significant gains. Although the rate of improvement moderates in later steps, the model continues to achieve substantial gains in accuracy and recall. This performance improvement comes with an associated computational cost for each iteration, which we detail in Appendix F.", "(+12.1%). These findings confirm that improving retrieval capability provides significant benefits for MI-RAG's performance, validating the synergistic design.", "6.3 TOP-K SENSITIVITY ANALYSIS\n\n| | Model | Metric | k = 10 | k = 20 |\n|---:|:-----------------|:-----------|:------------|:------------|\n| 0 | Gemma-3-4B | Acc Recall | 52.66 90.82 | 48.95 94.52 |\n| 1 | Gemini-2.5-Flash | Acc | 61.84 94.36 | 65.70 |\n| 2 | | Recall | | 95.97 |\n\nTable 9: Top-k sensitivity analysis on the InfoSeek validation subset. k indicates the number of retrieved image-text pairs.\n\nidence.", "Key Insights\n\nThe external feedback signals help in reducing retrieval rounds. These signals can come from different sources, at the LLM level (e.g., token generation confidence), at the answer level ( e.g., uncertainty or hallucination), and at the context level (e.g., relevancy or sufficiency). However, these methods may still retrieve noisy or irrelevant context, and complexity assessment remains a challenge.", "Abstract\n\nRetrieval-augmented generation (RAG) has been widely adopted to augment large language models (LLMs) with external knowledge for knowledge-intensive tasks. However, its effectiveness is often undermined by the presence of noisy (i.e., low-quality) retrieved passages. Enhancing LLMs' robustness to such noise is critical for improving the reliability of RAG systems. Recent advances have equipped LLMs with strong reasoning and self-reflection capabilities, allowing them to identify and correct errors in their reasoning process. Inspired by this ability, we propose Passage Injection-a simple yet effective method that explicitly incorporates retrieved passages into LLMs' reasoning process, aiming to enhance the model's ability to recognize and resist noisy passages. We validate Passage Injection under general RAG settings using BM25 as the retriever. Experiments on four reasoning-enhanced LLMs across four factual QA datasets demonstrate that Passage Injection significantly improves overall RAG performance. Further analysis on two noisy retrieval settings-random noise, where the model is provided irrelevant passages, and counterfactual noise, where it is given misleading passages-shows that Passage Injection consistently improves robustness. Controlled experiments confirm that Passage Injection can also effectively leverage helpful passages. These findings suggest that incorporating passages in LLMs' reasoning process is a promising direction for building more robust RAG systems. 1\n\n\u2217 Keping Bi is the corresponding author.", "1 The code and data can be found at: https://github.com/Trustworthy-InformationAccess/Passage-Injection\n\nThis work is licensed under a Creative Commons Attribution 4.0 International License. SIGIR-AP 2025, Xi'an, China\n\nACM ISBN 979-8-4007-2218-9/2025/12\n\n\u00a9 2025 Copyright held by the owner/author(s).\n\nhttps://doi.org/10.1145/3767695.3769515", "6 Conclusion\n\nIn this work, inspired by reasoning-enhanced LLMs' ability to detect and revise reasoning errors, we propose Passage Injection -a method that explicitly integrates retrieved passages into the model's reasoning process to enhance robustness against noisy information and improve overall RAG performance. Experimental results on four reasoning-enhanced LLMs across four factual QA datasets demonstrate that Passage Injection significantly improves overall RAG performance. Further analysis on two noisy retrieval settingsrandom noise, where the model is provided irrelevant passages, and counterfactual noise, where it is given misleading passages-shows that Passage Injection consistently improves LLMs' robustness to noisy passages. Controlled experiments confirm that Passage Injection can also effectively leverage helpful passages. These findings suggest that incorporating passages in LLMs' reasoning process is a promising direction for building more robust RAG systems.", "4 Experiments Setup\n\nDatasets . We conduct extensive evaluation across three distinct tasks: Open-domain QA , Long-form QA , and Fact verification . (i) For the Opendomain QA task, we selected Natural Questions (NQ) (Kwiatkowski et al., 2019), along with TriviaQA (Joshi et al., 2017) and WebQuestions (WebQA) (Berant et al., 2013). (ii) To assess longform generation capabilities, we utilized TruthfulQA (Lin et al., 2022), which requires the model to produce truthful answers when faced with questions to elicit false or misleading responses. (iii) Lastly, we utilized the FactKG dataset (Kim et al., 2023), which requires the model to determine the factuality of claims by leveraging knowledge from either parametric or the external source. See Table 9 and Appendix A.2 for dataset statistics and prompts used for each task.", "Baseline Methods . We compare CARE against several existing methods to address knowledge conflicts using different strategies.", "Robust training : It enhances the resilience of the LLMs through adversarial learning. For instance, RetRobust (Yoran et al., 2024) teaches the model to ignore irrelevant or distracting retrieved contexts, making LLM more robust. As a variant, we also apply our conflict-aware training strategy directly to the base LLM without using a context assessor, denoted as Direct FT .", "Decoding-based strategy : CAD (Shi et al., 2024) aims to assess and prioritize the reliability of retrieved documents during inference. ADACAD (Han et al., 2025) utilizes confidence-aware decoding to downweight unreliable contexts.", "Adaptive retrieval : Adaptive-RAG (Jeong et al., 2024) trains a classifier and question difficulty. SKR-kNN (Wang et al., 2023) embeds examples of correct and incorrect LLM generations", "| | LLM. | Method. | Open-Domain QA (Span EM).NQ | Open-Domain QA (Span EM).TriviaQA | Open-Domain QA (Span EM).webQA | Long-form QA (F1 / ROUGE-L).TruthfulQA | Long-form QA (F1 / ROUGE-L).TruthfulQA | Fact checking (Acc) FactKG. | Average. |\n|---:|:-----------|:-------------------------|:------------------------------|:------------------------------------|:---------------------------------|:-----------------------------------------|:-----------------------------------------|:------------------------------|:-------------------------|\n| 0 | | Fine-tuning LLMs | Fine-tuning LLMs | Fine-tuning LLMs | Fine-tuning LLMs | Fine-tuning LLMs | Fine-tuning LLMs | Fine-tuning LLMs | Fine-tuning LLMs |\n| 1 | | Direct FT | 0.469 | 0.708 | 0.389 | 0.102 | 0.069 | 0.542 | 0.380 |\n| 2 | | RetRobust | 0.459 | 0.719 | 0.412 | 0.152 | 0.134 | 0.583 | 0.410 |\n| 3 | | Without Fine-tuning LLMs | Without Fine-tuning LLMs | Without Fine-tuning LLMs | Without Fine-tuning LLMs | Without Fine-tuning LLMs | Without Fine-tuning LLMs | Without Fine-tuning LLMs | Without Fine-tuning LLMs |\n| 4 | | Closed-book | 0.290 | 0.577 | 0.366 | 0.273 | 0.249 | 0.531 | 0.381 |\n| 5 | Mistral-7B | RAG | 0.419 | 0.659 | 0.372 | 0.277 | 0.251 | 0.639 | 0.436 |\n| 6 | | CAD | 0.402 | 0.631 | 0.352 | 0.243 | 0.216 | 0.598 | 0.407 |\n| 7 | | ADACAD | 0.417 | 0.653 | 0.364 | 0.267 | 0.242 | 0.636 | 0.430 |\n| 8 | | Adaptive-RAG | 0.402 | 0.631 | 0.367 | 0.275 | 0.252 | 0.633 | 0.427 |\n| 9 | | SKR-kNN | 0.406 | 0.639 | 0.396 | 0.278 | 0.253 | 0.630 | 0.434 |\n| 10 | | Priori Judgment | 0.422 | 0.682 | 0.378 | 0.274 | 0.251 | 0.600 | 0.435 |\n| 11 | | CARE (Ours) | 0.447 | 0.696 | 0.432 | 0.279 | 0.256 | 0.638 | 0.458 |\n| 12 | | Fine-tuning LLMs | Fine-tuning LLMs | Fine-tuning LLMs | Fine-tuning LLMs | Fine-tuning LLMs | Fine-tuning LLMs | Fine-tuning LLMs | Fine-tuning LLMs |\n| 13 | | Direct FT | 0.472 | 0.711 | 0.360 | 0.100 | 0.067 | 0.305 | 0.336 |\n| 14 | | RetRobust | 0.461 | 0.726 | 0.380 | 0.081 | 0.061 | 0.644 | 0.392 |\n| 15 | | Without Fine-tuning LLMs | Without Fine-tuning LLMs | Without Fine-tuning LLMs | Without Fine-tuning LLMs | Without Fine-tuning LLMs | Without Fine-tuning LLMs | Without Fine-tuning LLMs | Without Fine-tuning LLMs |\n| 16 | | Closed-book | 0.345 | 0.630 | 0.465 | 0.266 | 0.239 | 0.637 | 0.430 |\n| 17 | LLaMA-3-8B | RAG | 0.447 | 0.684 | 0.377 | 0.247 | 0.225 | 0.661 | 0.440 |\n| 18 | | CAD | 0.394 | 0.613 | 0.326 | 0.164 | 0.142 | 0.610 | 0.375 |\n| 19 | | ADACAD | 0.428 | 0.666 | 0.362 | 0.214 | 0.193 | 0.667 | 0.422 |\n| 20 | | Adaptive-RAG | 0.458 | 0.690 | 0.438 | 0.245 | 0.223 | 0.661 | 0.452 |\n| 21 | | SKR-kNN | 0.449 | 0.675 | 0.427 | 0.246 | 0.224 | 0.660 | 0.447 |\n| 22 | | Priori Judgment | 0.458 | 0.704 | 0.406 | 0.254 | 0.231 | 0.666 | 0.453 |\n| 23 | | CARE (Ours) | 0.465 | 0.700 | 0.445 | 0.264 | 0.243 | 0.686 | 0.467 |", "Table 2: Evaluation results on Open-Domain QA (NQ, TriviaQA, WebQA), Long-form QA (TruthfulQA), and Fact Checking (FactKG), using Mistral-7B-Instruct and LLaMA-3-8B-Instruct as base LLMs. The best performance is marked in bold , and the second-best is underlined, among retrieval-augmented strategies that do not directly fine-tune the LLMs, including Decoding Strategy and Adaptive Retrieval approaches.", "and uses kNN search over these embeddings to decide whether retrieval is needed. Priori Judgement (Ren et al., 2025) examines the LLM's knowledge boundaries through prompts to determine whether to use external passages. At inference time, these methods decide whether to use external evidence and select either a closed-book or RAG response as the final answer.", "Evaluation Metrics . We adopt task-specific evaluation metrics that align with each benchmark. For Open-domain QA datasets, we report Span EM, measuring whether any ground-truth answer appears within the generated output. Unlike standard exact match, this evaluates containment rather than strict equivalence, making it better suited for assessing the knowledge encoded in LLMs. For the longform QA task, we use F1 and ROUGE-L scores. For the fact verification task FactKG , the model is required to generate either \"true\" or \"false.\" We report accuracy as the evaluation metric.", "Implementation Details . We mainly employ two base LLMs for CARE: Mistral-7B (Jiang et al.,", "2023) 3 and LLaMA-3-8B (Dubey et al., 2024) 4 , both in their instruct versions. To further assess the generality, we also evaluate CARE on Qwenbased model (Yang et al., 2025), as detailed in Appendix B. We employ ColBERTv2 (Santhanam et al., 2022), to retrieve top-1 context from Wikipedia for external knowledge through all datasets. To train the context accessor module efficiently, we use the LoRA (Hu et al., 2022) adapter in pre-training and fine-tuning. For the pre-training stage, we adopted prompts from xRAG (Cheng et al., 2024) and used two million randomly sampled contexts from the December 2021 Wikipedia dump. We use the NQ train for fine-tuning and hold out 10% for validation. Since CARE requires explicit positive and negative contexts, we filter out questions for which none of the top 100 retrieved passages contain an answer span. We set the number of memory tokens K as 16 for all experiments. We use a zero-shot setup to precisely\n\n3 mistralai/Mistral-7B-Instruct-v0.2\n\n4 meta-llama/Meta-Llama-3-8B-Instruct"]}, {"type": "entry", "index": 65, "question": "how do i evaluate non-question research queries like \u201cfind me papers like X but with Y\u201d", "answer": "You can treat a request like \u201cfind me papers like X but with Y\u201d as an information\u2011retrieval and result\u2011generation task, then evaluate it along two axes:\n\n1) How good is the retrieval?\n\n\u2022 Use similarity-based retrieval metrics: compare what your system retrieved against a human\u2011selected set of \u201cideal\u201d papers using relevance scores.\n  \u2013 You can base retrieval on lexical methods like BM25 or semantic embeddings (CodeBERT, UniXcoder, etc.) and compare how often the system\u2019s top results match or rank highly among the human\u2011relevant set [2][3].\n  \u2013 Under the hood, dense retrievers (like DPR) embed queries and documents into a vector space and rank by similarity; evaluation then checks whether high\u2011similarity items are actually the right ones [3].\n\n2) How good is the final output list/description?\n\n\u2022 String/structure similarity: If you have a \u201cgold\u201d query or filter (e.g., a reference SPARQL or search expression), compare the system\u2019s query to it using overlap scores such as BLEU or ROUGE, which measure how similar the generated structure is to a reference [1].\n\u2022 Execution-based metrics: Run both the system\u2019s query and the gold query, then compare the result sets. A relaxed exact match approach removes superficial differences (e.g., variable names), deduplicates lines, and compares processed result sets for equality or overlap [1]. This directly checks whether the user would see essentially the same papers.\n\u2022 Embedding-based answer similarity: If your system outputs a textual summary of the found papers instead of a formal query, compare that text to a human\u2011written summary using cosine similarity over embeddings to capture semantic closeness, not just word overlap [4].\n\nCombining these gives a practical evaluation: retrieval relevance, similarity of the underlying query/filter, and similarity of the presented results.\n", "tags": ["_untagged"], "full_faithfulness": 1.0, "full_answer_relevancy": 0.8607025424, "seed_faithfulness": 1.0, "seed_answer_relevancy": 0.8786682747000001, "context_relevance": 0.16685651123831732, "context_relevance_error": null, "context_relevance_reason": "The context passages primarily focus on SPARQL query generation and evaluation metrics, which do not directly address the user's question about evaluating non-question research queries. While there are mentions of retrieval techniques and evaluation processes, they lack specific guidance on how to handle queries like 'find me papers like X but with Y'. The relevance of the context to the user's specific inquiry is minimal.", "structure_citation": 0.6814479184260344, "structure_error": null, "structure_reason": "The Actual Output effectively addresses the evaluation of non-question research queries by outlining two axes: retrieval quality and final output quality. It includes specific metrics such as BLEU, ROUGE, and similarity-based retrieval techniques, which align well with the Context provided. However, while it references [1], [2], and [3], it does not clearly match all markers to specific snippets in the Context, leading to a slight deduction in points for potential missing citations.", "hallucination_score": 0.7993490091544598, "hallucination_error": null, "hallucination_reason": "The response effectively outlines a structured approach to evaluating non-question research queries, focusing on retrieval quality and output description. It references relevant metrics like BLEU and ROUGE, which align with the context provided. However, while the output is mostly supported by the context, it lacks specific examples or direct connections to the SciQA dataset mentioned, which could enhance clarity and relevance.", "contexts_used": ["5 Experiments\n5.1 Datasets\n\nWe conduct experiments on the SciQA benchmark dataset, a recently released resource designed to evaluate question answering systems over scholarly knowledge graphs [2]. SciQA provides a diverse set of natural language questions aligned with the Open Research Knowledge Graph (ORKG). SciQA contains 100 handcrafted natural language questions with paraphrases, corresponding human- and machine-readable SPARQL queries with their results. In addition, a set of 2465 questions has been semi-automatically derived from eight question templates.", "5.2 Baselines and the state of the art\n\nAs a baseline, we adopt a zero-shot setting in which the model generates SPARQL queries without any task-specific fine-tuning or example guidance. This setup evaluates the model's out-of-the-box ability to understand the task and map natural language questions to structured SPARQL queries. For the state-of-the-art comparison, we implement a one-shot approach in which the model is provided with the most semantically similar question from the training set, along with its corresponding SPARQL query, as an in-context demonstration. The most similar example is identified by computing cosine similarity between the input question and all training questions using SentenceBERT embeddings. This configuration has shown strong performance in recent studies by helping the model better understand the tasks [12, 14].", "Fig. 2: FIRESPARQL framework (yellow boxes) and evaluation setup (grey boxes)\n\nNLQ Prompt template RAG SKG Candidate properties and entities Fine-tuned LLM NLQ-SPARQL Pairs [1] Ontology Fine-tuning LoRA Final SPARQL Retrieved relevant context SPARQL Queries LLM SPARQL corrector Vanilla LLM Ground truth Eval 1: SPARQL generation performance Evaluation Answers to the generated SPARQL queries Ground truth answers\n\nEval 2: SAPRQL Execution\n\nperformance\nFIRESPARQL framework", "5.3 Implementation\n\nWe fine-tuned two instruction-tuned models, Llama 3.2-3B Instruct and Llama 3-8B Instruct, using 1,795 labeled samples from the SciQA training set. The models were trained under various epoch configurations (3, 5, 7, 10, 15, and 20) to analyze performance across training durations. All fine-tuning experiments were conducted on a single NVIDIA H100 GPU. We used DeepSeek-R1-DistillLlama-70B as the underlying model for retrieval-augmented generation (RAG).\n\nAll SPARQL queries are executed via Qlever [3]", "5.4 Evaluation metrics\n\nWe employ a combination of string-based and execution-based evaluation metrics to assess the quality of the generated SPARQL queries. Similar to other research, we use BLEU-4 and ROUGE scores, which measure token-level and n-gram overlaps, to evaluate the similarity between generated queries and the ground-truth queries. These metrics provide insights into how closely the structure and content of the generated queries align with the reference queries. Additionally, we assess the execution performance of the generated SPARQL queries using two variants of relaxed Exact Match: Success and All. The Relaxed Exact Match (success) metric considers only those queries that were syntactically valid, successfully executed against the ORKG RDF dump, and returned non-empty results. In contrast, the Relaxed Exact Match (all) metric evaluates the correctness of the query results across the entire test set, including queries that may have failed or returned empty results.", "Unlike the original Exact Match, which is very strict, our Relaxed Exact Match incorporates several preprocessing steps. First, we remove variable names from the returned results to avoid penalizing differences that do not affect semantics. Second, we split the results line by line and eliminate duplicate lines to normalize the output structure. Finally, we compare the processed sets using exact matching. This approach provides a more tolerant and realistic evaluation of query execution performance in scholarly knowledge graph settings. The abovementioned dual evaluation approach allows us to comprehensively analyze both the syntactic quality and the practical effectiveness of the generated SPARQL queries.", "5.5 Results\n\nTable 1 shows the results of different metrics on different strategies, different models, and different epochs. Figure 3 shows the results of different metrics with different epochs for LoRA fine-tuning. All the scores are the average scores of three runs. The standard deviation of BLEU-4, ROUGE scores, RelaxedEM(success), and RelaxedEM(all) across different model variants (e.g., zeroshot, one-shot, fine-tuning, and RAG) and training epochs is consistently low ( std < 0 . 0265 ), indicating stable performance across all three runs. Therefore, we use the average scores as a reliable and representative summary of model", "effectiveness. In the next section, we discuss the main takeaways from these results.", "| | Strategy | Model | Epoch | BLEU-4 | ROUGE-1 | ROUGE-2 | ROUGE-L | RelaxedEM (success) | RelaxedEM (all) |\n|---:|:--------------|:---------------------------------------------------------|:--------|---------:|----------:|----------:|----------:|----------------------:|------------------:|\n| 0 | zero-shot | llama-3.2-3b-Instruct | - | 0.03 | 0.36 | 0.18 | 0.35 | 0 | 0 |\n| 1 | zero-shot | llama-3-8b-Instruct | - | 0.03 | 0.39 | 0.18 | 0.38 | 0 | 0 |\n| 2 | zero-shot_rag | llama-3.2-3b-Instruct | - | 0.03 | 0.36 | 0.18 | 0.36 | 0 | 0 |\n| 3 | zero-shot_rag | llama-3-8b-Instruct | - | 0.03 | 0.36 | 0.18 | 0.38 | 0 | 0 |\n| 4 | oneshot | llama-3.2-3b-Instruct | - | 0.58 | 0.81 | 0.73 | 0.78 | 0.78 | 0.4 |\n| 5 | oneshot | llama-3-8b-Instruct | - | 0.38 | 0.61 | 0.5 | 0.59 | 0.89 | 0.29 |\n| 6 | ft_rag | llama-3.2-3b-Instruct-lora_ deepseekr1-distill-llama-70b | 20 | 0.32 | 0.63 | 0.51 | 0.6 | 0.42 | 0.06 |\n| 7 | ft_rag | llama3-8b-Instruct-lora_ deepseekr1-disttill-llama-70b | 15 | 0.58 | 0.81 | 0.72 | 0.77 | 0.85 | 0.29 |\n| 8 | ft | llama-3.2-3b-Instruct-lora | 3 | 0.67 | 0.86 | 0.79 | 0.83 | 0.88 | 0.53 |\n| 9 | ft | llama-3.2-3b-Instruct-lora | 5 | 0.62 | 0.83 | 0.75 | 0.79 | 0.71 | 0.36 |\n| 10 | ft | llama-3.2-3b-Instruct-lora | 7 | 0.52 | 0.77 | 0.67 | 0.73 | 0.79 | 0.27 |\n| 11 | ft | llama-3.2-3b-Instruct-lora | 10 | 0.56 | 0.79 | 0.7 | 0.76 | 0.7 | 0.22 |\n| 12 | ft | | 15 | 0.6 | 0.81 | 0.73 | 0.78 | 0.88 | 0.4 |\n| 13 | ft | | 20 | 0.7 | 0.86 | 0.79 | 0.84 | 0.8 | 0.54 |\n| 14 | | | 3 | 0.75 | 0.9 | 0.84 | 0.88 | 0.99 | 0.79 |\n| 15 | | | 5 | 0.74 | 0.9 | 0.85 | 0.87 | 0.98 | 0.73 |\n| 16 | | | 7 | 0.7 | 0.87 | 0.8 | 0.84 | 0.96 | 0.69 |\n| 17 | | llama-3-8b-Instruct-lora | 10 | 0.71 | 0.88 | 0.81 | 0.85 | 0.94 | 0.69 |\n| 18 | | | 15 | 0.77 | 0.91 | 0.86 | 0.9 | 0.98 | 0.85 |\n| 19 | | | 20 | 0.74 | 0.89 | 0.84 | 0.88 | 0.99 | 0.82 |", "Table 1: BLEU, ROUGE and RelaxedEM scores with different strategies, different models and different epochs", "A. Similarity-based Retrieval Techniques\n\nIn our experimental evaluation, we employ five distinct similarity-based retrieval techniques, including one lexical technique (i.e., BM25 [25]) and four semantic techniques (i.e., CodeBERT [26], UniXcoder [27], CoCoSoDa [28], and GTEQwen [29]):\n\nBM25 [25] extends the basic TF-IDF approach by incorporating document length normalization and non-linear term frequency scaling. In our implementation, we use BM25 to retrieve similar code snippets from a function corpus F . For a given code query, BM25 first tokenizes it into terms { t 1 , t 2 , ..., t n } and computes a relevance score for each function f \u2208 F as:\n\nThe IDF component is calculated similarly to the standard TF-IDF approach, but with smoothing factors:\n\nwhere N is the total number of functions in the corpus and df i is the number of functions containing term t i . The modified term frequency component TF mod introduces saturation and length normalization:\n\nwhere tf i is the frequency of term t i in function f , len f is the length of function f , and len avg is the average function length in the corpus. Parameters k and b control term frequency scaling and length normalization, respectively.", "CodeBERT [26] is built upon the RoBERTa-base and pretrained on both natural language and programming language corpus. It employs two types of objective modeling tasks, including Masked Language Modeling (MLM) and Replaced Token Detection (RTD), which enables the model to learn general representations.", "UniXcoder [27] is pre-trained using three objectives: masked language modeling, unidirectional language modeling, and denoising tasks. Additionally, UniXcoder incorporates two pre-training strategies: (1) a multi-modal contrastive learning approach that leverages Abstract Syntax Tree (AST) to enhance code representations, and (2) a cross-modal generation task utilizing code comments to align embeddings across different programming languages.", "CoCoSoDa [28], which has the same architecture as UniXcoder, enhances code representation through momentum contrastive learning. Specifically, the model employs dynamic data augmentation and negative sampling strategies to learn more robust and discriminative code embeddings.", "GTE-Qwen [29], a member of the GTE (General Text Embedding) family, is a decode-only retrieval model based on Qwen2 [30]. Different from the general Qwen model,\n\nGTE-Qwen incorporates bidirectional attention mechanisms to enrich contextual representations, making it particularly effective for multilingual understanding and retrieval tasks. We utilize GTE-Qwen2-1.5B-instruct in our experiment.", "Retriever Module (Dense Passage Retrieval).\n\nThe retriever's job is to efficiently identify which pieces of text in a large corpus are relevant to the input query x . Modern RAG implementations typically use dense retrievers like Dense Passage Retrieval (DPR) [45] in lieu of traditional keyword search. In DPR, a bi-encoder architecture is employed: a question encoder E q ( x ) maps the query x to a d -dimensional vector, and a passage encoder E p ( d ) maps each candidate document (or passage) d in the corpus to a d -dimensional vector in the same space. Relevance is assessed via a similarity score, usually the dot product of these vectors:\n\nAt query time, the retriever computes v q = E q ( x ) and then finds the topK documents whose vector E p ( d ) has highest inner product with v q . This search for maximum inner product can be implemented efficiently using Approximate Nearest Neighbor techniques (e.g., FAISS) to handle millions of documents in sub-linear time. The output of the retriever is the set Z = { z 1 , . . . , z K } of top-ranked documents and potentially their similarity scores. One can view the retriever as defining a probability distribution P ret ( z | x ) over documents z in the corpus, such that:\n\nwith the normalization \u2211 d \u2208C exp( E q ( x ) \u22a4 E p ( d )) (in practice approximated by summing over retrieved candidates rather than all of C ). In other words, the retriever assigns higher probability (or score)", "to documents whose embedding is most similar to the query's embedding. The DPR retriever is usually first trained on pairs of questions and relevant passages to ensure E q and E p produce representations that maximize dot-products for true Q-A pairs and minimize them for irrelevant pairs. The training objective is often a contrastive loss : for a given question q with a gold relevant passage p + and a set of negative passages { p -1 , . . . , p -N } , the encoder is trained to maximize sim ( q, p + ) while minimizing sim ( q, p -) for negatives. This can be formulated as a cross-entropy loss treating the positive passage as the correct class among one positive and N negatives:\n\nwhich encourages E q and E p to embed true pairs closer together than any negative pair [45]. After training, the retriever can generalize to new queries: it embeds the query and efficiently finds the nearest neighbor passages in the index. This retrieval step is crucial because it narrows down the evidence from potentially billions of tokens to a manageable subset that the generator will actually consider.", "Generator Module (Conditional Seq2Seq Model). The generator in a RAG pipeline is typically a sequence-to-sequence language model that produces the final answer or output text, given the input query and retrieved documents. Formally, the generator defines a conditional distribution P gen ( y | x, Z ) over output sequences, where Z = { z 1 , . . . , z K } are the retrieved passages. The generator is often initialized from a pre-trained transformer-based seq2seq model (such as BART or T5) to leverage rich language generation capabilities [52]. During generation, the model is provided with the question (or prompt) as well as the content of the retrieved documents. There are multiple ways to feed the retrieved context to the generator:\n\nIn an early fusion approach, one can concatenate the query x with the text of all retrieved passages into a single extended input sequence (perhaps with special separators) and have the seq2seq model attend to all of it at once. This is straightforward: the generator effectively treats the combined text as context and learns to pick out the relevant bits when producing the answer. However, this can be difficult if K is large, as the input may become very long.", "In the late fusion approach adopted by RAG [52], the generator considers one retrieved document at a time as a context and then marginalizes over the document choices (as in Eq. 1). Specifically, the RAG-Sequence variant fixes a single document z i as context for generating the entire output and computes P ( y | x ) by summing the probabilities P ( y | x, z i ) weighted by the retriever's confidence P ( z i | x ) . Another variant, RAG-Token, allows the generator to switch between documents at the token level, effectively marginalizing over the document choice for each generated token [52]. In both cases, the generator P gen ( y | x, z ) itself works like a standard seq2seq model: it factorizes over the output tokens y 1 , . . . , y T as \u220f T t =1 P \u03b8 ( y t | x, z, y <t ) , i.e. it generates one token at a time, attending to the input query x and the content of z (or multiple z 's if early fusion). The generator's architecture typically uses an encoder-decoder Transformer: the encoder encodes the combination of x and z , and the decoder produces y autoregressively.", "Because the generator is conditioning on retrieved evidence, it tends to produce outputs that are supported by that evidence. For example, if the query asks for a specific factual answer, the generator can copy or rephrase the needed information from one of the retrieved passages. This is in contrast to a vanilla language model which would attempt to rely on parametric knowledge (which might be outdated or incomplete). In sum, the generator module in RAG is responsible for fluent and coherent text generation, but crucially it is grounded by the retrieval context, which guides it toward accurate content.", "3 Methodology:\n\n3.2.2 Retrieval and Generation\n\nFor the naive Knowledge Graph (KG) retrieval pipeline, the retrieval process differs significantly. Instead of directly retrieving passages from the unsummarized corpus through vector search, we retrieve structured information from a", "Figure 8: K 2 RAG framework overview. Component (a): Knowledge Graph. Component (b): Hybrid Retriever containing Dense and Sparse retrievers. Component (c): Embeddings Model. Step (A): Knowledge Graph results. Step (B): Summarise Knowledge Graph results. Step (C): Generating sub-questions from chunks. Step (D): Generating sub-answers for sub-questions. Step (E): Generate the final answer from sub-answers and Knowledge Graph Results Summary.\n\nA Retrieve from Knowledge Graph B Summarise Results C Generate sub-questions from Results Summary D Answer Each Sub-question and Summarise Answer Embeddings Model Quantised LLM Hybrid Vector Retriever E\n\nGive final answer with\n\nsub-answers and Knowledge Graph User Question Final Answer SUMMARY\n\nknowledge graph that has been constructed using the unsummarized corpus and a quantized LLM. In this setup, the same quantized LLM is employed both for generating the knowledge graph and for querying it at retrieval time.", "Across all pipelines, including the KG-based approach, the retrieved information is subsequently passed to an unquantized LLM for final answer generation. Importantly, all pipelines share a common answer generation prompt, which is the same as that used in K 2 RAG [Prompt 3.1.3]. This standardization of the answer generation step allows for a controlled comparison across different naive retrieval strategies with K 2 RAG .", "3.3.1 Dataset Selection and Description\n\nIn order to perform a comprehensive evaluation on the proposed framework we have outlined two key requirements needed from the dataset as follows:\n\nDataset must contain question answer pairs in plain text format where answers are to be assumed as ground truth answers to the question.", "The question answer pairs must be constructed in reference to a document corpus which has all the required documents and their information from which ground truth answer to the questions are derived from.", "Due to time and resource constraints we were unable to create our own dataset from scratch satisfying these requirements and hence the MultiHop-RAG dataset [30] was identified and used as it satisfies the above key requirements needed to perform the analysis.", "The MultiHop-RAG dataset contains corpus data which was used for training along with a test dataset containing 2555 question answer pairs from which we performed evaluation of the pipeline. The corpus contains a 609 articles from a", "3.3.2 Evaluation Process\n\nWe used a K-fold evaluation framework where we split the data into K=10 folds. For each pipeline, we generated the answers for each question in each fold and captured answering times for the questions to compare the execution times [Figure 10] [Algorithm 3].", "To evaluate the accuracy of a pipeline's answer, we computed the similarity between the ground truth and the pipeline's output as a scoring function s of the plaintext ground truth and pipeline output for each question defined as:\n\nwhere o is the pipeline's output, t is the ground truth and e represents the embeddings model as a function to generate the vector embedding of a plaintext input.", "We randomly partitioned the dataset into ten equal folds to simulate real-world usage of the pipeline. This approach reflects scenarios in which different users pose diverse questions to the system. Random assignment of test instances to folds helps mitigate the risk of performance bias, which could arise if certain folds contained clusters of similar questions that align particularly well with the strengths of the pipeline. For evaluation, we employed cosine similarity", "Figure 9: Structure of competitor pipelines. Component (S): Semantic Search - Dense Vector Retrieval. Component (K): Keyword Search - Sparse Vector Retrieval. Component (H): Hybrid Search - Combination of Sparse and Dense Vector Retrieval. Component (KG): Knowledge Graph Retrieval\n\nRetrieve information 20% Sparse Database 80% Dense 100% Unquantised H S K KG\n\nto assess the semantic similarity between the predicted and ground truth answers, as it effectively captures the degree of semantic similarity. Additionally, we recorded the execution time for each query to analyze the system's efficiency.\n\nFigure 10: Evaluation Pipeline.\n\nGround Truth Model's Common System Prompt Cosine Similarity RAG Pipeline MultiHop-RAG Test Dataset\n\nK-th Subset of questions as Input\n\nStore Scores and Times Fold 1 to 10 Repeat Execution Time Scores List Times List"]}], "worst": [{"type": "entry", "index": 35, "question": "how often does entity linking mess up acronyms and lookalikes, and how do i test that", "answer": "The documents do not report a direct number for \u201chow often\u201d entity linking fails specifically on acronyms or look\u2011alike names.\n\nWhat we do see is that:\n- Even when structure is correct, LLMs often link to the wrong property or entity (semantic inaccuracies) in SPARQL generation over a scholarly KG [2]. This is the same failure mode you\u2019d see when an acronym (e.g., NMF, ACE) is mapped to the wrong node.\n- In medical KGs, entity linking relies on exact keywords and then semantic similarity of embeddings when no exact match exists [3]. This last step explicitly allows linking to a merely \u201csemantically related\u201d concept rather than the exact one, which is where lookalikes and acronym collisions can occur [3].\n- In conversational entity\u2011centric systems, the largest single error category is coreference/entity confusion (27% of failures) [4], showing that confusing similar mentions is a major practical problem even outside strict EL.\n\nHow to test this in your setting, using patterns from the papers:\n1) Build a targeted test set\n   - Create a small benchmark that stresses acronyms and visually/lexically similar entities, analogous to how MATE is used to stress cross\u2011modal entity linking [1].\n   - Include positive pairs (correct acronym\u2192entity) and hard negatives (different entities sharing acronym or near\u2011identical names).\n\n2) Run your full EL pipeline\n   - Use the same identifier / keyword / embedding\u2011based linking strategy described in MedKGent [3].\n\n3) Manually and/or LLM\u2011assisted evaluation\n   - Have humans or an LLM score whether each link is correct vs. incorrect, similar to the triple\u2011validity rubric used in MedKGent [3] and the SPARQL error audit in FIRESPARQL [2].\n\n4) Analyze error types\n   - Break down errors like the semantic vs. structural analysis in [2] and the error table in [4], but with categories \u201cacronym collision,\u201d \u201clookalike surface form,\u201d etc.\n\nThis will give you an empirical error rate for your own stack, focused specifically on acronyms and lookalikes.", "tags": ["long_tail_entities"], "full_faithfulness": 0.24, "full_answer_relevancy": 0.0, "seed_faithfulness": 0.8260869565000001, "seed_answer_relevancy": 0.0, "context_relevance": 0.208549739206933, "context_relevance_error": null, "context_relevance_reason": "The context provided does not directly address the user's question about how often entity linking messes up acronyms and lookalikes, nor does it offer methods for testing that. While it discusses entity linking and its evolution, it lacks specific information on the issues related to acronyms and lookalikes, which is the core of the user's inquiry.", "structure_citation": 0.5733559905428909, "structure_error": null, "structure_reason": "The Actual Output does not include any [n] markers, which is a significant shortcoming as per the evaluation steps. While it provides relevant information and claims about entity linking issues, it fails to reference specific snippets from the Context using the required markers. Additionally, some claims made are supported by the context, but without proper citations, the overall alignment with the evaluation criteria is weakened.", "hallucination_score": 0.6572611409640697, "hallucination_error": null, "hallucination_reason": "The response accurately identifies that the documents do not provide a specific frequency for how often entity linking fails with acronyms or lookalikes, which aligns with the input query. It also discusses relevant issues such as semantic inaccuracies and coreference confusion, which are pertinent to the topic. However, it lacks direct references to specific claims made in the context, such as the limitations of traditional EL methods or the role of exact keywords in medical KGs, which could have strengthened the response further.", "contexts_used": ["Entity Linking\n\nEntity Linking (EL) has evolved from text-only methods to Multimodal Entity Linking (MEL), and more recently to Cross-Modal Entity Linking (CMEL), which supports crossmodal reasoning. Traditional EL methods associate textual entities with their corresponding entries in a knowledge base, but overlook non-textual information (Shen, Wang, and Han 2015; Shen et al. 2023). MEL extends EL by incorporating visual information as auxiliary attributes to enhance alignment between entities and knowledge base entries (Gan et al. 2021; Liu et al. 2024b; Song et al. 2024).", "However, MEL does not establish cross-modal relations beyond these auxiliary associations, thereby limiting genuine cross-modal interaction.", "CMEL goes further by treating visual content as entities-aligning visual entities with their textual counterparts-to construct MMKGs and facilitate explicit crossmodal inference (Yao et al. 2023). Research on CMEL remains in its early stages, lacking a unified theoretical framework and robust evaluation protocols. The MATE benchmark is introduced to assess CMEL performance, but its synthetic 3D scenes fall short in capturing the complexity and diversity of real-world images (Alonso et al. 2025). To bridge this gap, we construct a CMEL dataset featuring greater real-world complexity and propose a spectral clustering-based method for candidate entity generation to drive further advances in CMEL research.", "3 Error type analysis on generated SPARQL queries\n\nDespite the improvements of LLMs on QA over SKGs, LLMs face limitations when handling KG-specific parsing. The experimental results conducted by S\u00f6ren Auer et al.[2] showed that only 63 out of 100 handcrafted questions could be answered by ChatGPT, of which only 14 answers were correct. To better understand why LLMs fail to generate the correct SPARQL query to a NLQ, we conduct a pilot experiment on using ChatGPT(GPT-4) with a random one-shot example to generate SPARQL queries for 30 handcrafted questions in the SciQA benchmark datasets.", "Insights from this pilot experiment revealed two major categories of errors LLMs tend to make in this task: semantic inaccuracies and structural inconsistencies. Semantic inaccuracies occur when LLMs fail to link the correct properties and entities in ORKG, despite generating SPARQL queries with correct structure. Our observations reveal that LLMs tend to rely on the example provided in the one-shot learning process to generate the correct structure for a certain type", "of questions, but often struggle with linking the correct properties and entities because LLMs do not learn the content of the underlying KG. Structural inconsistencies arise due to LLMs' lack of ontological schema of the underlying KG, leading to errors in query structure, such as missing or abundant links (triples), despite correctly linking to the mentioned entities or properties.", "Figure 1 shows the example of semantic inaccuracies and structural inconsistencies problem with the generated SPARQL queries in our pilot study. In the example of the semantic inaccuracies problem, ChatGPT failed to link the correct property orkgp:P15687; instead, it linked to a wrong property orkgp:P7101. In the example of the structural inconsistencies problem, the SPARQL query generated by ChatGPT directly links Contribution to Metrics, fails to detect the correct schema of the ORKG where Contribution and Metric are connected via Evaluation.", "Fig. 1: Examples of semantic inaccuracies and structural inconsistencies problem with the generted SPARQL queries\n\nSemantic inaccuracies Problem\n\nFail to link the correct properties and entities in ORKG\n\nWhat is the maximum sample size?\n\nContribution Evaluation Metric P34 P2006 P7046\n\nStructural inconsistencies Problem\n\nMake errors in query structure, such as missing or abundant links (triples)\n\nWhat are the metrics used by paper \"Using NMF-based text summarization\n\nto improve supervised and unsupervised classification?\n\norkgp:P15687 rdfs:label Sample size (n)\n\norkgp:P7101 rdfs:label has elements", "2 Results\n2.1 Method overview\n\nas its confidence score. For instance, if the triple (NPPA, Negative Correlate, Water) appears in 90% of the outputs, its confidence score is 0.9. Low-confidence triples (score < 0.6) are filtered out, and only high-confidence triples are retained for downstream graph construction. Each triple is also annotated with the PubMed ID of the source abstract and a timestamp, ensuring traceability and source attribution. For example, (NPPA, Negative Correlate, Water) would have a PubMed ID of 10494624 and a timestamp of 2000-01-01.", "As shown in Figure 1 c , for each retained triple, such as (NPPA, Negative Correlate, Water), the Constructor Agent checks its presence in the current KG. If absent ( i.e. , either the head or tail entities are missing), it is inserted; if present, its confidence score is updated according to Equation (1). The associated PubMed ID is appended, and the timestamp is updated to reflect the latest publication. For example, if an existing triple (NPPA, Negative Correlate, Water) has a confidence score of 0.7, PubMed ID 10691132, and timestamp 1999-12-31, and a new occurrence with a confidence score of 0.9, PubMed ID 10494624, and timestamp 2000-01-01 is encountered, the updated triple will have a confidence score of 0.97, PubMed IDs [10691132, 10494624], and a timestamp of 2000-01-01. If the head and tail entities are present but the relation differs, such as existing (NPPA, Associate, Water) vs. incoming (NPPA, Negative Correlate, Water), only the most appropriate relation is maintained. The Constructor Agent invokes the LLM to resolve the conflict by selecting the more suitable relation, considering both the existing and incoming triple's confidence scores and timestamps. If the LLM selects the new triple, the existing one is replaced; otherwise, no changes are made. The prompt design for relation conflict resolution is shown in Extended Data Figure 2 c . Together, the two agents extract structured medical facts and integrate them into a dynamic, time-aware KG. See more details in the Section 4.", "2.2 Structural Characterization of the Knowledge Graph\n\nIn this section, we detail the structural characteristics of the medical KG we constructed, with an emphasis on the distribution of node types, relationship types, and the confidence scores of relationship triples. We also present a visualization of a subgraph centered on COVID-19 to illustrate the graph's structure.", "Using the MedKGent framework, we extracted knowledge triples from the abstracts of 10,014,314 medical papers, with 3,472,524 abstracts (34.68%) yielding extractable triples. The relatively low extraction rate can be attributed to several factors: first, some abstracts lacked sufficient structured information for triple extraction; second, only triples with a confidence score exceeding 0.6 were retained, excluding those with lower confidence; and third, some triples extracted by LLMs contained formatting issues, such as extraneous or irrelevant characters, which were discarded. In total, our Extractor Agent identified 8,922,152 valid triples from the abstracts. However, the extracted triples contained a significant number of duplicates and conflicts. To resolve this, our Constructor Agent integrates the triples in chronological order. During this process, duplicates are merged, with the confidence score for each triple increasing in proportion to its frequency, reflecting greater certainty. For conflicting triples, where the same entity pair is associated with multiple relations, the Constructor Agent retains the most appropriate relationship. Following this consolidation, the final KG comprises 2,971,384 distinct triples.", "We conducted a comprehensive statistical analysis of the final constructed KG, which comprises 156,275 nodes. As shown in Figure 2 a , the node distribution is predominantly dominated by Gene and Chemical nodes, with smaller proportions of other entities such as Disease, Variant, Species, and CellLine. The KG includes 2,971,384 relationship triples (edges), representing a range of interactions between entities, as illustrated in Figure 2 b . The most common relationship type is 'Associate', followed by 'Negative Correlate' and 'Positive Correlate', indicating strong associations between medical entities. Less frequent relationships, such as 'Interact', 'Prevent', and 'Drug Interact', provide additional insights into the complexities of medical interactions. The distribution of confidence scores for these relationship triples, shown in Figure 2 c , with confidence values discretized to the nearest smaller 0.05 increment (rounding down to the closest multiple of 0.05), reveals a clear dominance of high-confidence triples. A significant proportion of triples exhibit confidence scores of 0.95, reflecting the cumulative increase in confidence resulting from the repetition of triples during the graph construction process. This high-confidence distribution reinforces the reliability and robustness of the KG.", "We visualized a local subgraph of the constructed KG with COVID-19 as the central node, highlighting five surrounding relationship triples, as shown in Figure 2 d . Each node is characterized by six key attributes: the Identifier, which uniquely references the node and normalizes multiple synonymous mentions to a standardized terminology entry; the Entity Type, which classifies the entity; the Terminology, which maps the entity type to its corresponding standard terminology; the Page Link, providing a reference to the entity in the Terminology; the Exact Keywords, which lists common names and aliases of the entity in lowercase; and the Semantic Embedding, a vector representation of the entity. In practice, these attributes facilitate entity linking within a query by matching entities to their corresponding nodes in the KG. When the Identifier of an entity in the query is available, entity linking can be efficiently performed using this unique reference. In the absence of an Identifier, precise matching", "Figure 2: A comprehensive statistical analysis and visualization of the constructed KG, consisting of 156,275 nodes and 2,971,384 relationship edges. a . Node distribution within the KG, with Gene and Chemical nodes predominating, and smaller proportions of Disease, Variant, Species, and CellLine. b . Relationship type distribution within the KG, highlighting the prevalence of 'Associate' relationships, followed by 'Negative Correlate' and 'Positive Correlate', with less common interactions such as 'Interact', 'Prevent', and 'Drug Interact'. c . The distribution of confidence scores for relationship triples, discretized to the nearest smaller 0.05 increment, ensures values are rounded down to the closest multiple of 0.05. This distribution reveals a clear dominance of high-confidence triples, particularly those with scores of 0.95, underscoring the robustness of the KG. d . Local subgraph visualization centered on COVID-19, displaying five surrounding relationship triples. Each node is characterized by key attributes, including Identifier, Entity Type, Terminology, Page Link, Exact Keywords, and Semantic Embedding, facilitating efficient entity linking through exact or similarity matching. The relationships in the KG are further enriched by attributes such as Confidence, PubMed IDs, and Timestamp, enhancing traceability, accuracy, and temporal relevance.\n\nCOVID -19 ACE2 Pneu- monia Lung Disea -ses MAD00 04J08 tociliz- umab Treat Identifier : MESH:C000718219 Entity Type : Chemical Terminology : NCBI MeSH Page Link", ": meshb.nlm.nih.gov/record/ui?ui=C000718219\n\nExact Keywords : [mad0004j08] Semantic Embedding : [- 0.12, \u2026, 0.10 ] : MESH:D000086382\n\nEntity Type:\n\nDisease\n\n: meshb.nlm.nih.gov/record/ui?ui=D000086382\n\n: [ncp, covid-19]\n0.25, \u2026, 0.09\n\nIdentifier:\n\nMESH:C502936\nChemical\n\nTerminology:\n\nNCBI MeSH\n\n: meshb.nlm.nih.gov/record/ui?ui=C502936\n\n: [mra, tocilizumab] 0.12, \u2026, 0.13 Associate 59272 Gene\n\n:\n\nNCBI Gene\n\n: www.ncbi.nlm.nih.gov/gene/59272\n\n: [ace2, ace2p]\n0.22, \u2026, 0.09]\nMESH:D011014\n\n: meshb.nlm.nih.gov/record/ui?ui=D011014\n\n: [pneumonia]\n0.18, \u2026, 0.01\nMESH:D008171\n\n: meshb.nlm.nih.gov/record/ui?ui=D008171\n\n: [lung diseases,lung damage]\n\n: [ 0.06, \u2026, 0.11 d a b Drug_Interact (0.1%) 0.70 0.65 'Prevent (0.79 0.75 7.89) (7.5%) 0.60 (8.1%) (5.4% (47.7%) 0.80 CellLine Positive (8.9%) (0.5%) Correlate 0.85 (19.9%) (10.3%) Variant (1.49) (5.9%) Cause (1.4% 0.90 (33.6%) Inhibit (1.2% Negative_Correlate Stimulate (0.5%) (13.7%) Species Compare (26.1%) Cotreat (1.0%)", "Figure 3: Comprehensive evaluation of extraction quality for relationship triples generated by the Extractor Agent. Systematic assessment of extraction accuracy using both automated evaluations by LLMs and independent manual expert review. a . Proportion of valid relationship triples (score \u2265 2.0) across relation types, as assessed by GPT4.1 on a randomly selected subset of 34,725 abstracts (83,438 triples). b . Proportion of valid relationship triples across relation types, as assessed by DeepSeek-v3 on the same subset. c . Validity rates from independent manual evaluation by three domain experts on a subset of 400 abstracts (1,060 triples), demonstrating high inter-expert consistency. d-f . Performance of GPT-4.1 and DeepSeek-v3 compared to three expert evaluations on the shared evaluation subset, reporting precision, recall, and F1 score. g . Pairwise inter-rater agreement between experts and LLMs quantified by Cohen's kappa coefficients, demonstrating substantial consistency across all evaluators.\n\nGPT-4.\n\nAutomated Evaluation\n\nDeepSeek-v3 Automated Evaluation\n\nManual Evaluation 0936| 0.0307 0,8875 0,8880 0 8700 0.7160 0.4\n\nExpert1's Evaluation as Reference\n\nExpert2's Evaluation as Reference\n\nExpert3's Evaluation as Reference\n\nPairvise Cohen's 0 9761 09761 0 0602 00760 0.9502 00537 0,9503 0 9440 0.5663 08143 0,8818 0 5446 0.6762 0,8853 0.5446 0.6906 06818 0.6008 0 6560 GPT-4,1 DeepSeek-v3 GPT-4.1 Correlale Corelate Cause Inhon Irhon Cotcat Inlatact Colrcat Kappa ison", "is achieved by checking whether the entity appears in the Exact Keywords list of a specific node. Alternatively, semantic vectors of the query entities can be compared with those in the KG to identify the most similar entities, enabling semantic similarity matching. This approach is particularly beneficial for entities with multiple names, ensuring accurate linking even when not all aliases are captured in the Exact Keywords list.", "The relationships between entities are characterized by three key attributes. Confidence reflects the reliability of the relationship, with higher values indicating greater certainty based on its frequency across multiple sources. The PubMed IDs attribute lists the PubMed identifiers of the papers from which the relationship is derived, enabling easy access to the original publications via the PubMed website 2 . If the relationship appears in multiple papers, all relevant PubMed IDs are included, further increasing the confidence score. Finally, Timestamp denotes the most recent occurrence of the relationship, specifically the publication date of the latest paper. Notably, while Timestamp captures only the latest appearance, the full temporal span of the relationship-including its earliest mention-can be readily retrieved through the associated PubMed IDs via the PubMed website. These attributes collectively enhance the traceability, accuracy, and temporal relevance of the relationships within the KG.", "4 Methods\n4.2.2 Constructor Agent\n\nA chemical/drug treats a disease. The Treat relationship typically occurs between Chemical and Disease.\n\nMeSH (Medical Subject Headings)\n\ndbSNP, otherwise HGNV format\n\nNCBI Taxonomy\nCell Line\nCellosaurus\n\nYour task is to select the most appropriate relation\n\nbetween two medical entities to form more\n\nreasonable knowledge triple.\n\nThere is an and Now, a new between e1 and\n\ne2 is proposed.\n\nPlease decide which relation should be retained\n\nbetween e1 and e2.\n\nIf r1 should be kept, respond with \"Y\".\n\nIf r2 should replace it, respond with \"N\".\n\nYou may consider the following two factors to assist\n\nyour decision:\n\n(1) The\n, and\nthat of\n\n;\n\n(2) The\nfor\n\nIn general, relations with\n\nhigher confidence scores or more recent timestamps are likely\n\nretained.\n\nYour output should contain only \"Y\" or \"N\". Do not\n\nprovide any explanations.\n\nOutput:\n\nc", "Extended Data Figure 2: a . Prompt template for relation extraction. Given a biomedical abstract and its extracted entities, the Extractor Agent prompts the LLM to infer semantic relations between entity pairs using a predefined relation set and textual descriptions. b . Reference terminologies for entity normalization. Each biomedical entity type is mapped to a standard terminology: Gene (NCBI Gene), Disease and Chemical (MeSH), Variant (dbSNP or HGNV), Species (NCBI Taxonomy), and Cell Line (Cellosaurus). c . Prompt design for relation conflict resolution. When conflicting relations exist between the same entity pair, the Constructor Agent prompts the LLM to select the most appropriate one based on confidence scores and timestamps. d . Schema for predefined relation types. The 12 core relation types-seven bidirectional and five unidirectional-are listed alongside their directionality, descriptions, and allowed entity-type combinations.", "4.3 Quality Assessment\n\nWe assessed the quality of relational triples extracted by the Extractor Agent through both automated and manual evaluations, leveraging two state-of-the-art LLMs-GPT-4.1 [74] and DeepSeek-v3 [75]-as well as three PhD students with interdisciplinary expertise in medicine and computer science. For each medical abstract and its corresponding set of extracted triples, individual triples were evaluated using a standardized four-level scoring rubric: 3.0 (Correct), 2.0 (Likely Correct), 1.0 (Likely Incorrect), and 0.0 (Incorrect). The evaluation prompt provided to both LLMs and human annotators is illustrated in Extended Data Figure 3 a .", "A relational triple was defined as valid if it received a score of \u2265 2 . 0 . The validity rate was calculated as:\n\nTo assess the reliability of automatic evaluation, we compared LLM-based assessments with human annotations on a shared evaluation subset, treating human judgments as ground truth. The precision, recall, and F 1 -score of the automatic evaluations were computed as:\n\nwhere TP, FP, and FN represent true positives, false positives, and false negatives, respectively. To further quantify inter-rater agreement, we calculated Cohen's Kappa coefficient [82] for each pair of evaluators, including both LLMs and human annotators, resulting in 10 pairwise comparisons across the five raters. The Kappa coefficient was computed as:\n\nwhere p 0 represents the observed agreement and p e denotes the expected agreement by chance. This analysis provides a quantitative measure of rating consistency across evaluators.", "4.4 Retrieval-Augmented Generation\n\nThe constructed KG serves as a reliable external source for information retrieval and can be integrated into LLMs via a RAG framework. By providing structured biomedical context, the KG enhances LLM performance across a range of medical QA benchmarks.", "Given a user query q , we first extract the set of medical entities present in the question, denoted as E q = { e q 1 , e q 2 , \u00b7 \u00b7 \u00b7 } . When using PubTator3 [80]-the same entity recognition tool employed during KG constructioneach extracted entity is assigned a unique identifier. This allows for efficient entity linking by matching these identifiers to the corresponding nodes N q = { n q 1 , n q 2 , \u00b7 \u00b7 \u00b7 } within the graph. Alternatively, if medical entities are extracted using other methods-such as prompting a LLM-they may lack standardized identifiers. In such cases, the extracted entity mentions are first converted to lowercase and matched against the Exact Keywords attribute of each node in the KG. A successful match enables linkage of the entity to the corresponding graph node. In both approaches, if an entity cannot be linked via its identifier or if its surface form does not appear in any node's Exact Keywords list, we apply a semantic similarity strategy to complete the entity linking process. Specifically, the embedding of the query entity is computed using the same model employed for generating node-level semantic representations ( i.e. , BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext [81]) and is compared against the Semantic Embedding of all nodes in the KG. The entity is then linked to the node with the highest semantic similarity score, which may correspond to either the exact concept or a semantically related medical entity. This entity linking framework-combining identifier-based matching, lexical normalization, and semantic embedding-ensures robust and flexible integration of KG-derived knowledge into downstream QA tasks.", "Following entity linking, we construct evidence subgraphs using a neighbor-based exploration strategy [86] to enhance the reasoning capabilities of LLMs. For each entity-linked node in the query-specific set N q , we retrieve its one-hop neighbors within the KG. Specifically, for each node n q i \u2208 N q , all adjacent nodes n q \u2032 i are identified, and the corresponding triples ( n q i , r, n q \u2032 i ) are appended to form a localized subgraph G q i . This expansion captures the immediate relational context surrounding the query entities, which is essential for enabling fine-grained medical reasoning. The complete evidence set for a given query is then defined as the union of these localized subgraphs: G q = { G q 1 , G q 2 , \u00b7 \u00b7 \u00b7 } . The resulting subgraph G q may contain a large number of relational triples, including redundant or irrelevant information, which can adversely impact LLM reasoning [87]. To address this, we leverage the LLM's inherent ranking capability to selectively filter high-value knowledge [88]. Given the question q and", "You are tasked with evaluating the validity of the knowledge triples extracted from the abstract of a medical paper.\n\nGiven the abstract (\n\n) of a medical paper and the extracted triples\n\n) from this abstract.\n\nEach triple is represented in the format:\n\n\"Head Entity Name (Alias1, Alias2) | Relationship Name | Tail Entity Name (Alias1, Alias2)\"\n\n,\n\nwith triples separated by ' $ '.", "Some entities may have no aliases or multiple aliases, which are separated by ', ' within the '()'.\n\nYour task is to evaluate the validity of each triple, with a particular focus on the\n\nrelationship\n\nit describes, based on the information provided in the abstract. Consider whether the stated relationship accurately\n\nreflects the connection between the head and tail entities as presented or implied in the text.", "For each triple, evaluate its validity using the following scoring scale and assign a confidence score:\n\n\u2022\n\nCorrect (3.0):\n\nThe relationship logically and accurately describes the relation between the head and tail entities as\n\nexplicitly mentioned or directly and strongly supported\n\nby the abstract. The\n\nrelationship type is\nprecise\n\nand the connection is\n\nundeniable\n\nbased on the text, requiring minimal inference.\n\nLikely Correct (2.0):\n\nThe relationship is\n\ngenerally acceptable and directionally correct\n\n. The core connection between the entities is\n\nvalid and supported by the text (explicitly, implicitly, or via\n\nreasonable inference)\n\n, even if the relationship type has\n\nminor inaccuracies or lacks ideal precision\n\n.\n\nLikely Incorrect (1.0):\n\nsubstantially inaccurate or misleading\n\nsignificantly misrepresenting\n\nthe connection described in the abstract, even if the entities are mentioned together.\n\nIncorrect (0.0):\n\nnot supported by the abstract whatsoever\n\n, is\n\nclearly and undeniably contradicted\n\nby the text, or involves a\n\nfundamental misunderstanding\n\nof the entities or their\n\nconnection as presented.\n\nOutput the evaluation in a fixed format:\n\nFirst line: 'Analysis: ' followed by the analysis of all triples, separated by '; '. Each triple's analysis should explain\n\nwhy\n\nthe specific confidence score (3.0, 2.0, 1.0, or 0.0) was assigned based on the criteria\n\nabove and the abstract's content.", "Second line: Only the numerical confidence scores for all triples, separated by ' $ ', in the same order as the input triples (e.g., 3.0 $ 2.0 $ 1.0 $ 0.0). This line must contain only numbers (formatted to one\n\ndecimal places like 3.0, 2.0, 1.0, 0.0), decimal points, and ' $ ' as separator, with no additional text or English letters.", "5 Results\n5.1 Main Results\n\n| | Model | FR (%) | DC (%) | UCS (/5) |\n|---:|:-------------------|:-----------|:-----------|:-----------|\n| 0 | Stateless LLM | 54.1 (0.4) | 48.3 (0.5) | 2.1 (0.1) |\n| 1 | Vector RAG | 71.6 (0.6) | 66.4 (0.7) | 3.4 (0.1) |\n| 2 | Entity-RAG | 75.9 (0.5) | 72.2 (0.6) | 3.7 (0.1) |\n| 3 | Semantic Anchoring | 83.5 (0.3) | 80.8 (0.4) | 4.3 (0.1) |\n\nTable 1: Overall performance on MultiWOZ-Long. Semantic Anchoring outperforms all baselines across metrics. Improvements in FR and DC are statistically significant at p < 0 . 01 ; UCS gains are significant at p < 0 . 05 . Values are mean \u00b1 stdev over three runs.", "Figure 2 analyzes how performance varies with session depth. While all models degrade as dialogue span increases, Semantic Anchoring sustains over 75% recall at 10 sessions, indicating stronger long-range tracking.", "5.2 Per-Dataset Breakdown\n\nTo test generality, we evaluate on DialogRE-L , which emphasizes relation extraction across sessions. Results in Table 2 show consistent improvements, though broader domains are needed to claim robustness.", "Figure 2: Factual Recall by session depth on MultiWOZ-Long. Semantic Anchoring exhibits the slowest degradation, maintaining > 75% recall at 10-session distance. Error bars denote standard deviation across three runs.\n\nFactual Recall vs. Session Depth (MultiWOZ-Long)\n\nStateless LLM Vector RAG Entity-RAG Semantic Anchoring Session Depth\n\n|---:|:-------------------|---------:|---------:|-----------:|\n| 0 | Stateless LLM | 49.8 | 44.1 | 2 |\n| 1 | Vector RAG | 68.7 | 62.5 | 3.2 |\n| 2 | Entity-RAG | 72.1 | 68.3 | 3.6 |\n| 3 | Semantic Anchoring | 81.4 | 77.9 | 4.2 |\n\nTable 2: Performance on DialogRE-L. Semantic Anchoring achieves consistent gains across metrics, suggesting effectiveness in relation extraction tasks that require long-range entity tracking.", "5.3 Ablation Studies\n\nTable 3 examines the role of linguistic components. Removing discourse tagging reduces FR by 4.7 points, while excluding coreference resolution reduces DC by 6.2 points. Eliminating all symbolic features collapses performance to Vector RAG levels. These results align with observed error patterns (\u00a75.6), underscoring the value of symbolic features.", "5.4 Qualitative Examples\n\nIn MultiWOZ-Long, when the user later asks 'Did he confirm the time for the taxi?' , Semantic Anchoring retrieves:\n\n[Entity: John Smith][CorefID: E17] confirmed the taxi is booked for 9 AM.", "By contrast, Vector RAG surfaces unrelated mentions of 'taxi.' Additional examples, including cases where Semantic Anchoring fails, are shown in Appendix C.", "| | Variant | FR (%) | DC (%) | UCS (/5) |\n|---:|:-------------------------|---------:|---------:|-----------:|\n| 0 | Full Model | 83.5 | 80.8 | 4.3 |\n| 1 | - Discourse Tagging | 78.8 | 75.6 | 4 |\n| 2 | - Coreference Resolution | 80.1 | 74.6 | 4.1 |\n| 3 | - Dependency Parsing | 81.2 | 78.5 | 4.1 |\n| 4 | Dense-only (Vector RAG) | 71.6 | 66.4 | 3.4 |\n\nTable 3: Ablation results on MultiWOZ-Long. Removing discourse or coreference modules significantly reduces FR and DC, respectively. Without all symbolic features, performance falls to the dense-only baseline.", "5.5 Human Evaluation\n\nFive trained annotators rated 50 randomly sampled conversations for User Continuity Satisfaction (UCS). Agreement was high ( \u03b1 = 0 . 81 ). As Table 1 shows, Semantic Anchoring achieves the highest UCS (4.3), with annotators noting better consistency in entity references. Full protocol details are in Appendix B.", "5.6 Error Analysis\n\nTable 4 categorizes common failures. Coreference mistakes (27%) and parsing errors (19%) are the most frequent, consistent with ablation findings. Discourse mislabeling (15%) often arises in sarcasm or overlapping speech. While overall error frequency is lower than dense retrieval, these remain open challenges.", "| | Error Type | Proportion of Failures |\n|---:|:----------------------|:-------------------------|\n| 0 | Parsing errors | 19% |\n| 1 | Coreference mistakes | 27% |\n| 2 | Discourse mislabeling | 15% |\n| 3 | Other / miscellaneous | 39% |\n\nTable 4: Error analysis on MultiWOZ-Long. Coreference mistakes are the most frequent error type, followed by parsing and discourse issues. These patterns align with ablation results."]}, {"type": "entry", "index": 45, "question": "how do i test it on \u201cnew paper contradicts old paper\u201d situations", "answer": "To test a system on \u201cnew paper contradicts old paper\u201d cases, you want an evaluation where two separate texts about the same topic disagree, and the model must (a) notice the conflict and (b) pinpoint where it is.\n\nThe MAGIC benchmark is designed exactly for inter\u2011document conflicts like this. It builds pairs of contexts from a knowledge graph, then slightly perturbs one of them so that the two texts end up making incompatible claims\u2014for example, different release orders of songs or different genres for the same work.[1] These are then converted into natural text passages, giving you two realistic, human\u2011like documents that disagree in subtle ways, often requiring multi\u2011step reasoning to see the contradiction.[1]\n\nTo test your system in a \u201cnew vs. old paper\u201d setting using this style:\n1. Treat each MAGIC instance as two papers: Context A = older paper, Context B = newer paper.[1]\n2. Feed both texts to your model via your usual RAG or reading pipeline.\n3. Ask it explicitly: (a) Is there any conflict between these two documents? and (b) If yes, quote or summarize the conflicting statements. MAGIC\u2019s own evaluation measures both conflict detection (Identification score) and precise localization of the conflicting spans (Localization score).[1]\n4. Score your system the same way: it gets credit only if it correctly says there is a conflict and points to the exact conflicting claims.[1]\n\nMAGIC includes both simple single\u2011hop and harder multi\u2011hop conflicts, so you can see how well your system handles straightforward and subtle contradictions, which closely mimics real \u201cnew paper overturns old paper\u201d situations.[1]", "tags": ["_untagged"], "full_faithfulness": 0.5833333333, "full_answer_relevancy": 0.8812546640000001, "seed_faithfulness": 0.7916666667000001, "seed_answer_relevancy": 0.8812546640000001, "context_relevance": 0.8129051644428735, "context_relevance_error": null, "context_relevance_reason": "The context passages provide substantial information about knowledge conflict in retrieval-augmented generation systems, which directly relates to the user's question about testing scenarios where new papers contradict old ones. The passages discuss the challenges of detecting conflicts and the proposed framework for evaluating inter-context knowledge conflicts, which aligns well with the user's inquiry. However, while the context is relevant, it could be more explicitly focused on practical testing methods for the specific scenario mentioned, which slightly limits the score.", "structure_citation": 0.8314206979239959, "structure_error": null, "structure_reason": "The Actual Output effectively addresses the evaluation steps by discussing the MAGIC benchmark, which is relevant to testing systems on inter-document conflicts. It includes multiple [1] markers that correspond to specific snippets in the Context, demonstrating a clear understanding of the task. However, while the claims made are generally supported by the Context, there is a slight lack of explicit citations for some claims, which could lead to minor deductions in the score.", "hallucination_score": 0.9108062771886773, "hallucination_error": null, "hallucination_reason": "The response effectively addresses the input by outlining a clear methodology for testing systems in scenarios where new papers contradict old ones. It accurately describes the MAGIC benchmark's purpose and how it generates inter-document conflicts, aligning well with the context provided. The steps for testing, including conflict detection and localization, are detailed and relevant, demonstrating a strong understanding of the evaluation criteria. However, a minor shortcoming is the lack of explicit mention of how to handle specific types of contradictions beyond the general framework, which could enhance clarity.", "contexts_used": ["Abstract\n\nKnowledge conflict often arises in retrievalaugmented generation (RAG) systems, where retrieved documents may be inconsistent with one another or contradict the model's parametric knowledge. Existing benchmarks for investigating the phenomenon have notable limitations, including a narrow focus on the question answering setup, heavy reliance on entity substitution techniques, and a restricted range of conflict types. To address these issues, we propose a knowledge graph (KG)-based framework that generates varied and subtle conflicts between two similar yet distinct contexts, while ensuring interpretability through the explicit relational structure of KGs. Experimental results on our benchmark, MAGIC, provide intriguing insights into the inner workings of LLMs regarding knowledge conflict: both open-source and proprietary models struggle with conflict detection-especially when multi-hop reasoning is required-and often fail to pinpoint the exact source of contradictions. Finally, we present in-depth analyses that serve as a foundation for improving LLMs in integrating diverse, sometimes even conflicting, information.", "1 Introduction\n\nRetrieval-augmented generation (RAG) has become the de facto standard for enhancing the performance of large language models (LLMs) by enabling updates to outdated knowledge and facilitating adaptation to specialized domains (Lewis et al., 2020). While effective, RAG's heavy reliance on retrieval quality introduces inherent risks. For instance, knowledge obtained from external sources may conflict with the model's parametric knowledge or contain inconsistencies within the retrieved documents themselves.", "Knowledge conflict (KC) is a recent research topic that covers issues related to the aforemen-\n\n\u2020 Equal contribution. * Corresponding author.", "Figure 1: Example of a three-hop conflict from our benchmark, MAGIC. Even advanced LLMs struggle to detect subtle inconsistencies across two contexts, such as conflicting release orders of two songs.\n\nKnowledge Graph A Context A \u22ef Shine was released after Bloom\n\nand included in the album\n\n+\n\n, which was followed by\n\nRepack\nKnowledge Graph B\npart of\n\nNo conflicts are detected!\n\nIs there a\nknowledge conflict\n\nbetween the two contexts?\n\nContext B\nwas followed by\n\n.\n\nThe song is part of , and \ud83d\udca5 (Imply \u00e0 )\n\n(The model fails to detect the 3-hop conflict.)\n\nfollowed\nby\n\ntioned scenarios and has been receiving attention in the field (Xu et al., 2024). An ideal LLM-based system should be robust against knowledge conflict, capable of integrating information from multiple sources-including those that may be contradictory-and ultimately generating reliable responses. However, its implementation is largely hindered by the challenge of detecting whether disagreements exist across different knowledge sources and, if so, pinpointing exactly where they occur.", "Numerous benchmarks have been introduced to evaluate the performance of LLMs in knowledge conflict detection (Hsu et al., 2021; Li et al., 2024; Jiayang et al., 2024; Hou et al., 2024). Nonetheless, we emphasize that existing research in this area has notable limitations. First, previous studies primarily focus on the question answering (QA) task, where conflicts occur only among multiple answer candidates for a given question (Chen et al., 2022; Xie et al., 2024; Marjanovic et al., 2024). Second, prior research often relies on overly simplistic techniques for dataset construction, e.g., entity substitution (Longpre et al., 2021; Chen et al., 2022), which are insufficient to capture the complex and subtle nature of knowledge conflicts. Third, while some studies attempt to categorize types of knowledge conflict (Hou et al., 2024; Marjanovic et al., 2024), systematic analysis distinguishing between forms, such as single-hop vs. multi-hop conflicts, is still lacking. Finally, existing benchmarks are largely concerned with exploring conflicts between parametric and external knowledge, while conflicts among multiple input documents remain underexplored (Jiayang et al., 2024; Hou et al., 2024). 1", "To alleviate these issues, we propose a framework for constructing a benchmark targeting intercontext knowledge conflict . It leverages knowledge graphs (KGs) as the primary source, from which subgraphs are extracted to serve as the basis for distinct knowledge chunks. These subgraphs are then perturbed by modifying nodes and edges to introduce conflicts. Finally, both original and altered graphs are converted into corresponding text passages using KG-to-text generation algorithms.", "By design, the proposed framework offers several advantages. Leveraging the relational structure of KGs enables greater diversity, complexity, and control in inducing conflicts within documents. Moreover, compared to text-based strategies, our approach improves interpretability and supports structured analysis by representing conflicting entities and relations in graph form (see Figure 1).", "Lastly, we conduct extensive analyses using MAGIC (A M ulti-Hop A nd G raph-Based Benchmark for I nterC ontext Conflicts), a novel dataset constructed through our framework. 2 MAGIC features complex inter-context conflict patterns, including simultaneous and multi-hop cases rarely seen in existing benchmarks. Experiments on MAGIC yield key insights into how LLMs perceive knowledge conflicts: (1) most models remain imperfect at detecting conflicts, especially when multi-hop reasoning is required; and (2) even when contradictions are detected, models often fail to localize the exact point of conflict.", "1 Knowledge conflict is commonly studied in three setups (Xu et al., 2024): (1) context-memory conflict , involving inconsistencies between parametric and external knowledge; (2) inter-context conflict , where contradictions exist among multiple input documents; and (3) intra-memory conflict , where inconsistent responses reflect variation in an LLM's training data. In this work, we focus on inter-context conflict .\n\n2 Available at: https://github.com/HYU-NLP/MAGIC.", "2 Related Work\n\nKnowledge conflict benchmarks Early studies on synthetically inducing sentences with knowledge conflicts (KC) primarily rely on entity substitution (Chen et al., 2022), which is often subsequently polished and paraphrased using LLMs (Xie et al., 2024; Gokul et al., 2025). Regarding domains and tasks, most prior work focuses on potential inconsistencies arising in QA, such as contradictory answers provided for a given question (Longpre et al., 2021). However, this paradigm has a clear limitation: most conflicts must appear near answer-related contexts, which strictly limits the range of possible variations. To overcome this, we adopt question-free, KG-based methods, which inherently support more diverse problem types. 3", "Inter-context knowledge conflict detection evaluates an LLM's ability to identify contradictions either across multiple input contexts (Jiayang et al., 2024; Hou et al., 2024; Marjanovic et al., 2024) or within a single document (Li et al., 2024). Existing benchmarks follow two main strategies: collecting conflicts curated from Wikipedia (Hou et al., 2024) and generating artificial ones using LLMs (Jiayang et al., 2024). The former offers realism but limited coverage, while the latter is scalable yet often less natural and less representative of real-world scenarios. We aim to combine the strengths of both by leveraging the factual grounding of KGs and the generative fluency of LLMs, ensuring both high quality and scalability. Finally, we highlight that little work has examined performance with respect to fine-grained conflict types. Departing from the common convention of using only two conflict categories, 4 we organize our dataset by the number of conflicts and the reasoning hops required for resolution, enabling more systematic analysis.", "KG-based dataset creation KGs play a crucial role in diverse tasks, e.g., fact verification (Kim et al., 2023), QA (Chen et al., 2024), and RAG (Sanmartin, 2024), by providing structured representations of knowledge. In addition, KGs can serve as valuable resources for dataset construction. For instance, Meng et al. (2022) introduce COUNTERFACT, a dataset designed to evaluate factual consistency and modifications in LLMs. In this paper, we also utilize KGs as a foundation\n\n3 Concurrently, Gokul et al. (2025) propose a query-free method but rely solely on LLMs, unlike our KG-based one.", "4 Jiayang et al. (2024): answer & factoid; Hou et al. (2024): explicit & implicit; Marjanovic et al. (2024): static & dynamic.", "Figure 2: Overview of the proposed KG-based framework for benchmarking inter-context knowledge conflict detection. It comprises three steps: (1) Subgraph Extraction, (2) Knowledge Conflict Generation, and (3) KG-to-Text Conversion, with details listed in \u00a73.\n\n(Still Losing You, genre,\n\nRock music)\n<Original Triplet>\n\nStep 2. Knowledge Conflict Generation\n\n\u2026\n\nStep 3. KG-to-Text Conversion\n\n<Original Context>\n\n\u22ef Still Losing You belongs\n\nto rock music genre \u22ef\n\nStep 1. Subgraph\nExtraction\n<Perturbed Context>\n\n\u22ef Still Losing You is a song of\n\nC&W and\ndistinct from\nthe\n\nalbum She Loves Car. It has\n\nmusic genre rock music \u22ef\n\n<Perturbed Triplet>\n\n(Still Losing You, instance of, C&W)\n\n(C&W,\n\n, She Loves Car)\n\n(She Loves Car, genre, Rock music)\n\nfor generating realistic and nuanced conflict statements. Bi et al. (2024) shares similarities with our work, as it also employs Wikidata triplets to induce knowledge conflicts. However, their approach is limited to retrieving seed entities for substitution rather than leveraging the full structure of KGs.", "4 Experimental Setups\n\nWith the MAGIC benchmark as a foundation, we conduct experiments to examine how LLMs handle inter-context knowledge conflicts. We evaluate various open-source and proprietary LLMs without task-specific training, instead prompting them to identify potential contradictions. In the following, we outline the LLMs, datasets, prompting strategies, and metrics used in our experiments.", "LLMs We use 5 LLMs: Llama 3.1 70B Instruct (Dubey et al., 2024), o1 (OpenAI, 2024b), Mixtral8x7B Instruct (MistralAI, 2023), Claude 3.5 Haiku (Anthropic, 2024), GPT-4o-mini (OpenAI, 2024a).", "Datasets Alongside MAGIC, we employ two existing benchmarks to highlight its strengths.\n\nECON (Jiayang et al., 2024): A dataset created by introducing evidence conflicts through two methods-answer conflicts and factoid conflicts-highlighting contradictions in supporting evidence. It contains 168 data instances.", "WikiContradict (Hou et al., 2024): A humanannotated QA benchmark utilizing Wikipedia's contradiction tags to capture real-world knowledge conflicts. It categorizes contradictions into explicit and implicit types. After deduplication, it comprises 103 data samples.", "MAGIC : It is constructed atop KGs, where conflicts are systematically induced from the un-", "| | Models / Datasets | ECON | WikiContradict | MAGIC |\n|---:|:--------------------|-------:|-----------------:|--------:|\n| 0 | Mixtral 8x7B | 46.43 | 52.43 | 37.92 |\n| 1 | Llama 3.1 70B | 81.41 | 78.79 | 72.86 |\n| 2 | Claude 3.5 Haiku | 83.33 | 61.17 | 60.28 |\n| 3 | GPT-4o-mini | 88.1 | 82.52 | 83.61 |\n| 4 | o1 | 74.4 | 74.76 | 68.06 |\n| 5 | Average | 74.73 | 69.93 | 64.54 |\n\nTable 2: ID scores (%) on three KC detection datasets. Lower scores indicate higher task complexity. The lowest value in each row is shown in bold .", "derlying relational structure. It encompasses both single-hop and multi-hop contradictions, with the number of conflicts dynamically varying across context pairs. In total, MAGIC consists of 1,080 carefully curated examples, with comprehensive statistics presented in Table 1. Remarkably, the scale of MAGIC surpasses that of existing benchmarks by a significant margin, offering a richer and more challenging resource for evaluating inter-context conflict detection.", "Prompting strategy Prior work (Jiayang et al., 2024; Hou et al., 2024) typically frames the task as a binary classification problem, relying on minimal prompts (see Appendix B). In contrast, we introduce a stepwise prompting strategy (refer to Figure 17 for details) designed to more fully probe the capabilities of LLMs for inter-context conflict detection.", "Metrics To account for the stochasticity of LLMs, all models perform three independent inference runs. We use two metrics for fine-grained evaluation, with scores averaged across all instances in each dataset. These metrics are manually computed by participating researchers, as automatic methods-such as LLM-as-a-judge-remain insufficiently reliable for this task. 13\n\nIdentification ( ID ) score: If a model fails to detect a conflict in any of the three attempts, it receives a score of 0; otherwise, it receives 1.", "Localization ( LOC ) score: We further evaluate LLMs' performance on conflict localization. A full score (1) is awarded only if all conflicting locations are correctly identified; otherwise, the score is 0. Note that this fine-grained evaluation has not been considered in previous work.", "13 If they were, further investigation into knowledge conflict detection would be unnecessary. See Appendix H for details.", "6 Further Analysis and Discussion\n\nTaxonomy-based analysis To systematically analyze conflict patterns, we apply our proposed conflict typology to ECON and WikiContradict. Rep-\n\n16 Still, exceptions exist depending on the used model.", "Figure 10: Comparison of ID and LOC scores on 1Single-Hop conflicts across three datasets.\n\n74.02 81.12 70.91 63.14 67.82 62.99 ECON WikiContradict MAGIC Score (%) ID Score LOC Score\n\nresenting these datasets as graphs allows us to highlight their characteristics. A key challenge is the lack of predefined ontologies or domain structures, which hinders the use of traditional ontology-based methods (van Cauter and Yakovets, 2024). To address this, we use LangChain (Chase, 2022) to construct reliable, schema-free KGs that support structured conflict representation.", "Figure 3 shows that 1-Single-Hop conflicts-the typically easiest case-are the most prevalent in prior datasets, accounting for 78% in ECON and 76% in WikiContradict. In contrast, MAGIC exhibits a balanced distribution across conflict types, with substantial proportions of 1-Multi-Hop (28%) and N-Multi-Hop (27%), underscoring its robustness as a benchmark.", "Length-based analysis In conflict detection, a reasonable hypothesis is that context length positively correlates with dataset difficulty, as longer documents tend to involve more complex linguistic structures. To validate this, we present lengthoriented anlysis in Figure 9. The left panel visualizes the total context length distribution among the three datasets, with MAGIC containing more longcontext examples. We suspect MAGIC's multi-hop and description-rich design contributes to this.", "Further, we group MAGIC into four bins based on context length and report their respective performance in the right panel of Figure 9. To balance group sizes, we use quantile-based binning: Q1 contains the shortest contexts, and Q4 the longest. As context length increases, both scores decline, with LOC dropping more sharply-indicating the growing difficulty of pinpointing conflicting spans in longer inputs.", "Comparison with existing KC datasets In addition to the results discussed in \u00a75, we compare MAGICwith existing datasets by focusing on the 1Single-Hop type-the only category shared across\n\nFigure 11: Comparison of difficulty between challenging subsets of WikiContradict and MAGIC, with scores averaged over all five LLMs.\n\n29.25 54.34 45.87 61.37 Accuracy (%)\n\nFigure 12: Comparison of ID scores for binary and multi-step prompts, tested on two models.\n\n55.61 59.22 78.57 83.61 82.52 88.1 GPT-4o-mini 34.42 45.63 50.6 73.83 78.79 81.41 Llama 3.1 70B Binary Multi-step (Ours)\n\nall datasets-to ensure a fair and consistent comparison. In ECON and WikiContradict, conflict categories are manually annotated. Figure 10 shows that even on relatively simple 1-Single-Hop conflicts, models perform worse on MAGIC than on ECON and WikiContradict, suggesting that our dataset poses challenges beyond surface-level contradiction.", "We further conduct a comparative study between the challenging subset of WikiContradict (Implicit) and the corresponding subset from our dataset (Multi-Hop in MAGIC). 17 Figure 11 reports that MAGIC proves more difficult for LLMs than WikiContradict. MAGIC yields an ID score up to 9% lower and a LOC score up to 16% lower compared to WikiContradict. This highlights MAGIC's intrinsic complexity, setting it apart from existing datasets.", "Impact of prompts While prior work (Jiayang et al., 2024; Hou et al., 2024) typically employs binary (yes/no) prompts for conflict detection-often oversimplifying the task-we use a multi-step technique that helps improve performance. To compare the effectiveness of these two distinct prompting strategies for conflict detection, we conduct experiments, with results reported in Figure 12.", "17 WikiContradict consists of explicit and implicit conflicts; the latter are considered more challenging due to their subtlety.", "Figure 13: ID scores of two LLMs on known (O) vs. unknown (X) instances, based on their parametric knowledge. Models show slightly stronger conflict detection when the relevant knowledge is already embedded.\n\nX\nO\n\n(Estimated) Existence of Parametric Knowledge\n\nLlama 4o-mini 69.39 69.51 72.55 79.71 ID Score (%)\n\nAcross all cases, the results show that our multistep prompting approach outperforms the na\u00efve prompt, achieving improvement up to 39.41%. This implies that although we employ a method superior to those commonly used in the literature, there remains substantial room for addressing the challenges posed by MAGIC.", "Impact of parametric knowledge In the literature on inter-context conflict, research has primarily focused on conflicts between two input documents, overlooking the influence of parametric knowledge which can significantly affect the performance of LLMs in knowledge conflict detection.", "To address this, we explore an underexamined setup by splitting a subset of MAGIC-instances with 1- and 2-conflicts-into two groups based on the estimated presence of parametric knowledge in LLMs. Concretely, we approximate the existence of parametric knowledge for a given triplet by posing a converted verification question-for example, 'Is Barack Obama Sr. the father of Barack Obama?'. We label a triplet as known if the model provides the correct answer in at least 4 out of 5 attempts, and as unknown if it succeeds in no more than 1.", "As shown in Figure 13, both GPT-4o-mini and Llama 3.1 achieve slightly higher ID scores on instances with prior knowledge. These findings suggest that models may detect conflicts more effectively when they already possess the relevant factual knowledge, although a more systematic investigation is left for future work.", "Impact of context order To probe whether context order influences model behavior, we evaluate both the original MAGIC and a variant in which the two contexts in each instance are reversed.", "As reported in Table 4, the ID scores remain largely consistent across models, with Mixtral 8x7B showing a slight increase and others nearly unchanged. These results indicate that context or-\n\n| | Models | Original | Swap | \u2206 (Swap-Orig) |\n|---:|:-----------------|-----------:|-------:|----------------:|\n| 0 | Mixtral 8x7B | 37.92 | 46.45 | 8.53 |\n| 1 | Llama 3.1 70B | 73.83 | 75.64 | 1.81 |\n| 2 | Claude 3.5 Haiku | 61.11 | 61.01 | -0.1 |\n| 3 | GPT-4o-mini | 83.61 | 81.86 | -1.75 |\n\nTable 4: Effect of context order on ID scores in MAGIC.", "| | Metric | ECON | WikiContradict | MAGIC |\n|---:|:------------|-------:|-----------------:|--------:|\n| 0 | Naturalness | 4.36 | 4.39 | 4.08 |\n| 1 | Realism | 4 | 4.72 | 4.26 |\n\nTable 5: Naturalness and realism scores for three KC datasets, evaluated by Claude 3.7 Sonnet.", "der does not significantly affect model performance on MAGIC, suggesting that our findings are robust to input ordering.", "Linguistic quality of MAGIC An essential requirement for MAGIC is that its conflict data be both realistic and stylistically similar to humanwritten text. To validate this, we use an LLM judge (Claude 3.7 Sonnet) to evaluate two aspects: (i) naturalness -whether the text is fluent and humanlike-and (ii) realism -whether the scenario appears plausible in real-world contexts. We randomly sample 155 contexts from three datasets to match overall length distributions. Each context is rated on a 0-5 scale, with the detailed evaluation prompts provided in Figures 18 and 19.", "As shown in Table 5, MAGIC achieves naturalness scores close to WikiContradict (-0.31), despite WikiContradict relying on real Wikipedia text. It further surpasses ECON in realism, showing that MAGIC generates scenarios that are both plausible and human-like. These results underscore MAGIC's reliability as a benchmark for advancing research in knowledge conflict detection."]}, {"type": "entry", "index": 53, "question": "when does using titles/abstracts beat full text retrieval", "answer": "The provided documents do not report a clear case where using only titles/abstracts decisively outperforms full\u2011text retrieval in terms of answer quality.\n\nInstead, they mostly highlight trade\u2011offs:\n\n1. In scientific survey generation, relying only on abstracts and metadata is explicitly described as a *limitation*: it \u201csignificantly limits the information coverage\u201d and makes surveys lack \u201cdepth and completeness\u201d compared to using full text [2]. This suggests full text is better for rich, in\u2011depth tasks, not worse.\n\n2. In biomedical QA (BioASQ), the retrieval system indexes PubMed titles and abstracts and gets good results, but this is framed as a *practical design choice* (PubMed abstracts are standardized and easy to index) rather than as being superior to using full articles [4]. The paper does not compare against full\u2011text retrieval; it only says that even using full abstracts required reranking to get adequate QA performance [4].\n\n3. For automatic survey generation, authors note that copyright restrictions forced them to use abstracts only, again implying that abstracts are used for legal/efficiency reasons, not because they are inherently better than full texts for quality [2].\n\nSo, within these documents, titles/abstracts are preferred mainly for efficiency, standardization, and legal constraints, while full text is expected to give *better* coverage and depth when it is available and computationally manageable [2][4]. There is no evidence here that titles/abstracts systematically \u201cbeat\u201d full text in effectiveness; they mainly win on cost and feasibility, not on information quality.", "tags": ["_untagged"], "full_faithfulness": 0.6666666667000001, "full_answer_relevancy": 0.0, "seed_faithfulness": 0.6666666667000001, "seed_answer_relevancy": 0.0, "context_relevance": 0.32672400966666454, "context_relevance_error": null, "context_relevance_reason": "The context passages provide some relevant information about retrieval methods and their effectiveness, but they do not directly address the specific question of when using titles/abstracts is preferable to full text retrieval. While there are mentions of limitations related to using abstracts, the overall relevance to the user's question is limited, resulting in a low score.", "structure_citation": 0.7463109367772365, "structure_error": null, "structure_reason": "The Actual Output effectively discusses the limitations of using titles/abstracts compared to full text, referencing specific snippets from the Context with [2] and [4] markers. It accurately highlights that the documents do not support the claim that titles/abstracts outperform full text in quality, aligning well with the evaluation steps. However, while the claims are well-supported, there is a slight lack of clarity in directly linking all claims to the corresponding snippets, which prevents a perfect score.", "hallucination_score": 0.5295564447751956, "hallucination_error": null, "hallucination_reason": "The response accurately identifies that the documents do not provide evidence that titles/abstracts outperform full-text retrieval, highlighting trade-offs and limitations. However, it fails to fully address the context provided, which discusses the challenges of using abstracts due to copyright restrictions and the potential for reduced depth and completeness. While it mentions efficiency and legal constraints, it does not sufficiently connect these points to the broader implications of retrieval methods discussed in the context.", "contexts_used": ["Abstract\n\nElectronic health records (EHRs) are long, noisy, and often redundant, posing a major challenge for the clinicians who must navigate them. Large language models (LLMs) offer a promising solution for extracting and reasoning over this unstructured text, but the length of clinical notes often exceeds even stateof-the-art models' extended context windows. Retrieval-augmented generation (RAG) offers an alternative by retrieving task-relevant passages from across the entire EHR, potentially reducing the amount of required input tokens. In this work, we propose three clinical tasks designed to be replicable across health systems with minimal effort: 1) extracting imaging procedures, 2) generating timelines of antibiotic use, and 3) identifying key diagnoses. Using EHRs from actual hospitalized patients, we test three state-of-the-art LLMs with varying amounts of provided context, using either targeted text retrieval or the most recent clinical notes. We find that RAG closely matches or exceeds the performance of using recent notes, and approaches the performance of using the models' full context while requiring drastically fewer input tokens. Our results suggest that RAG remains a competitive and efficient approach even as newer models become capable of handling increasingly longer amounts of text.", "5 Discussion and Future Directions\n\nLLMfor Automatic Survey Generation: Are We There Yet? The results in Section 4.1 indicate that neither Fully LLM-based nor RAG-based approaches have achieved human-level performance. As highlighted in (Liang et al., 2025; Tang et al., 2025), hallucinated information, such as fabricated references and factual inaccuracies, remains a critical challenge in LLM-generated surveys. Although RAG-based methods reduce hallucinations by retrieving external sources, the retrieved papers are often only topically relevant and misaligned with human preferences. While LLMs have demonstrated efficiency and the ability to generate content considered useful by human evaluators (Wang et al., 2024), our human evaluation results (\u00a74.5) reveal that, despite strong topical relevance, LLMgenerated surveys exhibit limited coverage and indepth analysis, both essential for high-quality scientific surveys. Therefore, while LLMs can assist in survey generation, they are still unable to independently craft surveys that meet academic standards at the current stage.", "Future Directions for Enhancing Survey Generation As shown in Section 4.1, quality-based ranking of candidate references effectively improves the citation performance of generated surveys. This can be further enhanced through several strategies. For example, citation network analysis can be employed to capture the global relationships among papers and identify influential studies. Additionally, analyzing human citation behavior-such as citation intent, frequency, and location in the textual context-can inform better reference selection mechanisms. Training reference selection models on human-annotated datasets is also a potential option for collecting literature suitable for survey generation. On the generation stage, relying solely on abstracts as input significantly limits the information coverage, as it fails to fully capture the paper's broader details. Future work could leverage full-text information to enable more comprehensive contextual understanding, as well as explore human-in-the-loop discourse control, factual consistency verification, and advanced long-document modeling to improve survey quality.", "Real-World Applicability and Deployment Our framework is designed with modular components, including embedding-based retrieval, cocitation expansion, and re-ranking, which can be parallelized or extended. For the retrieval stage, we choose S2ORC (Lo et al., 2020) as the external database because its papers are peer-reviewed and have full dataset downloads, which can be stored locally and used for a one-time embedding computation. In practice, it can be replaced with other sources such as arXiv 7 or PubMed 8 , depending on user needs. Additionally, numerous wellestablished embedding models are available on the MTEB leaderboard 9 , offering a range of trade-offs between accuracy, model size, and computational efficiency. For the co-citation expansion module, we rely on the OpenAlex (Priem et al., 2022b) database for citation analysis. OpenAlex also provides free APIs and allows bulk download of citation data. Similarly, users can replace OpenAlex with other citation databases, such as Scopus (Elsevier, 2025) and SciSciNet (Lin et al., 2023). As for the re-ranking, we assume it is highly adaptable to different downstream needs. Since we will release the SurveyGen , users can customize re-ranking strategies according to their specific preferences. In the generation stage, the results (\u00a74.1) show that open-source models (e.g., LLaMA-3.1-70B) can achieve competitive performance compared to closed-source commercial LLMs such as GPT-4.1. This offers users greater flexibility based on their budget, deployment needs, and infrastructure.", "Limitations\n\nWe acknowledge several limitations in our work.", "Input Limitation. For copyright reasons, our approach is restricted to using only abstracts and bibliographic metadata of the retrieved papers, without access to full-text content. This limitation may hinder the LLM's ability to capture finer-grained details and structural elements that are often present in full-length papers. Hence, the generated surveys may lack depth and completeness compared to human-written surveys that draw on the entire papers.", "Post-generation Refinement. To reduce API call costs, we did not perform post-generation refinement to the LLM outputs, such as language polishing, citation formatting, or structural adjustments. These post-processing steps could further improve the personalization and overall quality of the generated surveys. Also, our work focuses on generating textual survey content and does not include visual elements such as figures, tables, or diagrams, which are often present in published scientific surveys. Lastly, for longer surveys, models like Claude-3Haiku 10 may offer superior performance due to their extended context handling capabilities.", "Data Contamination. We acknowledge the possibility of data contamination, as some surveys or key references (\u00a73.1) used are open access and may have been included in the training data of the LLMs, potentially leading to slightly different performance estimates. Although we do not explicitly control for this factor in our evaluation process, such contamination is a general challenge in benchmarking LLMs on open-domain generation tasks (Xu et al., 2024). Moreover, since all baselines in our study are based on mainstream LLMs, any potential contamination would be shared and thus unlikely to impact the relative comparison.", "Evaluation Sample Scope. While our empirical evaluation focuses on a subset of 120 relatively short surveys spanning multiple disciplines-selected to balance cost and feasibility in academic settings -weexpect similar performance trends to hold across the full dataset. We encourage the broader research community to further benchmark models using our dataset and framework to extend our findings across broader contexts.\n\nhttps://www.anthropic.com/news/ claude-3-haiku", "C. Contextual Summary\n\nContextual summaries are generated to provide concise, relevant information about technical documents, facilitating more accurate and focused retrieval. For a document \ud835\udc51\ud835\udc57 , the summary \ud835\udc46 \ud835\udc57 is extracted using an attention mechanism that focuses on key themes and contextually important parts:\n\nHere, \ud835\udc4a\ud835\udc60 represents the learned parameters for the summarization task. This approach ensures that the model captures essential details from \ud835\udc51\ud835\udc57 , enhancing comprehension and retrieval accuracy.", "3. Our methodology\n\n3.1. Information Retrieval Pipeline\n\nTo support high-quality RAG for Phase A+, we developed an IR pipeline that integrates traditional lexical search with LLM-based query generation and semantic reranking (Fig. 1).", "If the initial query returns fewer than five documents, we invoke Gemini 2.5 Pro Preview (05-06) to automatically revise the query. The model is prompted to enhance retrieval recall by enabling approximate matching and omitting overly rare or domain-specific terms. This refinement step is done to improve the query coverage while maintaining relevance. Our experiments have shown that this process is required in less than 5% of the queries in the BioASQ 13 test set.", "We index all PubMed article titles and abstracts in an Elasticsearch instance, using BM25 retrieval as the ranking function. For each input question, we use Gemini 2.0 Flash to generate a structured Elasticsearch query that captures the semantic intent of the question using synonyms, related terms, and full boolean query string syntax rules supported by Elasticsearch. This query is validated using regular expressions and then is used to retrieve up to 10,000 documents.", "Following document retrieval, we apply a semantic reranking model (Google semantic-ranker-default004) to reduce the number of candidate documents [11]. This model re-scores the initially retrieved documents based on semantic similarity to the original question, allowing us to select the top 300 most relevant documents. This reranked subset is used for downstream RAG-based QA, since despite really long context supported by modern Transformer architectures [12, 13], we could not get adequate QA results on full article abstracts without this step.", "Finally, we have added additional IR searches to handle the cases where a QA step does not return a response based on the evidence retrieved from Elasticsearch. We have observed that Elasticsearch context might not provide sufficient evidence for QA in 3-7% of test cases for Phase A+, depending on the batch. An automated process is used to expand IR sources to address these cases. First, we are using a Google search restricted to PubMed sources to attempt to find new matches. If that fails, we extend our sources to include Home of the Office of Health Promotion and Disease Prevention, WebMD,\n\nThis multi-stage retrieval approach, combining LLM-generated queries, a traditional BM25 search, and semantic reranking, enables flexible, high-recall, and high-precision document selection tailored to complex biomedical queries.", "Figure 1: IR process\n\nPubMed corpus in Elasticsearch Query Generation (Gemini 2.0 Flash) Query Valida- tion and IR (BM25, \u2264 10,000 docs) Results < Refinement 2.5 Pro) Reranking (semantic- reranker-4) Top 300 Articles for RAG No Yes Refine\n\nHealthline, and Wikipedia. This ensures that we have an answer candidate for all questions in Phase A+ test sets.", "3.2. Question Answering Pipeline\n\nWe adopt a unified, zero-shot QA framework for both Phase A+ and Phase B of the challenge. While the core QA procedure remains consistent across phases, Phase A+ incorporates an additional IR step to verify the presence of candidate answers within relevant documents (described at the end of Section 3.1). This ensures that selected documents contain sufficient information to support answer generation.", "To generate candidate answers, we leverage several large language models (LLMs): Gemini 2.0 Flash, Gemini 2.5 Flash Preview (2025-04-17), and Claude 3.7 Sonnet (2025-02-19). Prompts are adjusted using examples derived from the BioASQ 11 test set, improving the response structure and quality.", "The system uses zero-shot prompting, tailored to the question type: Yes/No, Factoid, or List. We experiment with multiple types of input context: (1) IR-derived results from Phase A+, (2) curated snippets provided in Phase B, and (3) full abstracts of articles selected during Phase B. This allows us to examine the influence of context granularity on answer accuracy and completeness.", "To consolidate candidate answers, we perform a secondary synthesis step using Gemini 2.0 Flash. This model is prompted to resolve any contradictions, select the most precise and specific answer components, and integrate complementary information into a single, unified response. As part of this step, the model also returns a confidence score estimating the reliability of the synthesized answer. If the score is below a predefined threshold (0.5, determined empirically), the synthesis is re-run with reduced sampling temperature (from 0.1 to 0.0) to improve determinism. This synthesis process is evaluated using the BioASQ 12 dataset to ensure consistency with benchmark standards.", "Table 1\n\nResults of our runs on BioASQ 13 Phase A+, Yes/No questions.", "4 Experiments\n4.2 Performance Comparison\n\nTable 1 and Table 2 report the Recall scores on E-VQA and InfoSeek, respectively. QM-Retriever consistently outperforms all baseline methods, achieving average improvements of 9.9% (E-VQA) and 7.0% (InfoSeek) over the second-best approach. The strong recall performance ensures that mKG-RAG operates on highly relevant knowledge graphs constructed in the fine-grained retrieval phase, as further supported by our ablation studies. Additionally, the results reveal that V \u2192 Vretrieval consistently beats other unimodal and cross-modal configurations, underscoring the critical role of visual content in VQA tasks.", "| | MLLM.MLLM | E-VQA.E-VQA | InternVL3.8B | LLaMA-3.2.11B | LLaVA-v1.5.7B | LLaVA-v1.5.13B | DeepSeek-VL2.3B | DeepSeek-VL2.16B | Qwen2.5-VL.3B | Qwen2.5-VL.7B | Qwen2.5-VL.32B |\n|---:|:------------|:--------------|:---------------|:----------------|:----------------|:-----------------|:------------------|:-------------------|:----------------|:----------------|:-----------------|\n| 0 | Zero-shot | Single-Hop | 22.4 | 27.0 | 15.8 | 16.1 | 22.0 | 22.4 | 19.1 | 21.0 | 27.1 |\n| 1 | Zero-shot | All | 23.0 | 28.9 | 16.2 | 16.6 | 21.6 | 22.3 | 18.9 | 20.8 | 27.3 |\n| 2 | mKG-RAG | Single-Hop | 32.7 | 37.2 | 25.0 | 27.7 | 28.4 | 31.1 | 28.9 | 30.4 | 36.5 |\n| 3 | mKG-RAG | | \u2191 10.3 | \u2191 10.2 | \u2191 9.2 | \u2191 11.6 | \u2191 6.4 | \u2191 8.7 | \u2191 9.8 | \u2191 9.4 | \u2191 9.4 |\n| 4 | | All | 32.7 | 38.5 | 24.6 | 27.8 | 27.4 | 29.9 | 28.2 | 29.6 | 36.5 |\n| 5 | | | \u2191 9.7 | \u2191 9.6 | \u2191 8.4 | \u2191 11.2 | \u2191 5.8 | \u2191 7.6 | \u2191 9.3 | \u2191 8.8 | \u2191 9.2 |", "Table 4: VQA accuracy on E-VQA across different MLLM architectures with varying sizes\n\nQwen2-VL: Jozef KUBA\n\nGPT-4o: Matej Walch mKG-RAG: J. Langer\n\nQ : Who designed this museum?\n\nQwen2-VL \uff1a Indonesian\n\nGPT-4o: Indo-Saracenic mKG-RAG: Javanese\n\nQ : What is the architectural style of this mosque?\n\nQwen2-VL: 400\n\nGPT-4o:\n\nI can't answer without specific details.\n\nmKG-RAG: 250-metre\n\nQ : How long is the track at this velodrome?\n\nQwen2-VL: PARK\n\nGPT-4o: New York City Department of Parks and Recreation mKG-RAG: Abingdon Square Conservancy\n\nQ : Who is in charge of maintaining this park?\n\nQwen2-VL: 1000000\n\nGPT-4o: 26.04 km 2\n\nmKG-RAG: 3.82 km 2\n\nQ\n\n: What is the surface area of this lake?\n\nQwen2-VL: air space\n\nGPT-4o: a ring of dark spots mKG-RAG: brownish spots\n\nQ : What forms a ring on the larger end of the egg of this bird?", "Figure 4: Qualitative results of Qwen2-VL-7B, GPT-4o and mKG-RAG on E-VQA dataset.", "Results on E-VQA and InfoSeek. In this section, we compare mKG-RAG with Zero-shot MLLMs and RAG-based approaches on the benchmarks mentioned above. The results in Table 3 demonstrate that zero-shot MLLMs struggle with knowledge-based VQA tasks, particularly on the InfoSeek dataset. These limitations underscore the critical need for external knowledge integration. By augmenting LLaVA-More with mKG-RAG, we achieve substantial improvements, over 20.3% on E-VQA and 31.9% on InfoSeek, highlighting the value of retrieval augmentation.", "Furthermore, our method achieves state-of-the-art performance on both datasets. Under the finetuning setting, mKG-RAG * surpasses both mR2AG * and ReflectiVA * . Even without fine-tuning, mKG-RAG outperforms EchoSight by 1.2% and 1.7%, respectively. These results highlight the advantages of integrating RAG with multimodal KGs and demonstrate the effectiveness of our QMRetriever. Table 3 also includes two mKG-RAG variants that replace QM-Retriever with text-only and vision-only CLIP for entity/relationship retrieval, while still using documents retrieved by QMRetriever to construct multimodal KGs. In the text-only variant, both questions and image captions are used as queries to provide more context, explaining its better performance over the vision-only version. However, they both remain less effective than our full approach with QM-Retriever.", "Consistency across Architectures. In Table 4, we provide a detailed comparison of VQA scores across MLLMs of varying parameter sizes, including InternVL3 [58], LLaMA-3.2-Vision 2 , LLaVAv1.5 [34], DeepSeek-VL2 [52], and Qwen2.5-VL [3]. When enhanced with our mKG-RAG framework, these models achieve average performance gains of 9.4% on single-hop queries and 8.7% on overall scenarios, demonstrating the method's strong generalization across different architectures and scales.", "Qualitative Results. Figure 4 shows a qualitative comparison of mKG-RAG with zero-shot Qwen2VL and GPT-4o. While the latter two tend to produce plausible but incorrect or evasive responses, mKG-RAG consistently handles knowledge-intensive queries, especially those involving precise numerical and temporal reasoning.\n\n2 https://huggingface.co/meta-llama/Llama-3.2-11B-Vision", "| | Method. | E-VQA.Single-Hop | E-VQA.All | InfoSeek.Un-Q | InfoSeek.Un-E | InfoSeek.All |\n|---:|:--------------------|-------------------:|------------:|----------------:|----------------:|---------------:|\n| 0 | mKG-RAG | 38.4 | 36.3 | 41.4 | 39.6 | 40.5 |\n| 1 | w/o QM-Retriever | 34.2 | 31.6 | 38.9 | 37.9 | 38.4 |\n| 2 | w/o Graph Retrieval | 30.1 | 28.2 | 33.3 | 32.7 | 33 |\n| 3 | w/o Graph Expansion | 37.2 | 35 | 40.8 | 39.4 | 40.1 |\n\nTable 5: The ablation study of the design of mKG-RAG\n\n| | Model | Ret. Mode | K g = 1 | K g = 5 | K g = 10 | K g = 20 |\n|---:|:---------|:------------|----------:|----------:|-----------:|-----------:|\n| 0 | mKG-RAG\u2020 | Textual | 29.1 | 33.9 | 34.9 | 35.9 |\n| 1 | mKG-RAG\u2021 | Visual | 23 | 29.6 | 31 | 32 |\n| 2 | mKG-RAG | Multimodal | 29.2 | 35.1 | 36.3 | 36.9 |\n\nTable 6: The ablation study of how the retrieval number of entities/relationships affects the VQA accuracy on E-VQA.", "4.3 Ablation Study\n\nImpact of Coarse-grained Retrieval. To quantify the impact of coarse-grained document retrieval, we conduct an ablation experiment replacing QM-Retriever with visual-only CLIP (ViT-L/14@336) for topK d document selection. The results in Table 5 reveal significant performance drops: overall VQA accuracy of mKG-RAG decreases by 4.7% on E-VQA and 2.1% on InfoSeek. This ablation conclusively demonstrates the critical role of first-stage retrieval and QM-Retriever's superiority over unimodal alternatives.", "Effectiveness of Graph-based Retrieval. In our method, the entities and relationships extracted from documents form a distilled knowledge graph, reducing noise and enabling more effective retrieval than direct text chunk matching. To validate this insight, we replace graph-based retrieval with a naive chunk-based alternative. Specifically, we segment retrieved documents into fixed-size chunks and select those relevant to the given question and image description. As shown in Table 5, chunk-based retrieval leads to a substantial accuracy drop, 8.1% on E-VQA and 7.5% on InfoSeek."]}, {"type": "entry", "index": 46, "question": "does reranking bias toward certain writing styles/venues/authors or am i paranoid", "answer": "You\u2019re not paranoid\u2014reranking can absolutely introduce biases, including toward certain venues, authors, and indirectly toward particular writing styles.\n\n1. Bias from what features you use\nIn some systems, papers aren\u2019t just ranked by topical relevance, but also by \"quality\" signals like citation counts, author h\u2011index, and venue reputation.[2] This is explicit in QUAL-SG: papers are re-ranked using citation performance, author influence, and venue reputation.[2] That naturally favors:\n- Well\u2011cited, older or mainstream topics\n- Famous authors with high h\u2011indexes\n- Prestigious or \u201ccore\u201d venues\nLess-cited but innovative or niche work\u2014and authors from less-known venues\u2014will be pushed down.\n\n2. Bias from training data\nRerankers are trained on large collections of query\u2013document pairs; if those datasets over-represent certain domains, article structures, or writing styles, the model will implicitly learn to favor them.[3][4] The GeoGPT reranker emphasizes cleaning out \u201cnoisy or biased examples\u201d precisely because even a small number can \u201csignificantly degrade reranking performance,\u201d which includes skewing what it prefers.[3]\n\n3. Bias acknowledged but often not measured\nOne health-RAG study explicitly notes its limitation: it assumes relevant documents are already retrieved and \u201comits retrieval-stage factors such as ranking errors and bias,\u201d and calls for future work to study retrievers and rerankers under real-world conditions.[1] That\u2019s essentially an admission that reranking bias is real but under-explored.\n\nSo yes, reranking can systematically favor certain venues/authors and, via those signals, particular academic styles. It\u2019s not just in your head; it\u2019s a design and data issue.", "tags": ["rerankers"], "full_faithfulness": 0.7000000000000001, "full_answer_relevancy": 0.8758143842, "seed_faithfulness": 0.8, "seed_answer_relevancy": 0.8758135454, "context_relevance": 0.15555671730722426, "context_relevance_error": null, "context_relevance_reason": "The context passages primarily focus on methodologies and experimental setups related to reranking and evaluation metrics, but they do not directly address the user's question about whether reranking introduces bias toward certain writing styles, venues, or authors. While there is mention of bias-controlled pooling setups, the overall relevance to the user's specific concern is minimal, leading to a low score.", "structure_citation": 0.46602165449373983, "structure_error": null, "structure_reason": "The Actual Output includes claims supported by citations, but there are issues with citation matching. While citations [2], [3], and [4] are referenced, they do not correspond to any snippets in the provided Context, leading to unsupported claims. Additionally, the claim about bias in reranking is acknowledged but lacks direct support from the Context, which affects the overall reliability of the response.", "hallucination_score": 0.7409667366685813, "hallucination_error": null, "hallucination_reason": "The response accurately addresses the user's concern about reranking biases, providing specific examples of how biases can arise from feature selection and training data. It effectively references the context by discussing the implications of biased evidence pools and acknowledges the limitations of current studies, aligning well with the factual claims made in the input. However, while the response is strong, it could benefit from more explicit connections to the context provided, particularly regarding the experimental setups mentioned.", "contexts_used": ["3 Methodology\n3.3 Experimental Setup\n\nBased on the dataset statistics in Table 1, the number of possible document pairs quickly grows large. For example, with 193 helpful and 121 harmful documents in TREC 2020, the helpful-harmful condition alone would generate over 23,000 possible pairs; when combined with six adversarial variants per query in both TREC 2020 and 2021, the number of pairs becomes even larger. To keep computation feasible, we only sample up to ten document pairs for each condition per query. If there are fewer than ten valid pairs, all available combinations are included. Each document pair is then combined with user query framings to assess how sequencing and user assumptions affect the model outputs together.", "This setup mainly contributes to RQ3 by testing the influence of evidence order and combinations, while also providing insight into RQ2 by extending the single-document analysis to multi-document contexts, and RQ4 by evaluating how different query framings shape ground-truth alignment outcomes.", "3.3.3 Passage-Based Pooling Setup. The passage-based pooling setup moves beyond controlled pairings. In this setup, all documents in the dataset are first segmented into smaller chunks to enable segment-level retrieval. We segment each Manuscript submitted to ACM", "document with a sliding window of 512 words and a 256-word overlap. These segments, regardless of their original classification as helpful, harmful, or adversarial, are subsequently reranked using the MonoT5 reranker [54]. For each query, the ten highest-ranked segments are retained to form the retrieval pool, which is then provided to the language model as context in combination with one of three user query formulations. This setup thus reflects how LLMs would behave under conventional retrieval conditions, where retrieved evidence is shaped by ranking models rather than manually controlled.", "This setup primarily addresses RQ3 by examining the impact of various evidence types on model behavior, while simultaneously enhancing RQ2 by evaluating the persistence of document type effects observed in controlled environments within a conventional retrieval pipeline. This setup also examines RQ4 by analysing the interplay between query formulations and non-uniform retrieval pools in establishing ground-truth alignment results.", "3.3.4 Bias-Controlled Pooling Setup. The bias-controlled pooling setup examines how context pools that are biased towards a certain type of evidence (helpful or harmful) affect how models respond. This setup is inspired by the methodological framework of Pogacar et al. [61], who showed that imbalances in evidence presentation can affect human judgment. We apply this concept to the RAG context by creating two skewed retrieval pools: one biased towards helpful evidence and the other biased towards harmful evidence . We used the MonoT5 reranker to process each category of document segments (helpful, harmful, and adversarial) independently for every query, producing the top-10 helpful, top-10 harmful, and top-10 adversarial results for each attack type. These segments are then grouped into retrieval pools of fixed size (ten segments). In the helpful-biased condition, the eight highest-ranked segments from the top-10 helpful list are placed at the top of the pool, while the remaining two highest-ranked segments are selected from the top-10 harmful or adversarial lists. In the harmful-biased condition, the eight highest-ranked segments from the top-10 harmful or adversarial list are placed first, followed by the two highest-ranked segments from the top-10 helpful list. This setup contributes to RQ3 by testing imbalanced evidence and RQ4 by examining how user query formulations interact with skewed pools to affect alignment.", "3.4 Evaluation\n\nTo evaluate model performance across different experimental setups, we use a single metric, Ground-Truth Alignment (i.e., accuracy), which measures whether the stance of a generated response matches the ground-truth stance label for the query. A response is considered aligned if its predicted stance is identical to the ground-truth stance.", "Prior work has shown that LLMs can perform reliably as automatic evaluators [16, 29], often approaching humanlevel agreement. Following this line of work, we employ gemini-2.0-flash as an external stance classifier. To further validate our evaluation pipeline, we conduct a secondary classification of all responses using gpt-4o-mini and measure inter-rater agreement with gemini-2.0-flash and compute Cohen's Kappa score [26]. Agreement was 0.90 for TREC 2020 and 0.82 for TREC 2021. These values indicate what is conventionally characterized as 'almost perfect' agreement between the two classifiers and indicate the reliability of automated stance detection. We use gemini-2.0-flash as the only stance classifier for all of the experiments to avoid relying on the same model family for both generation and evaluation.", "For each experimental setup, we compute the Ground-Truth Alignment Rate, defined as the proportion of aligned responses relative to the total number of generated responses. The prompt used for stance classification of LLM responses is publicly available in the project's github repository.", "TREC 2021 Non-RAG Helpful Harmful Rewriter Paraphraser Fact-Inversion FSAP-IntraQ FSAP-InterQ Liar Ground-Truth Alignment (%) TREC 2020 GPT-4.1 Consistent Query Neutral Query Inconsistent Query", "Fig. 1. Results of the single-document setup with GPT-4.1 on TREC2020 and TREC 2021 . The x-axis shows the different document types provided to the LLMs, as well as the Non-RAG baseline, and the y-axis reports ground-truth alignment rate (%). Error bars indicate 95% confidence intervals estimated via bootstrapping. Across both datasets, providing helpful documents leads to the highest alignment rate, while presenting Liar documents results in the lowest. In addition, the results reveal a consistent user query framing trend in which the alignment rate is highest with consistent queries, lower with neutral queries, and lowest with inconsistent queries.", "6 Limitations\n\nOur study advances the understanding of RAG robustness under adversarial evidence, but it has limitations. First, we rely on ground-truth alignment as the primary evaluation metric, which does not fully capture other important aspects of response quality, such as completeness, clarity, or potential user trust effects. Future work should employ multidimensional evaluations to provide a more comprehensive assessment of system performance in sensitive domains.", "Second, our design assumes that relevant documents are already retrieved. This lets us isolate how document content and query framing influence model behavior, but omits retrieval-stage factors such as ranking errors and bias. Future work should extend this analysis to full RAG pipelines to examine how adversarial documents interact with retrievers and rerankers under real-world conditions.", "Finally, our evaluation relies on automated stance classification with gemini-2.0-flash, validated using gpt-4o-mini. While prior work shows LLMs can align well with human judgments [16, 29], automated classifiers may miss subtle inaccuracies or nuanced risks. Future work should include human-in-the-loop or multi-annotator studies to strengthen validity in safety-critical domains.", "2 Approach\n\n2.2 SurveyGen : Dataset Construction\n\nSince title-based filtering may still include nonsurvey articles, we applied an additional filtering step using abstracts to further refine the candidate set. Specifically, inspired by previous work that LLMs are effective as NLI models for evaluating factual consistency (Gubelmann et al., 2023; Chiang et al., 2024), we prompted three LLMs to clas-\n\n2 Data collected from S2ORC up to December 2024.", "sify whether a candidate paper is a survey-type article based on its title and abstracts, following three criteria: (1) Explicit declaration of survey intent (e.g., 'conducts a survey' or 'provides a survey' ). (2) Focus on survey papers, rather than proposing novel methodologies or experimental results. (3) Discussion of field-specific trends, challenges, or future directions. Papers without abstracts were excluded at this step. Based on these criteria, 6,851 out of 8,676 papers were identified as survey articles by a majority vote of the LLMs.", "We then retrieved the full-text data of these surveys using their paper IDs from the S2ORC bulks 3 . Here, the full-text data includes the full body of the survey with section divisions , as well as the citation locations of each reference within the survey. This allows us to obtain the structural outline of each survey paper and map references to their corresponding sections, which serve as the key input for Task 3. At this point, we removed papers that had fewer than 30 references or fewer than three top-level sections, as they are too short to serve as meaningful surveys. Finally, we obtained 4,205 papers suitable for survey generation and constructed the SurveyGen dataset, which includes 115,376 sections, 242,143 references directly cited within the surveys, and 5,062,596 references cited by these cited papers.", "The data format is outlined in Appendix H. Table 1 compares SurveyGen with other datasets for scientific document summarization.", "2.3 Quality-Related Indicators Supplement\n\nTo facilitate citation-based evaluation, we first supplemented all survey papers and their directly cited references with basic metadata (e.g., abstract, DOI,\n\n3 https://api.semanticscholar.org/api-docs/", "Figure 2: Overview of the QUAL-SG framework, which comprises two main stages: paper retrieval and survey generation . The retrieval stage includes three steps: (1) retrieving topic-relevant papers, (2) expanding with frequently co-cited papers, and (3) enriching them with quality-related metadata. Based on the retrieved set, the generation stage first re-ranks the papers from three key aspects, then prompts LLMs to perform tasks under different input conditions. Finally, we evaluate the generated surveys against human-written ones across multiple dimensions.\n\nS2ORC Survey topic Co-citation papers \u2026 h-index, i10-index, 2-year citedness, core journal h-index, publication count, total citation count citation count, influential citation count Citation performance Venue reputation Author influence Related papers Step 1:Paper Retrieval Step 2:Survey Generation Relevance Quality Diversity (1) Paper re-ranking (2) Task design | Quality indicators (3) Evaluation\n\nCitation:\n\n\u2606\n\nContent:\n\nStructural:\n\nLLM Generate VS Human Written Task1\n\n:\n\nSurvey fully generated by LLMs Task2 Survey generated from retrieved references Task3 human-curated references LLMs Topic\n\npublication venue, date, and research fields) from S2ORC, linked via corpus IDs. However, S2ORC does not provide sufficient metadata to measure the impact of academic papers.", "To address this limitation, we used DOIs of the involved papers to retrieve their corresponding metadata from the OpenAlex 4 (Priem et al., 2022a) database and enriched them with additional quality-related signals. Specifically, we incorporated three well-known bibliometric indicators to measure the quality of scientific publications: (1) citation performance : citation count and influential citation count 5 ; (2) author influence : h-index, publication count, and total citation count; and (3) venue reputation : h-index, i10-index, and CORE status of the publication venue (journal or conference) (Hicks et al., 2015; Donthu et al., 2021). As a result, each survey is now paired with its full text, complete with section divisions, and linked to its directly cited references. These references are further enriched with comprehensive multi-level quality indicators retrieved from OpenAlex, providing a robust foundation for evaluating generated surveys across multiple dimensions, such as citation accuracy, content quality, and structural consistency.", "Second-Level References Supplement: In some cases, influential works relevant to a survey may not be semantically aligned with its primary topic. For example, when retrieving literature on 'Deep Learning' , seminal works such as the 'Backpropagation algorithm' (Rumelhart et al., 1986) may obtain low semantic similarity scores as their titles and abstracts do not explicitly mention the topic. However, such papers are frequently cited by other retrieved references and are widely rec-\n\n4 https://openalex.org/\n\n5 Citations identified by Semantic Scholar as impactful in context, rather than mere mentions in the bibliography.", "ognized as foundational to the field. To address this issue and support advanced citation network analysis, we further enriched the metadata for 5.06 million references cited by the papers referenced in all surveys. For each of these references, we extracted essential bibliographic details, including the title, abstract, DOI, and citation count.", "2.4 QUAL-SG: QUALity-aware Literature Retrieval for Survey Generation\n\nWe propose QUAL-SG, a quality-aware extension of the naive RAG framework designed to improve the quality of retrieved literature for survey generation (Task 2). The overall framework is illustrated in Figure 2. As described in Section 2.1, in the Naive-RAG framework, the survey topic is used as a query to retrieve relevant papers from external databases. Formally, let q denote the topic derived from the human-written survey. Each candidate paper d i in the external database is represented by its abstract embedding. We define the semantic similarity score as:\n\nwhere v q and v d i are the embedding vectors of the query and the abstract of document d i , respectively.", "2.4 QUAL-SG: QUALity-aware Literature Retrieval for Survey Generation\n\nThe topn with the highest embedding similarity scores are selected to form the initial candidate set D = { d 1 , d 2 , . . . , d n } , where n is set to exceed the number of references in the corresponding human-written survey to ensure sufficient candidate coverage.", "Ethics Statement\n\nData Collection, Ethics, and Licensing. Our SurveyGen dataset is constructed based on S2ORC (Lo et al., 2020), a large corpus of scientific papers released under the CC BY-NC 4.0 11 . The dataset includes metadata extracted from the papers, such as author names, venue names, citation counts, and h-index values. No sensitive personal data (e.g., contact details or affiliations) is included. All metadata was collected in compliance with the terms of their sources and is used strictly for noncommercial academic research. The dataset is not intended for ranking or evaluating individuals or venues. We are committed to handling the data responsibly and ethically and will release our dataset under the same non-commercial license to ensure transparency and responsible data usage.", "Caution about Use of LLMs. While our QUALSG framework leverages LLMs to generate scientific surveys and strives to maintain the factual accuracy of the literature, there remains a concern of factual inconsistencies during the generation process. We advise users to critically evaluate the generated content, especially when using it for subsequent scientific research or practical applications. The LLM-generated survey is for reference only and should not be regarded as a substitute for peerreviewed articles or expert judgment.", "d. Reranker Model Fine-tuning\n\nThe reranker is a crucial component in Retrieval-Augmented Generation (RAG) systems. It refines the initial set of retrieved candidates by reordering them based on their semantic relevance to the query. Unlike embedding models, which encode queries and documents independently into fixed-length", "vectors, the reranker jointly processes the query and document as input and directly predicts a relevance score. By modeling token-level interactions between the query and document, the reranker captures richer semantic relationships, resulting in improved ranking of the top candidates. Given a query-document pair, the model outputs a raw relevance score, which is typically passed through a sigmoid function to produce a normalized value in the range [ 0 , 1 ] .", "Model and Training methods. We adopt an industry-standard cross-encoding scheme for reranking and use the BGE-M3 model (Chen et al., 2024) as our base. BGE-M3 is known for its strong generalization capabilities and is supported by a robust open-source ecosystem, making it well-suited for real-time applications.", "During training, we feed both positive and negative samples corresponding to each query into the model simultaneously. To optimize ranking performance, we apply the Multiple Negative Ranking (MNR) loss, defined as:\n\nwhere \ud835\udc41 is the total number, \ud835\udc36 is the number of negative samples per query, \ud835\udc5d pos is the predicted relevance score for the positive document, and \ud835\udc5d neg \ud835\udc56 is the score for the \ud835\udc56 -th negative document. This loss encourages the model to rank the positive document higher than its negative counterparts, thereby improving retrieval precision in downstream tasks.", "Dataset and Quality Improvement. To maximize the reranker's performance in the geoscience domain, we curated a large training corpus composed of both open-source and domain-specific ranking pairs. The domain-specific data were synthesized using Qwen, conditioned on abstracts from open-access scholarly articles in geography sourced from arXiv.", "After assembling all positive query-document pairs, we applied SimANS (Zhou et al., 2022) to mine hard negative samples from the candidate corpus, thus forming a balanced and challenging training dataset. Since both synthetic and open-source data were generated in batches, individual data points may vary in quality. Even a small number of noisy or biased examples can significantly degrade reranking performance. To address this, we implemented a data quality filtering process to improve the reliability of training examples. For each positive pair, we used the LLaMA3 model to assess answer quality based on a set of predefined criteria. Each sample was assigned a quality score ranging from 0 to 3, following the standards detailed in Table 8. Only samples with sufficient quality were retained for training.Data with a quality label of 0 is excluded from training.", "| | quality label | quality standards |\n|---:|:----------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 0 | 3 2 1 0 | completely correct, detailed, and covers all key information with clear and easy-to-understand expression. mostly correct, covers most key information, and is clearly expressed but with minor shortcomings. generally correct but has some errors or omissions, with mostly clear expression. significant errors or omissions, lacks sufficient detail, and is not clearly expressed. |\n\nTable 8 | data quality standards\n\nWe ultimately utilized diverse types of paired data to train the model. Open-source data included:\n\nQA pairs: DuReader (He et al., 2017), NQ, LCQMC (Liu et al., 2018), HotpotQA, etc.", "Retrieval-oriented pairs: T2Ranking (Xie et al., 2023), mMARCO, MS MARCO, etc.\n\nSimilarity task pairs: NLI (Koreeda and Manning, 2021), NLI-zh, AFQMC (Lee et al., 2022), PAWSX (Yang et al., 2019), etc.\n\nMultilingual data: Mr. TyDi.", "In total, we incorporated over 360,000 open-source data pairs, along with 30,000 synthetically generated QA pairs specifically tailored for geological science applications.", "For training with contrastive objectives, negative sample quality plays a critical role in reranker performance. Whenever available, we retain the original negative samples provided within each dataset. However, many datasets either lack negatives entirely or contain an insufficient number of challenging examples. To ensure training robustness, we construct a set of at least 16 negative samples per query for all datasets.", "When additional negatives are required, we use a weak retriever to select hard negative candidates from a large-scale public Wiki corpus or domain-specific geoscience libraries curated from scholarly literature. We then apply the SimANS sampling strategy (Zhou et al., 2022) to prioritize harder examples from the candidate pool. Each negative sample \ud835\udc5b\ud835\udc52\ud835\udc54 \ud835\udc65 is assigned a sampling probability based on the squared difference between its retriever score and that of the corresponding positive sample:\n\nwhere \ud835\udc5b\ud835\udc52\ud835\udc54 score and \ud835\udc5d\ud835\udc5c\ud835\udc60 score are the scores returned by the weak retriever, and \ud835\udf0e is a task-specific hyperparameter that adjusts the sharpness of the sampling distribution. F Sample ( \ud835\udc5b\ud835\udc52\ud835\udc54 \ud835\udc65 ) represents the probability of selecting \ud835\udc5b\ud835\udc52\ud835\udc54 \ud835\udc65 as a training negative.", "Introdction\n\nInformation retrieval (IR) (Voorhees 1999) plays a fundamental role in many natural language processing (NLP) tasks (Hambarde and Proenc \u00b8a 2023; Wang et al. 2024), especially in retrieval-augmented generation (RAG) systems (Xia et al. 2025; Liu et al. 2025). These systems retrieve relevant passages to support the generation process, improving the factual accuracy and reducing hallucinations (Lewis et al. 2020). In typical RAG pipelines, a fast retriever is often used to select a candidate set of passages from a large corpus, which are then re-scored by a reranker to prioritize passages that are more contextually relevant to the input query.", "Traditional reranking strategies, whether they operate at the pointwise (Nogueira et al. 2020a; Liang et al. 2023), pairwise (Yu et al. 2018; Qin et al. 2024), or listwise (Thonet et al. 2022) level, typically follow a two-step routine: score\n\n* Corresponding author.", "Copyright \u00a9 2026, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\n\nFigure 1: Impact of Top-K passage retrieval on multi-hop QA performance. F1, EM, and Recall scores are plotted for HotpotQA and MuSiQue datasets.\n\nTop2 Top3 Top5 Top8 Top10 TopK 0.0 0.2 0.4 0.6 0.8 Score HotpotQA F1 EM Recall 0.1 0.3 MuSiQue", "each passage on its relevance to the query, and then pick the top K candidates as the input for the generator (Li et al. 2023; Meng et al. 2024). Although effective for singlehop retrieval, this approach overlooks the interdependence among passages-a crucial factor in multi-hop and complex reasoning scenarios, including QFS, long-context processing, and high-level query reasoning (Edge et al. 2024; Qian et al. 2024). In many cases, the information required to answer a question is distributed across multiple sources, some of which might not be highly ranked. Consequently, traditional reranking methods face an inherent trade-off: if K is too small, essential but lower-ranked evidence may be omitted; if K is too large, irrelevant or distracting content may overwhelm the generator, leading to decreased performance.", "To further illustrate this point, we conduct a preliminary analysis on two widely used multi-hop question answering benchmarks, HotpotQA (Yang et al. 2018) and MuSiQue (Trivedi et al. 2022). Our findings highlight the key limitation of traditional reranking approaches: A fixed Top-K retrieval strategy limits the ability of RAG systems to handle multi-hop and complex queries As illustrated in Figures 1, the optimal value of K is highly querydependent, K =5 for HotpotQA and K =3 for MuSiQue. Increasing K from 2 to 8 improves recall by up to 13 . 21% and 53 . 26% on HotpotQA and MuSiQue, respectively, demonstrating the benefit of incorporating more contextual information. However, the F1 score drops sharply beyond K = 5 on HotpotQA and K = 3 on MuSiQue, indicating diminishing or even negative returns as irrelevant content begins to dilute the useful evidence.", "To address this problem, we propose to reframe reranking as a passage selection learning problem, where large language models are fine-tuned through supervised learning (SFT) to identify the minimal set of passages that collectively support answering a query. Rather than assigning scalar relevance scores to each passage, the proposed framework, Dynamic Passage Selector (DPS) , learns to reason about the combinatorial relevance. This formulation allows the model to explicitly capture the inter-passage dependencies critical for multi-hop question answering, understanding when a single passage would suffice or when multiple passages must be selected together.", "As illustrated in Figure 2, the number of passages required to answer the query varies between query A and query B. Traditional reranking approaches retrieve a fixed Top-K set of passages, which introduces redundant information that may compromise answer quality or lead to incorrect responses. In contrast, DPS dynamically selects the minimal yet sufficient subset of passages based on queryspecific needs, thereby ensuring the quality of generated content. Additionally, recent methods attempt to align retrieval and ranking in RAG pipelines (Li et al. 2024; Zhang et al. 2025a; Zuccon, Zhuang, and Ma 2025), they often rely on costly multi-stage API calls or require large-scale finetuning. By contrast, the proposed DPS can be seen as a plugand-play component that integrates seamlessly into existing RAG workflows without fine-tuning process or pipeline modifications.", "We validate our approach on a range of question answering benchmarks with varying levels of difficulty, including three popular multi-hop question answer datasets HotpotQA (Yang et al. 2018), MuSiQue (Trivedi et al. 2022), 2WikiMQA (Ho et al. 2020), as well as two out-of-domain datasets Legal and CS (Qian et al. 2024). Experimental results show that our dynamic passage selector consistently outperforms traditional point-wise rerankers and state-ofthe-art LLM-based list-wise rerankers, especially on complex multi-hop queries. The results demonstrate that DPS can dynamically identify contextually relevant passages, surpassing the limitations of fixed top-K retrieval. By more effectively aligning retrieval with generation, DPS leads to more robust end-to-end performance. Our main contributions are as follows:\n\nTo the best of our knowledge, this is the first work that identifies and addresses a critical limitation in RAG systems: the fixed Top-K retrieval strategy severely impairs RAG system's performance on multi-hop and complex queries.", "We treat reranking as a passage selection problem, finetune LLMs to dynamically select a minimal set of sufficient passages by modeling their interdependencies, enabling accurate retrieval for multi-hop and complex query, while remaining fully plug-and-play with existing RAG systems.", "Through extensive evaluations across five diverse question answering benchmarks, we demonstrate that DPS achieves state-of-the-art performance on both challeng-\n\ning multi-hop and out-of-domain datasets (e.g., 30.06% F1 increase over Qwen3-reranker on MuSiQue), validating its capabilities and robustness for RAG systems."]}, {"type": "entry", "index": 7, "question": "How do retrieval-augmented summarizers handle conflicting evidence across papers?", "answer": "Retrieval-augmented summarizers face two main kinds of conflict: disagreements among retrieved papers, and disagreements between those papers and the model\u2019s own internal knowledge.\n\nOne strategy is to add a dedicated summarization or aggregation step that explicitly filters and reconciles evidence. In the multi-agent counterspeech framework, retrieved static (curated knowledge base) and dynamic (web) evidence are first combined, then passed to a Summarization Agent whose job is to \u201cfilter and distill the most relevant details,\u201d remove redundancy, and \u201cresolve conflicting information,\u201d so that only the \u201cmost reliable and pertinent content is retained\u201d before the final response is generated [1]. In practice, this means the summarizer is prompted to compare studies, downplay outliers, and produce a single, coherent summary rather than echoing all sides.\n\nAnother strategy is to model which context to trust. Conflict-Aware Retrieval-Augmented Generation (CARE) builds a \u201ccontext assessor\u201d that encodes external documents into special memory-like embeddings so the model can distinguish more reliable from less reliable context when it conflicts with what it already \u201cknows\u201d [2]. This improves accuracy on QA and fact-checking tasks, suggesting the summarizer can selectively weight or discount conflicting papers instead of blindly averaging them [2].\n\nFinally, dedicated benchmarks such as MAGIC, ECON, and WikiContradict are used to test whether models can even detect and locate conflicts between pieces of retrieved evidence [3]. Results show that current LLMs still struggle, especially with complex, multi-hop conflicts [3], which means that, in practice, retrieval-augmented summarizers may miss or mishandle subtle disagreements unless they use explicit conflict-detection and resolution mechanisms like the ones above.", "tags": ["summarization_conflict"], "full_faithfulness": 0.7142857143, "full_answer_relevancy": 0.9246572655, "seed_faithfulness": 0.619047619, "seed_answer_relevancy": 0.9244687016, "context_relevance": 0.845464467646735, "context_relevance_error": null, "context_relevance_reason": "The context passages collectively provide substantial information on how retrieval-augmented summarizers, particularly the CARE framework, handle conflicting evidence. They detail the mechanisms of evidence retrieval, summarization, and the evaluation of conflicting information, directly addressing the user's question about managing conflicting evidence across papers. However, while the passages are relevant, they could be more explicitly connected to the specific processes of summarization in the context of conflicting evidence.", "structure_citation": 0.8033955696342335, "structure_error": null, "structure_reason": "The Actual Output effectively discusses strategies for handling conflicting evidence in retrieval-augmented summarizers, referencing specific methods and frameworks. It includes citations [1], [2], and [3], which correspond to relevant snippets in the Context. However, while the claims are generally supported by the cited snippets, there is a slight lack of clarity in how some claims directly relate to the citations, which prevents a perfect score.", "hallucination_score": 0.8531209373373756, "hallucination_error": null, "hallucination_reason": "The response effectively addresses the input question by outlining how retrieval-augmented summarizers manage conflicting evidence through various strategies, such as dedicated summarization steps and context assessment. It aligns well with the context provided, particularly in discussing the Summarization Agent's role in filtering and reconciling evidence. However, while it covers the main strategies, it could have included more specific examples or results from the benchmarks mentioned to further strengthen its claims.", "contexts_used": ["3 Multi-Agent Retrieval-Augmented Framework\n\n3.1 Generation\n\nThe counterspeech generation process involves leveraging LLMs to create effective, informative, and polite responses that counter health misinformation. The primary objective is to generate responses that not only provide factual corrections but also address misinformation in a respectful and engaging manner. Our proposed multi-agent system uses specialized agents that work in sequence to retrieve, filter, and process evidence, thereby optimizing the RAG pipeline (Figure 1).", "Information Retrieval To incorporate static and dynamic evidence, two separate agents are employed to retrieve evidence: one is a Static Retrieval Agent and another one is a Dynamic Retrieval Agent . These Agents collect information based on the health misinformation posts. The Static Retrieval Agent conducts an inquiry within a structured local knowledge base by employing a dual approach that integrates vector-based semantic search with keywordbased exact matching (Yuan et al., 2024; Sawarkar et al., 2024), ensuring that only valid medical knowledge is included in this process. In parallel, the Dynamic Retrieval Agent performs real-time web searches using the DuckDuckGo API, introducing the latest health misinformation corrections from authoritative sources. The two sources of knowledge are combined to produce the final evidence.", "Evidence Summarization To prevent information overload from combined evidence, we introduce a Summarization Agent to filter and distill the most relevant details combined. This step is crucial for eliminating redundancy and resolving conflicting information, ensuring that only the most reliable and pertinent content is retained for counterspeech generation. Specifically, we prompt the Summarization Agent to condense evidence into a clear and refined summary.", "Counterspeech Generation We integrate distilled evidence from the Summarized Agent into generation by prompting Counterspeech Generation Agent to produce evidence-based counterspeech.", "Counterspeech Refinement To further enhance accessibility and clarity, we employ the Refinement Agent to reframe and polish counterspeech, ensuring that the generated counterspeech is not only well-supported by evidence but also contextually relevant and trustworthy.", "3.2 Evaluation\n\nWe employ a comprehensive evaluation framework based on four key criteria: politeness, relevance, informativeness, and factual accuracy. These criteria ensure that the responses are engaging, respectful, factually sound, and effective in addressing health misinformation.", "(a) Politeness: Politeness in responses is essential in communication, which helps to foster user engagement and build a supportive health community (Shan et al., 2022). A polite counterspeech helps avoid potential backlash and is more likely to be accepted by users (Perez et al., 2025), as it encourages a respectful tone (Song et al., 2025a; Yue et al., 2024; He et al., 2023). To evaluate politeness, we utilize the Multilingual Politeness Classification Model (Srinivasan & Choi, 2022) to assess whether a response adheres to polite discourse norms.", "(b) Relevance: Relevant responses ensure the response directly addresses the specific health misinformation claim (Yue et al., 2024; He et al., 2023). We assess relevance using the BM25 relevance scoring algorithm (Robertson et al., 2009), which measures how well the generated response aligns with the original health misinformation content.", "(c) Informativeness: Informativeness reflects the amount of useful, relevant, and evidencebased content shared in the response. A highly informative counterspeech response not only corrects health misinformation but also provides actionable insights and explanations that improve user understanding and support informed decision-making (Chung et al., 2024). To measure informativeness, we employ LLM as an evaluator (Li et al., 2024b) with detailed instructions, assessing if the response provides valuable information (Hong et al., 2025).", "(d) Factual Accuracy: Factual accuracy demonstrates the reliability and trustworthiness of the generated response by ensuring that the information is correct and supported by evidence (Zhou et al., 2024). In counterspeech generation, presenting scientifically accurate facts can effectively correct misinformation and maintain user trust (Yue et al., 2024). We employ LLM evaluation (Li et al., 2024b) to check how accurate the generated answers are by comparing them to the gathered evidence. The model rates how accurate a response is by checking if it correctly presents scientific facts and does not include any false information.", "This paper presents ConflictAware Retrieval-Augmented Generation (CARE), a novel approach that enhances large language models (LLMs) by addressing context-memory conflict\u2014issues arising when retrieved context contradicts the model's internal knowledge. CARE includes a context assessor that encodes external contexts into memory embeddings that help the model discern reliable from unreliable information. The framework improves the LLM's accuracy on question-answering and fact-checking tasks, achieving an average performance gain of 5.0% over existing methods. The study categorizes existing solutions into adaptive retrieval, decoding strategies, and robust training, highlighting how CARE addresses knowledge conflicts more effectively while preserving the general capabilities of LLMs.", "4 Experimental Setups\n\nWith the MAGIC benchmark as a foundation, we conduct experiments to examine how LLMs handle inter-context knowledge conflicts. We evaluate various open-source and proprietary LLMs without task-specific training, instead prompting them to identify potential contradictions. In the following, we outline the LLMs, datasets, prompting strategies, and metrics used in our experiments.", "LLMs We use 5 LLMs: Llama 3.1 70B Instruct (Dubey et al., 2024), o1 (OpenAI, 2024b), Mixtral8x7B Instruct (MistralAI, 2023), Claude 3.5 Haiku (Anthropic, 2024), GPT-4o-mini (OpenAI, 2024a).", "Datasets Alongside MAGIC, we employ two existing benchmarks to highlight its strengths.\n\nECON (Jiayang et al., 2024): A dataset created by introducing evidence conflicts through two methods-answer conflicts and factoid conflicts-highlighting contradictions in supporting evidence. It contains 168 data instances.", "WikiContradict (Hou et al., 2024): A humanannotated QA benchmark utilizing Wikipedia's contradiction tags to capture real-world knowledge conflicts. It categorizes contradictions into explicit and implicit types. After deduplication, it comprises 103 data samples.", "MAGIC : It is constructed atop KGs, where conflicts are systematically induced from the un-", "| | Models / Datasets | ECON | WikiContradict | MAGIC |\n|---:|:--------------------|-------:|-----------------:|--------:|\n| 0 | Mixtral 8x7B | 46.43 | 52.43 | 37.92 |\n| 1 | Llama 3.1 70B | 81.41 | 78.79 | 72.86 |\n| 2 | Claude 3.5 Haiku | 83.33 | 61.17 | 60.28 |\n| 3 | GPT-4o-mini | 88.1 | 82.52 | 83.61 |\n| 4 | o1 | 74.4 | 74.76 | 68.06 |\n| 5 | Average | 74.73 | 69.93 | 64.54 |\n\nTable 2: ID scores (%) on three KC detection datasets. Lower scores indicate higher task complexity. The lowest value in each row is shown in bold .", "derlying relational structure. It encompasses both single-hop and multi-hop contradictions, with the number of conflicts dynamically varying across context pairs. In total, MAGIC consists of 1,080 carefully curated examples, with comprehensive statistics presented in Table 1. Remarkably, the scale of MAGIC surpasses that of existing benchmarks by a significant margin, offering a richer and more challenging resource for evaluating inter-context conflict detection.", "Prompting strategy Prior work (Jiayang et al., 2024; Hou et al., 2024) typically frames the task as a binary classification problem, relying on minimal prompts (see Appendix B). In contrast, we introduce a stepwise prompting strategy (refer to Figure 17 for details) designed to more fully probe the capabilities of LLMs for inter-context conflict detection.", "Metrics To account for the stochasticity of LLMs, all models perform three independent inference runs. We use two metrics for fine-grained evaluation, with scores averaged across all instances in each dataset. These metrics are manually computed by participating researchers, as automatic methods-such as LLM-as-a-judge-remain insufficiently reliable for this task. 13\n\nIdentification ( ID ) score: If a model fails to detect a conflict in any of the three attempts, it receives a score of 0; otherwise, it receives 1.", "Localization ( LOC ) score: We further evaluate LLMs' performance on conflict localization. A full score (1) is awarded only if all conflicting locations are correctly identified; otherwise, the score is 0. Note that this fine-grained evaluation has not been considered in previous work.", "13 If they were, further investigation into knowledge conflict detection would be unnecessary. See Appendix H for details.", "|---:|:--------------------|-------:|-----------------:|--------:|\n| 0 | Mixtral 8x7B | 35.71 | 40.78 | 17.4 |\n| 1 | Llama 3.1 70B | 54.49 | 51.52 | 37.92 |\n| 2 | Claude 3.5 Haiku | 66.07 | 52.43 | 42.5 |\n| 3 | GPT-4o-mini | 63.69 | 68.93 | 55 |\n| 4 | o1 | 64.88 | 65.48 | 49.72 |\n| 5 | Average | 57.09 | 55.74 | 40.51 |\n\nTable 3: LOC scores (%) on three KC detection datasets. Lower scores indicate higher complexity. We observe that models struggle more with pinpointing the exact location of a conflict than with detecting its presence. The lowest value in each row is shown in bold .", "7 Conclusion\n\nWe propose a KG-based benchmark, MAGIC, for inter-context knowledge conflict detection with greater diversity and complexity. Experimental results reveal the strengths and limitations of LLMs in handling knowledge conflicts. Despite recent progress, LLMs continue to struggle with conflict detection in complex cases, e.g., those requiring multi-hop reasoning. As a future direction, we aim to develop an optimized method to help models overcome these limitations.", "Ethics Statement\n\nWe release MAGIC, a benchmark dataset for intercontext knowledge conflict detection. The dataset is constructed from Wikidata5M, a publicly available resource under permissive licenses, and contains no personally identifiable information. While we carefully filtered and validated the generated contexts, we cannot fully guarantee the absence of biased or potentially offensive content. MAGIC is released solely for scientific research, with the aim of fostering the development of systems that can reliably handle conflicting information and support safe deployment in real-world applications."]}]}, "cases_answer_relevancy": {"best": [{"type": "entry", "index": 3, "question": "When does knowledge-graph RAG outperform dense retrievers on long-tail entity queries?", "answer": "Knowledge-graph RAG tends to outperform pure dense retrievers when the query is about a specific but rarely-mentioned (long\u2011tail) entity and the answer depends on how that entity is related to others, rather than on surface\u2011level wording.\n\nIn dense retrieval, passages are ranked by overall semantic similarity to the query text, which can pull in many \u201clook\u2011alike\u201d sentences that mention similar words but refer to different people, companies, or events. For example, in a financial KG, a dense retriever might return both \u201cSteve Jobs quits Apple\u201d and \u201cDavid Peter leaves Starbucks\u201d for a query like \u201cwho quit Apple,\u201d because both talk about someone leaving a company [3]. A knowledge graph that explicitly encodes entities and relations can restrict the search to the subgraph around the specific company node (Apple) and its outgoing \u201cquit/left\u201d relations, so only passages actually about quitting Apple are retrieved [3]. This is exactly the kind of precision you need for long\u2011tail entities that don\u2019t appear in many contexts.\n\nIndustrial systems that build custom KGs for domains like LinkedIn customer QA report substantially better retrieval metrics (MRR, Recall, NDCG) and downstream answer quality (BLEU, METEOR, ROUGE) than a baseline semantic search system, indicating advantages on specialized, often long\u2011tail user questions where entity structure matters [3].\n\nGraph-based RAG frameworks like GraphRAG and related methods show that organizing content into entity communities and subgraphs makes it easier to pull the right subgraph for complex, relation\u2011heavy queries instead of scanning many loosely related text chunks [3][4]. This helps especially when the query is vague or underspecified in wording but well-defined in terms of underlying entities and relationships\u2014exactly the setting where dense retrievers struggle with noise and near\u2011matches [3].", "tags": ["hybrid_vs_kg", "long_tail_entities"], "full_faithfulness": 1.0, "full_answer_relevancy": 0.9611225119000001, "seed_faithfulness": 1.0, "seed_answer_relevancy": 0.9660236389, "context_relevance": 0.755545002991404, "context_relevance_error": null, "context_relevance_reason": "The context passages provide substantial information about knowledge-graph RAG methods and their advantages over dense retrievers, particularly in handling long-tail entity queries. The mention of specific frameworks like GraphRAG and their ability to structure knowledge effectively addresses the user's question. However, while the context is relevant, it lacks direct comparisons or specific instances where knowledge-graph RAG outperforms dense retrievers, which would strengthen the response further.", "structure_citation": 0.7724855666803808, "structure_error": null, "structure_reason": "The Actual Output effectively discusses the advantages of knowledge-graph RAG over dense retrievers for long-tail entity queries, providing specific examples and insights. It includes citations [3] and [4], which correspond to relevant snippets in the Context. However, while most claims are well-supported, there are instances where the connection to the cited snippets could be clearer, particularly regarding the empirical metrics mentioned. This slight lack of clarity in supporting all claims fully prevents a perfect score.", "hallucination_score": 0.8688877288547415, "hallucination_error": null, "hallucination_reason": "The response clearly articulates when knowledge-graph RAG outperforms dense retrievers, specifically highlighting its effectiveness with long-tail entity queries. It accurately describes the limitations of dense retrieval methods and provides relevant examples, such as the distinction between retrieving similar sentences versus specific entity relationships. The claims made are well-supported by the context, particularly regarding the advantages of knowledge graphs in enhancing retrieval precision. However, while the response is strong, it could benefit from a more explicit connection to the specific metrics mentioned in the context, such as MRR and Recall, to fully align with the evaluation steps.", "contexts_used": ["Related Work\n\nRAG has emerged as a critical framework for enhancing LLMs by integrating external knowledge. Early approaches such as REALM (Guu et al. 2020) and DPR (Karpukhin et al. 2020) focus on encoding large text corpora into dense embeddings. In recent years, GraphRAG has become a new direction because it can structure fragmented knowledge. RAPTOR (Sarthi et al. 2024) and Microsoft's GraphRAG (Edge et al. 2025) both use hierarchical clustering: RAPTOR constructs recursive trees with multi-level summarization, and GraphRAG applies community detection with LLM-generated synopses, to support coarse-tofine retrieval and high-coverage responses. DALK (Li et al. 2024) and KGP (Wang et al. 2024) introduce dynamic KG construction and traversal agents, using LLMs to build domain-specific graphs and self-aware retrieval policies, to inject structural context while reducing noise. GFMRAG (Luo et al. 2025), G-Retriever (He et al. 2024), and LightRAG (Guo et al. 2025) combine graph neural encoders with specialized retrieval objectives: a querydependent GNN trained in two stages for multi-hop generalizability, a Prize Collecting Steiner Tree formulation to reduce hallucination and improve scalability, and a dual-level graph-augmented index for efficient, incrementally updatable lookup, respectively enabling accurate, scalable reasoning over graphs. HippoRAG (Guti\u00b4 errez et al. 2024), inspired by hippocampal memory processes, leverages Personalized PageRank for single-step multi-hop retrieval, delivering state-of-the-art efficiency and performance on both path following and path finding QA tasks. HippoRAG2 (Guti\u00b4 errez et al. 2025) further optimizes knowledge graph refinement and deeper passage integration. More details regarding related work are provided in the Appendix A.", "Retriever Module (Dense Passage Retrieval).\n\nThe retriever's job is to efficiently identify which pieces of text in a large corpus are relevant to the input query x . Modern RAG implementations typically use dense retrievers like Dense Passage Retrieval (DPR) [45] in lieu of traditional keyword search. In DPR, a bi-encoder architecture is employed: a question encoder E q ( x ) maps the query x to a d -dimensional vector, and a passage encoder E p ( d ) maps each candidate document (or passage) d in the corpus to a d -dimensional vector in the same space. Relevance is assessed via a similarity score, usually the dot product of these vectors:\n\nAt query time, the retriever computes v q = E q ( x ) and then finds the topK documents whose vector E p ( d ) has highest inner product with v q . This search for maximum inner product can be implemented efficiently using Approximate Nearest Neighbor techniques (e.g., FAISS) to handle millions of documents in sub-linear time. The output of the retriever is the set Z = { z 1 , . . . , z K } of top-ranked documents and potentially their similarity scores. One can view the retriever as defining a probability distribution P ret ( z | x ) over documents z in the corpus, such that:\n\nwith the normalization \u2211 d \u2208C exp( E q ( x ) \u22a4 E p ( d )) (in practice approximated by summing over retrieved candidates rather than all of C ). In other words, the retriever assigns higher probability (or score)", "to documents whose embedding is most similar to the query's embedding. The DPR retriever is usually first trained on pairs of questions and relevant passages to ensure E q and E p produce representations that maximize dot-products for true Q-A pairs and minimize them for irrelevant pairs. The training objective is often a contrastive loss : for a given question q with a gold relevant passage p + and a set of negative passages { p -1 , . . . , p -N } , the encoder is trained to maximize sim ( q, p + ) while minimizing sim ( q, p -) for negatives. This can be formulated as a cross-entropy loss treating the positive passage as the correct class among one positive and N negatives:\n\nwhich encourages E q and E p to embed true pairs closer together than any negative pair [45]. After training, the retriever can generalize to new queries: it embeds the query and efficiently finds the nearest neighbor passages in the index. This retrieval step is crucial because it narrows down the evidence from potentially billions of tokens to a manageable subset that the generator will actually consider.", "Generator Module (Conditional Seq2Seq Model). The generator in a RAG pipeline is typically a sequence-to-sequence language model that produces the final answer or output text, given the input query and retrieved documents. Formally, the generator defines a conditional distribution P gen ( y | x, Z ) over output sequences, where Z = { z 1 , . . . , z K } are the retrieved passages. The generator is often initialized from a pre-trained transformer-based seq2seq model (such as BART or T5) to leverage rich language generation capabilities [52]. During generation, the model is provided with the question (or prompt) as well as the content of the retrieved documents. There are multiple ways to feed the retrieved context to the generator:\n\nIn an early fusion approach, one can concatenate the query x with the text of all retrieved passages into a single extended input sequence (perhaps with special separators) and have the seq2seq model attend to all of it at once. This is straightforward: the generator effectively treats the combined text as context and learns to pick out the relevant bits when producing the answer. However, this can be difficult if K is large, as the input may become very long.", "In the late fusion approach adopted by RAG [52], the generator considers one retrieved document at a time as a context and then marginalizes over the document choices (as in Eq. 1). Specifically, the RAG-Sequence variant fixes a single document z i as context for generating the entire output and computes P ( y | x ) by summing the probabilities P ( y | x, z i ) weighted by the retriever's confidence P ( z i | x ) . Another variant, RAG-Token, allows the generator to switch between documents at the token level, effectively marginalizing over the document choice for each generated token [52]. In both cases, the generator P gen ( y | x, z ) itself works like a standard seq2seq model: it factorizes over the output tokens y 1 , . . . , y T as \u220f T t =1 P \u03b8 ( y t | x, z, y <t ) , i.e. it generates one token at a time, attending to the input query x and the content of z (or multiple z 's if early fusion). The generator's architecture typically uses an encoder-decoder Transformer: the encoder encodes the combination of x and z , and the decoder produces y autoregressively.", "Because the generator is conditioning on retrieved evidence, it tends to produce outputs that are supported by that evidence. For example, if the query asks for a specific factual answer, the generator can copy or rephrase the needed information from one of the retrieved passages. This is in contrast to a vanilla language model which would attempt to rely on parametric knowledge (which might be outdated or incomplete). In sum, the generator module in RAG is responsible for fluent and coherent text generation, but crucially it is grounded by the retrieval context, which guides it toward accurate content.", "2 Related Works\n\nSpare and Dense Retriever RAG methods: Hybrid approaches, such as the \"Blended RAG\" framework proposed by Sawarkar et al. [12], demonstrate that combining multiple retrievers can improve the retrieval of relevant passages for clearly defined queries. This framework integrates six distinct retrievers-including traditional sparse methods (e.g., BM25) and dense semantic search techniques-selected based on their performance across benchmark datasets like Natural Questions (NQ) [13], TREC-COVID [14], SqUAD [15], and HotPotQA [16].", "A key strength of this hybrid search technique is its ability to return relevant passages, aligning well with Research Goal 3 for well-specified queries. However, these methods do not fully address Research Goal 3 when handling vaguer questions. In such cases, the hybrid approach can retrieve extraneous or unnecessary information, thus failing to eliminate the \"needle in a haystack\" problem. This shortcoming is evident in evaluations on the CoQA dataset [17], where retrieval accuracies ranged only from 45% to 49%. The reliance on document metadata during retrieval-a common feature in these systems-further complicates matters, as such metadata is often minimal in real-world scenarios (e.g., limited to document names and creation dates), thereby increasing both the creation time of the information store (contradicting Research Research Goal 1) and the risk of returning irrelevant passages (contradicting Research Research Goal 3) leading to longer contexts (contradicting Research Goal 2).", "Figure 3: Results of Priyanka Mandikal et al. on 2 example queries [18].\n\nQuery: Do fibroblasts from CF patients grow at a\n\nQuery: What is the prognosis for patients who have\n\nnormal rate?\n\nhad an episode of respiratory failure?\n\nMethod P@1 P@3 P@5 P@10 VSR 33.33 SPECTERZ Hybrid 66.67 Sample Retrieval This paper analyzes the cell cycle of cystic\n\nThe diagnosis, management and prognosis of 17 infants\n\nfibrosis-derived fibroblasts and statistically\n\nwith cystic fibrosis (CF) and severe respiratory disease\n\ndemonstrates their failure support normal\n\nwere reviewed for the\n\nperiod 1968 to 1972.\n\nThe clinical\n\npopulation doubling time in vitro and indicates that this\n\ncourse these infants was characterized by is due largely\n\nto a failure of the\n\ncystic fibrosis-derived\n\nfailure to thrive and\n\ncells to enter the cell cycle in normal numbers.\n\nWe have malnutrition Despite vigorous therapy including no explanation for significant difference antibiotics there was\n\n60% mortality:\n\nA delay in the\n\nbehavior of these cells from patients with cystic fibrosis\n\ndiagnosis of CF from the onset of respiratory symptoms\n\nvis-a-vis either disease control fibroblasts, with a\n\nmean of six weeks was considered an important\n\nsave to conclude that proceeds from an intrinsic\n\nfactor affecting survival. This data supports the need for\n\nproperty of the cells themselves.", "developing a reliable screen test for CF at birth.", "In contrast, recent work by Mandikal et al. [18] has advanced a hybrid retrieval model that integrates a sparse TF-IDF retriever with a dense retriever based on fine-tuned SPECTER2 embeddings [19]. Their approach employs a weighted scoring mechanism, controlled by a hyperparameter \u03bb , to balance the contributions of the two retrieval methods. Empirical evaluations on a medical database concerning Cystic Fibrosis indicate that while dense retrieval alone underperforms, incorporating approximately 20% of the sparse score (i.e., setting \u03bb = 0 . 2 for the sparse component) significantly improves document retrieval accuracies across different retrieval ranks [Figure 3]. Crucially, this method indexes documents solely based on their content, thereby avoiding the pitfalls of metadata dependency. This contributes to reducing information store creation times (Research Goal 1) and partially mitigates the retrieval of unnecessary information (Research Goal 3), although it does not entirely resolve the challenge of handling vaguer queries.", "In summary, while hybrid retrieval approaches represent a significant advancement by returning more relevant passages for well-defined queries complying with Research Goal 3. However they remain imperfect in filtering out unnecessary information for vaguer questions which can lead to longer contexts. This limitation underscores the need for further refinement to minimize the \"needle in a haystack\" problem in large-scale, real-world applications and make a pipeline which is compliant with Research Goal 2 as well.", "Reranker RAG methods: To address the limitation which Hybrid-Search methods have in adhering to Research Goal 2 for vaguer questions, Reranker methods have been developed to reduce the size and improve the quality of the final context.", "Figure 4: A high level view of what a RAG pipeline with a reranker looks like.\n\nIst Stage 2nd Stage Knowledge Embedding-based LLM-based Docs Corpus retrieval reranking\n\nUsually this is a process called 2-step retrieval where the first step involves retrieving documents such as through sparse or dense retriever techniques and then a second stage where an LLM based reranker comes into play to decide which of the documents retrieved are most relevant to the query and sorts them in order [Figure 4] before passing it to an LLM for generation.", "Prior to the formal conceptualization of Retrieval-Augmented Generation (RAG), information retrieval research explored the use of document rerankers. For example, Noguiera et al.'s 2020 study, Document Ranking with a Pretrained Sequence-to-Sequence Model [20], proposed employing a sequence-to-sequence model as a document reranker. This work evaluated multiple fine-tuned T5 models of varying sizes, as well as a fine-tuned BERT model, on documents initially retrieved using the sparse BM25 keyword method. The incorporation of these rerankers led to improvements in retrieval performance across the Robust04, Core17, and Core18 datasets. However, a critical limitation of this framework was its reliance on large models, leaving unresolved whether the observed performance gains were due to the reranking strategy or simply the use of more powerful models.", "Subsequent advancements in RAG have integrated BERT-based rerankers into new pipelines. Michael Glass et al.'s Re 2 G framework [21] embeds a BERT-based reranker-similar to that of Noguiera et al.-within a RAG system. This integration has shown notable improvements in metrics such as Recall (for 5 documents), overall recall, precision, and accuracy on several datasets, as indicated by performance on the KILT leaderboard. Nevertheless, a significant drawback of this approach is the necessity to host an additional model, which directly conflicts with Goal 4: alleviating the monetary and computational costs associated with deploying full-scale LLMs.", "Further refinement in reranking approaches is demonstrated by Xuegang Ma et al., who fine-tuned a LLaMA-2-7B model for reranking in their RankLLaMA system [22]. Although RankLLaMA achieves superior reranking performance, it again relies on a large model comparable to LLaMA-2-7B. Such reliance on resource-intensive models undermines the overarching objective of Goal 4, as the additional computational and financial burden associated with hosting these rerankers contradicts the need for efficiency and cost reduction.", "In summary, while reranker models can mitigate the 'needle in a haystack' problem (Goal 2) by filtering out least relevant information, they are not a sustainable solution. Their requirement for additional large-scale models significantly increases computational and monetary costs, thereby violating Goal 4. Consequently, alternative strategies that achieve comparable retrieval performance without incurring such overhead are essential for the advancement of practical and efficient RAG systems.", "Knowledge Graph RAG methods: Knowledge graph-based solutions have been proposed as a promising alternative for enhancing the RAG process. The core idea behind employing knowledge graphs is to address semantic search challenges by organizing data according to intrinsic relationships. For example, although not originally applied within a RAG context, Dawei Cheng et al.'s 2020 work on constructing knowledge graphs for a financial price prediction tool [23] illustrates how grouping semantically related data into distinct sub-networks enables a query-such as 'who quit Apple'-to retrieve only the pertinent subset of information, rather than returning all semantically similar sentences (e.g., both 'Steve Jobs quits Apple' and 'David Peter leaves Starbucks'). This targeted retrieval helps mitigate the \"needle in a haystack\" problem (Goal 2) and increases the rate of retrieving relevant passages (Goal 3).", "Building on these insights, recent research has directly integrated knowledge graphs within RAG frameworks. For instance, Zhentao Xu et al. [24] developed a system for LinkedIn's customer question answering service by first", "creating a custom knowledge graph and then retrieving related nodes. The retrieved information is subsequently refined using GPT-4 to produce coherent responses. When compared with a baseline semantic search system, this pipeline significantly improved retrieval metrics-including Mean Reciprocal Rank, Recall, and Normalized Discounted Cumulative Gain-as well as evaluation measures such as BLEU, METEOR, and ROUGE, thereby supporting Goals 2 and 3.", "However, despite these promising advances, knowledge graph-based approaches fall short of meeting Goal 1. The process of constructing and training knowledge graphs is computationally intensive and time-consuming, which directly conflicts with the objective of reducing information store creation times.", "| | Model | EM | Fl Score | Accuracy | Hallucination |\n|---:|:--------------|-----:|:-----------|:-----------|:----------------|\n| 0 | Human | 63 | | | |\n| 1 | MHQA-GRN | 33.2 | | | |\n| 2 | Embedding-RAG | 28 | 37 | 46 | 30 |\n| 3 | KG-RAG | 19 | | 32 | 15 |\n\nFigure 5: Results comparing knowledge graph framework with semantic search methods [25].", "For example, Diego Sanmart\u00edn's 2024 work [25] demonstrated a Knowledge Graph RAG system that leverages LLMs to extract entity-relationship triples from unstructured text and employs a 'Chain of Explorations' retrieval strategy. While this approach shows potential for handling vague queries, the high training times required to build and maintain the knowledge graph significantly increase the overall overhead. Moreover, the compressed storage format-using entity-relationship triples-can lead to a loss of detailed contextual information, thereby limiting the system's ability to fully expand upon relevant topics during retrieval hence resulting in even lower EM, F1, Accuracy scores with only an improvement in Hallucination compared to a standard Semantic Search method [Figure 5].", "In summary, while knowledge graph-based solutions offer clear benefits in terms of improving semantic search and enhancing the retrieval of relevant passages (addressing Goals 2 and 3), they do not currently satisfy Goal 1 due to their high training times and associated computational costs. Addressing these inefficiencies is essential for realizing the full potential of knowledge graphs within efficient and practical RAG frameworks.", "Combining Knowledge Graphs and Vector based Search methods: The limitations inherent in a purely knowledge graph-based RAG framework suggest that combining the strengths of knowledge graphs with traditional semantic search methods may yield superior results. Bhaskarjit Sarmah et al. [26] propose such an integrated approach through their HybridRAG framework. This framework concatenates outputs from a knowledge graph query-implemented using Microsoft's GraphRAG [27] with content indexed by extracting entity relationships in chunk sizes of 1024-with results from a semantic search, which similarly employs chunk sizes of 1024 and the text-embedding-ada-002 model. The aggregated context is then provided to GPT-3.5 to generate the final answer. The dataset for this study was constructed by web scraping earnings call documents of companies listed on the NIFTY 50, and evaluation was performed using the RAGAS evaluation platform [28].", "The HybridRAG framework demonstrated improvements in metrics such as Faithfulness, Answer Relevancy Score, and Context Recall. These enhancements support Goal 2 by reducing the \"needle in a haystack\" problem and Goal 3 by increasing the rate of retrieving relevant passages for answering questions. However, the framework underperformed in Context Precision when compared to standalone implementations of both knowledge graph-based and vector database-based RAG systems. This shortcoming indicates that the current integration does not fully leverage the advantages of traditional vector search techniques, particularly hybrid search techniques, which is critical for fully meeting Goal 3.", "6 Conclusion\n\nThis paper introduces the K 2 RAG framework, which addresses key limitations of traditional RAG methods. By utilizing a summarized corpus, we achieved an average reduction of 93% in training times. When combined with quantized models and an advanced retrieval process, K 2 RAG outperformed all tested naive RAG methods in terms of answer accuracy, resource efficiency, and execution speed, even with more complex steps. As a result, K 2 RAG offers a more accurate and lightweight RAG pipeline. While the framework demonstrated a slight improvement in mean similarity and a higher Q 3 value, it primarily excelled in answering certain question types. For future work, we recommend testing K 2 RAG on additional datasets to assess its performance on a wider range of topics, and enhancing knowledge graph retrieval techniques to further improve answer accuracy across all question types by identifying a broader set of relevant topics.", "Knowledge Graph Based Retrieval-Augmented Generation\n\nTo better capture the relational nature of information, KGbased RAG has emerged as a prominent research direction. By representing knowledge as a graph of entities and relations, these methods aim to provide a more structured and semantically rich context for the LLM (Peng et al. 2024). Early approaches in this domain focused on leveraging graph structures for improved retrieval. For instance, GraphRAG (Edge et al. 2024) organizes documents into community-based KGs to preserve local context, while other methods like FastGraphRAG utilize graph-centrality metrics such as PageRank (Page et al. 1999) to prioritize more important nodes during retrieval. This subgraph retrieval approach has also proven effective in industrial applications like customer service, where KGs are constructed from historical support tickets to provide structured context (Xu et al. 2024). These methods marked a significant step forward by imposing a macro-structure onto the knowledge base, moving beyond disconnected text chunks.", "Recognizing the need for more fine-grained control and abstraction, subsequent works have explored more sophisticated hierarchical structures. HiRAG, the current stateof-the-art, clusters entities to form multi-level summaries (Huang et al. 2025a), while LightRAG (Guo et al. 2024) proposes a dual-level framework to balance global and local information retrieval. While these hierarchical methods have progressively improved retrieval quality, a critical gap persists in how the constructed graph structures are leveraged at query time. The retrieval process is often decoupled from the indexing structure; for instance, an initial search may be performed over a 'flattened' list of all nodes, rather than being directly guided by the indexed community or hierarchical relations. This decoupling means the rich structural information is primarily used for post-retrieval context expansion, rather than for guiding the initial, crucial step of identifying relevant information. This can limit performance on complex queries where the relations between entities are paramount, highlighting the need for a new paradigm where the retrieval process is natively co-designed with the knowledge structure."]}, {"type": "entry", "index": 0, "question": "What are pros and cons of hybrid vs knowledge-graph RAG for vague queries?", "answer": "For vague queries, hybrid (sparse+dense) RAG and knowledge\u2011graph RAG behave quite differently.\n\nPros of hybrid RAG (sparse + dense search):\n- Works very well for clearly defined, specific questions, returning highly relevant passages [2].\n- Indexing directly on document content avoids heavy metadata engineering and keeps store creation time relatively low [2].\n\nCons of hybrid RAG for vague queries:\n- When the question is vague, hybrid search tends to pull many loosely related passages, adding \u201cextraneous or unnecessary information\u201d and worsening the \u201cneedle in a haystack\u201d problem [2].\n- This leads to long, cluttered contexts and only modest retrieval accuracy on conversational datasets (\u224845\u201349% on CoQA) [2].\n\nPros of knowledge\u2011graph RAG for vague queries:\n- Structures data as entities and relations, so a query like \u201cwho quit Apple\u201d can target only the small, relevant sub\u2011network instead of every vaguely similar sentence, directly reducing the needle\u2011in\u2011a\u2011haystack issue [2].\n- Graph\u2011based QA (especially GraphRAG) excels on complex, thematic, and multi\u2011hop questions, giving richer, more coherent, and more explainable answers [1][2].\n- Tends to lower hallucination compared with pure semantic search [2] and supports provenance and reasoning\u2011style explanations [1].\n\nCons of knowledge\u2011graph RAG:\n- Building and updating the graph is computationally intensive and slow, so overall information\u2011store creation time is high [2].\n- Compressing text into triples can lose nuance, which can hurt exact\u2011match and F1 style metrics even when hallucinations drop [2].\n- GraphRAG brings added system complexity and higher runtime demands compared with lightweight retrievers [1].\n\nHybrid of KG + vector (HybridRAG) partly combines strengths\u2014better faithfulness, answer relevance, and recall\u2014but still struggles with context precision, meaning some unnecessary material remains in the final context [2].", "tags": ["hybrid_vs_kg"], "full_faithfulness": 1.0, "full_answer_relevancy": 0.9639235528000001, "seed_faithfulness": 0.46428571430000004, "seed_answer_relevancy": 0.9773926354, "context_relevance": 0.7954237852781298, "context_relevance_error": null, "context_relevance_reason": "The context passages provide substantial information relevant to the user's question about the pros and cons of hybrid vs knowledge-graph RAG for vague queries. They discuss the strengths and weaknesses of both approaches, particularly highlighting how knowledge graphs can enhance contextual understanding and reasoning capabilities, while also addressing the limitations of traditional RAG methods. However, some passages are more focused on specific methodologies rather than directly comparing the two approaches, which slightly detracts from the overall relevance.", "structure_citation": 0.715921354975597, "structure_error": null, "structure_reason": "The Actual Output effectively discusses the pros and cons of hybrid and knowledge-graph RAG for vague queries, with multiple claims supported by citations. However, while most citations correspond to relevant snippets in the Context, there are instances where the claims about retrieval accuracy and the 'needle in a haystack' problem could be better substantiated with specific references from the Context. Overall, the response is well-structured and informative, but slight discrepancies in citation support prevent a perfect score.", "hallucination_score": 0.8319241688561643, "hallucination_error": null, "hallucination_reason": "The response effectively outlines the pros and cons of both hybrid and knowledge-graph RAG approaches for vague queries, aligning well with the input's request. It provides specific strengths, such as hybrid RAG's efficiency for clear questions and knowledge-graph RAG's ability to target relevant sub-networks, while also addressing their shortcomings. However, while the output is detailed and informative, it could benefit from a more explicit connection to the context provided, particularly regarding the challenges of traditional RAG methods, which were not fully integrated into the analysis.", "contexts_used": ["1. Introduction\n\nRetrieval-Augmented Generation (RAG) has emerged as a powerful approach in natural language processing, particularly in question-answering systems. By combining the retrieval of relevant documents with the generative capabilities of large language models (LLMs), RAG models are able to generate contextually accurate and informative responses. Traditional RAG approaches typically rely on retrieving passages from vast corpora of unstructured text, which are then fed into a generative model to formulate an answer. While effective, this approach often faces challenges such as irrelevant retrieval, redundancy in answers, and limited capacity to understand structured relationships between concepts.", "Knowledge graph based question-answering (KG-QA) systems have gained significant attention in addressing these challenges. Unlike traditional RAG, which depends heavily on unstructured text retrieval, KG-QA leverages structured knowledge in the form of graph triplets, representing relationships between entities. The system can provide more precise, explainable, and contextually relevant answers by querying these triplets. Knowledge graphs offer a way to formalize the relationships between entities, making tracing and verifying the rationale behind a generated answer easier. When combined with LLMs, KG-QA systems can take advantage of both the structured reasoning capabilities of knowledge graphs and the natural language generation strengths of the LLMs.", "Creating high-quality knowledge graph triplets is crucial for the success of such systems. This process can be achieved through various methods, each with different strengths. Rule-based systems like spaCy leverage pre-defined linguistic rules and patterns to offer precision in extracting triplets from domain-specific texts. These systems could perform well and be finely tuned to some specific domain, such as the medical or legal domain, where the structure and vocabulary could be well defined. However, the actual system performance would depend on how well the rules are defined and how much knowledge the models can capture and embed in itself. At the same time, open-domain extractors like Stanford CoreNLP's OpenIE are more flexible, capturing a wider range of relations. They utilize language semantics and structure to push for identifying a subject, object, and relation in any sentence using limited or no training data. More recently, GraphRAG by Microsoft has emerged as a technique that leverages the power of LLMs for extracting precise structural information hierarchically from unstructured data by utilizing LLM inference at various stages of knowledge graph creation. It can create a more holistic view of the text and answer questions at different levels of abstraction, which can prove quite helpful more often than not.", "A key motivation for this research is to overcome the inherent limitations of traditional RAG approaches, which are adept at extracting localized, fact-based information yet often fall short when tasked with understanding the broader thematic context or handling complex, large-scale texts. By integrating knowledge graphs-capable of capturing structured relationships-into QA systems, we aim to enhance LLMs' reasoning capabilities, resulting in accurate, contextually richer responses, and more explainable. Furthermore, the comparative evaluation of diverse extraction methods such as spaCy, CoreNLP OpenIE, and GraphRAG is crucial to identifying the most effective techniques for constructing high-quality knowledge graphs. Leveraging open-source technologies for this study ensures replicability and practical applicability, ultimately guiding researchers and practitioners toward improved KG-QA systems better equipped to address real-world challenges. Therefore, this study not only fills a current research gap but lays the groundwork for future knowledge graph-based question-answering innovations.", "In this paper, therefore, we present a comparative study of three popular methods for generating graph triplets and integrating them with LLMs for question answering: (1) spaCy, (2) CoreNLP OpenIE, and (3) GraphRAG. Each method represents a unique approach to creating knowledge graphs and using them in QA systems. By evaluating them across broad parameters-such as performance, ease of use, accuracy, customizability, etc.-we aim to provide insights into their strengths and weaknesses and how they contribute to the effectiveness of KG-QA systems. This study aims to guide researchers and practitioners in selecting the most appropriate tools for their knowledge graph-based questionanswering tasks.\n\nThe key points of contribution of our work are as follows:\n\nWe present a fully reproducible framework that extracts relational triplets using spaCy, Stanford CoreNLP-OpenIE, and GraphRAG, constructs knowledge graphs, and integrates them into LLM prompts for enhanced question answering.", "Under identical conditions, we systematically evaluate each method on a whole range of parameters, filling a critical gap in the literature. Our evaluations are conducted on two distinct", "data sources-Shakespeare's play 'All's Well That Ends Well' and the RepliQA dataset-to ensure robustness and generalizability of our findings.", "By embedding structured relations into QA, we demonstrate that GraphRAG performs the best in complex, thematic queries, CoreNLP OpenIE offers the broadest factual coverage, and spaCy provides a lightweight, high-precision baseline.", "In addition to GPT-4 automated scoring, we incorporate domain expert judgments to assess answer coherence, reasoning ability, and contextual richness, yielding a more nuanced understanding of KG-augmented QA performance.", "Based on our dual-dataset empirical analysis, we recommend optimal extraction techniquesbalancing factual coverage, reasoning depth, and computational efficiency-along with best practices for graph-based question answering.", "We identify promising directions for advancing KG-based QA, like dynamic graph updates, hybrid extraction pipelines, multi-modal knowledge integration, and domain-specific adaptations.", "GraphRAG\n\nGraphRAG, a Microsoft Research innovation, represents a significant advancement in retrievalaugmented generation (RAG) by integrating structured knowledge graphs to enhance large language models' (LLMs) contextual understanding of private datasets. Unlike traditional RAG systems that rely on vector similarity searches, GraphRAG constructs an LLM-generated entity-relationship graph from source documents, enabling holistic analysis and semantic connections across disparate data points. GraphRAG addresses critical limitations in baseline RAG systems by transforming unstructured text into structured knowledge representations, particularly in synthesizing crossdocument insights and supporting verifiable assertions through source-linked evidence. It extracts relevant data from the knowledge graph, identifies the appropriate community related to the question, and utilizes the generated graph, entities, relations, claims, summaries, etc., to generate accurate and concise responses.", "4. Results and Discussion\n\n4.2.1.2. 'RepliQA'\n\nGraphRAG demonstrated consistently high performance with and without rubric, reflecting its strength in integrating structured knowledge and contextual cues. The small yet positive gain from rubric inclusion (4.61 to 4.66) further supports its capacity to incorporate guidance effectively. Overall, these results not only underscore GraphRAG's superiority but also suggest that rubric augmentation can enhance model outputs, particularly for models that can flexibly integrate external guidance.", "Then, all of the responses for a particular question, along with the question and the context were given to the GPT-4 model, and asked to relatively rank the responses from best to worst. The following table shows how many times the respective responses were ranked first or second in the rankings given by GPT-4.", "| | Method | Rank 1 Count | Rank 2 Count |\n|---:|---------:|---------------:|:--------------------------|\n| 0 | 9 | 6 | Dataset Answer |\n| 1 | 15 | 3 | SpaCy (without rubric) |\n| 2 | 9 | 12 | SpaCy (with rubric) |\n| 3 | 0 | 4 | CoreNLP (without rubric) |\n| 4 | 2 | 9 | CoreNLP (with rubric) |\n| 5 | 4 | 8 | GraphRAG (without rubric) |\n| 6 | 11 | 8 | GraphRAG (with rubric) |", "The rank-based comparison highlights an interesting distinction between spaCy and GraphRAG. SpaCy (without rubric) secures the highest number of rank-1 placements (15), suggesting that it occasionally produces responses that are judged as the best in isolated comparisons. However, when evaluated in terms of overall reliability, GraphRAG demonstrates greater consistency. GraphRAG with rubric, for example, secures 11 rank-1s and 8 rank-2s (19 in total), reflecting a balance of strong answers without significant drops in quality. When considered alongside the average score results, it becomes clear that while spaCy excels in producing standout answers on occasion, GraphRAG achieves a more stable and dependable performance across the dataset.", "The effect of rubric use is also evident. For both spaCy and CoreNLP, introducing rubric guidance shifts performance: spaCy's rank-1 count drops from 15 to 9, but its rank-2 placements rise significantly from 3 to 12, indicating that rubric enforcement pushes its answers toward consistently 'good' quality, even if fewer are rated as the absolute best. CoreNLP shows a similar trend, with rubric use improving its top-2 presence (from 4 to 11), though it still lags behind the other methods. GraphRAG, in contrast, benefits less dramatically from rubric introduction because it is already strong without rubric (12 top-2 placements versus 19 with rubric). This suggests that rubric guidance stabilizes weaker models (spaCy, CoreNLP) by reducing variability, whereas GraphRAG already exhibits high robustness and only gains marginally with rubric.", "In summary, rubric-based evaluation enhances consistency across models, ensuring fewer poorquality answers, but its effect is most pronounced for methods prone to variability. GraphRAG's relative insensitivity to rubric presence underscores its inherent robustness, while spaCy's shift from rank-1 dominance to rank-2 consistency reveals a trade-off between peak and average performance.", "NOTE: It is important to note that comparing spaCy, CoreNLP, and GraphRAG on speed or memory usage is not meaningful due to their inherently different processing approaches-spaCy uses a lightweight Cython-based pipeline, CoreNLP employs a multi-stage Java framework with deep linguistic analysis, and GraphRAG leverages LLMs for hierarchical graph-based reasoning-making direct benchmarking of resource usage inherently unequal.", "4.2.3. Pre-processing Requirements\n\nThe spaCy-based approach requires extensive manual preprocessing, including text segmentation, custom dependency parsing, and triplet extraction, demanding significant coding effort. The CoreNLP method automates many linguistic analyses through its OpenIE module but still necessitates server setup and some manual processing for question analysis and triplet matching. In contrast, GraphRAG automates the entire preprocessing pipeline, handling text ingestion, knowledge graph construction, and retrieval processes internally, thereby minimizing manual intervention. The following table summarizes the preprocessing requirements for each method:", "| | Aspect | spaCy | CoreNLP | GraphRAG |\n|---:|:-------------------------------------|:--------------|:------------------------------|:-----------------------------------------------------|\n| 0 | Setup complexity | Low | Medium (Java server required) | Very low |\n| 1 | Custom patterns &rules | Built-in | | Dependency parsing Fully automated |\n| 2 | Hand-rolled via patterns | OpenIE module | | Triplet extraction Internal to GraphRAG (using LLMs) |\n| 3 | Question matching/filter Custom code | | Custom code | Handled by framework (using prompts) |\n| 4 | Graph construction | Manual | Manual | Automated |\n| 5 | Total manual effort | High | Moderate | Low |\n\nTable 17: Comparison table for the 'Pre-processing Requirements' subparameter", "4.3. Added GraphRAG advantages\n\nGraphRAG offers several advantages in the context of LLM + graph-based question answering. It excels at handling both structured and unstructured data, leveraging knowledge graphs to capture relationships between entities while extracting insights from unstructured text. This dual capability enhances response accuracy and comprehensiveness. Additionally, GraphRAG supports incremental learning, allowing for dynamic updates to the knowledge graph without requiring full retraining, which is particularly beneficial for rapidly evolving domains.", "The system's schema flexibility allows for easy modification or extension of the underlying schema, supporting dynamic or schema-less environments. Furthermore, GraphRAG is designed for distributed processing, leveraging cloud-native architectures to ensure scalability and high performance. GraphRAG also provides provenance tracking, tracing the origin of each piece of information used in generating responses. This feature enhances transparency and trustworthiness by grounding outputs in structured formats, reducing hallucinations and providing confidence scoring. Overall, GraphRAG's comprehensive capabilities make it a powerful tool for complex AI applications across various domains, offers a robust solution for question answering, ensuring privacy, and supporting advanced reasoning and processing capabilities.", "5. Conclusion and Future Work\n\nIn this work, we have presented a fully reproducible framework that leverages three distinct open-source methods-spaCy, Stanford CoreNLP-OpenIE, and GraphRAG-for extracting relational triplets, constructing knowledge graphs, and integrating them with large language models (LLMs) to enhance question-answering. Through systematic evaluation under identical conditions, we found that spaCy delivers a lightweight, high-precision baseline: it excels at extracting clear, well-formed subject-predicate-object triplets with minimal false positives, yet its coverage diminishes when faced with idiomatic or complex sentence structures. CoreNLP-OpenIE , by contrast, achieves the broadest factual coverage across varied text types, uncovering nuanced relations even in less canonical phrasing; however, this comes at the expense of higher computational overhead and the need for more aggressive filtering to remove noisy or overly granular triplets. GraphRAG stands out for its reasoning prowess: its tight coupling of graph-structured context with LLM prompting yields the richest, most coherent responses on thematic and multi-hop queries, albeit with added system complexity and increased runtime demands. Our empirical results-validated by GPT-4 automated scoring and domain expert judgments-demonstrate that each method occupies a unique point in the precision-coverage-reasoning trade-off space, thereby offering clear guidance on tool selection for diverse QA scenarios.", "Based on these insights, we recommend a hybrid pipeline that begins with spaCy's high-precision filters to eliminate spurious relations, followed by CoreNLP-OpenIE's expansive extraction to maximize recall and culminates with GraphRAG's deep contextual reasoning to generate the final answer. To support real-time and dynamic applications, knowledge graphs should be incrementally updated via streaming triplet ingestion, ensuring that the QA system remains aligned with evolving information. Incorporating lightweight expert-in-the-loop validations-such as crowd-sourced or calibrated domain checks-can further prune residual noise and calibrate LLM prompts for heightened coherence. Finally, practitioners should tune extraction thresholds and prompt templates to the specific demands of their domain, balancing latency, accuracy, and interpretability according to task requirements.", "Looking ahead, a promising research avenue lies in unifying triplet extraction and reasoning within end-to-end hybrid architectures. One possible avenue can be where the LLMs are designed such that building knowledge graphs is a part of their own learning process, using trainable components that can be adjusted during training. This way, the model can automatically learn which relationships and structures are most helpful for answering questions. This could eliminate the need for separate, multistep RAG pipelines, making the system more efficient and integrated. A key question for future work is whether graph-centric retrieval-directly querying over learned graph embeddings-can fully supplant traditional chunk-based RAG methods without sacrificing multi-hop inference capabilities or explainability. To date, our study has been anchored in text-centric extraction; expanding this comparison to include retrieval over vectorized graph representations will yield valuable insights into the fundamental limits of RAG versus graph-based strategies.", "Beyond retrieval and model architecture, scalable evaluation and multimodal integration represent critical frontiers. Automated, graph-based coherence metrics-such as entailment scoring over relation paths-could reduce our reliance on costly expert assessments, while calibrated crowd-sourcing frameworks can facilitate large-scale benchmarking. On the content side, extending knowledge graphs to encapsulate visual, tabular, and temporal data will enable QA systems to operate over charts, images, and time series, unlocking new applications in scientific analysis, financial forecasting, and beyond. Finally, developing domain-specific schema adaptation and incremental graph-updating strategies will ensure that KG-enhanced QA systems remain robust and current across rapidly evolving knowledge domains. These combined efforts will bring us closer to QA systems capable of human-level reasoning, contextual richness, and real-world applicability.", "2 Related Works\n\nSpare and Dense Retriever RAG methods: Hybrid approaches, such as the \"Blended RAG\" framework proposed by Sawarkar et al. [12], demonstrate that combining multiple retrievers can improve the retrieval of relevant passages for clearly defined queries. This framework integrates six distinct retrievers-including traditional sparse methods (e.g., BM25) and dense semantic search techniques-selected based on their performance across benchmark datasets like Natural Questions (NQ) [13], TREC-COVID [14], SqUAD [15], and HotPotQA [16].", "A key strength of this hybrid search technique is its ability to return relevant passages, aligning well with Research Goal 3 for well-specified queries. However, these methods do not fully address Research Goal 3 when handling vaguer questions. In such cases, the hybrid approach can retrieve extraneous or unnecessary information, thus failing to eliminate the \"needle in a haystack\" problem. This shortcoming is evident in evaluations on the CoQA dataset [17], where retrieval accuracies ranged only from 45% to 49%. The reliance on document metadata during retrieval-a common feature in these systems-further complicates matters, as such metadata is often minimal in real-world scenarios (e.g., limited to document names and creation dates), thereby increasing both the creation time of the information store (contradicting Research Research Goal 1) and the risk of returning irrelevant passages (contradicting Research Research Goal 3) leading to longer contexts (contradicting Research Goal 2).", "Figure 3: Results of Priyanka Mandikal et al. on 2 example queries [18].\n\nQuery: Do fibroblasts from CF patients grow at a\n\nQuery: What is the prognosis for patients who have\n\nnormal rate?\n\nhad an episode of respiratory failure?\n\nMethod P@1 P@3 P@5 P@10 VSR 33.33 SPECTERZ Hybrid 66.67 Sample Retrieval This paper analyzes the cell cycle of cystic\n\nThe diagnosis, management and prognosis of 17 infants\n\nfibrosis-derived fibroblasts and statistically\n\nwith cystic fibrosis (CF) and severe respiratory disease\n\ndemonstrates their failure support normal\n\nwere reviewed for the\n\nperiod 1968 to 1972.\n\nThe clinical\n\npopulation doubling time in vitro and indicates that this\n\ncourse these infants was characterized by is due largely\n\nto a failure of the\n\ncystic fibrosis-derived\n\nfailure to thrive and\n\ncells to enter the cell cycle in normal numbers.\n\nWe have malnutrition Despite vigorous therapy including no explanation for significant difference antibiotics there was\n\n60% mortality:\n\nA delay in the\n\nbehavior of these cells from patients with cystic fibrosis\n\ndiagnosis of CF from the onset of respiratory symptoms\n\nvis-a-vis either disease control fibroblasts, with a\n\nmean of six weeks was considered an important\n\nsave to conclude that proceeds from an intrinsic\n\nfactor affecting survival. This data supports the need for\n\nproperty of the cells themselves.", "developing a reliable screen test for CF at birth.", "In contrast, recent work by Mandikal et al. [18] has advanced a hybrid retrieval model that integrates a sparse TF-IDF retriever with a dense retriever based on fine-tuned SPECTER2 embeddings [19]. Their approach employs a weighted scoring mechanism, controlled by a hyperparameter \u03bb , to balance the contributions of the two retrieval methods. Empirical evaluations on a medical database concerning Cystic Fibrosis indicate that while dense retrieval alone underperforms, incorporating approximately 20% of the sparse score (i.e., setting \u03bb = 0 . 2 for the sparse component) significantly improves document retrieval accuracies across different retrieval ranks [Figure 3]. Crucially, this method indexes documents solely based on their content, thereby avoiding the pitfalls of metadata dependency. This contributes to reducing information store creation times (Research Goal 1) and partially mitigates the retrieval of unnecessary information (Research Goal 3), although it does not entirely resolve the challenge of handling vaguer queries.", "In summary, while hybrid retrieval approaches represent a significant advancement by returning more relevant passages for well-defined queries complying with Research Goal 3. However they remain imperfect in filtering out unnecessary information for vaguer questions which can lead to longer contexts. This limitation underscores the need for further refinement to minimize the \"needle in a haystack\" problem in large-scale, real-world applications and make a pipeline which is compliant with Research Goal 2 as well.", "Reranker RAG methods: To address the limitation which Hybrid-Search methods have in adhering to Research Goal 2 for vaguer questions, Reranker methods have been developed to reduce the size and improve the quality of the final context.", "Figure 4: A high level view of what a RAG pipeline with a reranker looks like.\n\nIst Stage 2nd Stage Knowledge Embedding-based LLM-based Docs Corpus retrieval reranking\n\nUsually this is a process called 2-step retrieval where the first step involves retrieving documents such as through sparse or dense retriever techniques and then a second stage where an LLM based reranker comes into play to decide which of the documents retrieved are most relevant to the query and sorts them in order [Figure 4] before passing it to an LLM for generation.", "Prior to the formal conceptualization of Retrieval-Augmented Generation (RAG), information retrieval research explored the use of document rerankers. For example, Noguiera et al.'s 2020 study, Document Ranking with a Pretrained Sequence-to-Sequence Model [20], proposed employing a sequence-to-sequence model as a document reranker. This work evaluated multiple fine-tuned T5 models of varying sizes, as well as a fine-tuned BERT model, on documents initially retrieved using the sparse BM25 keyword method. The incorporation of these rerankers led to improvements in retrieval performance across the Robust04, Core17, and Core18 datasets. However, a critical limitation of this framework was its reliance on large models, leaving unresolved whether the observed performance gains were due to the reranking strategy or simply the use of more powerful models.", "Subsequent advancements in RAG have integrated BERT-based rerankers into new pipelines. Michael Glass et al.'s Re 2 G framework [21] embeds a BERT-based reranker-similar to that of Noguiera et al.-within a RAG system. This integration has shown notable improvements in metrics such as Recall (for 5 documents), overall recall, precision, and accuracy on several datasets, as indicated by performance on the KILT leaderboard. Nevertheless, a significant drawback of this approach is the necessity to host an additional model, which directly conflicts with Goal 4: alleviating the monetary and computational costs associated with deploying full-scale LLMs.", "Further refinement in reranking approaches is demonstrated by Xuegang Ma et al., who fine-tuned a LLaMA-2-7B model for reranking in their RankLLaMA system [22]. Although RankLLaMA achieves superior reranking performance, it again relies on a large model comparable to LLaMA-2-7B. Such reliance on resource-intensive models undermines the overarching objective of Goal 4, as the additional computational and financial burden associated with hosting these rerankers contradicts the need for efficiency and cost reduction.", "In summary, while reranker models can mitigate the 'needle in a haystack' problem (Goal 2) by filtering out least relevant information, they are not a sustainable solution. Their requirement for additional large-scale models significantly increases computational and monetary costs, thereby violating Goal 4. Consequently, alternative strategies that achieve comparable retrieval performance without incurring such overhead are essential for the advancement of practical and efficient RAG systems.", "Knowledge Graph RAG methods: Knowledge graph-based solutions have been proposed as a promising alternative for enhancing the RAG process. The core idea behind employing knowledge graphs is to address semantic search challenges by organizing data according to intrinsic relationships. For example, although not originally applied within a RAG context, Dawei Cheng et al.'s 2020 work on constructing knowledge graphs for a financial price prediction tool [23] illustrates how grouping semantically related data into distinct sub-networks enables a query-such as 'who quit Apple'-to retrieve only the pertinent subset of information, rather than returning all semantically similar sentences (e.g., both 'Steve Jobs quits Apple' and 'David Peter leaves Starbucks'). This targeted retrieval helps mitigate the \"needle in a haystack\" problem (Goal 2) and increases the rate of retrieving relevant passages (Goal 3).", "Building on these insights, recent research has directly integrated knowledge graphs within RAG frameworks. For instance, Zhentao Xu et al. [24] developed a system for LinkedIn's customer question answering service by first", "creating a custom knowledge graph and then retrieving related nodes. The retrieved information is subsequently refined using GPT-4 to produce coherent responses. When compared with a baseline semantic search system, this pipeline significantly improved retrieval metrics-including Mean Reciprocal Rank, Recall, and Normalized Discounted Cumulative Gain-as well as evaluation measures such as BLEU, METEOR, and ROUGE, thereby supporting Goals 2 and 3.", "However, despite these promising advances, knowledge graph-based approaches fall short of meeting Goal 1. The process of constructing and training knowledge graphs is computationally intensive and time-consuming, which directly conflicts with the objective of reducing information store creation times.", "| | Model | EM | Fl Score | Accuracy | Hallucination |\n|---:|:--------------|-----:|:-----------|:-----------|:----------------|\n| 0 | Human | 63 | | | |\n| 1 | MHQA-GRN | 33.2 | | | |\n| 2 | Embedding-RAG | 28 | 37 | 46 | 30 |\n| 3 | KG-RAG | 19 | | 32 | 15 |\n\nFigure 5: Results comparing knowledge graph framework with semantic search methods [25].", "For example, Diego Sanmart\u00edn's 2024 work [25] demonstrated a Knowledge Graph RAG system that leverages LLMs to extract entity-relationship triples from unstructured text and employs a 'Chain of Explorations' retrieval strategy. While this approach shows potential for handling vague queries, the high training times required to build and maintain the knowledge graph significantly increase the overall overhead. Moreover, the compressed storage format-using entity-relationship triples-can lead to a loss of detailed contextual information, thereby limiting the system's ability to fully expand upon relevant topics during retrieval hence resulting in even lower EM, F1, Accuracy scores with only an improvement in Hallucination compared to a standard Semantic Search method [Figure 5].", "In summary, while knowledge graph-based solutions offer clear benefits in terms of improving semantic search and enhancing the retrieval of relevant passages (addressing Goals 2 and 3), they do not currently satisfy Goal 1 due to their high training times and associated computational costs. Addressing these inefficiencies is essential for realizing the full potential of knowledge graphs within efficient and practical RAG frameworks.", "Combining Knowledge Graphs and Vector based Search methods: The limitations inherent in a purely knowledge graph-based RAG framework suggest that combining the strengths of knowledge graphs with traditional semantic search methods may yield superior results. Bhaskarjit Sarmah et al. [26] propose such an integrated approach through their HybridRAG framework. This framework concatenates outputs from a knowledge graph query-implemented using Microsoft's GraphRAG [27] with content indexed by extracting entity relationships in chunk sizes of 1024-with results from a semantic search, which similarly employs chunk sizes of 1024 and the text-embedding-ada-002 model. The aggregated context is then provided to GPT-3.5 to generate the final answer. The dataset for this study was constructed by web scraping earnings call documents of companies listed on the NIFTY 50, and evaluation was performed using the RAGAS evaluation platform [28].", "The HybridRAG framework demonstrated improvements in metrics such as Faithfulness, Answer Relevancy Score, and Context Recall. These enhancements support Goal 2 by reducing the \"needle in a haystack\" problem and Goal 3 by increasing the rate of retrieving relevant passages for answering questions. However, the framework underperformed in Context Precision when compared to standalone implementations of both knowledge graph-based and vector database-based RAG systems. This shortcoming indicates that the current integration does not fully leverage the advantages of traditional vector search techniques, particularly hybrid search techniques, which is critical for fully meeting Goal 3."]}, {"type": "entry", "index": 1, "question": "How do reranker-based RAG pipelines trade off latency, cost, and context precision?", "answer": "Reranker-based RAG pipelines add a second step after initial retrieval: an extra model (often an LLM or large encoder) re-scores the retrieved passages and filters them before generation [2]. This directly shapes three things:\n\n1. Context precision\n\u2013 By filtering out less relevant passages, rerankers tackle the \u201cneedle-in-a-haystack\u201d problem and improve recall, precision, and accuracy of what is passed to the generator [2].\n\u2013 More advanced selectors like Dynamic Passage Selector learn to pick a variable set of passages, reducing missed evidence when K is too small and cutting noise when K is too large, which boosts multi-hop reasoning quality [1].\n\n2. Latency\n\u2013 Reranking introduces an extra inference step over many candidate passages, so end\u2011to\u2011end response time increases.\n\u2013 Similar patterns are seen in KG-based RAG: raising the retrieval limit improves accuracy but increases query time and token usage [3]\u2014reranking more items has a comparable effect.\n\n3. Monetary and computational cost\n\u2013 Hosting a dedicated reranker (e.g., BERT, T5, or LLaMA\u20112\u20117B) significantly increases VRAM and compute requirements, conflicting with goals of low-cost deployment [2]. Studies argue that, despite better passage selection, such large rerankers are \u201cnot a sustainable solution\u201d because of this overhead [2].\n\u2013 Some work finds that reranking top\u2011k (e.g., top\u201130) gives only marginal end\u2011task gains in complex \u201cmixture-of-knowledge\u201d settings, suggesting the extra cost may not always be justified [4].\n\u2013 Alternative designs like KeyKnowledgeRAG deliberately avoid large rerankers, using smaller models and summarization to keep VRAM, cost, and latency low while still improving context quality over naive RAG [2].\n\nIn practice, rerankers trade higher cost and slower responses for cleaner, more focused contexts\u2014and whether that trade is worthwhile depends on query complexity and deployment constraints.", "tags": ["rerankers", "benchmarks_eval", "retrieval_tuning", "scaling_cost"], "full_faithfulness": 1.0, "full_answer_relevancy": 0.9639890720000001, "seed_faithfulness": 1.0, "seed_answer_relevancy": 0.0, "context_relevance": 0.8209369753062011, "context_relevance_error": null, "context_relevance_reason": "The context passages provide substantial information relevant to the user's question about the trade-offs in reranker-based RAG pipelines, particularly regarding latency, cost, and context precision. The passages discuss the limitations of traditional reranking methods, the introduction of the Dynamic Passage Selector (DPS), and the implications of hybrid retrieval methods. However, while the context is informative, it could more explicitly address the specific trade-offs mentioned in the question, particularly in terms of latency and cost, which are less directly covered.", "structure_citation": 0.8050857124175226, "structure_error": null, "structure_reason": "The Actual Output effectively addresses the trade-offs of latency, cost, and context precision in reranker-based RAG pipelines, providing detailed explanations supported by citations. It includes claims with [n] markers that correspond to snippets in the Context, and the cited snippets align well with the claims made. However, there are minor shortcomings, such as the presence of a citation [4] that lacks a corresponding snippet in the Context, which slightly detracts from the overall completeness of the response.", "hallucination_score": 0.8121582261821636, "hallucination_error": null, "hallucination_reason": "The response accurately identifies and discusses the trade-offs of latency, cost, and context precision in reranker-based RAG pipelines, aligning well with the context provided. It effectively explains how rerankers improve context precision by filtering passages and addresses the increase in latency and costs associated with additional models. However, while it mentions the Dynamic Passage Selector, it does not explicitly connect this to the context's emphasis on the need for adaptive evidence selection, which slightly detracts from its completeness.", "contexts_used": ["Abstract\n\nRetrieval-augmented generation (RAG) systems are often bottlenecked by their reranking modules, which typically score passages independently and select a fixed Top-K size. This approach struggles with complex multi-hop queries that require synthesizing evidence across multiple documents, creating a trade-off where small K values omit crucial information and large K values introduce noise. To address this, we introduce the D ynamic P assage S elector (DPS), a novel reranking framework that treats passage selection as a supervised learning problem. Unlike traditional point-wise or list-wise methods, DPS is fine-tuned to capture interpassage dependencies and dynamically select the most relevant set of passages for generation. As a seamless plugand-play module, DPS requires no modifications to the standard RAG pipeline. Comprehensive evaluations on five benchmarks show that DPS consistently outperforms stateof-the-art rerankers and fine-tuning methods. Notably, on the challenging MuSiQue dataset, DPS improves the F1-score by 30.06% and 15.4% over strong baselines like Qwen3reranker and RankingGPT, respectively. Our results demonstrate that by enabling adaptive evidence selection, DPS substantially enhances reasoning capabilities in complex RAG scenarios.", "2 Related Works\n\nSeveral studies have explored and implemented advanced RAG (Retrieval-Augmented Generation) systems to overcome the limitations of the naive RAG approach discussed in the introduction. A significant focus of these works lies in developing novel methods to enhance vector database-centric retrieval systems, evaluated across a diverse set of metrics pertinent to vector-based retrieval. Additionally, some studies assess the performance of RAG solutions leveraging Knowledge Graphs [11], while one investigation examines the outcomes of naively combining Knowledge Graph-based and semantic search results for question-answering tasks. These related works also highlight certain limitations, ranging from increased resource demands to the need for novel strategies or enhancements to improve system performance.", "Addressing the limitations of naive Retrieval-Augmented Generation (RAG) pipelines has led to the integration of hybrid retrieval methods that combine sparse (keyword-based) and dense (semantic) retrievers.", "Spare and Dense Retriever RAG methods: Hybrid approaches, such as the \"Blended RAG\" framework proposed by Sawarkar et al. [12], demonstrate that combining multiple retrievers can improve the retrieval of relevant passages for clearly defined queries. This framework integrates six distinct retrievers-including traditional sparse methods (e.g., BM25) and dense semantic search techniques-selected based on their performance across benchmark datasets like Natural Questions (NQ) [13], TREC-COVID [14], SqUAD [15], and HotPotQA [16].", "A key strength of this hybrid search technique is its ability to return relevant passages, aligning well with Research Goal 3 for well-specified queries. However, these methods do not fully address Research Goal 3 when handling vaguer questions. In such cases, the hybrid approach can retrieve extraneous or unnecessary information, thus failing to eliminate the \"needle in a haystack\" problem. This shortcoming is evident in evaluations on the CoQA dataset [17], where retrieval accuracies ranged only from 45% to 49%. The reliance on document metadata during retrieval-a common feature in these systems-further complicates matters, as such metadata is often minimal in real-world scenarios (e.g., limited to document names and creation dates), thereby increasing both the creation time of the information store (contradicting Research Research Goal 1) and the risk of returning irrelevant passages (contradicting Research Research Goal 3) leading to longer contexts (contradicting Research Goal 2).", "Figure 3: Results of Priyanka Mandikal et al. on 2 example queries [18].\n\nQuery: Do fibroblasts from CF patients grow at a\n\nQuery: What is the prognosis for patients who have\n\nnormal rate?\n\nhad an episode of respiratory failure?\n\nMethod P@1 P@3 P@5 P@10 VSR 33.33 SPECTERZ Hybrid 66.67 Sample Retrieval This paper analyzes the cell cycle of cystic\n\nThe diagnosis, management and prognosis of 17 infants\n\nfibrosis-derived fibroblasts and statistically\n\nwith cystic fibrosis (CF) and severe respiratory disease\n\ndemonstrates their failure support normal\n\nwere reviewed for the\n\nperiod 1968 to 1972.\n\nThe clinical\n\npopulation doubling time in vitro and indicates that this\n\ncourse these infants was characterized by is due largely\n\nto a failure of the\n\ncystic fibrosis-derived\n\nfailure to thrive and\n\ncells to enter the cell cycle in normal numbers.\n\nWe have malnutrition Despite vigorous therapy including no explanation for significant difference antibiotics there was\n\n60% mortality:\n\nA delay in the\n\nbehavior of these cells from patients with cystic fibrosis\n\ndiagnosis of CF from the onset of respiratory symptoms\n\nvis-a-vis either disease control fibroblasts, with a\n\nmean of six weeks was considered an important\n\nsave to conclude that proceeds from an intrinsic\n\nfactor affecting survival. This data supports the need for\n\nproperty of the cells themselves.", "developing a reliable screen test for CF at birth.", "In contrast, recent work by Mandikal et al. [18] has advanced a hybrid retrieval model that integrates a sparse TF-IDF retriever with a dense retriever based on fine-tuned SPECTER2 embeddings [19]. Their approach employs a weighted scoring mechanism, controlled by a hyperparameter \u03bb , to balance the contributions of the two retrieval methods. Empirical evaluations on a medical database concerning Cystic Fibrosis indicate that while dense retrieval alone underperforms, incorporating approximately 20% of the sparse score (i.e., setting \u03bb = 0 . 2 for the sparse component) significantly improves document retrieval accuracies across different retrieval ranks [Figure 3]. Crucially, this method indexes documents solely based on their content, thereby avoiding the pitfalls of metadata dependency. This contributes to reducing information store creation times (Research Goal 1) and partially mitigates the retrieval of unnecessary information (Research Goal 3), although it does not entirely resolve the challenge of handling vaguer queries.", "In summary, while hybrid retrieval approaches represent a significant advancement by returning more relevant passages for well-defined queries complying with Research Goal 3. However they remain imperfect in filtering out unnecessary information for vaguer questions which can lead to longer contexts. This limitation underscores the need for further refinement to minimize the \"needle in a haystack\" problem in large-scale, real-world applications and make a pipeline which is compliant with Research Goal 2 as well.", "Reranker RAG methods: To address the limitation which Hybrid-Search methods have in adhering to Research Goal 2 for vaguer questions, Reranker methods have been developed to reduce the size and improve the quality of the final context.", "Figure 4: A high level view of what a RAG pipeline with a reranker looks like.\n\nIst Stage 2nd Stage Knowledge Embedding-based LLM-based Docs Corpus retrieval reranking\n\nUsually this is a process called 2-step retrieval where the first step involves retrieving documents such as through sparse or dense retriever techniques and then a second stage where an LLM based reranker comes into play to decide which of the documents retrieved are most relevant to the query and sorts them in order [Figure 4] before passing it to an LLM for generation.", "Prior to the formal conceptualization of Retrieval-Augmented Generation (RAG), information retrieval research explored the use of document rerankers. For example, Noguiera et al.'s 2020 study, Document Ranking with a Pretrained Sequence-to-Sequence Model [20], proposed employing a sequence-to-sequence model as a document reranker. This work evaluated multiple fine-tuned T5 models of varying sizes, as well as a fine-tuned BERT model, on documents initially retrieved using the sparse BM25 keyword method. The incorporation of these rerankers led to improvements in retrieval performance across the Robust04, Core17, and Core18 datasets. However, a critical limitation of this framework was its reliance on large models, leaving unresolved whether the observed performance gains were due to the reranking strategy or simply the use of more powerful models.", "Subsequent advancements in RAG have integrated BERT-based rerankers into new pipelines. Michael Glass et al.'s Re 2 G framework [21] embeds a BERT-based reranker-similar to that of Noguiera et al.-within a RAG system. This integration has shown notable improvements in metrics such as Recall (for 5 documents), overall recall, precision, and accuracy on several datasets, as indicated by performance on the KILT leaderboard. Nevertheless, a significant drawback of this approach is the necessity to host an additional model, which directly conflicts with Goal 4: alleviating the monetary and computational costs associated with deploying full-scale LLMs.", "Further refinement in reranking approaches is demonstrated by Xuegang Ma et al., who fine-tuned a LLaMA-2-7B model for reranking in their RankLLaMA system [22]. Although RankLLaMA achieves superior reranking performance, it again relies on a large model comparable to LLaMA-2-7B. Such reliance on resource-intensive models undermines the overarching objective of Goal 4, as the additional computational and financial burden associated with hosting these rerankers contradicts the need for efficiency and cost reduction.", "In summary, while reranker models can mitigate the 'needle in a haystack' problem (Goal 2) by filtering out least relevant information, they are not a sustainable solution. Their requirement for additional large-scale models significantly increases computational and monetary costs, thereby violating Goal 4. Consequently, alternative strategies that achieve comparable retrieval performance without incurring such overhead are essential for the advancement of practical and efficient RAG systems.", "Knowledge Graph RAG methods: Knowledge graph-based solutions have been proposed as a promising alternative for enhancing the RAG process. The core idea behind employing knowledge graphs is to address semantic search challenges by organizing data according to intrinsic relationships. For example, although not originally applied within a RAG context, Dawei Cheng et al.'s 2020 work on constructing knowledge graphs for a financial price prediction tool [23] illustrates how grouping semantically related data into distinct sub-networks enables a query-such as 'who quit Apple'-to retrieve only the pertinent subset of information, rather than returning all semantically similar sentences (e.g., both 'Steve Jobs quits Apple' and 'David Peter leaves Starbucks'). This targeted retrieval helps mitigate the \"needle in a haystack\" problem (Goal 2) and increases the rate of retrieving relevant passages (Goal 3).", "Building on these insights, recent research has directly integrated knowledge graphs within RAG frameworks. For instance, Zhentao Xu et al. [24] developed a system for LinkedIn's customer question answering service by first", "creating a custom knowledge graph and then retrieving related nodes. The retrieved information is subsequently refined using GPT-4 to produce coherent responses. When compared with a baseline semantic search system, this pipeline significantly improved retrieval metrics-including Mean Reciprocal Rank, Recall, and Normalized Discounted Cumulative Gain-as well as evaluation measures such as BLEU, METEOR, and ROUGE, thereby supporting Goals 2 and 3.", "However, despite these promising advances, knowledge graph-based approaches fall short of meeting Goal 1. The process of constructing and training knowledge graphs is computationally intensive and time-consuming, which directly conflicts with the objective of reducing information store creation times.", "| | Model | EM | Fl Score | Accuracy | Hallucination |\n|---:|:--------------|-----:|:-----------|:-----------|:----------------|\n| 0 | Human | 63 | | | |\n| 1 | MHQA-GRN | 33.2 | | | |\n| 2 | Embedding-RAG | 28 | 37 | 46 | 30 |\n| 3 | KG-RAG | 19 | | 32 | 15 |\n\nFigure 5: Results comparing knowledge graph framework with semantic search methods [25].", "For example, Diego Sanmart\u00edn's 2024 work [25] demonstrated a Knowledge Graph RAG system that leverages LLMs to extract entity-relationship triples from unstructured text and employs a 'Chain of Explorations' retrieval strategy. While this approach shows potential for handling vague queries, the high training times required to build and maintain the knowledge graph significantly increase the overall overhead. Moreover, the compressed storage format-using entity-relationship triples-can lead to a loss of detailed contextual information, thereby limiting the system's ability to fully expand upon relevant topics during retrieval hence resulting in even lower EM, F1, Accuracy scores with only an improvement in Hallucination compared to a standard Semantic Search method [Figure 5].", "In summary, while knowledge graph-based solutions offer clear benefits in terms of improving semantic search and enhancing the retrieval of relevant passages (addressing Goals 2 and 3), they do not currently satisfy Goal 1 due to their high training times and associated computational costs. Addressing these inefficiencies is essential for realizing the full potential of knowledge graphs within efficient and practical RAG frameworks.", "5 Discussion\n\n5.1 Achieving Research Goal 1: Reduce information store creation and update times\n\nK 2 RAG effectively reduces the creation times for Knowledge Graph and Sparse and Dense vector databases, such as the Semantic and BM25-based Keyword databases. Performing text summarization on the training corpus led to an average reduction in training times of up to 93% , with only 25 minutes spent on summarization.", "5.1 Achieving Research Goal 1: Reduce information store creation and update times\n\nFor instance, the Semantic database creation, which originally took 4010 seconds (67 minutes) , was reduced to 441 seconds (7.35 minutes) when using the summarized corpus. Even when accounting for the summarization process, the total time was 32.25 minutes , significantly lower than training with the full corpus. Similarly, Knowledge Graph creation time dropped from nearly 18 hours to around 1 hour , thanks to the reuse of the summarized corpus. This approach not only saves time and resources but also reduces the cost of LLM-based Knowledge Graph creation, which is typically expensive to host.", "5.2 Achieving Research Goal 2: Mitigate the Needle-in-a-Haystack problem\n\nK 2 RAG effectively mitigates the \"Needle in a Haystack\" problem, which can hinder LLM performance when handling large contexts. By integrating the Longformer-based led-base-book-summary summarizer at key stages of the retrieval pipeline, the framework reduces context size while preserving essential information.", "5.2 Achieving Research Goal 2: Mitigate the Needle-in-a-Haystack problem\n\nK 2 RAG maintained answer quality and even outperformed naive pipelines on certain questions, as reflected in the higher accuracy and Q 3 quantile. This approach allows for improved LLM performance without a bloated context, preventing the model from missing critical details and enhancing the final answer quality.", "5.3 Achieving Research Goal 3: Improve retrieval accuracy\n\nThe K 2 RAG framework enhances passage retrieval through two key components. First, the knowledge component identifies relevant topics for answering the question. Based on these results, the system further explores each topic within the knowledge graph to extract more concrete and relevant chunks. This approach enables the development of more comprehensive answers. Similar to Research Goal 2, this strategy improves the quality of the final LLM-generated responses.", "5.4 Achieving Research Goal 4: Lower Temporal and Computational Overhead\n\nWhile rerankers enhance passage retrieval accuracy, they require an additional large model, increasing resource demands. Similarly, the naive Knowledge Graph RAG pipeline depends on two LLMs: Mistral-7B-FP16 (14GB VRAM) and Mistral-7B-Q4 (4.1GB VRAM).", "5.4 Achieving Research Goal 4: Lower Temporal and Computational Overhead\n\nIn contrast, the K 2 RAG framework streamlines the process by utilizing only Mistral-7B-Q4 (4.1GB VRAM) for querying the Knowledge Graph, question generation, and final answer generation, along with a small summarization model requiring just 648MB VRAM. This results in a total VRAM usage of 4.8GB, allowing for efficient RAG pipelines on smaller machines while significantly reducing computational and monetary costs.", "5.4 Achieving Research Goal 4: Lower Temporal and Computational Overhead\n\nMoreover, K 2 RAG achieves faster response times and superior answer quality compared to naive RAG pipelines. This is evident in its reduced question-answering times relative to the naive Knowledge Graph Search RAG, enabling K 2 RAG to process more queries in a span of time.", "5.5 Limitations\n\nWhile our RAG framework shows a slight mean accuracy advantage over naive methods, a mean similarity of 0.57, despite a high Q 3 value, indicates limited performance gains on most questions. This suggests it excels only at certain\n\n| | Pipeline | Memory Footprint |\n|---:|:-------------------------|:-------------------|\n| 0 | Semantic Search Pipeline | 14.3GB |\n| 1 | Keyword Search Pipeline | 14.3GB |\n| 2 | Hybrid Search Pipeline | 14.3GB |\n| 3 | Knowledge Graph Search | 18.1GB |\n| 4 | KeyKnowledgeRAG | 5GB |\n\nTable 4: Table of Memory Footprints", "question types, but the narrow scope restricts broader improvement. Reliance on broad knowledge graph searches may also reduce precision. A more focused retrieval strategy could yield more precise topic pinpointing and higher accuracy.", "6 DISCUSSION AND LIMITATION\n\nLessons Learned. During the evaluation of RAGs, we identified critical trade-offs between accuracy and computational cost. Tests revealed that response accuracy depends on the scale of the local KG and the entity retrieval limit per query. Increasing the retrieval limit from the default to more entities markedly improved accuracy but incurred higher token consumption and prolonged query times. This highlights the necessity of balancing query performance and cost in large-scale RAG-based workflows. Additionally, while model predictions occasionally diverged from the constructed hypothesis chains, domain experts emphasized that such discrepancies do not diminish the validity of the chains themselves. They highlighted that rigorously derived hypotheses-even without full alignment with retrieval results-retain significant values, particularly in uncovering novel insights.", "Generalization and Scalability. The hypothesis-driven architecture has demonstrated broad applicability across diverse research domains, particularly in scientific discovery tasks involving KG-based node and link prediction. When integrated with RAG, the system offers a cost-effective and flexible alternative to domain-specific fine-tuning, enabling more efficient adaptation to new research areas. Given the generalizability of RAG and the system's modular KG-based design, most components are transferable across domains, requiring only the substitution of domain-specific KGs and predictive models. Domain experts have also identified promising applications in areas such as protein structure and function prediction via interaction networks, as well as novel materials discovery. However, the system's performance is largely dependent on the quality of the underlying KGs and the effectiveness of the associated predictive models. As advances in textbased KG extraction continue to improve accuracy and robustness, the system's cross-domain applicability is expected to expand further.", "Limitations. While RAG supports text-to-KG conversion, the process remains resource-intensive and prone to inaccuracies, especially in data-rich domains. To reduce risk, we restricted integration to small, verified updates using public KGs. However, the slow pace of KG updates compared to rapid scientific progress highlights the need for more accurate, automated extraction methods. Although RAG helps mitigate hallucinations by retrieving from traceable sources, such issues persist. Future improvements could include fact-verification modules-e.g., cross-referencing authoritative sources or applying confidence thresholds to flag uncertain outputs. Given the complexity of hypothesisdriven discovery, some interaction complexity is unavoidable, though we anticipate continued simplification as AI reliability advances.", "2 Experiments\n\n2.2 RQ1: Effectiveness of RAG under Mixture-of-Knowledge Scenarios.\n\n1.64 1.81 0.62 0.57 0.85 0.45 1.13 0.91 (e) Qwen-3-32B Pubmed Wiki Pes2o C4 Github Math SE Book Arxiv CC SimpleQA SciQ ARC-C MMLU MMLU-Pro CSBench 0.72 16.28 3.14 0.90 1.20 0.88 1.46 4.35 1.10 1.40 0.30 2.00 0.50 1.90 1.70 0.60 2.60 0.17 0.09 0.00 0.26 0.34 0.44 0.25 0.37 0.22 0.39 0.20 0.43 0.68 0.96 0.49 0.64 0.48 0.52 0.82 0.79 0.74 0.40 0.51 0.28 (f) GPT-4o\n\nand mid-sized models, while the gains diminish as model size increases. For the strongest models, retrieval offers marginal or even slightly negative gains, suggesting that larger models are increasingly able to internalize domain knowledge without the need for external retrieval. These results highlight the generalizability of our findings and demonstrate that the impact of retrieval augmentation is robust across different knowledge domains.", "2.3 RQ 2: Instance-level Analysis of Multiple Retrieval Sources\n\nTo investigate whether different retrieval sources provide unique advantages, we conduct an instancelevel study (Figure 2) measuring the proportion of queries that can be solved only by retrieving from a specific corpus-compared to using all sources or no retrieval at all. Our results show that a significant fraction of cases (e.g., 8%-39% for Llama-3.18B) depend exclusively on retrieval from particular corpora. Crucially, no single retrieval source consistently outperforms others across all query types , highlighting the need for dynamic, query-specific routing to the most relevant corpus to maximize RAG performance in heterogeneous, real-world knowledge environments.", "2.4 RQ 3: Effectiveness of Reranking\n\nOne possible reason for the limited improvement in RAG performance is the inherent limitations of", "Figure 3: Performance with rerank on retrieval effectiveness across different datasets and models.\n\nPubmedWiki Pes2o GithubMath Book Arxiv All llama-3.2-3b llama-3.1-8b qwen3-4b qwen3-8b qwen3-32b gpt-4o-mini gpt-4o -0.63 7.11 -0.36\n\n-0.57 -0.23 -0.49 -0.07 -0.58\n\n2.04 8.49 3.02 31.90 4.42 9.05 3.93 4.32 3.67 4.94 3.19 12.8934.09 -0.82 3.50 0.21\n\n-0.70 -0.60 -0.70 -0.48 -0.75\n\n0.77 -0.85 2.93 0.14\n\n-0.76 -0.62 -0.69 -0.45 -0.76\n\n0.69\n3.81\n1.99\n\n-0.71 -0.13 -0.78 -0.68 -0.75 -0.58 -0.81\n\n0.27\n2.53\n-0.46\n\n-0.33 -0.01 -0.39 -0.29 -0.36 -0.26 -0.32\n\n1.89\n-0.24\n0.23\n\n-0.27 -0.24 -0.27 -0.24 -0.25 -0.26 -0.23 -0.17\n\n(a) SimpleQA -0.22 -0.10 0.24 -0.26 0.11 -0.20 -0.32 0.41 -0.01 0.06 0.03 -0.14 0.46 -0.15 0.07 -0.09 0.19 -0.11 -0.19 0.18\n\n-0.25 -0.13 -0.21 -0.01 -0.23 -0.06 -0.20 -0.08 -0.29 -0.02\n\n-0.32 -0.15 -0.24 -0.10 -0.28 -0.11 -0.27 -0.11 -0.35 -0.07 -0.01\n\n-0.20 -0.15 -0.25 -0.10 -0.19 -0.14 -0.18 -0.14 -0.24 -0.09 -0.05\n\n-0.14 -0.13 -0.18 -0.07 -0.13 -0.09 -0.13 -0.10 -0.13 -0.08 -0.04\n\n(b) SciQ", "-0.32 -0.15 -0.24 -0.10 -0.28 -0.11 -0.27 -0.11 -0.35 -0.07 -0.01\n\n-0.20 -0.15 -0.25 -0.10 -0.19 -0.14 -0.18 -0.14 -0.24 -0.09 -0.05\n\n-0.14 -0.13 -0.18 -0.07 -0.13 -0.09 -0.13 -0.10 -0.13 -0.08 -0.04\n\n(c) ARC-C -0.00 0.05 -0.03 0.04 -0.05 0.10 0.16 0.01 0.02 0.15 -0.04 -0.00 -0.02 -0.04 -0.02 -0.05 -0.06 -0.04 -0.01 -0.02\n\n-0.03 -0.01 -0.04 -0.00 -0.05\n\n-0.06 -0.02 -0.03 -0.02 -0.04 -0.02 -0.05 -0.02 -0.05 -0.01\n\n-0.03 -0.02 -0.02 -0.02 -0.03 -0.02 -0.03 -0.02 -0.04 -0.02 -0.01\n\n-0.01 -0.02 -0.02 -0.01 -0.01 -0.01 -0.01 -0.01 -0.01 -0.01 -0.01\n\n(d) MMLU -0.06 -0.00 -0.04 -0.07 0.12 0.31 -0.04\n\n-0.08 -0.02 -0.04 -0.02 -0.05 -0.02 -0.04 -0.01 -0.06 -0.01\n\n-0.06 -0.02 -0.05 -0.03 -0.08 -0.02 -0.07 -0.02 -0.09 -0.01", "-0.02 -0.01 -0.02 -0.02 -0.01 -0.00 -0.01 -0.01 -0.02 -0.01\n\n(e) MMLU-Pro\n0.08\n0.13\n\n-0.12 -0.01 -0.04 -0.01 -0.06\n\n-0.05 -0.01 -0.09 -0.03\n\n-0.14 -0.06 -0.09 -0.05 -0.07 -0.05 -0.08 -0.07 -0.11 -0.04 -0.03\n\n-0.10 -0.06 -0.09 -0.06 -0.06 -0.04 -0.04 -0.04 -0.09 -0.04 -0.05\n\n-0.03 -0.01 -0.02 -0.02 -0.02 -0.01 -0.00 -0.00 -0.01 -0.01\n\n-0.02 -0.01 -0.02\n\n-0.02 -0.01 -0.01 -0.00 -0.01 -0.00 -0.00\n\n(f) CSBench\n\nFigure 4: Performance comparison of routing strategies across MMLU and MMLU-Pro datasets.\n\nmmlu_pro 0.0 0.2 0.4 0.6 0.8 Accuracy no retrieval all sources qwen3-4b (plain) qwen3-8b (plain) qwen3-32b (plain) qwen3-4b (cot) qwen3-8b (cot) qwen3-32b (cot) Upper Bound", "retrievers. To investigate this, inspired by prior work demonstrating benefits from adding reranking to the RAG pipeline (Shao et al., 2024; Yu et al., 2024), we apply reranking to the top-30 retrieved results. As shown in Figure 3, reranking yields only marginal gains across datasets. This suggests that improving retrieval quality through reranking is insufficient in mixture-of-knowledge scenarios. The retriever's limited capacity and restricted access to relevant knowledge highlight the need for deeper integration between knowledge sources, retrieval mechanisms, and generative models.", "2.5 RQ 4: Evaluating LLMs as Routers for Mixture-of-Knowledge Retrieval\n\nPrevious analyses highlight the need for adaptive retrieval mechanisms that dynamically route queries to the most relevant corpus based on topic context. Here, we investigate whether current LLMs can effectively perform query routing. Figure 4 evaluates LLMs from the Qwen-3 series (4B, 8B, and 32B parameters) as 'routers' that select among heterogeneous knowledge sources at inference time. We compare plain prompting versus chain-of-thought prompting on the MMLU and MMLU-Pro datasets, which cover diverse general and professional domains.", "2.5 RQ 4: Evaluating LLMs as Routers for Mixture-of-Knowledge Retrieval\n\nSpecifically, we benchmark: (1) no-retrieval baselines; (2) static retrieval from all corpora ('all sources'); (3) LLM-prompted routing variants (plain and chain-of-thought); and (4) an oracle router upper bound. Surprisingly, neither prompting strategy consistently outperforms static retrieval. In fact, both routing approaches often underperform compared to simply retrieving from all sources, and occasionally fall below no-retrieval baselines. Chain-of-thought prompting provides only marginal improvements, while scaling model size yields negligible or even negative returns.", "2.5 RQ 4: Evaluating LLMs as Routers for Mixture-of-Knowledge Retrieval\n\nWe attribute this failure to two main factors: (1) Inaccurate relevance estimation: Without dedicated training, LLMs struggle to reliably identify which corpus holds the needed information, especially amid overlapping or stylistically diverse", "2.5 RQ 4: Evaluating LLMs as Routers for Mixture-of-Knowledge Retrieval\n\ncorpora. Minor routing errors propagate to poor retrieval quality. (2) Training-inference mismatch: LLM training typically lacks explicit multi-source comparison tasks, limiting the effectiveness of prompt-based routing as a meta-reasoning problem. Future directions should focus on learned routing modules trained with supervision or reinforcement learning, or on tightly integrated RAG systems that jointly optimize source selection and generation."]}, {"type": "entry", "index": 19, "question": "how does agentic rag differ from normal rag?", "answer": "Normal (traditional) RAG is usually a fixed pipeline: the system takes your question, runs a one-shot retrieval over a database (often text only), and then the model answers based on whatever was retrieved. The steps are static and linear\u2014\u201cretrieve, then read\u201d\u2014and do not change much based on query complexity or feedback. This works for simple questions but struggles with multi-step reasoning, complex tasks, and nuanced tool use [1][3].\n\nAgentic RAG adds an explicit decision\u2011making agent (or multiple agents) that actively controls how retrieval and reasoning happen. Instead of one fixed pass, the agent can:\n- Plan a retrieval strategy and decompose the task into sub\u2011queries or subtasks [1][3].\n- Run multi\u2011turn retrieval, refining searches and summarizing evidence over several steps [1][2][5].\n- Route the query through specialized tools or modules (e.g., domain routers, search routers, tissue classifiers, VQA agents) and choose between different retrieval modes (global vs partitioned, text vs image, hybrid) [1][5].\n- Decide whether RAG is needed at all, how many rewrites to perform, and which knowledge partition to search [1].\n- Learn better retrieval and reasoning policies via reinforcement learning, treating the corpus as an environment and optimizing end\u2011to\u2011end for answer quality and reasoning traceability [1][2][4].\n\nIn short, normal RAG is a static helper that fetches context once; agentic RAG is a learned controller that can adaptively plan, call tools, iterate retrieval, and provide more traceable, multi-step reasoning.\n", "tags": ["_untagged"], "full_faithfulness": 0.9354838710000001, "full_answer_relevancy": 0.9647742447000001, "seed_faithfulness": 0.64, "seed_answer_relevancy": 0.9622178753, "context_relevance": 0.939901613035366, "context_relevance_error": null, "context_relevance_reason": "The context passages provide a comprehensive explanation of how Agentic RAG differs from traditional RAG, specifically highlighting the incorporation of autonomous agents and their capabilities in planning retrieval strategies and multi-step reasoning. This directly addresses the user's question about the differences between the two systems, demonstrating strong relevance and alignment with the evaluation steps.", "structure_citation": 0.6267600946717622, "structure_error": null, "structure_reason": "The Actual Output includes several claims supported by citations [1], [2], [3], and [5], which correspond to snippets in the Context. However, there are unsupported claims made in the output, such as the detailed functionalities of Agentic RAG that are not fully substantiated by the provided Context. Additionally, while some citations match, others do not have clear corresponding snippets, leading to a deduction in points for unsupported claims and potential mismatches.", "hallucination_score": 0.9116312055142106, "hallucination_error": null, "hallucination_reason": "The response effectively distinguishes between normal RAG and agentic RAG, accurately reflecting the claims made in the input. It highlights the static nature of traditional RAG and contrasts it with the dynamic, decision-making capabilities of agentic RAG, which aligns well with the context provided. The explanation of multi-turn retrieval and the role of agents in planning and adapting retrieval strategies is well-supported by the context, demonstrating a strong understanding of the concepts. However, a minor shortcoming is the lack of specific examples or references to the context that could further enhance the explanation.", "contexts_used": ["2 Related Work\n\n2.1 Multimodal Agentic RAG\n\nRetrieval-Augmented Generation mitigates the limitations of large language models in knowledge completeness and factual consistency by incorporating external knowledge sources [10, 11]. Traditional RAG systems, such as Na\u00a8 \u0131ve RAG [12, 13] and Advanced RAG [14, 15], typically follow a static, linear 'retrieve-then-read' workflow. While effective for simple queries, they struggle with complex tasks requiring multi-step reasoning, context-aware adaptation, or tool use [16]. Agentic RAG extends the RAG paradigm by embedding autonomous agents into the retrieval and reasoning process [17]. These agents can plan retrieval strategies [18], invoke external tools [19], reflect and revise outputs [20], and collaborate across multiple roles or modalities [21, 22, 23]. Depending on design, they may operate as single decision-makers (e.g., router agents) or as specialized multi-agent systems for handling heterogeneous data sources. Despite these advances, existing Agentic RAG research in the medical domain has focused predominantly on text or structured data [24, 25], overlooking the diagnostic richness encoded in medical images. However, fields like pathology rely heavily on visual features, patterns in tissue morphology, staining, and spatial arrangements that cannot be captured through text alone.", "2.2 Reinforcement Learning for Medical VLMs\n\nReinforcement learning (RL) provides a promising paradigm for aligning the outputs of VLMs with clinical accuracy requirements, especially in high-risk medical domains where hallucinated descriptions can lead to severe consequences [26]. A core challenge in medical VLMs is ensuring factual alignment between visual evidence (e.g., pathology slides, radiology images) and textual outputs [27, 28, 29].", "However, direct end-to-end RL of large VLMs remains highly impractical. The main limitations include the scarcity of high-quality reward data from physicians [30], the instability of RL training on large models, and the lack of interpretability in the learned behaviors [31]. These issues make it difficult to deploy RL-fine-tuned models safely in clinical settings. To address this, recent approaches have turned to agent-centric strategies, where RL is applied not to the internal parameters of VLMs, but to the decision-making policies of external agents that interact with them. These agents may learn how to craft better queries, validate initial answers, or select relevant external knowledge iteratively [4, 22, 23]. This decoupled optimization process not only reduces risk but also enhances transparency and controllability.", "3 Methodology\n3.3 Multimodal Fusion\n\nExplanation To intuitively understand how Equation (1) influences the re-ranking process, we present the similarity matrices of the first- and second-ranked documents with respect to the query text embedding and image embedding in Figure 3. As shown in Figure 3, the second-ranked document exhibits a more uniform similarity, or attention, across the tokens of the document embedding. In contrast, the first-ranked document demonstrates more concentrated and higher similarity on fewer tokens of the document embedding. In this case, the high similarity of the first-ranked document to the query is meaningful, as it is caused by focused attention on the most informative tokens. On the other hand, the high similarity of the second-ranked document to the query results from more diffuse attention, likely caused by noise. Thus, although the second-ranked feature initially has the highest text similarity in the retrieval process, after re-ranking by the fusion formula, it drops to second place, while the original second-ranked document is promoted to", "Figure 2: An illustration of the multi-turn retrieval and summarization process.\n\nFinal Answer:\n\n<think>The Indian-file pattern is a hallmark of invasive lobular carcinoma, where\n\ntumor cells infiltrate the stroma in single-file lines. Lobular carcinoma cells are\n\ntypically small and uniform with scant cytoplasm and round nuclei. In contrast,\n\nductal carcinoma (B) forms solid nests and trabeculae, papillary carcinoma (C) has\n\nfibrovascular cores in papillary structures, and mucinous carcinoma (D) shows tumor\n\ncells in mucin pools. The observation of small cells, uniform nuclei, and single-file\n\ninfiltration aligns specifically with lobular carcinoma morphology.</think>\n\n<answer>A</answer>\n\nText Query:\n\nSmall, uniform tumor cells\n\ninfiltrate in an Indian-file\n\npattern. Which breast cancer\n\ntype is this?\n\nA. Lobular breast\ncancer\n\nB. Ductal breast cancer\n\nC. Papillary breast cancer\n\nD. Mucinous breast cancer\n\nExternal\nKnowledge\n\nTool Call1: {\"name\": \"rag\", \"parameters\": {\"query\": \"Histological\n\nfeatures of invasive lobular carcinoma of the breast,\n\nincluding\nIndian-file pattern\n\"}}\n\n{\"name\": 'classifier\", \"parameters\": {'partition\": 'Breast\"}}\n\nTool Call2: {\"name\": \"rag\", \"parameters\": {\"query\": \"How to\n\ndifferentiate ductal , papillary , and mucinous breast carcinoma histologically\"}} Text Retrieval\n\nMax Sim.\n\n\u2026\n\nTop20 Pages for each query\n\nPatho-R1 Pathology Reasoning Model Rerank by the similiary of image query Crop\n\n& summarize info.\n\nDatabase\n\n:\n\nover 200k pages of textbook over 150m vectors Vrag Agent Multi-Turn Intercation Think\n\nImage Query:", "Ground Truth: A\n\nQ1: Histological features of invasive lobular\n\ncarcinoma of the breast, including Indian-file\n\npattern\n\nA: Histological features of invasive lobular\n\ncarcinoma of the breast include uniform small\n\nround tumor cells infiltrating the stroma in a\n\nsingle-file (Indian-file) arrangement and\n\ncircumferentially around ducts in a target-like\n\npattern.\n\nQ2: How to differentiate ductal, papillary, and\n\nmucinous breast carcinoma histologically\n\nA: Histologically, ductal carcinoma shows glandular,\n\npapillary, cribriform, or diffuse growth patterns,\n\noften forming nests, trabeculae, or cords. Papillary\n\ncarcinoma is characterized by prominent papillary\n\nstructures with central fibrovascular cores.\n\nMucinous carcinoma displays abundant extracellular\n\nmucin, with tumor cells floating loosely in mucin\n\npools.\n\nSummarized Information\nInvasive lobular\ncarcinoma\n\n.\n\nCharacteristic\nhistologic features\n\nare: one cell wide\n\nfiles of round regular tumour cells ( Indian file )\n\nTop1 for tool call 1\n\nand lobular carcinomas represent75% and 25% of metastases\u2026, , cribriform or diffuse patterns\u2026 mucinous carcinoma B Hybrid endometrioid- like/\n\nTop1 for tool call 2\n\nAgentic Router(ours)\n\nthe top rank due to its combination of high similarity and concentrated attention.", "3.4 Agentic Diagnostic Workflow with Multimodal Evidence Tracing\n\nOur system adopts an intelligent multi-agent workflow that transforms a diagnostic query into an evidence-grounded conclusion. Given a user input (e.g., a pathology image and candidate diagnoses), the Agentic Router module first parses the query, decomposes it into sub-tasks aligned with each diagnostic candidate, and formulates a high-level retrieval plan. It delegates retrieval and evidence aggregation to the VRAG Agent , which operates under its guidance.", "The VRAG Agent executes a multi-turn retrieval process against the multimodal knowledge base. It first conducts text-based retrieval using candidate-specific keywords, and then re-ranks the retrieved entries by evaluating imagetext similarity with the query. Through iterative evidence refinement and summarization, as illustrated in Figure 2, the agent constructs a structured prompt containing the top 1ranked visual evidence for each candidate. This prompt is then passed to a specialized vision-language model, which performs contrastive reasoning to produce a diagnosis and evidence-grounded report. Details of each module and step are provided in Appendix B.", "3.5 Tool Integrated RL for Agentic Router\n\nTraditional RAG systems are often treated as static pipelines, without adapting their behavior to each query's complexity. To overcome this limitation, we introduce a reinforcement learning (RL) framework that enables an agent to learn dynamic invocation and routing strategies [35]. The agent's task is to generate a decision path specifying whether\n\nFigure 3: Illustration of the reranking process utilizing modality fusion.\n\nOriginal Text Query:\n\nThis image shows\nmonophasic synovial sarcoma\n\n. What are its key histologic features?\n\nA.", "Dense, uniform spindle cells with scant cytoplasm in short, intersecting fascicles.\n\nB.\n\nBiphasic: glandular epithelium with spindle cell stroma.\n\nC.\n\nScattered adipocytes in fibrous stroma with atypia.\n\nD.\n\nLarge polygonal cells with coarse eosinophilic granules.\n\nRewrited Search Query:\n\nWhat are the histological features of\n\n?\n\nEach query token\n\nembedding is matched to the\n\nmost similar image-patch embedding\n\non\n\nindexed pages, and their similarity scores are summed to rank the pages.\n\nPatho-Fusion\n\nScore:\n\nSimilarity Matrix Image 0.6638 0.6671 Final Top1 Cytologic appearance of Ewing sarcoma\n\n/PNET ...\n\n0.5878 Top20 from \u2026\u2026 Monophasic synovial\n\nsarcoma.\n\nThe tumor\n\n...\n\n0.6558\n\u2026 monophasic\nsynovial sarcoma\n\nwith ...\n\n0.6190 0.6602 Lose with an adenocarcinoma-like appearance of the\n\nepithelial component.", "and how to call the RAG system. Formally, given an input query Q orig, the agent's policy \u03c0 outputs a path P , optimized to maximize the expected similarity to a ground-truth decision path P gt\n\nThe hierarchical reward R final compares the generated path to the target path step by step (see Algorithm 1). The agent performs a sequence of decisions to construct the path:\n\nDecision 1: Whether to invoke RAG?", "-Path A (No RAG Invocation): If the agent decides False , the decision process terminates. This is for simple queries that can be answered without external knowledge. Final path: { rag : False }\n\n-Path B (Invoke RAG): If True , proceed to the next decision.", "Decision 2: How to decompose the task? (Only applies if RAG is invoked)\n\nThe agent may choose to rewrite the query one or more times to better surface its core semantics and improve retrieval quality. This is not a mechanical rewriting process, but a targeted transformation to better align with the retrieval engine.\n\n\u2022\n\nDecision 3: Whether to use a tissue-specific classifier? (Only applies if RAG is invoked) The agent decides whether to enable a classifier to restrict retrieval to a relevant knowledge partition.", "-Path B.1 (Global Retrieval): If False , RAG retrieves documents from the full corpus. Final path: { rag : True , rewrite count : n, classifier : False }\n\n-Path B.2 (Classifier-Based Retrieval): If True , proceed to the next decision.", "ABSTRACT\n\nRetrieval-Augmented Generation (RAG) mitigates hallucination in LLMs by incorporating external knowledge, but relies on chunk-based retrieval that lacks structural semantics. GraphRAG methods improve RAG by modeling knowledge as entity-relation graphs, but still face challenges in high construction cost, fixed one-time retrieval, and reliance on long-context reasoning and prompt design. To address these challenges, we propose Graph-R1 , an agentic GraphRAG framework via end-to-end reinforcement learning (RL). It introduces lightweight knowledge hypergraph construction, models retrieval as a multi-turn agent-environment interaction, and optimizes the agent process via an end-to-end reward mechanism. Experiments on standard RAG datasets show that Graph-R1 outperforms traditional GraphRAG and RL-enhanced RAG methods in reasoning accuracy, retrieval efficiency, and generation quality. Our code is publicly available 1 .", "8 Discussion and Future Direction\n\n8.2 Implications for Proprietary Data\n\nA key development in RAG is its application to enterprise AI, enabling access to proprietary data without embedding sensitive information into model parameters [35]. This architecture supports data privacy and ensures outputs remain grounded in up-to-date knowledge [74]. Unlike traditional LLMs, which require retraining to update internal knowledge, RAG allows retrieval components to be independently refreshed [74].", "However, enterprise RAG deployments introduce new security concerns. The retriever could inadvertently expose confidential information if strict access controls are not enforced. Organizations must implement robust authentication mechanisms, ensuring that retrieved documents align with user permissions. Privacy-preserving techniques such as encrypted search indices and federated retrieval [58] are promising solutions to mitigate risks.", "The ability of RAG to function over proprietary knowledge bases has made it a preferred choice for industries handling sensitive information, including finance, healthcare, and legal sectors. As enterprises scale RAG systems, optimizing retrieval latency and ensuring regulatory compliance will be paramount.", "8.3 Future Research Directions\n\nKey avenues for advancing RAG include:\n\nMulti-hop retrieval : RAG pipelines typically retrieve single passages, but many queries require reasoning over multiple documents. Standard RAG models struggle with multi-hop questions [88], motivating research on multi-step retrieval and reasoning (e.g., chained or iterative RAG methods).", "Secure and privacy-aware retrieval : As RAG is applied to sensitive data, privacy becomes critical. Recent work proposes encrypting knowledge bases and embeddings to prevent unauthorized access while preserving performance [107], and integrating differential privacy and secure computation to guard against information leakage [42].", "Multimodal and agentic RAG : Extending RAG beyond text can leverage rich context. Multimodal RAG (MRAG) systems incorporate images, video, or other modalities into retrieval and generation, reducing hallucinations and improving performance on visual queries [59]. Separately, Agentic RAG embeds autonomous agents into the retrieval pipeline, enabling dynamic planning, tool use, and multi-step reasoning in the loop [83].", "Structured knowledge integration : Traditional RAG relies on vector search over unstructured text, which can miss complex relationships. Incorporating structured knowledge (e.g., knowledge graphs) can improve semantic understanding. Surveys and systems demonstrate that linking RAG to knowledge graphs yields more accurate and coherent answers [14, 24].", "Real-time or streaming retrieval : Many applications require RAG to operate on live data streams. For example, systems like StreamingRAG build evolving knowledge graphs from streaming inputs to provide timely context updates, achieving much faster throughput and higher temporal relevance than static RAG [79].", "8.4 Emerging Practical Applications\n\nRAG is finding use in a range of new applications:\n\nEnterprise search : Companies employ RAG to query internal document repositories. By building RAG pipelines around enterprise vector databases, organizations can retrieve and", "synthesize answers from private knowledge with strong performance, scalability, and security guarantees [89].\n\nReal-time assistants : Personal or domain-specific AI assistants can use RAG to retrieve current information (e.g., news, stock prices, weather) on the fly. The RAG framework naturally supports grounding LLM responses in real-time data, improving accuracy and user trust [33].", "Fact-checking : Integrating RAG into fact-checking pipelines can automate evidence retrieval. Recent systems use RAG-based reasoning (e.g., chain-of-RAG) to fetch and synthesize evidence for claims, improving veracity predictions in political and news domains [1].", "End-to-End Agentic RAG System Training for Traceable Diagnostic Reasoning\n\nQiaoyu Zheng 1,2 , Yuze Sun 1 , Chaoyi Wu 1 , Weike Zhao 1,2 , Pengcheng Qiu 1,2 , Yongguo Yu 3 , Kun Sun 3 , Yanfeng Wang 1,2 , Ya Zhang 1,2, \u2020 and Weidi Xie 1,2, \u2020\n\n3 Xinhua Hospital affiliated to Shanghai Jiao Tong University School of Medicine, Shanghai, China\n\nShanghai Jiao Tong University, Shanghai, China Shanghai AI Laboratory, Shanghai, China", "End-to-End Agentic RAG System Training for Traceable Diagnostic Reasoning\n\nAccurate diagnosis remains a central challenge for medical large language models due to inherent knowledge limitations and hallucinations. While retrieval-augmented generation (RAG) and tool-augmented agentic methods show potential in mitigating these issues, their suboptimal utilization of external knowledge and the decoupling of the feedback-reasoning traceability, stemming from insufficient supervision, remain key limitations. To address these challenges, We introduce Deep-DxSearch, an agentic RAG system trained end-to-end with reinforcement learning (RL) that enables steer tracebale retrieval-augmented reasoning for medical diagnosis . In Deep-DxSearch, we first construct a large-scale medical retrieval corpus comprising patient records and reliable medical knowledge sources to support retrieval-aware reasoning across diagnostic scenarios. More crucially , we frame the LLM as the core agent and the retrieval corpus as its environment, using tailored rewards on format, retrieval, reasoning structure, and diagnostic accuracy, thereby evolving the agentic RAG policy from large-scale data through RL.", "End-to-End Agentic RAG System Training for Traceable Diagnostic Reasoning\n\nExperiments demonstrate that our end-to-end agentic RL training framework consistently outperforms prompt-engineering and training-free RAG approaches across multiple data centers. After training, DeepDxSearch achieves substantial gains in diagnostic accuracy, surpassing strong diagnostic baselines such as GPT-4o, DeepSeek-R1, and other medical-specific frameworks for both common and rare disease diagnosis under in-distribution (ID) and out-of-distribution (OOD) settings. Moreover, ablation studies on reward design and retrieval corpus components confirm their critical roles, underscoring the uniqueness and effectiveness of our approach compared with traditional implementations. Finally, case studies and interpretability analyses highlight improvements in Deep-DxSearch's diagnostic policy, providing deeper insight into its performance gains and supporting clinicians in delivering more reliable and precise preliminary diagnoses. Data, code, and checkpoints are available at https://github.com/MAGIC-AI4Med/Deep-DxSearch .", "6 Conclusion\n\nIn this work, we proposed the QA-Dragon , a query-aware dynamic RAG system tailored for complex and knowledge-intensive VQA tasks. Unlike the traditional RAG system, which retrieves the text and image separately, the QA-Dragon integrates a domain router, a search router, and orchestrates hybrid retrieval strategies via dedicated text and image search agents. Our system supports multimodal, multi-turn, and multi-hop reasoning through orchestrating both text and image search agents in a hybrid setup. Extensive evaluations on the Meta CRAG-MM Challenge (KDD Cup 2025) demonstrate that our system significantly improves answer accuracy, surpassing other strong baselines."]}, {"type": "entry", "index": 21, "question": "how are people combining rag with graph databases?", "answer": "People are combining RAG with graph databases by turning raw documents into structured graphs and then using those graphs as the main retrieval layer before generation.\n\n1) Build a knowledge graph from documents\n- Systems extract subject\u2013predicate\u2013object triples (like \u201cBank X \u2013 must report \u2013 transaction Y\u201d) from regulatory or domain documents and store them as nodes and edges in a graph [3].\n- In some domains, they also include images and visual relations, forming a multimodal knowledge graph, not just text [1].\n- For medical use cases, tools (e.g., Gemini) are used to generate knowledge graphs that capture relationships between diseases, treatments, and outcomes from guidelines and papers [5].\n\n2) Index the graph for RAG\n- The graph elements (triples, entities, relations) are embedded and stored alongside the original text sections and metadata in a unified vector database. This lets the system do both graph-style reasoning and vector search with one store [3].\n- RAG research explicitly highlights \u201cintegration of structured knowledge bases like databases or graphs\u201d as a key evolution beyond pure text retrieval [4].\n\n3) Graph-aware retrieval before generation\n- Some frameworks use a dual-stage process: first recall relevant documents, then retrieve fine-grained entities and relationships from the graph, often using a specialized retriever tuned to the question [1].\n- Other systems perform triplet-level retrieval so the LLM sees the precise \u201cwho-did-what-to-whom\u201d facts, improving alignment and traceability for complex regulatory questions [3].\n\n4) Generation with structured context\n- The LLM (or multimodal LLM) is then prompted with both the raw text snippets and the graph-derived facts/subgraphs, improving factual accuracy, reasoning over relationships, and explainability [1][3][5].", "tags": ["_untagged"], "full_faithfulness": 1.0, "full_answer_relevancy": 0.9823590249, "seed_faithfulness": 0.8571428571, "seed_answer_relevancy": 0.9823590249, "context_relevance": 0.8595550480858849, "context_relevance_error": null, "context_relevance_reason": "The context passages provide relevant information about the integration of Retrieval-Augmented Generation (RAG) with multimodal knowledge graphs (KGs) specifically for knowledge-based Visual Question Answering (VQA) tasks. This directly addresses the user's question about how RAG is combined with graph databases. However, while the passages are informative, they could be more explicitly focused on practical examples or applications of this integration, which would enhance the relevance to the user's inquiry.", "structure_citation": 0.8386400428116669, "structure_error": null, "structure_reason": "The Actual Output effectively incorporates claims supported by the Context, with multiple [n] markers corresponding to relevant snippets. However, while most claims are well-supported, there are minor instances where the connections could be clearer, particularly in the explanation of the dual-stage retrieval process. Overall, the response demonstrates a strong understanding of how RAG integrates with graph databases, but slight improvements in citation clarity could enhance the overall alignment.", "hallucination_score": 0.8970637544747755, "hallucination_error": null, "hallucination_reason": "The response effectively outlines how RAG is combined with graph databases by detailing the process of building knowledge graphs, indexing them for retrieval, and using graph-aware retrieval methods. It aligns well with the claims made in the input, demonstrating a clear understanding of the integration of structured knowledge into the RAG framework. The output supports the context provided, particularly in addressing the challenges of unstructured data in knowledge-intensive tasks. However, a minor shortcoming is the lack of specific examples or case studies that could further illustrate the practical applications of these methods.", "contexts_used": ["Abstract\n\nRecently, Retrieval-Augmented Generation (RAG) has been proposed to expand internal knowledge of Multimodal Large Language Models (MLLMs) by incorporating external knowledge databases into the generation process, which is widely used for knowledge-based Visual Question Answering (VQA) tasks. Despite impressive advancements, vanilla RAG-based VQA methods that rely on unstructured documents and overlook the structural relationships among knowledge elements frequently introduce irrelevant or misleading content, reducing answer accuracy and reliability. To overcome these challenges, a promising solution is to integrate multimodal knowledge graphs (KGs) into RAG-based VQA frameworks to enhance the generation by introducing structured multimodal knowledge. Therefore, in this paper, we propose a novel multimodal knowledge-augmented generation framework ( mKG-RAG ) based on multimodal KGs for knowledge-intensive VQA tasks. Specifically, our approach leverages MLLM-powered keyword extraction and vision-text matching to distill semantically consistent and modality-aligned entities/relationships from multimodal documents, constructing high-quality multimodal KGs as structured knowledge representations. In addition, a dual-stage retrieval strategy equipped with a question-aware multimodal retriever is introduced to improve retrieval efficiency while refining precision. Comprehensive experiments demonstrate that our approach significantly outperforms existing methods, setting a new state-of-the-art for knowledge-based VQA.", "1 Introduction\n\nVisual Question Answering (VQA) [2, 19] is a challenging task at the intersection of vision and language understanding, requiring models to interpret images and answer related questions. This capability has enabled remarkable advances in various domains, including medical image diagnosis [33] and customer service support [12]. Recently, due to the powerful visual-linguistic understanding and reasoning capabilities, Multimodal Large Language Models ( MLLMs ) [35, 30, 50, 7] have provided a promising solution to conventional VQA tasks. For instance, LLaVA [35] demonstrates strong\n\n\u2217 Corresponding author", "zero-shot performance on commonsense VQA by leveraging pre-trained visual encoders for image representation alongside the reasoning capabilities of large language models (LLMs). Despite notable advancements, MLLMs face critical limitations in knowledge-intensive VQA scenarios [40, 6] (termed knowledge-based VQA ), particularly those requiring encyclopedic knowledge, long-tail factual recall, or contextual reasoning beyond immediate visual inputs. As illustrated in Figure 1 (a), when queried about the latest renovation date of a stadium, typical MLLMs exhibit two characteristic failure modes: generating plausible but factually incorrect responses or refusing to answer altogether. These issues stem from the scarcity of relevant knowledge in MLLMs' training corpus and the inherent difficulty of memorizing low-frequency facts [6].", "Recently, Retrieval-Augmented Generation ( RAG ) [16] has shown great potential in addressing these challenges by leveraging external knowledge databases to supplement the internal knowledge of MLLMs, thereby enabling more accurate answer generation [32, 4, 11]. Specifically, multiple query-relevant documents are retrieved from external knowledge databases and serve as in-context information to augment the generation process of MLLMs. Despite their success, vanilla RAGbased VQA methods that rely on unstructured documents or paragraphs often introduce irrelevant or even misleading information [38, 51], thereby compromising the accuracy and reliability of generated answers. Moreover, these approaches typically overlook the structural relationships among knowledge elements, limiting the reasoning capabilities of MLLMs. As illustrated in Figure 1 (b), the presence of noisy and unstructured context makes it difficult for MLLMs to identify and leverage relevant supporting evidence. To overcome these limitations, a promising direction is to retrieve structured knowledge, such as Knowledge Graphs (KGs) [23] for augmentation generation [22, 15, 59]. However, in the VQA setting, which inherently involves multimodal reasoning, relying solely on textual KGs is suboptimal, as both modalities are crucial for identifying relevant knowledge. Therefore, integrating Multimodal Knowledge Graphs into the retrieval-augmented VQA framework presents a more robust solution to generate reliable and precise responses in knowledge-intensive scenarios, as illustrated in Figure 1 (c).", "However, retrieving relevant knowledge from multimodal knowledge graphs to enhance the generation of knowledge-based VQA tasks is exceptionally challenging. First, offthe-shelf multimodal KGs [36] are generally built around common entities, and often lack the encyclopedic or long-tail knowledge required by knowledge-intensive questions, rendering them ineffective for direct use in knowledge-based VQA. Moreover, current knowledge sources used in knowledge-based VQA [40, 6] are typically organized in unstructured documents containing substantial contextual noise, making it challenging to extract well-structured entities and relationships essential for constructing high-quality multimodal KGs. Furthermore, a large-scale knowledge graph constructed from millions of documents, each potentially containing hundreds of entities and relationships, significantly expands the search space. Consequently, performing direct retrieval over such a graph is computationally inefficient and adversely affects retrieval precision.", "To address the challenges above, this paper proposes mKG-RAG , a novel\n\nFigure 1: Illustration of issues in knowledge-based VQA. (b) Vanilla RA methods suffer from retrieving unstructured knowledge from external documents via unimodal retrievers. (c) Our mKG-RAG augments MLLMs with structural information from multimodal knowledge graphs.\n\nVisual-only\nSearch\nI can't give\n\nthe information.\n\nWhen were the latest renovations of this\n\nstadium? [Answer: 2010] In 2012 Factual Error Lack of Knowledge (a) Knowledge-based VQA\n\n(b) Vanilla RAG-based VQA Framework\n\n(c) mKG-RAG: RAG with Multimodal Knowledge Graphs\n\nIn 2018 In 2010 Textual-only Multimodal Graph Traversal Ski Stadium Marcialonga\n\nRelation:\n\nthe stadium was\n\nrenovated in 2010 for the\n\nWorld Ski Championship.\n\nSkier Word Ski Championship Cross-country Ski Stadium\n\nThe Cross country and\n\nbiathlon center Fabio Can\u00e0l,\n\nuntil December 2018 named\n\nLago di T\u00e9sero Cross Coun-\n\ntry Stadium is a sport venue\n\nlocated in the village of ...\n\nUnstructured Documents\nStructured Knowledge Graphs\nInaccurate Retrieval", "retrieval-augmented generation framework integrated with multimodal knowledge graphs designed to enhance the reasoning capabilities of MLLMs in knowledge-based VQA tasks. More specifically, a multimodal knowledge graph construction module is introduced to transform unstructured multimodal documents, such as Wikipedia articles, into structured knowledge representations. This module leverages MLLM-powered keyword extraction and vision-text alignment to extract semantically consistent and modality-aligned entities and relationships from external multimodal documents. To enable efficient retrieval, mKG-RAG develops a dual-stage search paradigm that combines a coarse-grained document recall and a fine-grained entity/relationship retrieval. The coarse stage efficiently narrows the search space by identifying candidate documents likely to contain relevant evidence, while the fine stage refines the results by retrieving query-relevant entities and relationships from multimodal KGs that are dynamically constructed from these potentially noisy documents. During retrieval, unlike previous methods that rely on isolated unimodal retrievers, we introduce a question-aware multimodal retriever trained on a high-quality question-evidence dataset to further enhance retrieval precision within the proposed search paradigm. Comprehensive evaluations on two frequently used benchmarks demonstrate the superior performance of mKG-RAG, achieving an accuracy of 36.3% on E-VQA and 40.5% on InfoSeek.\n\nThe contributions of this work are summarized as follows:", "We propose mKG-RAG, a novel multimodal knowledge-augmented generation framework that integrates RAG with multimodal KGs to enhance the knowledge reasoning of MLLMs. To the best of our knowledge, this is the first work to investigate the potential of multimodal knowledge graphs in knowledge-intensive VQA tasks.", "Our framework develops a multimodal KG construction pipeline, allowing the extraction of image-text aligned entities and relations from multimodal documents. Additionally, a dual-stage retrieval schema with a question-aware multimodal retriever enables us to unleash the potential of RAG incorporated with multimodal KGs.", "Extensive experiments demonstrate that mKG-RAG significantly outperforms strong baselines, setting new state-of-the-art results on E-VQA and InfoSeek.", "2 RELATED WORK\n2.1 Vision-Language Models\n\nVision-language models (VLMs) have emerged as powerful tools for multimodal tasks, including deepfake detection. The rapid advancement of large language models (LLMs) such as GPT-4 [1] and LLaMA [66] has driven the evolution of VLMs, enabling seamless integration of vision and language modalities. Early approaches to VLMs, such as CLIP [52], utilized contrastive learning to align vision and language representations in a shared embedding space. Subsequent models like BLIP [37] introduced more advanced pretraining strategies, incorporating retrieval-augmented learning and cross-attention mechanisms to enhance contextual understanding.", "Recent advancements, such as LLaVA [40] and Qwen2-VL [68], have integrated ViT with LLMs, enabling vision-conditioned text generation and multimodal reasoning. Building on these foundations, models like Qwen2.5-Omni [77] introduce novel architectures and techniques to handle complex multimodal tasks, such as video dialogue and reasoning, while maintaining strong performance in visual and linguistic understanding.", "2.2 Deepfake Detection Methods\n\nImage deepfake detection has evolved significantly with advancements in AI technologies. Traditional methods [6, 9, 67], primarily based on Convolutional Neural Networks (CNNs), focus on identifying pixel-level anomalies such as unnatural textures and lighting inconsistencies. These Non-VLM-based approaches, exemplified by models like XceptionNet [10] and MesoNet [2], have shown high precision in specific scenarios but often struggle with generalization due to their reliance on dataset-specific features.", "More recently, the integration of VLMs has introduced a new paradigm in deepfake detection. VLM-based methods leverage the reasoning and cross-modal capabilities of these models to provide more flexible and interpretable detection. Models such as AntifakePrompt [7], ForgeryGPT [42], ForgeryTalker [38], ForgerySleuth [63], SIDA [27] and So-Fake-R1 [28] utilize the capabilities of VLM to enhance detection accuracy and provide robust support for localization and explanation tasks. In the video domain, BusterX [74] and BusterX++ [75] are outstanding representatives. Most of these methods rely heavily on extensive manual annotation of masks or textual explanatory descriptions. Additionally, the interpretable descriptions learned by current models tend to be coarse. In contrast,", "Figure 2: Framework of RAIDX: It integrates retrieval-augmented prompting with GRPO-enhanced reasoning for deepfake detection. A ViT and RAG module construct informative prompts using similar images, which are processed by a partially trainable LLM to output saliency-guided localization and fine-grained explanations without requiring any supervision.\n\nViT\n\nIs this image real?\n\nPrompt Engineering\n\nPlease carefully analyze this image\n\nand provide a detailed explanation.\n\nLLM\nLoRA(GRPO Training)\n<think>\n\nThe image depicts a cowboy on horseback chasing a steer through a dusty environment, likely during a rodeo or cattle drive scene. The lighting\n\nsuggests a sunset or sunrise, casting a warm glow over the scene. The dust kicked up by the animals adds to the realism of th e setting. \\n\\n###\n\nAnalysis:\\n1. **Lighting and Shadows**: While the warm directional lighting appears consistent, subtle inconsistencies are visible around the\n\nhorse's hooves and the steer's legs, where shadows and contact points seem unnaturally soft or floating,\n\nsuggesting an imprecise simulation of\n\nground interaction. \\n2. **Anatomical Irregularities**: The hat shows unnatural curvature and blending with the background light, especially\n\naround the edges. The face lacks realistic texture, and fine facial details.", "The horse's face appears slightly stylized, with a glossy finish and\n\ndisproportionately smooth edges around the nostrils and eyes. The hooves, do not deform the dust or ground as expected and cast shadows that\n\nappear artificially rendered\n\u2026\u2026\n\n\\n\\n### Conclusion:\n\nDespite its strong visual coherence, this image reveals several key synthetic artifacts\n\n-\n\nincluding smooth facial features, anatomically\n\nimplausible hooves and tails, and overly uniform dust patterns. These features strongly indicate that the image is AI-generated rather than a real\n\nphotographic capture </think> <answer>FAKE</answer> RAG Tokenizer RAIDX Trainable Frozen\n\nRAIDX relies on the GRPO reinforcement learning strategy, which can produce relatively accurate saliency maps and fine-grained textual explanation outputs without requiring any manual text or mask annotations.", "2.3 Retrieval-Augmented Generation\n\nRetrieval-Augmented Generation (RAG) [21] is a general paradigm which enhances LLMs by including relevant information retrieved from external databases into the input. RAG typically consists of three phases: indexing , retrieval , and generation . Indexing constructs external databases and their retrieval index from external data sources. Retrieval utilizes these indexes to fetch the relevant document chunks as context, given a user query. Generation integrates the retrieved context into the input prompt for LLMs, and LLMs then generate the final output based on the augmented inputs. RAG has been widely used in various domains [34, 61, 81]. Although RAG has achieved many successful cases in LLM tasks, there are few examples [45, 51] of its integration with VLMs. To the best of our knowledge, RAIDX is the first to apply RAG in the deepfake detection task.", "2.4 Reinforcement Learning in VLMs\n\nThe integration of reinforcement learning (RL) into multimodal systems has emerged as a promising direction, particularly for tasks requiring alignment between vision and language. Recent advances in techniques such as Proximal Policy Optimization (PPO) [57] and Group Relative Policy Optimization (GRPO) [15] have demonstrated their potential to enhance reasoning quality and factual correctness in language models, often by optimizing model outputs through structured feedback. GRPO, in particular, has gained significant attention due to its effectiveness in improving the reasoning capabilities of LLMs by leveraging rule-based rewards. In the visual domain, the application of RL to VLMs has shown substantial promise [8, 26, 60]. Experimental results from the VLM-R1 [60] demonstrate that RL-based models not only deliver competitive performance on visual understanding tasks but also surpass supervised fine-tuning (SFT) in generalization ability. To the best of our knowledge, RAIDX is the first to apply the GRPO reinforcement learning training strategy to the field of deepfake detection, which significantly reduces reliance on manual mask annotations and text annotations-forming the foundation of our annotation-efficient approach.", "Abstract\n\nRegulatory compliance question answering (QA) requires precise, verifiable information, and domainspecific expertise, posing challenges for Large Language Models (LLMs). In this work, we present a novel multi-agent framework that integrates a Knowledge Graph (KG) of Regulatory triplets with RetrievalAugmented Generation (RAG) to address these demands. First, agents build and maintain ontology-free KG by extracting subject-predicate-object (SPO) triplets from regulatory documents and systematically cleaning, normalizing, deduplicating, and updating them. Second, these triplets are embedded and stored along with their corresponding textual sections and metadata in a single enriched vector database, allowing for both graph-based reasoning and efficient information retrieval. Third, an orchestrated agent pipeline leverages triplet-level retrieval for question answering, ensuring high semantic alignment between user queries and the factual 'who-did-what-to-whom' core captured by the graph. Our hybrid system outperforms conventional methods in complex regulatory queries, ensuring factual correctness with embedded triplets, enabling traceability through a unified vector database, and enhancing understanding through subgraph visualization, providing a robust foundation for compliance-driven and broader auditfocused applications.", "Retrieval-Augmented Generation (RAG)\n\nRAGis a hybrid approach that integrates retrieval-based and generation-based techniques to improve the performance of language models on knowledge-intensive tasks. By leveraging external knowledge sources, RAG enables models to produce more accurate and contextually relevant responses. The paradigm has gained significant attention due to its ability to combine the strengths of retrieving pertinent documents and generating coherent text. In the real-world, RAG systems are widely adopted with online web search ( e.g., OpenAI (Hurst et al. 2024), and Gemini (Comanici et al. 2025)) and industrial domains with credential documents.", "Early research on RAG established its effectiveness across various natural language processing tasks (Lewis et al. 2020). Subsequent studies have proposed advancements, such as improved retrieval mechanisms using dense retrieval methods (Karpukhin et al. 2020) and the integration of structured knowledge bases like databases or graphs (Edge et al. 2024). RAG has also shown promise in multi-task and few-shot learning scenarios (Izacard et al. 2023), where the retrieval component compensates for limited training data by accessing external information. However, challenges remain, particularly in optimizing the retrieval process, which depends heavily on the quality of the input query-an issue that motivates the exploration of query rewriting (Ma et al. 2023).", "4 Method\n4.1 Datasets\n\nIn this study, we collected 3,683 AP spinal X-rays with corresponding radiological reports from 3,022 patients at X Hospital between December 2019 and July 2023, with all data collection protocols approved by the Institutional Review Board (IRB). The dataset was randomly partitioned into training, validation, and testing sets following an 8:1:1 ratio, resulting in 2,946 samples for training, 368 samples for validation, and 369 samples for testing. To address the inherent class imbalance common in medical datasets, we implemented a stratified sampling approach that maintained consistent distribution of scoliosis severity across all partitions, ensuring that the proportion of each scoliosis category (normal, mild, moderate, and severe) remained constant across all sets. To minimise input image variance, all X-rays were standardized by cropping to a uniform size of 896 \u00d7 448 pixels, ensuring that the entire spinal column was captured within each image.", "4.2 Visual Prompting Strategies\n\nWe established MLLMs zero-shot predictions as baselines and investigated how performance is affected by visual prompts provided by spine keypoint detection", "models. As shown in Figure 1, we designed three visual prompting strategies: Curved Spine Midline (CSM), Vertebral Connection Line (VCL), and Segmented Vertebrae Marks (SVM). These visual prompts were designed to provide models with critical information about spinal curvature and structure. For SDLD and SDDDtasks, we differentiated thoracic, thoracolumbar, and lumbar regions using distinct colors to provide anatomical localization information.", "4.3 Retrieval-Augmented Generation (RAG) Approach\n\nTo enhance model performance on knowledge-intensive tasks, we implemented a specialized RAG framework utilizing an AIS-specific knowledge database. This database was constructed by integrating authoritative sources including clinical practice guidelines, research publications from PubMed, and patient education resources from organizations such as the Scoliosis Research Society (SRS) [15, 24, 23, 2, 6]. To optimize information retrieval, we employed Gemini to generate structured knowledge graphs that capture key relationships between AIS concepts, treatments, and outcomes [25].", "4.4 DKA and PECA Data Population\n\nOur evaluation datasets were designed to assess different aspects of AIS understanding. The DKA dataset consists of multiple-choice questions targeting professional medical knowledge, while the PECA dataset simulates patient-centered scenarios requiring both clinical accuracy and appropriate communication. To ensure comprehensive coverage, we developed questions spanning diagnosis, treatment options, management strategies, and daily living accommodations for patients with varying degrees of spinal deformity. A rigorous quality control process involving two junior doctors and verification by a senior physician was implemented to ensure clinical relevance, accuracy, and appropriate difficulty levels across all questions.", "4.5 Evaluation\n\nFor the VSA task, we addressed dataset class imbalance by employing a comprehensive evaluation approach combining F1 score, AUC, and accuracy metrics. This multi-metric strategy ensures balanced assessment of model performance across both majority and minority classes. For the DKA task, we used accuracy as our primary performance metric, as it directly measures the models' proficiency in selecting correct answers within the multiple-choice format. In evaluating the PECA task, we implemented a structured human evaluation protocol wherein three junior physicians independently assessed model responses using a five-dimensional framework on a 5-point Likert scale. The detailed assessment criteria, outlined in Table 1, were specifically designed to evaluate both clinical accuracy and communication effectiveness, providing comprehensive insight into models' capabilities in addressing patient concerns across varying degrees of spinal deformity."]}], "worst": [{"type": "entry", "index": 2, "question": "Which benchmarks best capture hallucination rates in hybrid RAG systems?", "answer": "No single benchmark perfectly captures hallucination rates in hybrid RAG systems, but several families come close from different angles.\n\nFor RAG-style agents (including hybrid or multi-modal), challenge-style setups that explicitly log \u201challucination rate\u201d at the query level are the most direct. The CRAG\u2011MM 2025 setting tracks per\u2011query hallucinations, missing answers, and accuracy, and reports a hallucination rate alongside a truthfulness score for each system variant, making it very suitable for evaluating how often a RAG pipeline fabricates content versus abstains or answers correctly [3].\n\nFact\u2011checking/verification datasets such as FEVER, HoVeR, QuanTemp, and AveriTeC are also strong proxies because they require comparing model claims to retrieved evidence and judging whether they are supported, refuted, or unverifiable\u2014exactly the distinction underlying hallucinations in RAG [4]. These capture whether a hybrid system\u2019s generated claims stay faithful to evidence.\n\nEnd\u2011to\u2011end RAG evaluation frameworks like RAGAS, ARES, and CRUX explicitly assess answer faithfulness to retrieved documents and analyze how retrieval errors propagate into generation, which is central for measuring hallucinations in hybrid RAG pipelines [4].\n\nMetaRAG\u2019s own enterprise dataset is an example of a targeted hallucination benchmark, with human labels of whether responses are hallucinated given retrieved context, but it is private and the authors note the need for public benchmarks with minimal pretraining overlap and finer-grained factoid labels to better evaluate hallucinations in RAG systems [1].\n\nFinally, broader factuality benchmarks like TruthfulQA and BioGEN, used in DSCC\u2011HS, track factual consistency and hallucination scores in long-form generation, and can complement RAG-specific tests even though they are not RAG-only benchmarks [2].", "tags": ["hybrid_vs_kg", "benchmarks_eval"], "full_faithfulness": 1.0, "full_answer_relevancy": 0.0, "seed_faithfulness": 1.0, "seed_answer_relevancy": 0.0, "context_relevance": 0.7386454574615074, "context_relevance_error": null, "context_relevance_reason": "The context passages provide substantial information about hallucination detection in hybrid RAG systems, particularly through the evaluation of MetaRAG. The passages discuss experiments, datasets, evaluation protocols, and metrics that are directly relevant to understanding how hallucination rates are captured. However, while the context is informative, it could be more explicitly focused on identifying specific benchmarks that capture hallucination rates, which would strengthen the alignment with the user's question.", "structure_citation": 0.7524463226888918, "structure_error": null, "structure_reason": "The Actual Output effectively discusses various benchmarks for capturing hallucination rates in hybrid RAG systems and includes citations [1], [2], [3], and [4]. Each citation corresponds to relevant snippets in the Context, supporting the claims made. However, while the claims are generally well-supported, there is a slight lack of clarity in how some benchmarks are directly linked to the specific evaluation of hallucination rates, which prevents a perfect score.", "hallucination_score": 0.8566786756537, "hallucination_error": null, "hallucination_reason": "The response accurately identifies multiple benchmarks that capture hallucination rates in hybrid RAG systems, aligning well with the context provided. It discusses specific benchmarks like CRAG-MM 2025, FEVER, and MetaRAG, which are relevant to the evaluation of hallucinations as outlined in the context. The output effectively explains how these benchmarks assess hallucination rates, demonstrating a strong understanding of the topic. However, it could have included a brief mention of the limitations or challenges in using these benchmarks, which would have provided a more balanced view.", "contexts_used": ["4 Experiments\n\nWe conducted experiments to evaluate MetaRAG on its ability to detect hallucinations in retrieval-augmented generation (RAG). The evaluation simulates a realistic enterprise deployment setting, in which a chatbot serves responses generated from internal documentation. Our focus is on the detection stage, that is, identifying when an answer contains unsupported (hallucination) or fabricated information. Prevention and mitigation are important but they are outside the scope of this work.", "4.1 Dataset\n\nThe evaluation dataset is a proprietary collection of 23 internal enterprise documents , including policy manuals, procedural guidelines, and analytical reports, none of which were seen during LLM training. Each document was segmented into chunks of a few hundred tokens, and retrieval used cosine similarity over text-embedding-3-large , with the topk = 3 chunks appended to each query.", "We then collected a set of user queries and corresponding chatbot answers. Each response was labeled by human annotators as either hallucinated or not, using the retrieved context as the reference. The final evaluation set contains 67 responses , of which 36 are labeled as not hallucinated and 31 as hallucinated .", "To preserve confidentiality, we do not release the full annotated dataset. However, the complete annotation guidelines are included in the supplementary material.", "4.2 Evaluation Protocol\n\nMetaRAG produces fine-grained, factoid-level hallucination scores , whereas the available ground truth labels are at the response level . To align with these existing labels, we evaluate MetaRAG as a binary classifier by thresholding the hallucination score H ( Q,A,C ) at \u03c4 = 0 . 5 . We report standard classification metrics: Precision, Recall, F1 score and accuracy. Latency is also recorded to assess feasibility for real-time deployment.", "4.2.1 Case Studies in Identity-Sensitive Domains\n\nBeyond quantitative evaluation, we also provide qualitative illustrations of MetaRAG in identity-sensitive scenarios. To illustrate how MetaRAG's span-level scores can enable identity-aware safeguards without inferring protected attributes, we present two stylized examples. These are not part of the quantitative evaluation in Section 4, but highlight potential deployment scenarios.", "Healthcare (pregnancy). Auser asks: 'Can pregnant women take ibuprofen for back pain?' The model answers: 'Yes, ibuprofen is safe throughout pregnancy .' However, the retrieved context specifies that ibuprofen is contraindicated in the third trimester. MetaRAG flags the span 'safe throughout pregnancy' with a high factoid score ( S i = 0 . 92 ), yielding a response-level score H = 0 . 92 . Under the policy hooks described in Section 3.6, the topic tag pregnancy triggers a stricter threshold ( \u03c4 identity = 0 . 3 , lower than the general case), span highlighting, a forced citation requirement, and possible escalation to human review.", "Migration (refugee rights). A user asks: 'Do LGBTQ+ refugees automatically receive protection in country X?' The model claims that such protections are automatic , but the retrieved legal text provides no evidence of this. MetaRAG flags the unsupported claim 'automatically receive protection' with a moderate score ( S i = 0 . 5 ), yielding a response-level score H = 0 . 5 . Although this score would sit at the decision boundary under a general threshold ( \u03c4 general = 0 . 5 ), the stricter identity-aware threshold ( \u03c4 identity = 0 . 3 ) ensures it is flagged for this case. Under the policy hooks, the topic tag asylum/refugee enforces citation and may escalate the response to a human reviewer. In a chatbot deployment, the system would abstain from returning the unsupported answer and instead notify the user that expert verification is required.", "These qualitative vignettes complement our quantitative evaluation by showing how MetaRAG's flagged spans can be turned into concrete safeguards in identity-sensitive deployments.", "6 Discussion\n6.1 Practical Implications\n\nOperational Efficiency : Detecting hallucinations simultaneously with content delivery reduces the need for costly downstream human verification.", "6.2 Ethical Considerations\n\nBeyond technical performance, hallucination detection intersects directly with questions of fairness, accountability, and identity harms. Hallucinations in chatbot systems pose risks that extend beyond factual inaccuracies: they can reinforce harmful stereotypes, undermine user trust, and misrepresent marginalized communities in identitysensitive contexts.", "Reinforced stereotypes: Language models are known to reproduce and amplify societal biases, as demonstrated by benchmarks such as StereoSet [24] and WinoBias [34]. In identity-sensitive deployments, hallucinated outputs risk reinforcing these biases in subtle but harmful ways.", "Trust erosion: Chatbots are only adopted at scale in high-stakes domains if users trust their outputs. Surveys on hallucination consistently highlight that exposure to unsupported or fabricated content undermines user trust in LLM systems [14, 20].", "7 Conclusion\n\nHallucinations in RAG-based conversational agents remain a significant barrier to trustworthy deployment in real-world applications. We introduced MetaRAG , a metamorphic testing framework for hallucination detection in retrieval-augmented generation (RAG) that operates without requiring ground truth references or access to model internals. Our experiments show that MetaRAG achieves strong detection performance on a challenging proprietary dataset, aligning with prior benchmark studies. Beyond general reliability, MetaRAG's factoid-level localization also supports identity-aware deployment by surfacing unsupported claims in sensitive domains (e.g., healthcare, migration, labor). Looking ahead, we see MetaRAG as a step toward safer and fairer conversational AI, where hallucinations are not only detected but also connected to safeguards that protect users in identity-sensitive contexts. This connection to identity-aware AI ensures that hallucination detection does not treat all users as homogeneous but provides safeguards that reduce disproportionate risks for marginalized groups.", "Identity harms: Misrepresentations in generated responses may distort personal narratives or marginalize underrepresented groups, aligning with broader critiques that technical systems can reproduce social inequities if identity considerations are overlooked [11, 29].", "Figure 4. Pareto front analysis for hallucination detection performance. Each point represents a MetaRAG configuration; Pareto-optimal points (non-dominated) are highlighted. Subplots show: (Left) F1 vs. average token usage, (Center) F1 vs. average total execution time, (Right) Precision vs. Recall. Pareto-optimal points represent configurations with no strictly better alternative in both accuracy and cost. Configuration IDs correspond to Table 2.\n\nFl vs token usage\n\nFl vs total time\n\nRecall vs Precision 0.940 Pareto front 7.0 0.98 70000 0.935 6.5 0.96 0.930 60000 0.94 6.0 5.5 0.92 0.925 50000 5.0 0.90 0.920 4.5 0.88 40000 0.915 4.0 0.86 y = x 0.910 3.5 30000 Average token usage\n\nAverage total time (s)\n\nPrecision (mean)\n\nBy detecting hallucinations in a black-box, reference-free manner, MetaRAG supports safer deployment of RAG-based systems, particularly in settings where fairness, identity, and user well-being are at stake.", "6.3 Limitations and Future Work\n\nWhile MetaRAG demonstrates strong hallucination detection performance, several limitations remain:\n\nDataset Scope: The study relies on a private, domain-specific dataset. This may limit external validity. Future work should focus on curating or constructing public benchmarks designed to avoid overlap with LLM pretraining corpora, enabling more robust generalizability .", "Annotation Granularity: We lack factoid-level ground truth, which reduces our ability to assess fine-grained reasoning accuracy. Providing such annotations in future datasets would support deeper consistency evaluations .", "Policy Hooks Not Evaluated: The identity-aware deployment hooks introduced in Section 3.6 are presented only as a design concept. In our implementation, we used a fixed threshold of \u03c4 = 0 . 5 across all queries. Future research should implement and measure the effectiveness of topic-aware thresholds, forced citation, and escalation strategies in real-world chatbot deployments .", "Topic as Proxy (Design Limitation): In Section 3.6, we suggest topic tags (e.g., pregnancy, asylum, labor) as privacy-preserving signals for stricter safeguards, rather than inferring protected attributes. This was not implemented in our experiments. As a design idea, it may also miss cases where risk is identity-conditioned but the query appears generic. Future work should explore how to operationalize such topic-aware safeguards and investigate richer, privacy-preserving signals that better capture identity-sensitive risks .", "Model Dependency: Current findings hinge on specific LLMs (GPT-4.1 variants). As models evolve, the behavior of MetaRAG\n\nmay shift. Future efforts should validate MetaRAG across opensource and emerging models to reinforce its robustness .", "ABSTRACT\n\nHallucination remains a critical barrier to the reliable deployment of Large Language Models (LLMs) in high-stakes applications. Existing mitigation strategies, such as Retrieval-Augmented Generation (RAG) and post-hoc verification, are often reactive, inefficient, or fail to address the root cause within the generative process. Inspired by dual-process cognitive theory, we propose D ynamic S elfreinforcing C alibration for H allucination S uppression (DSCC-HS), a novel, proactive framework that intervenes directly during autoregressive decoding. DSCC-HS operates via a two-phase mechanism: (1) During training, a compact proxy model is iteratively aligned into two adversarial roles-a Factual Alignment Proxy (FAP) and a Hallucination Detection Proxy (HDP)-through contrastive logit-space optimization using augmented data and parameter-efficient LoRA adaptation. (2) During inference, these frozen proxies dynamically steer a large target model by injecting a real-time, vocabulary-aligned steering vector (computed as the difference between FAP and HDP logits) at each decoding step, requiring no modification to the target model. This plug-and-play approach enables lightweight, scalable, and proactive hallucination suppression. Our experiments on the TruthfulQA and BioGEN benchmarks demonstrate DSCC-HS's state-of-the-art performance. On TruthfulQA, DSCC-HS achieves a 49.82% accuracy and a 99.2% Factual Consistency Rate (FCR) while reducing the hallucination score to 0.8 , significantly outperforming strong baselines like ITI and DOLA. On the long-form BioGEN benchmark, DSCC-HS attains the highest FActScore of 46.50 and the lowest incorrectness rate of 11.49 , showcasing its robustness in generating factually consistent long texts. These results validate DSCC-HS as a principled and efficient solution for enhancing LLM factuality.", "4 Experiments\n\nIn this section, we present our experimental setup and evaluation results. To assess the effectiveness of our multimodal RAG pipeline under settings aligned with the CRAG-MM 2025 challenge. All local experiments were conducted on the same hardware as the challenge environment: a single NVIDIA L40S GPU with 48GB of memory. For the generated answer, we employed the GPT-4o-mini [7] model as the evaluation LLM.", "4.1 Metrics\n\nWeevaluate our system using the official metric suite defined by the CRAG-MM challenge. Specifically, we report the total number of queries, correct answers, missed answers, and hallucinations. From these, we derive key indicators such as accuracy, hallucination rate, and missing rate. Additionally, we compute the truthfulness score to assess factual grounding.", "4.2 Single-source Augmentation\n\nTo evaluate the effectiveness of our method, we conduct an ablation study on Task 1 under local evaluation, as shown in Table 1. The goal is to assess the incremental impact of the key component in our verification-centric pipeline.", "The LLaMA Vision Only baseline refers to the vision-language model (VLM) directly answering questions from image and text input without any external retrieval or augmentation. This setting yields a relatively high accuracy of 25.00%, but suffers from severe hallucination issues (59.62%) and a negative truthfulness score of -34.62%, indicating that the answers often conflict with the ground truth. This is expected, as the VLM attempts to answer all questions regardless of its knowledge limitations, resulting in a low", "| | | Accuracy (%) | Missing Rate (%) | Hallucination Rate (%) | Truthfulness Score (%) |\n|---:|:---------------------------|---------------:|-------------------:|-------------------------:|-------------------------:|\n| 0 | LLaMA Vision Only | 25 | 15.38 | 59.62 | -34.62 |\n| 1 | RAG Agent | 27.88 | 9.62 | 62.5 | -34.62 |\n| 2 | w/o CoV & Self-Consistency | 3.85 | 95.19 | 0.96 | 2.88 |\n| 3 | w/o CoV | 4.81 | 95.19 | 0 | 4.81 |\n| 4 | Ours | 14.42 | 82.69 | 2.88 | 11.54 |\n\nTable 1: Ablation study on Task 1 single-source augmentation under local evaluation.", "missing rate but poor factual reliability. The RAG Agent baseline incorporates external retrieval via the official search API, enhancing access to relevant knowledge. This improves the accuracy to 27.88% and slightly reduces the missing rate to 9.62%. However, hallucination rate increases further to 62.50%, and the truthfulness score remains at -34.62%, showing that naive RAG without verification can introduce more misleading content. To further assess the role of verification, we remove key components from our pipeline to observe their individual effects. The w/o CoV & SelfConsistency variant disables both the self-consistency check and the Chain-of-Verification (CoV). As a result, the model becomes overly conservative, with a missing rate of 95.19% and very low accuracy (3.85%). While hallucination rate drops sharply to 0.96%, the overall coverage and utility of the model are severely degraded. A similar trend is observed in the w/o CoV variant, which enables self-consistency but disables CoV. Although slightly more accurate (4.81%), it still fails to provide meaningful coverage. Our final agent, denoted as Ours , integrates all proposed components. This configuration strikes the best balance between caution and informativeness. It achieves a truthfulness score of 11.54% , the highest among all systems, with an acceptable hallucination rate of 2.88% and an improved accuracy of 14.42%. The results demonstrate that each verification stage contributes to reducing hallucinations while selectively increasing answer confidence, enabling the agent to provide more trustworthy responses.", "Our method is developed specifically for the Single-source Augmentation (Task 1) setting. The same pipeline and implementation are directly applied to Task 2 and Task 3 during the CRAG-MM challenge phase. Thus, we focus our discussion on Task 1.", "6 Datasets and Evaluation Benchmarks\n\nThe evaluation of IR and RAG systems relies on diverse datasets that test different aspects of retrieval and answer generation capabilities, including the retrieval, ranking, and answer quality.", "6.1 IR Specific\n\nInformation retrieval has a long history of evaluation campaigns, including those from TREC, CLEF, NTCIR, and FIRE. The queries used in these collections are often developed to strike a balance between having too many relevant documents in the target collection (which can be too easy to retrieve and too difficult to properly annotate (Voorhees et al., 2022)) and too few relevant documents. Sometimes challenging topics are also developed deliberately (Voorhees, 2005). The topic development process often involves manual reformulation of queries to ensure good coverage of relevance assessments. Mirroring the annotation process itself, it can be beneficial for automated retrieval systems to also perform various forms of query understanding (expansion or rewriting) to help ensure high recall. Hence, the approaches described in Section 3 show performance gains.", "Comprehensive benchmarks like BEIR (Thakur et al., 2021) offer heterogeneous evaluation across 17 diverse datasets spanning multiple domains and retrieval tasks, enabling zero-shot generalization assessment. In addition, evaluation sets such as TREC DL (Deep Learning) and its variants (Craswell et al., 2020, 2021, 2022, 2023) are used to evaluate IR systems.", "6.2 Question Answering\n\nQuestion Answering datasets for retrieval evaluation fall into two primary categories based on complexity. Single-hop QA benchmarks include Natural Questions (NQ) (Kwiatkowski et al., 2019), TriviaQA (Joshi et al., 2017), SQuAD (Rajpurkar et al., 2016), and PopQA (Mallen et al., 2023) test the RAG system's ability to retrieve and utilize information to generate the final answer. Multi-hop QA benchmarks comprise 2WikiMultiHopQA (Ho et al., 2020) (questions requiring evidence from exactly two Wikipedia articles via bridging/comparison), HotpotQA (Yang et al., 2018), and MuSiQue (Trivedi et al., 2022), which features compositionally complex questions constructed from interconnected sub-questions. Due to the dependency on intermediate answers, such questions cannot be answered through isolated single-step retrieval. Therefore, this design naturally motivates the RAG systems to decompose the original query and new queries based on the intermediate answers help in the retrieval of better context.", "6.3 Fact Checking/Verification\n\nThese datasets assess models' ability to verify claims against retrieved evidence. FEVER (Fact Extraction and VERification) (Thorne et al., 2018) is an open-domain fact-checking benchmark that requires retrieval over a large collection of Wikipedia articles and training NLI models to classify claims as supported, refuted, or not having enough information based on Wikipedia evidence. While FEVER deals with simple claims, HoVeR (Jiang et al., 2020) proposes a fact-checking benchmark with claims that require multi-hop reasoning and multi-turn retrieval. QuanTemp (V et al., 2024) is the first to propose a large open-domain benchmark for fact-checking numerical claims. It comprises claims that require interleaving claim decomposition and retrieval based on the verification output of sub-claims. This relates to query-level feedback and generation-time feedback. AveriTeC (Schlichtkrull et al., 2023) is a real-world claim verification dataset that includes diverse claim types with supporting or refuting evidence gathered through web search, making it particularly valuable for evaluating RAG systems on authentic fact-checking scenarios that require verification against potentially noisy or contradictory web sources.", "6.4 Complex Reasoning\n\nEmerging benchmarks introduce novel relevance criteria requiring awareness of reasoning structure. BRIGHT (SU et al., 2025) defines document relevance not by topical alignment but by whether passages contain logical constructs (deductive steps, analogies, constraints) necessary to derive answers. This challenges lexical retrievers that lack inference-awareness. However, reasoningaugmented retrieval methods, such as CoT (Chainof-Thought), have shown performance gains. Further, there exist reasoning/agentic tasks like Deep Research (Wu et al., 2025), GPQA (Rein et al., 2023), MATH500 (Cobbe et al., 2021), which involve access to the web search engine, have been used in the RAG setting (Li et al., 2025a; Wei et al., 2025). However, due to the absence of the cor- pus, it is hard to evaluate the retrieval performance. Recent benchmarks like BrowseComp-Plus (Chen et al., 2025b) provide a curated corpus for Deep Research tasks and enable the evaluation of retrieval performance.", "6.5 Evaluation Metrics\n\nEvaluating retrieval systems and RetrievalAugmented Generation (RAG) pipelines is critical for ensuring the accuracy, relevance, and reliability of generated responses. Retrieval evaluation typically focuses on metrics such as recall@k, precision, mean reciprocal rank (MRR), and hit rate, which assess how effectively the system retrieves pertinent documents or passages given a query. In contrast, RAG evaluation is more holistic, combining retrieval quality with generation fidelity and coherence. Common approaches include measuring answer correctness using exact match (EM) or F1 score, assessing faithfulness to retrieved evidence to detect hallucinations, and evaluating relevance and fluency through human or automated scoring (e.g., BLEU, ROUGE, or BERTScore). Recent frameworks like RAGAS (ExplodingGradients, 2024), ARES (Saad-Falcon et al., 2024), and CRUX (Ju et al., 2025) also emphasize end-to-end evaluation, where the interplay between retrieval accuracy and generation quality is analyzed to identify bottlenecks-such as irrelevant documents leading to incorrect answers-making comprehensive evaluation essential for diagnosing and improving RAG system performance.", "Challenges . The current RAG evaluation methods mainly focus on the retrieval and final answer performance. However, Reasoning RAG systems are highly dependent on intermediate reasoning steps and retrieval rounds. Therefore, it is also important to consider additional evaluation dimensions such as computational cost, efficiency, or number of retrieval rounds."]}, {"type": "entry", "index": 5, "question": "How does Reciprocal Rank Fusion compare to weighted hybrid scoring on arXiv-style corpora?", "answer": "The provided documents do not report any direct comparison between Reciprocal Rank Fusion (RRF) and a weighted hybrid scoring scheme on arXiv-style corpora, so we cannot say which performs better under those conditions.\n\nFrom the documents, we only know the following about RRF:\n\n1. RRF is a rank-based fusion method that combines the rankings from multiple retrievers. For a document d, its fused score is computed as the sum over retrievers of 1 / (k + rank_i(d)), where rank_i(d) is the position of d in retriever i\u2019s ranked list and k is a constant that dampens the influence of very high ranks [2]. This means RRF emphasizes documents that are consistently ranked near the top across methods.\n\n2. RRF is used to fuse dense and sparse retrieval results in a mixed retriever for harm-reduction RAG, helping integrate semantic and keyword-based matches [2].\n\n3. It is also used as a baseline hybrid retrieval method that combines dense and sparse search results in a large GraphRAG-style system [4].\n\n4. In a medical multimodal RAG framework, Reciprocal Rank Fusion is used to combine image-to-text and text-to-text retrieval results, before comparing against a more specialized retriever, showing that the specialized method can outperform this simple fusion, but without any link to arXiv-style corpora [3].\n\nNone of the documents define or evaluate a \u2018weighted hybrid scoring\u2019 scheme, nor do they discuss retrieval experiments on arXiv or arXiv-like scientific-paper corpora. Therefore, a data-driven comparison between RRF and weighted hybrid scoring on arXiv-style corpora is not available in the supplied material.", "tags": ["hybrid_vs_kg", "rrf_hybrid"], "full_faithfulness": 0.9444444444000001, "full_answer_relevancy": 0.0, "seed_faithfulness": 0.7000000000000001, "seed_answer_relevancy": 0.0, "context_relevance": 0.2556171844453696, "context_relevance_error": null, "context_relevance_reason": "The context passages provided do not directly address the user's question about the comparison between Reciprocal Rank Fusion and weighted hybrid scoring on arXiv-style corpora. While some passages mention Reciprocal Rank Fusion, they lack specific information or analysis related to weighted hybrid scoring, making them largely irrelevant to the user's inquiry.", "structure_citation": 0.5576464375676288, "structure_error": null, "structure_reason": "The response correctly identifies that there are no direct comparisons between Reciprocal Rank Fusion (RRF) and weighted hybrid scoring in the provided documents, which aligns with the evaluation steps. However, while it includes claims supported by snippets (e.g., the description of RRF), it fails to utilize the [n] markers effectively, as it does not cite the specific snippets corresponding to the claims made. This lack of citation for the claims results in a moderate score.", "hallucination_score": 0.6101977113787885, "hallucination_error": null, "hallucination_reason": "The response accurately identifies that there is no direct comparison between Reciprocal Rank Fusion (RRF) and weighted hybrid scoring on arXiv-style corpora, which aligns with the context provided. However, while it elaborates on RRF's characteristics and applications, it lacks specific details about weighted hybrid scoring, which diminishes the completeness of the answer. The absence of relevant information about weighted hybrid scoring limits the overall effectiveness of the response.", "contexts_used": ["Conclusion\n\nThe observed increase in query length suggests that the current training scheme, constrained by limited samples and a single epoch, may inadvertently favor verbose queries. Future work could explore larger-scale training or refined reward mechanisms to balance query conciseness with retrieval effectiveness. Additionally, enhancing the alignment between rewriters and semantic retrievers through advanced reward policies or training strategies could unlock further performance gains.", "In conclusion, RL-QR offers a scalable, annotation-free solution for query optimization, with broad applicability to diverse retrievers and databases. Its demonstrated success on industrial in-house data underscores its potential to transform real-world retrieval systems, paving the way for more efficient and versatile RAG applications.", "Chan, C.-M.; Xu, C.; Yuan, R.; Luo, H.; Xue, W.; Guo, Y.; and Fu, J. 2024. Rq-rag: Learning to refine queries for retrieval augmented generation. arXiv preprint arXiv:2404.00610 .", "Chen, J.; Xiao, S.; Zhang, P.; Luo, K.; Lian, D.; and Liu, Z. 2024. BGE M3-Embedding: Multi-Lingual, MultiFunctionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation. arXiv:2402.03216.", "Comanici, G.; Bieber, E.; Schaekermann, M.; Pasupat, I.; Sachdeva, N.; Dhillon, I.; Blistein, M.; Ram, O.; Zhang, D.; Rosen, E.; et al. 2025. Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multimodality, Long Context, and Next Generation Agentic Capabilities. arXiv preprint arXiv:2507.06261 .", "Cormack, G. V.; Clarke, C. L.; and Buettcher, S. 2009. Reciprocal rank fusion outperforms condorcet and individual rank learning methods. In Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval , 758-759.", "Edge, D.; Trinh, H.; Cheng, N.; Bradley, J.; Chao, A.; Mody, A.; Truitt, S.; Metropolitansky, D.; Ness, R. O.; and Larson, J. 2024. From local to global: A graph rag approach to query-focused summarization. arXiv preprint arXiv:2404.16130 .", "Faysse, M.; Sibille, H.; Wu, T.; Omrani, B.; Viaud, G.; Hudelot, C.; and Colombo, P. 2024. Colpali: Efficient document retrieval with vision language models. In The Thirteenth International Conference on Learning Representations .", "Feng, H.; Wei, S.; Fei, X.; Shi, W.; Han, Y.; Liao, L.; Lu, J.; Wu, B.; Liu, Q.; Lin, C.; et al. 2025. Dolphin: Document image parsing via heterogeneous anchor prompting. arXiv preprint arXiv:2505.14059 .", "Fix, E. 1985. Discriminatory analysis: nonparametric discrimination, consistency properties , volume 1. USAF school of Aviation Medicine.", "Hurst, A.; Lerer, A.; Goucher, A. P.; Perelman, A.; Ramesh, A.; Clark, A.; Ostrow, A.; Welihinda, A.; Hayes, A.; Radford, A.; et al. 2024. Gpt-4o system card. arXiv preprint arXiv:2410.21276 .", "Answer Requirements:\n\nStart with the precise numerical value and units.\n\nInclude route of administration if specified.\n\nInclude frequency/timing context if mentioned.", "Explain significance of the dosage (e.g., threshold, therapeutic, etc.)\n\nAvoid referencing 'the text' or 'the source'\n\nKnowledge Content: Here is the knowledge content to use: knowledge\n\nFormat your response as:\n\n'question': 'Question text', 'answer': 'Numerical value + units + explanation from source',\n\n'context': 'The exact sentences of context information you used from the source'\n\nTable 2: System instruction used for safety boundary check tasks.", "(i) You are tasked with answering a yes/no question related to harm reduction practices based on reliable information.\n\n(ii) You must respond in exactly this format:\n\nYes/No , followed by a brief explanation of the substantial reason(s) behind.", "Knowledge Retrieval : We implemented a mixed retrieval method using dense and sparse retrievers. Specifically, we leverage dense retriever (using Sentence-Transformers (Reimers and Gurevych, 2019)) to identify semantically related harm reduction information. We then implement sparse retriever BM25 (Robertson et al., 2009) using precise term-matching to accurately retrieve specific substance names and dosage information that may not be optimally captured through dense retriever alone.", "Knowledge Reranking : To effectively integrate retrieved knowledge, we employ the Reciprocal Rank Fusion (RRF) method, which combines the results from both retrieval methods using a rank-based fusion: score ( d ) = \u2211 i 1 k + rank i ( d ) , where d denotes the target document and rank i ( d ) denotes document d 's rank in the i -th retrieval method, and k is a constant that mitigates the impact of high rankings.", "The RAG scheme integrates with LLMs when providing harm reduction information. Its model response R is formulated as:\n\nwhere K denotes the retrieved knowledge.", "5 Experiments\n5.2 Main Results\n\nMed-LVLMs: Figure 3 shows that HeteroRAG, with a 7B parameter size, outperforms most advanced Med-LVLMs, which contain 4 to 5 times more parameters across multiple datasets. This indicates that the proposed framework advances the medical multimodal capabilities of existing MedLVLMs to a higher level.", "5.3 Effectiveness of Retrieved Knowledge\n\nWe conduct ablation studies to evaluate the contribution of knowledge sources as shown in Table 3. The 'Original' and 'SFT' settings represent the performance of the original Lingshu-7B and Lingshu-7B after SFT on the original training set, which does not include reports and documents. The other configurations examine HeteroRAG's performance when either reports or documents are removed. The results show that retrieved knowledge significantly improves Med-LVLM's performance compared to the Original baseline. The performance improvements from super-\n\n| | Models | Rad. | Oph. | Pat. |\n|---:|:------------|:-------|:-------|:-------|\n| 0 | BiomedCLIP | 30.20 | 13.45 | 28.85 |\n| 1 | PMC-CLIP | 30.05 | 19.80 | 23.35 |\n| 2 | PubMedCLIP* | 13.35 | - | - |\n| 3 | MM-Retinal* | - | 4.65 | - |\n| 4 | QuiltNet* | - | - | 39.65 |\n| 5 | FactMM-RAG* | 44.25 | - | - |\n| 6 | RULE* | 31.80 | 18.90 | - |\n| 7 | MMed-RAG* | 31.80 | 18.90 | 30.20 |\n| 8 | ModCLIPs* | 79.40 | 47.55 | 77.35 |\n\nTable 4: Image-to-text recall@5 of different retrievers. The asterisks (*) denote the modality-specific retrievers.", "vised fine-tuning alone are insufficient to compensate for the absence of knowledge. When reports or documents are excluded, the performance degradation confirms that both sources are important for HeteroRAG's knowledgeintensive inference. Furthermore, all five corpora contribute to Med-LVLM's capacity.", "5.4 Effectiveness of ModCLIPs\n\nWe evaluate ModCLIPs against other medical retrievers on image-to-text report retrieval tasks, as shown in Table 4. Generalist retrievers include BiomedCLIP and PMCCLIP (Lin et al. 2023). Modality-specific retrievers include PubMedCLIP (Eslami, Meinel, and De Melo 2023), MMRetinal (Wu et al. 2024b), QuiltNet (Ikezogwo et al. 2023), FactMM-RAG, RULE and MMed-RAG. Using the test set described in Section 4.1 with recall@5 as our evaluation metric, our experiments demonstrate that ModCLIPs consistently outperform competing methods across all three modalities. This superior performance can be attributed to", "| | Methods | OMVQA-Rad | OMVQA-Oph | Quilt-VQA |\n|---:|:----------------|------------:|------------:|------------:|\n| 0 | CLIP | 76.83 | 84.42 | 63.85 |\n| 1 | MQG w/o DPO w/o | 82.08 | 91.25 | 69.97 |\n| 2 | | 81.08 | 88.17 | 65.6 |\n| 3 | SFT | 78.75 | 86 | 64.14 |\n\nTable 5: Performance of HeteroRAG under two ablation settings: replacing MQG with CLIP-based retrieval, and removing the training stages of MQG.", "Figure 4: Accuracy and disalignment metrics of Lingshu-7B trained with different methods and data.\n\nOriginal SFT HKPT w/o CMA w/o KU w/o KR Accuracy (\u2191) MD (\u2193) KUD (\u2193) KID (\u2193)\n\ntwo key advantages: (1) Single-modality training yields significantly better modality-specific understanding compared to mixed-modality approaches, and (2) our training data offers more comprehensive coverage and greater diversity within each modality.", "5.5 Effectiveness of MQG\n\nWe further investigate the effectiveness of MQG in Table 5. First, the MQG in HeteroRAG is replaced with a CLIP retrieval module. Specifically, for each medical visual question, the ModCLIPs are employed to retrieve documents through both image-to-text and text-to-text retrieval. The two retrieval results are combined using Reciprocal Rank Fusion. We also ablate the DPO and SFT training stages of the MQG. The number of retrieved documents remains consistent. Our experiments demonstrate that MQG retrieves more relevant documents compared to standard CLIP methods. This improvement can be attributed to better alignment of MQG and each corpus's characteristics. Furthermore, both the SFT and DPO training stages prove essential in developing MQG.", "5.6 Alignment Effectiveness of HKPT\n\nTo evaluate the alignment effectiveness of HKPT, we introduce three additional metrics besides answer accuracy: Modality Disalignment (MD), Knowledge Usage Disalignment (KUD), and Knowledge Interference Disalignment (KID). MD corresponds to CMA in Section 4.3, KUD corresponds to KU, and KID corresponds to KR. MD measures\n\n| | Models | OMVQA-Rad | OMVQA-Oph | Quilt-VQA |\n|---:|:---------------|------------:|------------:|------------:|\n| 0 | LLaVA-Med-7B | 53.67 | 56.83 | 66.18 |\n| 1 | + HeteroRAG | 60.17 | 71.42 | 69.39 |\n| 2 | HuatuoGPT-V-7B | 72.08 | 81.83 | 66.18 |\n| 3 | + HeteroRAG | 78.17 | 84.5 | 71.43 |\n| 4 | Lingshu-7B | 74.92 | 80.83 | 49.27 |\n| 5 | + HeteroRAG | 82.08 | 91.25 | 69.97 |\n\nTable 6: Model performance when the HeteroRAG framework is applied to different Med-LVLMs.", "the proportion that the Med-LVLM correctly answers with the irrelevant image among cases where it correctly answers with the original image. KUD measures the proportion that the Med-LVLM succeeds when any retrieval source (report, document, or report+document) is introduced among cases where it fails without retrieval. KID measures the proportion that the Med-LVLM fails when any retrieval source is introduced among cases where it succeeds without retrieval.", "Figure 4 shows the average metrics on OMVQA-Rad, OMVQA-Oph, and Quilt-VQA. The 'SFT' method refers to SFT using the training dataset with documents and reports added. The results demonstrate that HKPT improves overall accuracy compared to both the original and SFT models. Moreover, all three types of disalignment are significantly reduced by the HKPT method. We further conduct ablation studies on each type of preference pair in HKPT, including CMA, KU, and KR. The results confirm that each component effectively enhances the corresponding alignment capability as expected.", "5.7 Compatibility Analysis\n\nTo analyze the compatibility of the HeteroRAG framework with different Med-LVLMs, we apply it to LLaVA-Med7B and HuatuoGPT-V-7B besides Lingshu-7B. Specifically, the ModCLIPs and MQG in HRM are kept unchanged, as they are universal across different downstream readers. The HKPT process is performed separately for each MedLVLM. Results in Table 6 show that HeteroRAG brings consistent improvements over all Med-LVLMs. This indicates that HeteroRAG can be transferred to diverse Med-LVLMs.", "2 Preliminaries\n\nLet C = { \ud835\udc50 1 , \ud835\udc50 2 , . . . , \ud835\udc50 \ud835\udc36 } be an index of textual passages (i.e. we would refer to them as chunks interchangeably) and q be a natural language question that is provided as an input to our system. Retrieving items from C relevant to q can be achieved by using a base retrieval function \u210e \ud835\udc58 base ( q , C ) \u2286 C for returning a ranked list of items from C , in descending order according to a particular retrieval score.", "In the context of this challenge, given an input query q \u2032 , a dense retriever on top of C : \u210e \ud835\udc58 dense ( q \u2032 , C ) can be implemented using the provided Pinecone index. Similarly, a sparse retrieval step: \u210e \ud835\udc58 sparse ( q \u2032 , C ) can be achieved using the provided OpenSearch instance. A baseline retrieval step can be also implemented as a hybrid combination of passages coming from a dense and sparse retriever. Each hybrid retrieval search step returns the top\ud835\udc58 items from an index of interest by aggregating the results of dense and sparse retrieval using Reciprocal Rank Fusion (RRF) [1], as follows:\n\nwhere \u210e \ud835\udc58 dense , \u210e \ud835\udc58 sparse \u2286 C are functions returning sets of items from C , in descending order according to score dense and scoresparse respectively.", "6 Evaluation of RAG Systems\n\nThe evaluation of RAG systems is multifaceted, as performance depends not only on the generative model but also on the quality of the retrieval pipeline. A robust evaluation framework must assess retrieval accuracy, answer quality, factuality, latency, and scalability. This section provides a structured overview of RAG evaluation criteria, benchmarks, and the impact of architectural design choices. Summary of this section is in Table Table 1", "6.1 Evaluation Dimensions and Metrics\n\nRAG performance is commonly assessed along the following dimensions:\n\nRetrieval Accuracy. Key metrics include:\n\nRecall@ k - Measures the proportion of queries where a relevant document appears among the topk retrieved results.", "Mean Reciprocal Rank (MRR) - Captures the average inverse rank of the first relevant document, rewarding early placement.", "Mean Average Precision (MAP) -Evaluates the quality of ranked retrieval across relevant items.", "6.2 Benchmarks and Datasets\n\nSeveral benchmarks are widely used to evaluate RAG systems:\n\nNatural Questions (NQ) , TriviaQA , and WebQuestions for open-domain QA [52].", "FEVER and AveriTeC for fact-checking, emphasizing verifiability.\n\nKILT benchmark suite integrates QA, dialog, slot filling, and entity linking over Wikipedia.", "BEIR evaluates retrieval across 31 zero-shot tasks in domains like biomedicine and finance.\n\nMTRAG targets multi-turn conversations requiring sequential retrieval and reasoning.", "TREC RAG Track (2024-) defines unified evaluation of retrieval, generation, and support quality over MS MARCO with metrics like nugget recall and citation coverage [71].", "6.3 Retrieval-Augmented Generation Assessment System\n\nRAGAS (Retrieval-Augmented Generation Assessment System) is an evaluation framework specifically designed for assessing and improving the factuality and grounding of RAG systems. Unlike conventional metrics that measure superficial linguistic overlap, RAGAS emphasizes the alignment between generated content and retrieved documents, providing explicit signals regarding factual correctness and attribution quality. By systematically measuring how well the generated outputs are supported by the retrieved evidence, RAGAS helps identify and penalize hallucinations-instances where the model generates plausible but unsupported statements. Consequently, employing RAGAS during model training or iterative fine-tuning guides RAG systems toward producing outputs firmly grounded in verifiable sources, substantially improving factual accuracy and reducing the incidence of hallucinated information.", "| | Category | Metric/Tool | Description |\n|---:|:-------------------|:---------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 0 | Retrieval Accuracy | Recall@ k MRR MAP | Proportion of queries with a relevant doc in top- k results Average inverse rank of the first relevant document Mean precision across all relevant retrieved documents |\n| 1 | Generation Quality | Exact Match (EM), F1 BLEU, ROUGE Faithfulness Rate | Measures overlap with ground-truth answers N-gram based overlap metrics for generation Consistency with retrieved context (manual/automated) |\n| 2 | Efficiency | Latency (retrieval/generation) Memory/Compute Usage | Time required for each processing step Resource demands for model operation |\n| 3 | Scalability | Index Size vs. Accuracy Adaptability | Impact of increasing corpus size on performance Ability to incorporate new data without retraining |\n| 4 | Benchmarks | Natural Questions, TriviaQA FEVER, AveriTeC KILT BEIR MTRAG TREC RAG Track | Open-domain QA evaluation Fact-checking datasets Multi-task benchmark over Wikipedia 31-task zero-shot retrieval benchmark Multi-turn reasoning benchmark Unified eval. of retrieval, generation, citation |\n| 5 | Tooling | RAGAS | Evaluates factual consistency and grounding |", "Table 1: Evaluation Dimensions, Metrics, Benchmarks, and Tools for RAG Systems", "B. Results\n\nThe results of the experiments are summarized in Tables I and II, which presents the performance metrics of TechnicalEmbeddings compared to several baseline models. The allmpnet-base-v2 model [15], generates high-quality sentence embeddings through fine-tuning BER and testing various pooling strategies. All-MiniLM-L6-v2 [16] employs multihead self-attention relationship distillation to reduce parameters and improve efficiency. The BGE series [17], including bge-small-en and bge-base-en, effectively extracts and cleans semantically relevant text pairs from large Chinese web corpora, aiding weak supervision training. The results clearly illustrate the effectiveness of the TechnicalEmbeddings framework in improving technical question answering performance across two distinct datasets. In Table I, which evaluates the Rust-Docs-QA dataset, TechnicalEmbeddings achieves a Mean Average Precision (MAP) of 0.2238 and a Mean Reciprocal Rank (MRR) of 0.2249. These scores represent a notable enhancement over the baseline models, particularly the all-mpnet-base-v2, which records an MAP of 0.1734 and an MRR of 0.1817. The ability of Technical-Embeddings to achieve higher precision and recall scores -0.0785 and 0.3364, respectively -demonstrates its effectiveness in retrieving relevant documents, thereby addressing user needs more effectively.", "| | Model | MAP | MRR | Precision | Recall |\n|---:|:---------------------|-------:|-------:|------------:|---------:|\n| 0 | all-mpnet-base-v2 | 0.1734 | 0.1817 | 0.0745 | 0.2745 |\n| 1 | all-MiniLM-L6-v2 | 0.1892 | 0.1897 | 0.0765 | 0.3235 |\n| 2 | bge-small-en | 0.2096 | 0.2196 | 0.0784 | 0.3039 |\n| 3 | bge-base-en | 0.1942 | 0.2039 | 0.0773 | 0.3186 |\n| 4 | Technical-Embeddings | 0.2238 | 0.2249 | 0.0785 | 0.3364 |\n\nTABLE I PERFORMANCE OF DIFFERENT RETRIEVAL MODELS ON T H E RUSTDOCS-QA DATASET.", "Moving to Table II, which presents the performance metrics on the RAG-EDA dataset, we observe that TechnicalEmbeddings matches the best baseline model, bge-small-en, with an MAP and MRR score of 0.6926. This alignment suggests that our framework produces results on par with", "leading models in the field. Furthermore, TechnicalEmbeddings outperforms bge-small-en in terms of Recall, achieving a score of 0.8111 compared to bge-small-en's 0.8000. Such consistency across multiple performance metrics underscores the robustness and reliability of our model in retrieving pertinent technical content, which is essential for users navigating complex documentation.", "|---:|:---------------------|-------:|-------:|------------:|---------:|\n| 0 | all-mpnet-base-v2 | 0.6 | 0.6 | 0.623 | 0.6889 |\n| 1 | all-MiniLM-L6-v2 | 0.5648 | 0.5648 | 0.5824 | 0.6333 |\n| 2 | bge-small-en | 0.6926 | 0.6926 | 0.7201 | 0.8 |\n| 3 | bge-base-en | 0.6352 | 0.6352 | 0.6686 | 0.7667 |\n| 4 | Technical-Embeddings | 0.6926 | 0.6926 | 0.723 | 0.8111 |\n\nTABLE II PERFORMANCE OF DIFFERENT RETRIEVAL MODELS ON T H E RAGEDA DATASET", "The comparative analysis of these results highlights the advantages of incorporating synthetic query generation contextual summary, and prompt tuning into the TechnicalEmbeddings framework. The significant improvements in MAP, MRR, precision, and recall indicate that our approach effectively captures the nuances of technical language. By simulating real-world user interactions through synthetic queries, the model is better equipped to understand diverse query types and structures, which contributes to its superior performance in technical question answering."]}, {"type": "entry", "index": 6, "question": "What\u2019s the impact of chunk size and overlap on faithfulness for academic PDFs?", "answer": "From the available evidence, chunk size clearly affects faithfulness, while the impact of overlap is not directly measured.\n\nIn the K\u00b2RAG study, they evaluated different chunk sizes on a real-world financial report (Lyft 2021) and measured answer faithfulness and relevancy. As chunk size decreased from very large to moderately sized chunks, faithfulness improved sharply: 2048 tokens \u2192 80.37 faithfulness, 1024 \u2192 94.26, 512 \u2192 97.59, 256 \u2192 97.22, and then dropped slightly at 128 \u2192 95.74 [1]. Relevancy showed a similar pattern, peaking around 256\u2013512 tokens [1].\n\nThis suggests there is a \u201csweet spot\u201d around 256\u2013512 tokens per chunk for academic-style PDFs: \n- Too large chunks (e.g., 2048 tokens) mix many topics, so the model pulls in extra, loosely related content and becomes less faithful to the specific question. \n- Too small chunks (e.g., 128 tokens) may miss supporting context, so the model has to fill gaps, again hurting faithfulness. \n- Mid-sized chunks (256\u2013512) are large enough to contain a complete argument or section, but small enough to stay tightly focused, which helps faithful, on-point answers.\n\nOn overlap, K\u00b2RAG defines a general chunking formula with a configurable overlap O and reports using overlaps like 20 tokens for vector stores and 100 for the knowledge-graph index, but it does not provide an ablation or faithfulness scores for different overlap values [1]. So we cannot say, from these documents alone, how overlap quantitatively impacts faithfulness, only that some overlap is used to keep continuity across chunks.", "tags": ["benchmarks_eval", "retrieval_tuning"], "full_faithfulness": 1.0, "full_answer_relevancy": 0.0, "seed_faithfulness": 0.9615384615, "seed_answer_relevancy": 0.0, "context_relevance": 0.698562281953917, "context_relevance_error": null, "context_relevance_reason": "The context passages provide substantial information about the impact of chunk size and overlap on faithfulness, particularly in the section discussing the chunking strategy and its effects on retrieval accuracy and faithfulness metrics. Specific details, such as the configurations used for chunk sizes and overlaps, and their correlation with faithfulness and relevancy metrics, directly address the user's question. However, some passages are less relevant, which slightly detracts from the overall coherence of the response.", "structure_citation": 0.7948482283021601, "structure_error": null, "structure_reason": "The Actual Output effectively discusses the impact of chunk size on faithfulness, supported by specific data from the K\u00b2RAG study, which corresponds to the provided Context. It includes citations for the claims made, particularly regarding the faithfulness scores associated with different chunk sizes. However, while it mentions overlap, it does not provide specific faithfulness scores or detailed analysis for overlap, which slightly detracts from its completeness in addressing the question.", "hallucination_score": 0.8328297735363401, "hallucination_error": null, "hallucination_reason": "The response effectively addresses the impact of chunk size on faithfulness, providing specific data from the K\u00b2RAG study that supports the claim. It clearly outlines how different chunk sizes affect faithfulness and relevancy, aligning well with the input question. However, while it mentions that the impact of overlap is not directly measured, it could have provided more context or examples regarding the implications of overlap on faithfulness, which would strengthen the overall response.", "contexts_used": ["3 Methodology:\n\n3.1 K 2 RAG Framework\n\nTo meet the research goals, the K 2 RAG framework implements four techniques:\n\n1. Knowledge Graph: K 2 RAG employs a knowledge graph component to organize and interconnect topics within the corpus [Figure 8(a)]. Unlike traditional chunking approaches, which struggle to balance the trade-off", "between providing sufficient context and avoiding irrelevant information, the knowledge graph addresses this issue by structuring content into interconnected nodes which enables knowledge-rich and more context-aware retrieval capable of handling complex or vague queries that vector-based search methods often struggle with.", "2. Hybrid Search: To minimize false positive embeddings-where semantically similar but irrelevant chunks are retrievedK 2 RAG integrates a hybrid retriever [Figure 8(b)]. Inspired by Mandikal et al.'s paper [18], we weight dense vector and sparse vector retrieval at an optimized 80%/20% ratio. This hybrid method significantly improves retrieval accuracy, helping reduce noise by ensuring the most relevant content is retrieved.", "3. Summarization: To combat the \"Needle-in-Haystack\" issue, K 2 RAG incorporates summarization at multiple stages in the pipeline [Figure 8 (B) and (E)]. To speed up indexing, documents are summarized before being indexed into vector and knowledge graph stores. Additionally retrieved content is summarized at query time to further refine the context provided to the LLM. By summarizing at both indexing and retrieval, we ensure concise, high-quality input for generation.", "4. Lightweight Models: For VRAM resources efficiency, K 2 RAG leverages a lightweight Longformer-based text summarizer [29], along with quantized LLMs to produce a low VRAM and resource-efficient pipeline without compromising output quality.", "3.1.1 Corpus Summarization Process\n\nTo address research goal 1, we decrease training times of the K 2 RAG framework compared to naive RAG frameworks by summarizing the training corpus to reduce its size while preserving as much information as possible. Hence a Longformer based model [29] fine tuned for long text summarization is used as it's self-attention mechanism is highly optimized for capturing and processing large chunks of texts compared to those seen in traditional transformer models. This helps speed up training times of information store components used in K 2 RAG . To create our summarized corpus we iterate over each article in the corpus and then summarize it using the loaded in Longformer summarizer model. Then these results are stored into a CSV format from which then they can be loaded in for training the RAG pipeline [Algorithm 1] [Figure 6].", "3.1.2 Indexing\n\nThe chunking strategy to generate a set of chunks C d from a single document or text d is defined as:\n\nFigure 6: How to generate summaries of corpus.\n\nSummarise Document Load corpus For each Longformer Encoder Decoder Model Store into list Repeat Export to CSV", "where S is the desired chunk size, O is the desired chunk overlap, Nt d is the total number of tokens in the document d . The inner set denotes a chunk constructed from tokens x (( i -1) \u2217 ( S -O ))+1 to x (( i -1) \u2217 ( S -O ))+ S and L t is the limit of tokens to process to ensure the number of chunks of size S and overlap O are maximized.", "We then apply the chunking strategy defined in (1) across all documents in the summarized corpus cor sum and take the union of chunks to give us the final set of chunks C to pass to the knowledge graph and vector stores for indexing:\n\nThe configuration used to train all information retrievers is S = 256 , O = 20 when chunking for the dense and sparse vector stores indexing, and S = 300 , O = 100 when chunking for knowledge graph indexing.", "This was decided in accordance with Xiaohua Wang et al.'s paper for vector stores indexing which showed 512 and 256 token chunks achieved the best results in Faithfulness and Relevancy metrics [9] [Figure 7]. Using the chunk size less than 512 is used to help promote in smaller contexts generated to pass to the generator LLM to address the \"Needle-in-Haystack\" challenge [8] by ensuring enough information is contained within the chunks retrieved and minimising noisy information.", "| | Chunk Size | lyft_2021.Average Faithfulness | lyft_2021.Average Relevancy |\n|---:|-------------:|---------------------------------:|------------------------------:|\n| 0 | 2048 | 80.37 | 91.11 |\n| 1 | 1024 | 94.26 | 95.56 |\n| 2 | 512 | 97.59 | 97.41 |\n| 3 | 256 | 97.22 | 97.78 |\n| 4 | 128 | 95.74 | 97.22 |\n\nFigure 7: Chunk sizes and their effects on Answer Faithfulness and Answer Relevancy.", "3.1.3 Retrieval and Generation\n\nFor all pipelines the following system prompt was used for generation of the final answer:", "| | 0 |\n|---:|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 0 | Answer Generation System Prompt |\n| 1 | Additional Information: context from retrieval Instruction: You are a smart LLM who gives an answer to the question in as little words as possible using the additional information provided above. Question: question Short Answer: |\n\nWe establish the top k chunks C q retrieved from the dense and sparse vector databases for a question q is a hybrid retriever [Figure 8 (b)] defined as:\n\nwhere S is the set of scores computed for each chunk in C (2) using a scoring function scr of the question q , chunk c and a weighting parameter \u03bb :", "Where e represents the dense embeddings model as a function and z represents the sparse embedding function.", "The first step in the K 2 RAG pipeline [Figure 8] is to use a knowledge graph trained on summarized data to generate an answer containing the identified topics relevant to the query [Figure 8(A)]. Then a Longformer model is used to summarize the results obtained from the knowledge graph [Figure 8(B)]. Over the knowledge graph results summary, we split it into chunks of S = 128 and O = 10 following our text chunking strategy (1) . This is so that each chunk contains information about a clear topic needed to help answer the question from which for each chunk we use a quantized LLM to create a sub-question out of it [Figure 8(C)] with the following specialized prompt:", "3.2 Implemented Naive RAG Pipelines\n\nWe implemented 4 common naive RAG pipelines;\n\nSemantic - Retrieving only from a Dense Vector Database.\n\nKeyword - Retrieving only from a Sparse Vector Database.", "Hybrid - Retrieving from both Dense and Sparse Vector Database.\n\nKnowledge Graph (KG) - Retrieving only from a Knowledge Graph.", "3.2.1 Indexing\n\nThe indexing process for each information store is the same as in K 2 RAG but with chunks C created with the unsummarized corpus cor unsum instead:", "3.2.2 Retrieval and Generation\n\nAll na\u00efve retrieval-augmented pipelines in our framework follow a two-step process: (1) retrieval of information relevant to the input question, and (2) passage of the retrieved information to an unquantized large language model (LLM) for answer generation. In the case of our Semantic, Keyword, and Hybrid pipelines, we use a hybrid retrieval mechanism that combines semantic similarity and keyword matching. The weighting parameter \u03bb controls the balance between the two retrieval strategies: a value of \u03bb = 1 corresponds to purely semantic retrieval [Figure 9(S)], \u03bb = 0 corresponds to purely keyword-based retrieval [Figure 9(K)], and \u03bb = 0 . 8 gives higher preference to semantic retrieval while still incorporating keyword-based signals [Figure 9(H)].", "For the naive Knowledge Graph (KG) retrieval pipeline, the retrieval process differs significantly. Instead of directly retrieving passages from the unsummarized corpus through vector search, we retrieve structured information from a", "Figure 8: K 2 RAG framework overview. Component (a): Knowledge Graph. Component (b): Hybrid Retriever containing Dense and Sparse retrievers. Component (c): Embeddings Model. Step (A): Knowledge Graph results. Step (B): Summarise Knowledge Graph results. Step (C): Generating sub-questions from chunks. Step (D): Generating sub-answers for sub-questions. Step (E): Generate the final answer from sub-answers and Knowledge Graph Results Summary.\n\nA Retrieve from Knowledge Graph B Results C Generate sub-questions from Results Summary D Answer Each Sub-question and Summarise Answer Embeddings Quantised LLM Hybrid Vector Retriever E\n\nGive final answer with\n\nsub-answers and Knowledge Graph User Question Final Answer SUMMARY\n\nknowledge graph that has been constructed using the unsummarized corpus and a quantized LLM. In this setup, the same quantized LLM is employed both for generating the knowledge graph and for querying it at retrieval time.", "Model Size: Large vs. Mid-size\n\nTo study the role of scale, we compare: (1) Qwen3-235B-A22B (Yang et al. 2025), the flagship model with 235 billion total parameters and 22 billion active, optimized for long-context reasoning, and (2) Qwen3-30B-A3B (Yang et al. 2025), a 30B-parameter variant with 3B active, designed for faster inference and reduced hardware costs.", "Figure 4: Overall performance of different systems across regions. (Subject to aesthetic changes.)\n\nAnswer Quality Faithfulness Recall@30 0.0 0.2 0.4 0.6 0.8 1.0 1.2 Score 69.6% N/A 68.2% 97.0% 9.5% 88.0% 96.9% 93.2% Overall Performance General LLMs Vanilla RAG GridCodex\n\n| | Model | Answer Quality | Faithfulness |\n|---:|:----------|-----------------:|---------------:|\n| 0 | 235B-A22B | 0.852 | 0.968 |\n| 1 | 30B-A3B | 0.74 | 0.995 |\n\nTable 3: Model size impact (Netherlands dataset).", "As shown in Table 3, scaling from 30B to 235B parameters improves answer quality by 11.2%. The larger model demonstrates stronger multi-hop reasoning, cross-clause referencing, and ambiguity resolution due to greater representational capacity and expert routing. However, this gain comes at a cost: faithfulness drops slightly (0.968 vs. 0.995), as the 235B model integrates retrieved context more aggressively, occasionally introducing inferred connections not explicitly present in the documents. This highlights a key trade-off-larger models enable richer reasoning but may lean toward speculative integration, whereas smaller models remain more conservative and strictly grounded in retrieved evidence.", "3 Experiments\n3.1 Experimental Setup\n\nWe split DentalQA into training and test sets in a 4:1 ratio while preserving each subfield's proportions, and report all results on the held-out test set. MCQperformance is measured by Accuracy; MAQ by Accuracy, Precision, Recall and F1; and OEQ and DEF by BERTScore F1 (Zhang et al., 2019). We conduct our experiments on multiple popular LLMs. For general LLMs, we select DeepSeek-V3, DeepSeek-R1, GPT-4o, GPT-4o-mini, LLaMA-3.23B-Instruct, LLaMA-3.1-8B-Instruct (Grattafiori et al., 2024) and Qwen-2.5-1.5/3B/7B/14B/32BInstruct (Qwen et al., 2025). For medical LLMs, we select BioMistral-7B (Labrak et al., 2024), HuatuoGPT2-7B (Chen et al., 2024) and LLaMA3-8B-UltraMedical (Zhang et al., 2024). We evaluate in a zero-shot setting using task-specific prompt templates (Appendix A.3). Experiments are conducted on eight NVIDIA RTX 3090 GPUs.", "3.2 Domain Adaptation on Qwen2.5-3B\n\nTo enhance dentistry-specific knowledge and capabilities, we adopt three adaptation strategies based on Qwen-2.5-3B-Instruct. (a) Supervised FineTuning (SFT): Full-model fine-tuning on the DentalQA training split for four epochs with a learning rate of 1e-6 and batch size 16 using bfloat16 precision. (b) Retrieval-Augmented Generation (RAG): At inference, retrieve the top-5 most relevant passages from DentalCorpus via FAISS with bge-m3 embeddings and prepend them to the prompt (Appendix A.3). (c) SFT + RAG: Combine the above supervised fine-tuning with retrieval augmentation during inference.", "3.3 Results\n\nThe main results are presented in Table 1, where we report the performance of 14 LLMs and our domain adaptation results.", "Overall Trends. Performance varies markedly by language. On DentalBench-ZH, DeepSeek-R1 achieves state-of-the-art accuracy on both MCQ", "| | Model | DentalQA-ZH.MCQ.ACC | DentalQA-ZH..ACC | DentalQA-ZH.MAQ.P | DentalQA-ZH.MAQ.R | DentalQA-ZH..F1 | DentalQA-ZH.OEQ.BERTScore | DentalQA-ZH.DEF.BERTScore | DentalQA-EN.MCQ.ACC | DentalQA-EN.OEQ.BERTScore | DentalQA-EN.DEF.BERTScore |\n|---:|:--------------------------------|:--------------------------------|:--------------------------------|:--------------------------------|:--------------------------------|:--------------------------------|:--------------------------------|:--------------------------------|:--------------------------------|:--------------------------------|:--------------------------------|\n| 0 | General LLMs | | | | | | | | | | |\n| 1 | GPT-4o | 64.86 | 37.30 | 87.75 | 81.74 | 84.63 | 27.23 | 21.60 | 73.98 | 31.28 | 29.21 |\n| 2 | GPT-4o-mini | 51.65 | 29.73 | 81.36 | 87.37 | 84.26 | 26.48 | 19.50 | 60.59 | 34.55 | 29.42 |\n| 3 | Deepseek-V3 | 69.28 | 41.35 | 87.22 | 86.23 | 86.72 | 27.79 | 17.78 | 68.28 | 27.65 | 25.73 |\n| 4 | Deepseek-R1 | 76.06 | 43.51 | 88.64 | 86.68 | 87.65 | 26.77 | 15.81 | 60.04 | 20.91 | 18.58 |\n| 5 | Llama-3.2-3B | 38.22 | 7.30 | 72.24 | 65.01 | 68.44 | 19.88 | 15.13 | 48.96 | 28.13 | 26.13 |\n| 6 | Llama-3.1-8B | 40.80 | 10.27 | 77.45 | 67.49 | 72.13 | 16.69 | 4.96 | 55.60 | 25.31 | 21.75 |\n| 7 | Qwen2.5-1.5B | 45.58 | 13.24 | 76.67 | 77.55 | 77.11 | 21.74 | 8.57 | 38.09 | 26.10 | 21.59 |\n| 8 | Qwen2.5-3B | 48.63 | 19.19 | 77.70 | 80.37 | 79.01 | 20.89 | 11.16 | 41.77 | 34.48 | 29.62 |\n| 9 | Qwen2.5-7B | 60.29 | 26.22 | 83.08 | 79.22 | 81.11 | 26.37 | 11.59 | 49.23 | 26.28 | 22.12 |\n| 10 | Qwen2.5-14B | 66.48 | 33.51 | 84.05 | 85.01 | 84.53 | 25.47 | 12.69 | 50.49 | 26.68 | 21.93 |\n| 11 | Qwen2.5-32B | 70.86 | 39.46 | 85.50 | 86.15 | 85.82 | 26.02 | 11.65 | 58.34 | 26.59 | 22.78 |\n| 12 | Medical LLMs | | | | | | | | | | |\n| 13 | BioMistral-7B | 25.44 | 5.68 | 76.33 | 47.43 | 58.51 | 14.48 | 14.06 | 34.96 | 34.50 | 29.55 |\n| 14 | HuatuoGPT2-7B | 22.51 | 6.22 | 74.50 | 67.54 | 70.85 | 25.38 | 21.04 | 25.47 | 15.50 | 16.55 |\n| 15 | Llama-3-8B-UltraMedical | 30.32 | 11.08 | 72.86 | 81.52 | 76.95 | 18.74 | 9.18 | 46.10 | 26.76 | 24.80 |\n| 16 | Domain Adaptation on Qwen2.5-3B | Domain Adaptation on Qwen2.5-3B | Domain Adaptation on Qwen2.5-3B | Domain Adaptation on Qwen2.5-3B | Domain Adaptation on Qwen2.5-3B | Domain Adaptation on Qwen2.5-3B | Domain Adaptation on Qwen2.5-3B | Domain Adaptation on Qwen2.5-3B | Domain Adaptation on Qwen2.5-3B | Domain Adaptation on Qwen2.5-3B | Domain Adaptation on Qwen2.5-3B |\n| 17 | Qwen2.5-3B | 48.63 | 19.19 | 77.70 | 80.37 | 79.01 | 20.89 | 11.16 | 41.77 | 34.48 | 29.62 |\n| 18 | w . SFT | 54.58 | 25.60 | 75.57 | 93.24 | 83.48 | 22.42 | 15.29 | 47.90 | 37.74 | 30.79 |\n| 19 | w . RAG | 54.45 | 21.35 | 74.88 | 91.17 | 82.22 | 30.18 | 22.13 | 48.74 | 36.47 | 30.04 |\n| 20 | w . SFT+RAG | 60.06 | 29.07 | 77.30 | 93.46 | 84.62 | 30.06 | 20.85 | 52.15 | 37.68 | 29.65 |", "Table 1: Overall Performance on DentalQA. We use Accuracy (ACC), Precistion (P), Recall (R), F1, and BERTScore F1 (BERTScore) as our metrics. Bold indicates the best result, and underline indicates the second best.", "and MAQ, with DeepSeek-V3 and Qwen2.5-32B close behind. Conversely, on DentalBench-EN, GPT-4o leads across these tasks. In both languages, however, open-ended tasks (OEQ and DEF) trail far behind MCQ and MAQ, underscoring enduring challenges in domain-specific generative reasoning and terminology.", "General Models vs. Medical Models. Although medical LLMs perform relatively well on OEQ and DEF, they fall markedly short of general-purpose models on MCQ and MAQ. For example, Llama3.1-8B consistently outperforms its medical counterpart across all multiple-choice tasks, suggesting that medical tuning may insufficiently capture dentistry-specific factual knowledge.", "Impact of Model Scale. In the Qwen-2.5 family, scaling improves MCQ and MAQ notably but yields limited gains on OEQ and DEF, suggesting factual recall benefits more from model size than generative reasoning does.", "Domain Adaptation. Both SFT and RAG improve MCQ and MAQ, but RAG shows a larger impact on open-ended tasks (e.g., OEQ-ZH BERTScore: +9.29 vs. +1.53). Combining both yields additive gains-especially on MCQ and MAQ (+11.43 and\n\n+9.88). For OEQ/DEF, SFT+RAG offers clear benefit over SFT alone in Chinese, while in English the effect is less consistent, indicating language sensitivity in retrieval effectiveness.", "6. Analysis\n\n6.1. Case Study: Bridging Parametric and Non-Parametric Methods\n\nMemory Decoder fundamentally learns to compress the knowledge stored in large non-parametric datastores into a compact parametric model, combining the memorization capabilities of retrieval methods with the efficiency and generalization of parametric approaches. To validate this hypothesis, we conducted case studies on WikiText-103 examining how different methods assign probabilities to specific tokens.", "As shown in Table 5, Memory Decoder exhibits two crucial capabilities:\n\nLong-tail Knowledge: For factual information like \"Jacobi\" and \"1906\", Memory Decoder assigns dramatically higher probabilities than the base model (68.94% vs. 0.12% and 98.65% vs. 1.57%), successfully capturing the memorization benefits of non-parametric methods.", "Semantic Coherence: For function words and logical continuations like \"on\" and \"C\", Memory Decoder maintains probabilities closer to the base model rather than following kNN-LM's lower probabilities, demonstrating its ability to preserve coherent language modeling capabilities that pure retrieval methods sacrifice.", "| | Long-tail Knowledge Learning.Context (target token underlined) | Long-tail Knowledge Learning.MemDec | Long-tail Knowledge Learning.kNN | Long-tail Knowledge Learning.Base LM |\n|---:|:---------------------------------------------------------------------------------------------------------------|:--------------------------------------|:-----------------------------------|:---------------------------------------|\n| 0 | he starred alongside actors Mark Strong and Derek Jacobi | 68.94% | 9.39% | 0.12% |\n| 1 | The launch of HMS Dreadnought in 1906 by the Royal Navy raised the stakes | 98.65% | 40.62% | 1.57% |\n| 2 | Semantic Coherence and Reasoning | Semantic Coherence and Reasoning | Semantic Coherence and Reasoning | Semantic Coherence and Reasoning |\n| 3 | Context (target token underlined) | MemDec | kNN | Base LM |\n| 4 | In 2000 Boulter had a guest-starring role on the television series The Bill | 40.11% | 8.07% | 45.51% |\n| 5 | ...three tank squadrons for special overseas operations, known as 'A', 'B' and ' C ' Special Service Squadrons | 50.10% | 10.76% | 63.04% |", "Table 5 | Probability assignments for specific tokens by different methods. Orange section : Memory Decoder excels at capturing long-tail factual knowledge, assigning dramatically higher probabilities than the base model. Cyan section : For semantic coherence, Memory Decoder intelligently balances between kNN-LM and base model probabilities, preserving linguistic fluency.", "| | | GPT2-small | GPT2-medium | GPT2-large | GPT2-xl | Avg |\n|---:|:-----------------------|-------------:|--------------:|-------------:|----------:|------:|\n| 0 | Base | 24.89 | 18.29 | 15.8 | 14.39 | 18.34 |\n| 1 | DAPT | 14.76 | 12.78 | 11.1 | 10.16 | 12.2 |\n| 2 | + MemDec-small (124M) | 13.36 | 12.25 | 11.53 | 10.93 | 12.01 |\n| 3 | + MemDec-medium (345M) | 12.08 | 11.59 | 10.92 | 10.43 | 11.26 |\n| 4 | + MemDec-large (774M) | 11.67 | 11.23 | 10.83 | 10.28 | 11 |\n\nTable 6 | Performance comparison with different Memory Decoder sizes. Even small Memory Decoders achieve competitive performance with full-parameter DAPT while maintaining plug-and-play capability.", "These observations confirm that Memory Decoder occupies a unique position: it enhances memorization of domain-specific and long-tail knowledge like non-parametric methods, while maintaining the generalization and reasoning capabilities inherent to parametric models.", "6.2. Impact of Memory Decoder Size\n\nTable 6 examines how Memory Decoder size affects performance across the GPT2 family. As Memory Decoder size increases, performance consistently improves across all base models, with the large variant achieving the best average perplexity. These results validate that Memory Decoder provides an efficient alternative to full model fine-tuning: practitioners can choose the decoder size based on their computational constraints while maintaining the crucial advantage of preserving the original model's capabilities.", "6.3. Ablation on the pre-training objective\n\nWe compare Memory Decoder against logit interpolation with a DAPT model. Table 7 shows Memory Decoder consistently outperforms DAPT interpolation across all GPT2 scales, with an average gain of", "| | Base Model | Baseline PPL | + DAPT-small | + MemDec-small |\n|---:|:-------------|---------------:|---------------:|:-----------------|\n| 0 | GPT2-small | 24.89 | 15.95 | 13.36 (-2.59) |\n| 1 | GPT2-medium | 18.29 | 14.26 | 12.25 (-2.01) |\n| 2 | GPT2-large | 15.8 | 13.13 | 11.53 (-1.60) |\n| 3 | GPT2-xl | 14.39 | 12.3 | 10.93 (-1.37) |\n| 4 | Average | 18.34 | 13.91 | 12.01 (-1.90) |\n\nTable 7 | Memory Decoder vs. DAPT model interpolation on WikiText-103. Both use 124M parameter models with different training objectives.", "1.90 perplexity points. Notably, the gains persist even at larger scales, confirming that our hybrid pre-training objective provides complementary value beyond what standard language modeling objectives can achieve, regardless of model size."]}, {"type": "entry", "index": 29, "question": "best rag eval benchmarks and why?", "answer": "There isn\u2019t a single \u201cbest\u201d RAG benchmark; you normally need a small suite that covers different failure modes.\n\n1) General retrieval quality\n- BEIR: 17 heterogeneous search tasks, great for testing if your retriever generalizes beyond one domain [1].\n- TREC Deep Learning + TREC 2024 RAG Track: strong, human\u2011curated IR and now end\u2011to\u2011end RAG comparisons, with industrial baselines via Ragnar\u00f6k [1][4].\n\n2) Open\u2011domain and multi\u2011hop QA (classic RAG setting)\n- Natural Questions, TriviaQA, SQuAD, PopQA: good for single\u2011hop knowledge lookup and answer accuracy (EM/F1) [1][5].\n- HotpotQA, 2WikiMultiHopQA, MuSiQue: require multi\u2011step reasoning and multi\u2011hop retrieval; include supporting\u2011fact metrics, so you can see if retrieval really brings the needed evidence [1][5].\n- KILT: multi\u2011task (QA, fact\u2011checking, etc.) with a shared corpus, widely used to compare RAG models across tasks [4].\n\n3) Fact\u2011checking / hallucination stress\u2011tests\n- FEVER, HoVeR, QuanTemp, AveriTeC: test if the system can verify claims (including multi\u2011hop and numeric claims) against retrieved evidence, which is central for reliable RAG [1].\n- RAGTruth: corpus for fine\u2011grained hallucination analysis in RAG outputs [4].\n\n4) Complex reasoning and long\u2011context RAG\n- BRIGHT and complex tasks like Deep Research, GPQA, MATH500, BrowseComp\u2011Plus: stress reasoning\u2011aware retrieval and multi\u2011step search planning, closer to real \u201cagentic\u201d RAG [1].\n- LongBench plus HiCBench: designed to probe long\u2011document RAG and chunking; HiCBench is particularly good for diagnosing how retrieval and chunking affect evidence recall and answer quality [2].\n\n5) End\u2011to\u2011end RAG evaluation frameworks\n- RAGAS, ARES, CRUX, and Auepora: not datasets but toolkits that jointly score retrieval, faithfulness, and answer correctness, helping you see where your RAG pipeline breaks [1][3][4].", "tags": ["benchmarks_eval"], "full_faithfulness": 1.0, "full_answer_relevancy": 0.0, "seed_faithfulness": 0.6538461538, "seed_answer_relevancy": 0.0, "context_relevance": 0.8364495596147826, "context_relevance_error": null, "context_relevance_reason": "The context passages provide substantial information about various evaluation benchmarks relevant to Retrieval-Augmented Generation (RAG) systems, addressing the user's question about the best RAG evaluation benchmarks. Specific datasets and their purposes are discussed, such as BEIR and TREC, which directly relate to the evaluation of RAG systems. However, while the context is informative, it could be more explicitly focused on the 'best' benchmarks, as the user requested, rather than just listing various datasets and their characteristics.", "structure_citation": 0.7305989070626036, "structure_error": null, "structure_reason": "The Actual Output includes several claims supported by citations, and most of these citations correspond to snippets in the provided Context. However, there are some instances where the citations [1] and [4] are used multiple times without clear differentiation, which could lead to confusion regarding their specific references. Additionally, while the majority of claims are well-supported, a few claims could benefit from more explicit connections to the cited snippets, particularly in the context of complex reasoning and multi-hop QA. Overall, the response demonstrates a strong understanding of RAG benchmarks and their evaluation, but minor issues with citation clarity prevent a perfect score.", "hallucination_score": 0.8915353393638871, "hallucination_error": null, "hallucination_reason": "The response accurately identifies a range of RAG benchmarks and their purposes, aligning well with the context provided. It covers various aspects such as general retrieval quality, open-domain QA, fact-checking, and complex reasoning, all of which are supported by the context. However, while the output is comprehensive, it could benefit from a more explicit connection to the evaluation metrics mentioned in the context, which would enhance its alignment further.", "contexts_used": ["6 Datasets and Evaluation Benchmarks\n\nThe evaluation of IR and RAG systems relies on diverse datasets that test different aspects of retrieval and answer generation capabilities, including the retrieval, ranking, and answer quality.", "6.1 IR Specific\n\nInformation retrieval has a long history of evaluation campaigns, including those from TREC, CLEF, NTCIR, and FIRE. The queries used in these collections are often developed to strike a balance between having too many relevant documents in the target collection (which can be too easy to retrieve and too difficult to properly annotate (Voorhees et al., 2022)) and too few relevant documents. Sometimes challenging topics are also developed deliberately (Voorhees, 2005). The topic development process often involves manual reformulation of queries to ensure good coverage of relevance assessments. Mirroring the annotation process itself, it can be beneficial for automated retrieval systems to also perform various forms of query understanding (expansion or rewriting) to help ensure high recall. Hence, the approaches described in Section 3 show performance gains.", "Comprehensive benchmarks like BEIR (Thakur et al., 2021) offer heterogeneous evaluation across 17 diverse datasets spanning multiple domains and retrieval tasks, enabling zero-shot generalization assessment. In addition, evaluation sets such as TREC DL (Deep Learning) and its variants (Craswell et al., 2020, 2021, 2022, 2023) are used to evaluate IR systems.", "6.2 Question Answering\n\nQuestion Answering datasets for retrieval evaluation fall into two primary categories based on complexity. Single-hop QA benchmarks include Natural Questions (NQ) (Kwiatkowski et al., 2019), TriviaQA (Joshi et al., 2017), SQuAD (Rajpurkar et al., 2016), and PopQA (Mallen et al., 2023) test the RAG system's ability to retrieve and utilize information to generate the final answer. Multi-hop QA benchmarks comprise 2WikiMultiHopQA (Ho et al., 2020) (questions requiring evidence from exactly two Wikipedia articles via bridging/comparison), HotpotQA (Yang et al., 2018), and MuSiQue (Trivedi et al., 2022), which features compositionally complex questions constructed from interconnected sub-questions. Due to the dependency on intermediate answers, such questions cannot be answered through isolated single-step retrieval. Therefore, this design naturally motivates the RAG systems to decompose the original query and new queries based on the intermediate answers help in the retrieval of better context.", "6.3 Fact Checking/Verification\n\nThese datasets assess models' ability to verify claims against retrieved evidence. FEVER (Fact Extraction and VERification) (Thorne et al., 2018) is an open-domain fact-checking benchmark that requires retrieval over a large collection of Wikipedia articles and training NLI models to classify claims as supported, refuted, or not having enough information based on Wikipedia evidence. While FEVER deals with simple claims, HoVeR (Jiang et al., 2020) proposes a fact-checking benchmark with claims that require multi-hop reasoning and multi-turn retrieval. QuanTemp (V et al., 2024) is the first to propose a large open-domain benchmark for fact-checking numerical claims. It comprises claims that require interleaving claim decomposition and retrieval based on the verification output of sub-claims. This relates to query-level feedback and generation-time feedback. AveriTeC (Schlichtkrull et al., 2023) is a real-world claim verification dataset that includes diverse claim types with supporting or refuting evidence gathered through web search, making it particularly valuable for evaluating RAG systems on authentic fact-checking scenarios that require verification against potentially noisy or contradictory web sources.", "6.4 Complex Reasoning\n\nEmerging benchmarks introduce novel relevance criteria requiring awareness of reasoning structure. BRIGHT (SU et al., 2025) defines document relevance not by topical alignment but by whether passages contain logical constructs (deductive steps, analogies, constraints) necessary to derive answers. This challenges lexical retrievers that lack inference-awareness. However, reasoningaugmented retrieval methods, such as CoT (Chainof-Thought), have shown performance gains. Further, there exist reasoning/agentic tasks like Deep Research (Wu et al., 2025), GPQA (Rein et al., 2023), MATH500 (Cobbe et al., 2021), which involve access to the web search engine, have been used in the RAG setting (Li et al., 2025a; Wei et al., 2025). However, due to the absence of the cor- pus, it is hard to evaluate the retrieval performance. Recent benchmarks like BrowseComp-Plus (Chen et al., 2025b) provide a curated corpus for Deep Research tasks and enable the evaluation of retrieval performance.", "6.5 Evaluation Metrics\n\nEvaluating retrieval systems and RetrievalAugmented Generation (RAG) pipelines is critical for ensuring the accuracy, relevance, and reliability of generated responses. Retrieval evaluation typically focuses on metrics such as recall@k, precision, mean reciprocal rank (MRR), and hit rate, which assess how effectively the system retrieves pertinent documents or passages given a query. In contrast, RAG evaluation is more holistic, combining retrieval quality with generation fidelity and coherence. Common approaches include measuring answer correctness using exact match (EM) or F1 score, assessing faithfulness to retrieved evidence to detect hallucinations, and evaluating relevance and fluency through human or automated scoring (e.g., BLEU, ROUGE, or BERTScore). Recent frameworks like RAGAS (ExplodingGradients, 2024), ARES (Saad-Falcon et al., 2024), and CRUX (Ju et al., 2025) also emphasize end-to-end evaluation, where the interplay between retrieval accuracy and generation quality is analyzed to identify bottlenecks-such as irrelevant documents leading to incorrect answers-making comprehensive evaluation essential for diagnosing and improving RAG system performance.", "Challenges . The current RAG evaluation methods mainly focus on the retrieval and final answer performance. However, Reasoning RAG systems are highly dependent on intermediate reasoning steps and retrieval rounds. Therefore, it is also important to consider additional evaluation dimensions such as computational cost, efficiency, or number of retrieval rounds.", "2 Related Works\n\nTraditional Text Chunking. Text chunking divides continuous text into meaningful units like sentences, phrases, and words, with our focus on sentence-level chunking. Recent works have explored various approaches: [Cho et al., 2022] combines text chunking with extractive summarization using hierarchical representations and determinantal point processes (DPPs) to minimize redundancy, [Liu et al., 2021] presents a pipeline integrating topical chunking with hierarchical summarization, and [Zhang et al., 2021] develops an adaptive sliding-window model for ASR transcripts using phonetic embeddings. However, these LSTM and BERT[Devlin et al., 2019] based methods face limitations from small context windows and single-level chunking capabilities.", "RAG-oriented Document Chunking. Recent research has explored content-aware document chunking strategies for RAG systems. LumberChunker[Duarte et al., 2024] uses LLMs to identify semantic shifts,but may miss hierarchical relationships. PIC[Wang et al., 2025] proposes pseudo-instruction for document chunking, guide chunking via document summaries, though its single-level approach may oversimplify document structure. AutoChunker[Jain et al., 2025] employs tree-based representations but primarily focuses on noise reduction rather than multi-level granularity. Late Chunking[G\u00fcnther et al., 2024] embeds entire documents before chunking to preserve global context, but produces flat chunk lists without modeling hierarchical relationships. In contrast, our HierarchyChunk method creates multi-level document representations, chunking from coarse sections to fine-grained paragraphs. This enables RAG systems to retrieve information at appropriate abstraction levels, effectively bridging fragmented knowledge gaps and providing comprehensive document understanding.", "Limitations of Existing Text Chunking Benchmarks. The evaluation of text chunking and RAG methods heavily relies on benchmark datasets. Wiki-727[Koshorek et al., 2018],VT-SSum[Lv et al., 2021] and NewsNet[Wu et al., 2023] are typically chunked into flat sequences of paragraphs or sentences, without capturing the multi-level organization (e.g., sections, subsections, paragraphs) inherent in many real-world documents. This single-level representation limits the ability to evaluate chunking methods that aim to preserve or leverage document hierarchy, which is crucial for comprehensive knowledge retrieval in complex RAG scenarios. While Qasper[Dasigi et al., 2021], HotpotQA[Yang et al., 2018] and GutenQA[Duarte et al., 2024] are designed for RAG-related tasks, they do not specifically provide mechanisms or metrics for evaluating the efficacy of document chunking strategies themselves. Their focus is primarily on end-to-end RAG performance, where the impact of chunking is implicitly measured through retrieval and generation quality. This makes it challenging to isolate and assess the performance of different chunking methods independently, hindering systematic advancements in hierarchical document chunking. Our work addresses these gaps by proposing a method that explicitly considers multi-level document chunking and constructs a novel benchmark from a chunking perspective.", "5 Experiments\n5.3 Experimental Settings\n\nIn the model training of HiChunk, Gov-report[Huang et al., 2021], Qasper[Dasigi et al., 2021] and Wiki727[Koshorek et al., 2018] are the train datasets, which are publicly available datasets with explicit document structure. We use Qwen3-4B[Team, 2025] as the base model, with a learning rate of 1e-5 and a batch size of 64. The maximum length of training and inference is set to 8192 and 16384 tokens, respectively. Meanwhile, the length of each sentence is limited to within 100 characters. Due to the varying sizes of chunks resulting from semantic-based chunking, we limit the length of the retrieved context based on the number of tokens rather than the number of chunks for a fair comparison. The maximum length of the retrieved context is set to 4096 tokens. We also compare the performance of different chunking methods under different retrieved context length settings in subsection 5.6. In the RAG evaluation process, we consistently use Bge-m3[Chen et al., 2024] as the embedding model for context retrieval. As for the response model, we use three different series of LLMs with varying scales: Llama3.1-8b[Dubey et al., 2024], Qwen3-8b, and Qwen3-32b[Team, 2025].", "5.4 Chunking Accuracy\n\nTo comprehensively evaluate the performance of the semantic-based chunking method, we conducted experiments using two publicly available datasets, along with the proposed benchmark, to assess the cutpoint accuracy of the chunking method. Since the SC and LC chunking methods are limited to performing single-level chunking, we evaluated only the F1 scores for the initial level of chunking points and the F1 scores without regard for the hierarchy of chunking points. The evaluation results are presented in Table 3. In the Qasper and Gov-report datasets, which serve as in-domain test sets, the HC method shows a significant improvement in chunk accuracy compared to the SC and LC methods. Additionally, in HiCBench, an out-of-domain test set, the HC method exhibits even more substantial accuracy improvements. These findings demonstrate that HC enhances the base model's performance in document chunking by focusing exclusively on the chunking task. Moreover, as indicated in the subsequent experimental results presented in subsection 5.5, the accuracy improvement of the HC method in document chunking leads to enhanced performance throughout the RAG pipeline. This includes improvements in the quality of evidence retrieval and model responses.", "| | Chunk.Method | Qasper.F 1 L 1 | Qasper.F 1 L 2 | Qasper.F 1 L all | Gov-Report.F 1 L 1 | Gov-Report.F 1 L 2 | Gov-Report.F 1 L all | HiCBench.F 1 L 1 | HiCBench.F 1 L 2 | HiCBench.F 1 L all |\n|---:|:---------------|-----------------:|:-----------------|-------------------:|---------------------:|:---------------------|-----------------------:|-------------------:|:-------------------|---------------------:|\n| 0 | SC | 0.0759 | - | 0.1007 | 0.0298 | - | 0.0616 | 0.0487 | - | 0.1507 |\n| 1 | LC | 0.5481 | - | 0.6657 | 0.1795 | - | 0.5631 | 0.2849 | - | 0.4858 |\n| 2 | HC | 0.6742 | 0.5169 | 0.9441 | 0.9505 | 0.8895 | 0.9882 | 0.4841 | 0.3140 | 0.545 |\n\nTable 3. Chunking accuracy. HC means the result of HiChunk without fixed size chunking. The best result is in bold .", "5.5 RAG-pipeline Evaluation\n\nWeevaluated the performance of various chunking methods on the LongBench, Qasper, GutenQA, OHRBench and HiCBench datasets, with the results detailed in Table 4. The performance of each subset in LongBench is shown in Table A1. The results demonstrate that the HC200+AM method achieves either optimal or suboptimal performance on most LongBench subsets. When considering average scores, LumberChunk remains a strong baseline. However, as noted in Table 2, both GutenQA and OHRBench datasets exhibit the feature of evidence sparsity, meaning that the evidence related to QA pairs is derived from only a few sentences within the document. Consequently, the different chunking methods show minimal variation in evidence recall and response quality metrics on these datasets. For instance, using Qwen3-32B as the response model on the GutenQA dataset, the evidence recall metrics of FC200 and HC200+AM are 64.5 and 65.53, and the Rouge metrics are 44.86 and 44.94, respectively. Another example is OHRBench dataset, the evidence recall metrics and Rouge metrics of FC200, LC, HC200 and HC200+AM are very close. In contrast, the Qasper and HiCBench datasets contain denser evidence, where a better chunking method results in higher evidence recall and improved response quality. Again using Qwen3-32B as an example, on the T 1 task of HiCBench dataset, the evidence recall metric for FC200 and HC200+AM are 74.06 and 81.03, the Fact-Cov metrics are 63.20 and 68.12, and the Rouge metrics are 35.70 and 37.29, respectively. These findings suggest that the evidence-dense QA in the HiCBench dataset is better suited for evaluating the quality of chunking methods, enabling researchers to more effectively identify bottlenecks within the overall RAG pipeline.", "| | Chunk Method.Chunk Method | LongBench Score.LongBench Score | Qasper.ERec | Qasper.F1 | GutenQA.ERec | GutenQA.Rouge | OHRBench( T 0 ).ERec | OHRBench( T 0 ).Rouge | HiCBench( T 1 ).ERec | HiCBench( T 1 ).FC | HiCBench( T 1 ).Rouge | HiCBench( T 2 ).ERec | HiCBench( T 2 ).FC | HiCBench( T 2 ).Rouge |\n|---:|:----------------------------|:----------------------------------|:--------------|:------------|:---------------|:----------------|:-----------------------|:------------------------|:-----------------------|:---------------------|:------------------------|:-----------------------|:---------------------|:------------------------|\n| 0 | Llama3.1-8B | Llama3.1-8B | Llama3.1-8B | Llama3.1-8B | Llama3.1-8B | Llama3.1-8B | Llama3.1-8B | Llama3.1-8B | Llama3.1-8B | Llama3.1-8B | Llama3.1-8B | Llama3.1-8B | Llama3.1-8B | Llama3.1-8B |\n| 1 | FC200 | 42.49 | 84.08 | 47.26 | 64.43 | 30.03 | 67.03 | 51.01 | 74.84 | 47.82 | 28.43 | 74.61 | 46.79 | 30.97 |\n| 2 | SC | 42.12 | 82.08 | 47.47 | 58.30 | 28.58 | 62.65 | 49.10 | 72.14 | 46.80 | 28.43 | 73.49 | 45.28 | 30.92 |\n| 3 | LC | 42.73 | 87.08 | 48.20 | 63.67 | 30.22 | 68.42 | 51.85 | 76.64 | 50.84 | 29.62 | 76.12 | 49.12 | 32.01 |\n| 4 | HC200 | 43.17 | 86.16 | 48.09 | 65.13 | 29.95 | 68.25 | 51.33 | 78.52 | 49.87 | 29.38 | 78.76 | 49.11 | 31.80 |\n| 5 | +AM | 42.90 | 87.49 | 48.95 | 65.47 | 30.33 | 67.84 | 51.92 | 81.59 | 55.58 | 30.04 | 80.96 | 53.66 | 33.04 |\n| 6 | Qwen3-8B | Qwen3-8B | Qwen3-8B | Qwen3-8B | Qwen3-8B | Qwen3-8B | Qwen3-8B | Qwen3-8B | Qwen3-8B | Qwen3-8B | Qwen3-8B | Qwen3-8B | Qwen3-8B | Qwen3-8B |\n| 7 | FC200 | 43.95 | 84.32 | 45.10 | 64.50 | 33.47 | 67.07 | 48.18 | 74.06 | 47.35 | 33.83 | 72.95 | 43.45 | 35.27 |\n| 8 | SC | 43.54 | 82.22 | 44.55 | 58.37 | 32.71 | 62.18 | 46.79 | 71.42 | 46.07 | 33.30 | 72.36 | 42.97 | 34.76 |\n| 9 | LC | 44.83 | 87.43 | 46.05 | 63.67 | 33.87 | 68.79 | 49.28 | 75.53 | 48.27 | 34.12 | 75.14 | 46.80 | 35.93 |\n| 10 | HC200 | 43.90 | 86.49 | 45.95 | 65.20 | 33.89 | 68.57 | 49.06 | 77.68 | 47.37 | 34.30 | 78.10 | 46.20 | 36.32 |\n| 11 | +AM | 44.41 | 87.85 | 45.82 | 65.53 | 34.15 | 68.31 | 49.61 | 81.03 | 50.75 | 35.26 | 80.65 | 49.02 | 37.28 |\n| 12 | Qwen3-32B | Qwen3-32B | Qwen3-32B | Qwen3-32B | Qwen3-32B | Qwen3-32B | Qwen3-32B | Qwen3-32B | Qwen3-32B | Qwen3-32B | Qwen3-32B | Qwen3-32B | Qwen3-32B | Qwen3-32B |\n| 13 | FC200 | 46.33 | 84.32 | 46.49 | 64.50 | 44.86 | 67.07 | 46.89 | 74.06 | 63.20 | 35.70 | 72.95 | 60.87 | 37.17 |\n| 14 | SC | 46.29 | 82.22 | 46.39 | 58.37 | 43.59 | 62.18 | 45.43 | 71.26 | 61.09 | 35.64 | 72.36 | 59.23 | 37.09 |\n| 15 | LC | 47.43 | 87.43 | 46.82 | 63.67 | 44.45 | 68.79 | 47.92 | 75.53 | 64.76 | 36.15 | 75.14 | 62.75 | 38.02 |\n| 16 | HC200 | 46.71 | 86.49 | 46.99 | 65.20 | 44.83 | 68.57 | 47.71 | 77.68 | 63.93 | 36.55 | 78.10 | 62.51 | 38.26 |\n| 17 | +AM | 46.92 | 87.85 | 47.25 | 65.53 | 44.94 | 68.31 | 47.89 | 81.03 | 68.12 | 37.29 | 80.65 | 66.36 | 37.37 |", "Table 4. RAG-pipeline evaluation on LongBench, Qasper, GutenQA, OHRBench and HiCBench. Evidence Recall and Fact Coverage metric is represented by ERec and FC respectively. The best result is in bold , and the sub-optimal result is in underlined", "5.6 Influence of Retrieval Token Budget\n\nSince HiCBench is more effective in assessing the performance of chunking methods, we evaluated the impact of our proposed method on the T 1 task of HiCBench under different retrieve token budgets: 2k, 2.5k, 3k, 3.5k and 4k tokens. We compared the effects of various chunking methods by calculating the Rouge metrics between responses and answers, as well as the Fact-Cov metrics. The experimental findings are illustrated in Figure 3. The results demonstrate that a larger retrieval token budget usually leads to better response quality, so it is necessary to compare different chunking methods under the same retrieval token budget. HC200+AM consistently achieves superior response quality across various retrieve token budget", "settings. These experimental results underscore the effectiveness of HC200+AM method. We further present the correspond curves of the evidence recall metrics in subsection A.2.", "Figure 3. Performance of HiCBench( T 1 ) under different retrieval token budget from 2k to 4k.\n\n2k 2.5k 3k 3.5k 4k 27.5 28.0 28.5 29.0 29.5 30.0 Rouge Llama3.1-8b FC200 SC LC HC200 HC200+AM 32.0 32.5 33.0 33.5 34.0 34.5 35.0 35.5 Qwen3-8b 36.0 36.5 37.0 Qwen3-32b Fact Cov(%)", "5.7 Effect of Maximum Hierarchical Level\n\nIn this section, we examine the impact of limiting the maximum hierarchical level of document structure obtained by HiChunk. The maximum level ranges from 1 to 4, denoted as L 1 to L 4, while LA represents no limitation on the maximum level. We measure the evidence recall metric on different settings. As shown in Figure 4. This result reveals that the Auto-Merge retrieval algorithm degrades the performance of RAG system in the L 1 setting due to the overly coarse-grained semantics of L 1 chunks. As the maximum level increases from 1 to 3, the evidence recall metric also gradually improves and remains largely unchanged thereafter. These findings highlight the importance of document hierarchical structure for enchancing RAG systems.", "Figure 4. Evidence recall metric across different maximum level on HiCBench( T 1 and T 2 ).\n\nQwen3-8b/32b Evidence Recall L1 L2 L3 L4 LA Fact-Cov", "2 Background\n2.1 LLMs\n\nLLMs are transformer-based neural networks [25, 29] trained on massive text corpora to acquire broad language understanding and reasoning capabilities. Their performance can be further enhanced through several complementary strategies. Supervised fine-tuning (SFT) adapts pretrained models to specific domains or tasks by updating their parameters on curated labeled data [19, 8, 30], while reinforcement learning from human feedback (RLHF) aligns their behavior with human preferences through reward-based optimization [16]. Both methods directly modify the model's parametric memory, embedding new knowledge into its weights, yet this knowledge remains static and can quickly become obsolete, especially in rapidly evolving or highly regulated domains [17].", "In contrast, prompt learning [33, 12] and prompt engineering [22] guide model behavior without altering its parameters, using carefully crafted instructions or few-shot exemplars to induce desired task behavior at inference time. Relatedly, chain-of-thought (CoT) [27] prompting encourages models to generate intermediate reasoning steps before producing an answer, which improves multi-step reasoning but does not expand the model's knowledge base. While these approaches enhance task adaptability and reasoning ability, they cannot update or extend the underlying knowledge stored in the model's parameters, leaving LLMs vulnerable to hallucinations, especially in specialized or fast-changing domains.", "These limitations motivate the use of RAG, which complements LLMs and the above approaches with non-parametric memory by retrieving relevant external documents at inference time, thereby grounding their outputs in accurate, verifiable, and up-to-date information.", "2.2 RAG in General\n\nRetrieval-augmented generation (RAG) has emerged as a central paradigm for enhancing LLMs by conditioning them on external knowledge sources rather than relying solely on parametric memory. By retrieving relevant information from corpora, databases, or the web, RAG mitigates key limitations of static LLMs, including hallucination, factual drift, and domain obsolescence.", "Recent surveys have mapped the evolution of RAG systems. Gao et al. [5] and Wu et al. [28] categorize approaches, analyzing design dimensions such as retrieval granularity (passages vs. documents), retriever-generator integration (late fusion vs. joint training), and memory augmentation. They also highlight persistent challenges, including reducing hallucinations, handling outdated or", "Figure 2: Overview of our Retrieval-Augmented Generation (RAG) pipeline for radio regulations QA, combining FAISS-based retrieval with LLM-based answer generation\n\nRadio Regulations Embdedding Database Large Language Model Embedder Question Similarity Search (FAISS) Context Correct Answer\n\nincomplete knowledge, and supporting efficient updates in dynamic domains. In parallel, Yu et al. [31] emphasize the need for principled evaluation that captures the hybrid nature of retrieval and generation. Together, these works establish RAG as a promising direction for improving factuality, adaptability, and transparency in LLMs while underscoring unresolved research questions.", "2.3 Evaluating RAG\n\nEvaluating RAG remains a challenging open problem due to its hybrid nature. Yu et al. [31] provide a comprehensive survey and propose Auepora , a unified evaluation framework that organizes assessment along three axes: Target , Dataset , and Metric .", "On the Target axis, they distinguish between component-level and end-to-end goals. For retrieval, the focus is on relevance (alignment between query and retrieved documents) and accuracy (alignment between retrieved content and ground-truth candidates). For generation, they emphasize relevance (response-query alignment), faithfulness (consistency with supporting documents), and correctness (agreement with reference answers). Beyond these, they highlight broader system requirements such as low latency, diversity, robustness to noisy or counterfactual evidence, and calibrated refusal, which are critical for making RAG systems usable and trustworthy in practice.", "On the Dataset axis, they note that widely used Q&A-style benchmarks often capture only narrow aspects of RAG behavior, motivating domain-specific testbeds that reflect dynamic knowledge and application-specific constraints (e.g., legal, medical, or financial domains).", "On the Metric axis, they collate traditional rank-based retrieval measures (e.g., MAP, MRR [24]), text generation metrics (e.g., ROUGE [4], BLEU [18], BERTScore [32]), and the growing use of LLM-as-a-judge for assessing faithfulness and overall quality. While LLM-based evaluation shows promise, they caution that alignment with human preferences and the need for transparent grading rubrics remain unresolved challenges.", "Our evaluation choice. In contrast to prior work that primarily reuses general-purpose benchmarks, we construct our own domain-specific testbed: a multiple-choice dataset derived directly from the ITU Radio Regulations [11] using an automated pipeline with LLM generation, automated judging, and human verification (Section 4.5). This design ensures that the benchmark reflects realistic regulatory queries while retaining ground-truth answers. Accordingly, our end-to-end metric is simply answer accuracy . To disentangle retrieval from generation, we further introduce a domain-tailored retrieval metric aligned with component-level relevance and accuracy . Technical details of this metric are developed in Section 4.1.", "Benchmarks and evaluation:\n\nKnowledge-intensive benchmarks track RAG progress. OpenQA tasks (Natural Questions, TriviaQA, HotpotQA) and the KILT benchmark suite (including QA, fact-checking, slot filling, etc.) are standard evaluation sets. In 2023, RAG models dominated many KILT tasks and few-shot QA challenges. New evaluation tools also emerged: for example, the RAGAS framework provides reference-free metrics for RAG pipelines, and the RAGTruth corpus (Niu et al. 2024) enables fine-grained analysis of hallucinations in RAG outputs. 2023 also witnessed a major spike in publications and adoption of RAG. Some other works are on Active RAG [38], Improving domain adaptation of RAG models for open question answering [85], content filtering for RAG [94], and RAG with self-memory [16].", "2024 - Recent Advances. In 2024, research on RAG continues to push on secure retrieval frameworks, multi-hop reasoning, and domain specialization. Several groups explore differentiable retrievers that can be tuned in an end-to-end pipeline, while others investigate merging large-context attention with retrieval indexing. Meanwhile, new techniques aim to reduce the chance the generator ignores retrieved data or merges contradictory documents incorrectly. RAG-based chat systems in healthcare, finance, and law now incorporate advanced fact-checking modules to ensure that only vetted external sources influence the final output. Some notable works in 2024 are Evaluating Retrieval Quality in RAG [78], Benchmarking RAG for Medicine [97], Benchmarking LLM in RAG [13], Review of RAG for AI Generated Content [106], Unifying Context Ranking with RAG [100], Searching for the Best Practice in RAG [93], Finetuning Vs RAG for Less Popular Knowledge [86], Integrating RAG wit LLM in Nephrology [61], RAG for Copyright Protection [23], RAG for Textual Graph Understanding and Question Answering [31], Interactive AI with RAG for NExt\n\nGeneration Networking [103], Web Application for RAG: Implementation and Testing [72], Overcoming Challenges for Long Input in RAG [39], and Adapting Language Model for Domain Specific RAG [104]. Some specifics in RAG development as at 2024 are discussed below:", "Infrastructure and standardized evaluation: The community recognised a need for common tooling and shared tasks. Ragnar\u00f6k introduced a reusable end-to-end framework and provided industrial baselines for the inaugural TREC 2024 RAG Track [70]. Beyond code, evaluation methodology itself became a research focus: ARAGOG proposed automatic grading of RAG outputs that correlates with human judgements, analysing retrieval precision and answer similarity across advanced pipelines [20]. These efforts mark a shift from anecdotal demos to systematic, reproducible assessment.", "Adaptive retrieval and self-reflection: Building on ideas such as Self-RAG, several 2024 works taught models to decide when-and how much-to retrieve . SAM-RAG dynamically filters documents and verifies both evidence and final answers in multimodal contexts, improving accuracy without unnecessary retrieval calls [102]. For complex visual-question-answering, OmniSearch plans multi-hop retrieval chains on the fly, demonstrating large gains on the new Dyn-VQA benchmark [53]. These results confirm that retrieval policies, not just retriever quality, matter for difficult queries.", "Multimodal RAG breaks out: Where earlier RAG research was text-only, 2024 saw a surge in multimodal extensions. SAM-RAG and OmniSearch both combine text and image evidence, while concurrent frameworks (e.g. mR 2 AG and M3DocRAG) introduce retrieval-reflection loops or structured vision-language indexes. Surveys published this year chart the design space and highlight open issues such as cross-modal alignment and vision-aware reranking [102, 53].", "Progress in dense retrieval: High-recall retrievers remain the backbone of every RAG system. 2024 research emphasised unsupervised or instruction-tuned retrievers that avoid costly labelled data, building on contrastive pre-training techniques and LLM-augmented embedding models. These retrievers power the top submissions in the TREC track and underpin production deployments discussed in industrial white papers.", "Rapid growth and forward outlook: A bibliometric snapshot counted more than 1,200 RAG-related papers on arXiv in 2024 alone [106],", "compared with fewer than 100 the previous year, underscoring the field's rapid maturation. Looking ahead, challenges include ultra-fast retrieval over trillion-token corpora, faithfulness verification for multi-hop reasoning, and energy-efficient multimodal indexing. Nevertheless, 2024 firmly established RAG as the default strategy for grounding large language (and vision-language) models in up-to-date, attributable knowledge.", "RAG + Graphs in QA\n\nTo address these limitations of traditional RAG, particularly in retrieving contextually precise and semantically rich information, research has tried to integrate structured knowledge sources and graphbased reasoning into the retrieval pipeline. One notable direction of work is LLM-based retrieval, which incorporates knowledge graphs information into the generation process of LLMs. The LLMs are augmented using the retrieved facts from the KG [26], leading to a clear dependence on the quality of extracted graph information, using it to generate responses. Research has also been done towards augmenting knowledge graph information, retrieved through some semantic or some other similarity index, in the prompts given to the LLM [27], to help the model do zero-shot question-answering. Some researchers have tried a different approach to fact retrieval, where the model tries different queries, using structured query languages, until the desired information comes through [28]. All of these approaches have used KGs as an external source to retrieve information from and answer the questions [29]. Then, aligning the retrieval process with LLM even more closely, some researchers have proposed methods which use LLMs in intermediate steps as well to plan the retrieval and judge whether the retrieved information is relevant or not [29,30], continuing the process until the desired output emerges.", "Another interesting direction of work is integrating GNNs with LLMs, which leverages graph neural networks to enhance retrieval and re-ranking using learned graph representations, along with generation capabilities of LLMs. There have been approaches such as GNN-RAG [31], which have tried combining language understanding abilities of LLMs with the reasoning abilities of GNNs in a retrieval-augmented generation (RAG) style. Other methods of GNN-LLM alignment, have been classified into symmetric and asymmetric alignment [32]. Symmetric alignment refers to the equal treatment of the graph and text modalities during the alignment process[33, 34, 35]. Asymmetric alignment focuses on allowing one modality to assist or enhance the other, here leveraging the capabilities of GNNs to improve the LLMs [36, 37, 38].", "The continual evolution of such hybrid architectures reflects the dynamic nature of the QA landscape and its responsiveness to complex information needs. Given the diversity of methods explored to enhance QA systems-from rule-based techniques to advanced neural and hybrid models-it becomes essential to establish robust mechanisms for evaluating their effectiveness. It is thus important to discuss the evaluation paradigms that compare and benchmark these systems. Extractive QA benchmarks almost universally use Exact Match (EM)-the strict proportion of predictions that character-for-character match a ground-truth answer-and F1 score, the harmonic mean of token-level precision and recall, to evaluate answer quality. On SQuAD v2.0 [39], human annotators achieve around 86.831 EM and 89.452 F1 on the test set, whereas state-of-the-art models now exceed 90 EM and 93 F1. Natural Questions [40] extends this paradigm to long-answer (paragraph) and short-answer (span or yes/no) annotations drawn from real Google search queries. Meanwhile, multiple-choice datasets like OpenBookQA [41]-designed to mimic open-book science exams-use simple accuracy. Complex reasoning benchmarks push beyond single-span extraction. HotpotQA [42], a multi-hop dataset built on pairs of Wikipedia articles, evaluates both answer-span EM/F1 and supporting-fact EM/F1, plus a joint metric requiring both to be correct; even top models achieve only ~72 joint F1[43] under the full-wiki setting, far below human performance of ~82 average F1 and ~96 upper bound F1. These core metrics and datasets underpin broader QA evaluation: multi-task suites like GLUE/MMLU [44, 45] include QA subtasks to probe general language understanding, while specialized frameworks such as MT-bench [46] ('LLM as judge') and automated platforms like Scale Evaluation layer on top to assess conversational and retrieval-augmented QA in real-world scenarios.", "This review has highlighted the broad spectrum of methodologies and evaluation strategies used in enhancing and assessing question answering systems. With this foundation in place, we now transition to the specific tools and frameworks employed in our research for graph-based question answering. In the following section, we briefly introduce and contextualize spaCy, Stanford CoreNLP, and GraphRAG-three diverse and widely-used tools we have utilized for knowledge graph construction and integration with LLMs."]}, {"type": "entry", "index": 30, "question": "neighbor expansion for chunks in rag is that a thing?", "answer": "Yes, \u201cneighbor expansion\u201d is a real idea in RAG-style systems, but it\u2019s mainly discussed in the context of graphs rather than plain text chunks.\n\n1. In mKG-RAG, they explicitly do l\u2011hop neighbor expansion on a knowledge graph: once they\u2019ve built a subgraph of entities and relations relevant to the question, they expand it by following neighbors 1 or more hops away to pull in related facts that might be missing from the initial retrieval [1]. When they remove this graph expansion step, accuracy drops on their VQA benchmarks, which shows the expansion is useful [1].\n\n2. A similar idea appears in a large\u2011scale GraphRAG variant (GeAR). They cap the number of expansion steps n=2 and note that graph expansion is mostly helpful for multi\u2011hop questions; for simple questions they can skip using external triples and graph expansion to save compute [3]. This is again expansion over neighbors in a graph, not just adjacent text.\n\n3. For plain text chunk RAG, the standard pipeline is: split docs into chunks, embed them, retrieve top\u2011k most similar chunks, then pass them to the generator [5]. The documents here don\u2019t describe a named \u201cneighbor expansion for chunks\u201d, but they do show related ideas like expanding or refining the retrieved pool via rankers and feedback (\"ranker-based expansion\") [4], and replacing chunk retrieval with graph-based retrieval plus expansion [1].\n\nSo: neighbor expansion is a thing in graph-based RAG (GraphRAG / KG-RAG) and conceptually similar ideas exist for refining or expanding retrieved text sets, but the documents don\u2019t describe a standardized, named technique that does \"neighbor expansion\" purely over adjacent text chunks.", "tags": ["_untagged"], "full_faithfulness": 1.0, "full_answer_relevancy": 0.0, "seed_faithfulness": 0.8260869565000001, "seed_answer_relevancy": 0.0, "context_relevance": 0.021946061336980727, "context_relevance_error": null, "context_relevance_reason": "The context passages provided do not address the user's question about 'neighbor expansion for chunks in rag.' The passages focus on various aspects of VQA accuracy, performance comparisons, and retrieval-augmented generation techniques, but none specifically mention or relate to the concept of neighbor expansion in the context of RAG. Therefore, the relevance to the user's inquiry is nonexistent.", "structure_citation": 0.4858207639318423, "structure_error": null, "structure_reason": "The Actual Output includes claims supported by citations, but there are issues with citation matching. While it references concepts like 'l-hop neighbor expansion' and 'graph expansion', the citations [1], [3], [4], and [5] do not directly correspond to the claims made in the context provided. Additionally, the output suggests that 'neighbor expansion' is not a standardized technique for text chunks, which is a valid point, but it lacks clarity on how the cited snippets support this conclusion. Overall, while there is some alignment, the lack of precise citation support and clarity on claims leads to a moderate score.", "hallucination_score": 0.719524003772658, "hallucination_error": null, "hallucination_reason": "The response accurately identifies 'neighbor expansion' as a concept in RAG-style systems, particularly in graph contexts, which aligns with the input query. It provides specific examples from mKG-RAG and GraphRAG, demonstrating a clear understanding of the topic. However, it notes that there is no standardized technique for 'neighbor expansion' in plain text chunks, which is a valid point but could be seen as a limitation in fully addressing the query. Overall, the claims made are well-supported by the context provided.", "contexts_used": ["4 Experiments\n4.2 Performance Comparison\n\nTable 4: VQA accuracy on E-VQA across different MLLM architectures with varying sizes\n\nQwen2-VL: Jozef KUBA\n\nGPT-4o: Matej Walch mKG-RAG: J. Langer\n\nQ : Who designed this museum?\n\nQwen2-VL \uff1a Indonesian\n\nGPT-4o: Indo-Saracenic mKG-RAG: Javanese\n\nQ : What is the architectural style of this mosque?\n\nQwen2-VL: 400\n\nGPT-4o:\n\nI can't answer without specific details.\n\nmKG-RAG: 250-metre\n\nQ : How long is the track at this velodrome?\n\nQwen2-VL: PARK\n\nGPT-4o: New York City Department of Parks and Recreation mKG-RAG: Abingdon Square Conservancy\n\nQ : Who is in charge of maintaining this park?\n\nQwen2-VL: 1000000\n\nGPT-4o: 26.04 km 2\n\nmKG-RAG: 3.82 km 2\n\nQ\n\n: What is the surface area of this lake?\n\nQwen2-VL: air space\n\nGPT-4o: a ring of dark spots mKG-RAG: brownish spots\n\nQ : What forms a ring on the larger end of the egg of this bird?", "Figure 4: Qualitative results of Qwen2-VL-7B, GPT-4o and mKG-RAG on E-VQA dataset.", "Results on E-VQA and InfoSeek. In this section, we compare mKG-RAG with Zero-shot MLLMs and RAG-based approaches on the benchmarks mentioned above. The results in Table 3 demonstrate that zero-shot MLLMs struggle with knowledge-based VQA tasks, particularly on the InfoSeek dataset. These limitations underscore the critical need for external knowledge integration. By augmenting LLaVA-More with mKG-RAG, we achieve substantial improvements, over 20.3% on E-VQA and 31.9% on InfoSeek, highlighting the value of retrieval augmentation.", "Furthermore, our method achieves state-of-the-art performance on both datasets. Under the finetuning setting, mKG-RAG * surpasses both mR2AG * and ReflectiVA * . Even without fine-tuning, mKG-RAG outperforms EchoSight by 1.2% and 1.7%, respectively. These results highlight the advantages of integrating RAG with multimodal KGs and demonstrate the effectiveness of our QMRetriever. Table 3 also includes two mKG-RAG variants that replace QM-Retriever with text-only and vision-only CLIP for entity/relationship retrieval, while still using documents retrieved by QMRetriever to construct multimodal KGs. In the text-only variant, both questions and image captions are used as queries to provide more context, explaining its better performance over the vision-only version. However, they both remain less effective than our full approach with QM-Retriever.", "Consistency across Architectures. In Table 4, we provide a detailed comparison of VQA scores across MLLMs of varying parameter sizes, including InternVL3 [58], LLaMA-3.2-Vision 2 , LLaVAv1.5 [34], DeepSeek-VL2 [52], and Qwen2.5-VL [3]. When enhanced with our mKG-RAG framework, these models achieve average performance gains of 9.4% on single-hop queries and 8.7% on overall scenarios, demonstrating the method's strong generalization across different architectures and scales.", "Qualitative Results. Figure 4 shows a qualitative comparison of mKG-RAG with zero-shot Qwen2VL and GPT-4o. While the latter two tend to produce plausible but incorrect or evasive responses, mKG-RAG consistently handles knowledge-intensive queries, especially those involving precise numerical and temporal reasoning.\n\n2 https://huggingface.co/meta-llama/Llama-3.2-11B-Vision", "| | Method. | E-VQA.Single-Hop | E-VQA.All | InfoSeek.Un-Q | InfoSeek.Un-E | InfoSeek.All |\n|---:|:--------------------|-------------------:|------------:|----------------:|----------------:|---------------:|\n| 0 | mKG-RAG | 38.4 | 36.3 | 41.4 | 39.6 | 40.5 |\n| 1 | w/o QM-Retriever | 34.2 | 31.6 | 38.9 | 37.9 | 38.4 |\n| 2 | w/o Graph Retrieval | 30.1 | 28.2 | 33.3 | 32.7 | 33 |\n| 3 | w/o Graph Expansion | 37.2 | 35 | 40.8 | 39.4 | 40.1 |\n\nTable 5: The ablation study of the design of mKG-RAG\n\n| | Model | Ret. Mode | K g = 1 | K g = 5 | K g = 10 | K g = 20 |\n|---:|:---------|:------------|----------:|----------:|-----------:|-----------:|\n| 0 | mKG-RAG\u2020 | Textual | 29.1 | 33.9 | 34.9 | 35.9 |\n| 1 | mKG-RAG\u2021 | Visual | 23 | 29.6 | 31 | 32 |\n| 2 | mKG-RAG | Multimodal | 29.2 | 35.1 | 36.3 | 36.9 |\n\nTable 6: The ablation study of how the retrieval number of entities/relationships affects the VQA accuracy on E-VQA.", "4.3 Ablation Study\n\nImpact of Coarse-grained Retrieval. To quantify the impact of coarse-grained document retrieval, we conduct an ablation experiment replacing QM-Retriever with visual-only CLIP (ViT-L/14@336) for topK d document selection. The results in Table 5 reveal significant performance drops: overall VQA accuracy of mKG-RAG decreases by 4.7% on E-VQA and 2.1% on InfoSeek. This ablation conclusively demonstrates the critical role of first-stage retrieval and QM-Retriever's superiority over unimodal alternatives.", "Effectiveness of Graph-based Retrieval. In our method, the entities and relationships extracted from documents form a distilled knowledge graph, reducing noise and enabling more effective retrieval than direct text chunk matching. To validate this insight, we replace graph-based retrieval with a naive chunk-based alternative. Specifically, we segment retrieved documents into fixed-size chunks and select those relevant to the given question and image description. As shown in Table 5, chunk-based retrieval leads to a substantial accuracy drop, 8.1% on E-VQA and 7.5% on InfoSeek.", "Contribution of Graph Expansion. mKG-RAG enhances the constructed subgraph through l -hop neighbor expansion, effectively capturing potentially missing but relevant knowledge connections. Table 5 shows that omitting graph expansion leads to consistent performance drops of 1.3% (E-VQA) and 0.4% (InfoSeek), demonstrating its critical contribution to our mKG-RAG.", "Impact of Varying Retrieval Number. In Table 6, we further analyze the impact of K g , the number of retrieved entities and relationships, on our method. As K g increases from 1 to 20, the overall accuracy of mKG-RAG and its variants gradually improves, as higher recall rates enhance the likelihood of capturing relevant knowledge. However, when K g > 10 , the benefit diminishes due to longer contexts and more noise. Thus, setting K g = 10 offers a practical trade-off. Notably, mKG-RAG still performs competitively even at K g = 1 , thanks to its graph expansion strategy, which enables the model to gather additional relevant information.", "ABSTRACT\n\nFine-tuning is an immensely resource expensive process when trying to retrain Large Language Models (LLMs) to have access to a larger bank of knowledge. To alleviate this issue there have been many different fine-tuning techniques proposed which have shown good progress in trying to reduce time and computational resources to achieve fine-tuning but with LLMs becoming more intelligent and larger, this issue continues to arise. Hence a new method of enabling knowledge expansion on LLMs had to be devised. Retrieval-Augment-Generate (RAG) is a class of techniques where information is stored in a database and appropriate chunks of information are retrieved to help answer the question. However there are many limitations to naive RAG implementations. This paper proposes the KeyKnowledgeRAG ( K 2 RAG ) framework to address the scalability and answer accuracy limitations associated with naive RAG implementations. This framework takes inspiration from divide-and-conquer ideology, and combines dense and sparse vector search, knowledge graphs and text summarization to help address these limitations. The framework also involves a data preprocessing step to reduce training times. The MultiHopRAG dataset was used for evaluation where the our implemented K 2 RAG pipeline was trained with the document corpus and then evaluated with the test data. Results have shown that there is an improvement in answering questions compared to common Naive RAG implementations where K 2 RAG achieved the highest mean answer similarity at 0.57 and also was able to answer more questions with more ground truth similarity with a highest Q 3 quartile at 0.82 . Additionally our proposed framework is trainable much faster due to the inclusion of a train data corpus summarization step which reduces training times of the individual components by 93% on average. Furthermore K 2 RAG has shown to operate even faster than a traditional knowledge graph based naive RAG implementation with a mean execution times reduced by up to 40% . In addition to superior question-answering capabilities, K 2 RAG offers being a more scalable solution with VRAM requirements reduced by 3 times in comparison to implementations of several naive RAG pipelines evaluated in this paper. Hence K 2 RAG can help companies in sophisticated decision-making through implementing a more lightweight and robust question-answering systems built on internal documents.", "5 Experiments\n\nFor the external knowledge graph, we use the full Wikidata 1 dump, filtering out any triples whose object is a string literal. We include one alias 2 for each included entity by creating a separate triple with 'alias' as predicate. We use a separate Pinecone sparse index for storing this data.", "We set the maximum number of steps \ud835\udc5b = 2. Throughout our experiments, we identified that benefits of graph expansion for simpler questions coming from DataMorgana were limited. Consequently, we opted for a more efficient implementation that does not use Wikidata triples and graph expansion during the first iteration. Questions requiring multi-hop reasoning would require additional iterations, and consequently, the full pipeline described in Section 4 is used for \ud835\udc5b > 1.", "In order to monitor improvement in the pipeline, we built our evaluation according to the suggested methodology, focusing on correctness and faithfulness. Experiments were conducted by constructing a sample of questions using DataMorgana [3].", "5.1 Using DataMorgana\n\nFollowing the best practices presented by Filice et al., we divide the set of users into novice and expert with equal probability, and further define a set of question and answer type categories. We expand upon their original set by incorporating the 'path-following' and 'path-finding' multi-hop question categorisation introduced by Gutierrez et al.. Moreover, we refrain from including the 'linguistic variation' question type and redistribute their probability mass among the remaining categories. Table 1 presents our final taxonomy of question and answer types.", "| | Question | Do frilled lizards and geoducks share any reproductive characteristics? | How come I always have to reset the high limit switch on my hot tub heater after draining and refillin the spa? |\n|---:|:-----------------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 0 | Proximal triples T \u2032 q ( \ud835\udc5b ) from read -ing FineWeb chunks | [('Pacific Geoducks', 'larvae swimming duration', 'first 48 hours after hatching'), ('Pacific Geoducks', 'fertilization method', 'external fertilization'), ('Pacific Geoducks', 'release eggs', '7 to 10 million eggs'), ('Pacific Geoducks', 'reproductive method', 'broadcast spawning'), ('Pacific Geoducks', 'development stage', 'develop a tiny foot and drop to the ocean floor in a few weeks')] | [('faulty parts', 'can cause', 'high limit switch to trip'), ('high limit switch', 'trips due to', 'water temperature exceeding safe limits'), ('primary operating thermostat', 'failure can lead to', 'high limit switch tripping'), ('blocked or clogged vents', 'can cause', 'high limit switch to trip'), ('thermistor', 'failure can lead to', 'high limit switch tripping')] |\n| 1 | Wikidata triples T q ( \ud835\udc5b ) linked by proximal triples | [('Larval development in the Pacific oyster and the impacts of ocean acidification: Differential genetic effects in wild and domesticated stocks', 'cites work', 'Gene expression correlated with delay in shell formation in larval Pacific oysters (Crassostrea gigas) exposed to experimental ocean acidification provides insights into shell formation mechanisms.'), ('Egg consumption and risk of cardiovascular disease: three large prospective US cohort studies, systematic review, and updated meta-analysis', 'cites work', 'Land, irrigation water, greenhouse gas, and reactive nitrogen burdens of meat, eggs, and dairy production in the United States'), ('Cryptic diversity, geographical endemism and allopolyploidy in NE Pacific seaweeds', 'cites work', 'Temporal windows of reproductive opportunity reinforce species barriers in a marine broadcast spawning assemblage.'), ('The Probable Method of Fertilization in Terrestrial Hermit Crabs Based on a Comparative Study of Spermatophores', 'published in', 'Pacific Science'), ('Pacific', 'located in the administrative territorial entity', 'Long Beach'), ('Genetic variation of wild and hatchery populations of the Pacific oyster Crassostrea gigas assessed by microsatellite markers', 'cites work', 'Isolation and characterization of di- and tetranucleotide microsatellite loci in geoduck clams, Panopea abrupta.')] | [('STUDY OF EFFECT OF CONSECUTIVE HEATING ON THERMOLUMINESCENCE GLOW CURVES OF MULTI-ELEMENT TL DOSEMETER IN HOT GAS-BASED READER SYSTEM', 'published in', 'Radiation Protection Dosimetry'), ('Heat killing of Bacillus subtilis spores in water is not due to oxidative damage', 'cites work', 'A superoxide dismutase mimic protects sodA sodB Escherichia coli against aerobic heating and stationary-phase death.'), ('Mineralogy of Sn-W-As-Pb-Zn-Cu-bearing alteration zones in intracontinental rare metal granites (Central Mongolia)', 'cites work', 'The 'chessboard' classification scheme of mineral deposits: Mineralogy and geology from aluminum to zirconium'), ('Water as a reservoir of nosocomial pathogens', 'cites work', 'Superficial and systemic illness related to a hot tub'), ('Journal of Research of the U. S. Geological Survey, 1974, volume 2, issue 4', 'has part(s)', 'A mineral separation procedure using hot Clerici solution'), ('Optimal Water-Power Flow-Problem: Formulation and Distributed Optimal Solution', 'published in', 'IEEE transactions on control of network systems'), ('Hot tub-associated dermatitis due to Pseudomonas aeruginosa. Case report and review of the literature', 'published in', 'Archives of Dermatology')] |", "Table 2: Example of misalignment between FineWeb and Wikidata. The green keywords indicate the topic of the proximal triples, while the red keywords indicate the topic of the linked Wikidata triples.", "2 Preliminaries\n2.1 Retrieval System\n\nThe core objective of a retrieval system is to identify and rank a subset of documents ( d 1 , d 2 , ..., d k ) from a large corpus C based on their estimated relevance to a query q . Classical retrieval approaches, such as BM25 (Robertson et al., 1995), rely on exact term matching and produce sparse relevance scores. In contrast, dense retrieval methods employ neural encoders to project queries and documents into a shared embedding space, enabling semantic similarity matching (Karpukhin et al., 2020). Since first-stage retrievers often produce noisy candidates, modern pipelines incorporate a second-stage re-ranking step using more expressive models. This includes LLM-based rankers (Pradeep et al., 2023b;\n\nMa et al., 2024; Sun et al., 2023) and reasoningaugmented models such as ReasonIR (Shao et al., 2025), Rank-1 (Weller et al., 2025), and RankR1 (Zhuang et al., 2025), which refine the initial rankings by modeling deeper interactions between the query and candidate documents.", "2.2 Retrieval-Augmented Generation\n\nRetrieval-Augmented Generation (RAG) (Lewis et al., 2020b) is a hybrid paradigm that enhances the generative capabilities of large language models (LLMs) by incorporating non-parametric external knowledge during inference. This design mitigates well-documented limitations of standalone LLMs, including hallucinations, confident but incorrect outputs, and inability to reflect up-to-date or domain-specific information due to static pretraining (Hurst et al., 2024; Grattafiori et al., 2024; Yang et al., 2025a).", "RAG introduces an explicit retrieval step: for a query q , a retriever selects a set of topk documents { d 1 , . . . , d k } from an external corpus. A generator G then conditions on both q and the retrieved context to produce the output a = G ( q, d 1 , . . . , d k ) where G is typically an encoderdecoder or decoder-only LLM fine-tuned to integrate retrieved evidence into its generation process.", "2.3 Challenges in RAG\n\nA central challenge in RAG is that generation quality is tightly coupled with retrieval quality. If relevant (i.e., answer-containing) documents are not retrieved, or if irrelevant context is included, the generator is prone to producing incorrect or misleading outputs (Cuconasu et al., 2024, 2025; Liu et al., 2024).", "Consequently, improving the topk retrieval results is crucial. This can be viewed both as a selection problem (how to retrieve the most relevant documents) and a filtering problem (how to suppress distracting or noisy context). To this end, several methods have been proposed that incorporate various forms of feedback , ranging from simple lexical overlap to more sophisticated agentic or reasoning-based signals, to guide and refine the retrieval process.", "In this survey, we systematically categorize these feedback mechanisms and analyze their effectiveness across different components of the RAG pipeline. We focus on how feedback is acquired, represented, and integrated into retrieval, with the", "Figure 1: Illustration of feedback signals across the RAG pipeline. Feedback can modify the query (e.g., rewriting), the retrieved pool (e.g., ranker-based expansion), or the generation loop (e.g., retrieval triggers based on uncertainty).\n\nRetrieval Reranker results Generation Query Encoder representation new results Ranker Relevance Dynamic Understanding Generated Output Feedback levels Query Level Retrieval Level Generation Time Query Understanding Ranker Relevance Dynamic Retrieval - Query Expansion - Query Rewriting\n\n- Query Vector Adaption\n\n- Query Decomposition\n- Result refinement\n\n- When to retrieve\n\n- What (new query) to retrieve\n\naim of providing a comprehensive taxonomy and highlighting open research challenges.", "Preliminary\n\nIn a retrieval-augmented generation (RAG) system, a collection of documents is first split into small chunks D , which are then embedded into vectors using an embedding model f . Given a query q , the retriever R first searches top k relevant chunks D k = { d 1 , d 2 , ..., d k } \u2282 D from the collection according to the similarity between the query and chunks, i.e., D k = R ( D,q ) . The similarity s is calculated using the inner product (IP) or L2 norm between the query embedding q = f ( q ) and chunk embeddings d = f ( d ) , \u2200 d \u2208 D . Finally, the retrievals and query are incorporated into a prompt P ( q, D k ) , which is used as the input of the generator model g to generate final answer, i.e., \u02c6 a = G ( P ( q, D k )) . We list all notations and abbreviations in the appendix.", "Related Work Retrieval-Augmented Generation\n\nRetrieval-Augmented Generation was introduced as a powerful paradigm to mitigate the intrinsic knowledge limitations of LLMs by grounding them in external information (Lewis et al. 2020). The standard RAG framework operates by retrieving relevant text chunks from a corpus and providing them as context to an LLM for answer generation (Wang et al. 2024). While effective, this approach is fundamentally constrained by the 'chunking dilemma': small, fine-grained chunks risk losing critical context, whereas larger chunks often introduce significant noise and dilute the LLM's focus (Tonellotto et al. 2024).", "Substantial research has been dedicated to overcoming this limitation. One line of work focuses on improving the retriever itself, evolving from sparse methods like BM25 (Robertson, Zaragoza et al. 2009) to dense retrieval models such as DPR (Karpukhin et al. 2020) and Contriever (Izacard et al. 2021), which learn to better capture semantic relevance. Another direction targets the indexing and organization of the source documents (Jiang et al. 2023). Recent advancements have explored creating hierarchical summaries of text chunks, allowing retrieval to occur at multiple levels of granularity. For instance, RAPTOR builds a tree of recursively summarized text clusters, enabling retrieval of both fine-grained details and high-level summaries (Sarthi et al. 2024). Despite these improvements, these methods still largely treat knowledge as a linear sequence or a simple tree of text. They do not explicitly model the complex, non-hierarchical relations that often exist between different entities and concepts within a document. This limitation hinders their ability to answer complex queries that require reasoning over these intricate connections, motivating the shift towards KG-based RAG methods."]}]}, "tag_summary": {"hybrid_vs_kg": {"count": 6, "full_faithfulness": 0.9907407407333334, "full_answer_relevancy": 0.48069701715, "seed_faithfulness": 0.82225274725, "seed_answer_relevancy": 0.48375901503333335, "hallucination_score": 0.8002200092663321}, "rerankers": {"count": 4, "full_faithfulness": 0.905769230775, "full_answer_relevancy": 0.91111818985, "seed_faithfulness": 0.7673076923, "seed_answer_relevancy": 0.44167449864999997, "hallucination_score": 0.8072128061271628}, "benchmarks_eval": {"count": 9, "full_faithfulness": 0.9842592592555557, "full_answer_relevancy": 0.6021095109222222, "seed_faithfulness": 0.7230644869444445, "seed_answer_relevancy": 0.49649746352222224, "hallucination_score": 0.8158568492774914}, "retrieval_tuning": {"count": 6, "full_faithfulness": 0.9487878787833334, "full_answer_relevancy": 0.6183284099, "seed_faithfulness": 0.8129949760166667, "seed_answer_relevancy": 0.30751619355, "hallucination_score": 0.8522254150926344}, "scaling_cost": {"count": 5, "full_faithfulness": 0.98666666666, "full_answer_relevancy": 0.9040124968800001, "seed_faithfulness": 0.8673118279600001, "seed_answer_relevancy": 0.7109721192, "hallucination_score": 0.8674707730882112}, "long_tail_entities": {"count": 3, "full_faithfulness": 0.7466666666666667, "full_answer_relevancy": 0.6385430647, "seed_faithfulness": 0.9420289855, "seed_answer_relevancy": 0.6401318079666667, "hallucination_score": 0.7725971849594414}, "rrf_hybrid": {"count": 1, "full_faithfulness": 0.9444444444000001, "full_answer_relevancy": 0.0, "seed_faithfulness": 0.7000000000000001, "seed_answer_relevancy": 0.0, "hallucination_score": 0.6101977113787885}, "summarization_conflict": {"count": 3, "full_faithfulness": 0.9047619047666666, "full_answer_relevancy": 0.9208238305333333, "seed_faithfulness": 0.7896825396666666, "seed_answer_relevancy": 0.9230415584333335, "hallucination_score": 0.8796200625657745}, "bm25_sparse": {"count": 1, "full_faithfulness": 1.0, "full_answer_relevancy": 0.9545066822, "seed_faithfulness": 1.0, "seed_answer_relevancy": 0.9543717850000001, "hallucination_score": 0.7916426850595131}, "_untagged": {"count": 38, "full_faithfulness": 0.9425873930263159, "full_answer_relevancy": 0.7501303158289472, "seed_faithfulness": 0.7418341982078946, "seed_answer_relevancy": 0.7524851187552631, "hallucination_score": 0.8387171485554044}}, "context_relevance_avg": 0.6073, "context_relevance_median": 0.7069, "structure_citation_avg": 0.6916, "structure_citation_median": 0.736, "hallucination_score_avg": 0.8329, "hallucination_score_median": 0.85, "estimated_cost_usd": 0.148}
